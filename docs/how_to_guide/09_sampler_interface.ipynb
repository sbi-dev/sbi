{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose sampling algorithms\n",
    "\n",
    "`sbi` implements three methods: NPE, NLE, and NRE. When using NPE, the trained neural network directly approximates the posterior. Thus, sampling from the posterior can be done by sampling from the trained neural network. The neural networks trained in NLE and NRE approximate the likelihood(-ratio). Thus, in order to draw samples from the posterior, one has to perform additional sampling steps, e.g. Markov-chain Monte-Carlo (MCMC). In `sbi`, the implemented samplers are:\n",
    "\n",
    "- Markov-chain Monte-Carlo (MCMC)\n",
    "\n",
    "- Rejection sampling\n",
    "\n",
    "- Variational inference (VI)\n",
    "\n",
    "- Importance sampling (IS)\n",
    "\n",
    "Which sampler should you choose? And how can you do this in the `sbi` toolbox?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit recommendations\n",
    "\n",
    "For NPE:\n",
    "-  use the `DirectPosterior`. If you have a likelihood available, you can refine the posterior with importance sampling (see tutorial [here]())\n",
    "\n",
    "For NLE or NRE:\n",
    "- If you have very few parameters (<3), use `RejectionPosterior` or `MCMCPosterior`\n",
    "- If you have a medium number of parameters (3-10), use `MCMCPosterior`\n",
    "- If you have many parameters (>10) and the `MCMCPosterior` is too slow, use the `VIPosterior`. Optionally combine the `VIPosterior` with an `ImportanceSamplingPosterior` to improve its accuracy.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- `MCMCPosterior`: very accurate, but can be slow if you have many parameters.  \n",
    "- `VIPosterior`: can be much faster if you have many parameter. May be inaccurate.  \n",
    "- `RejectionPosterior`: accurate and fast, but only for very few parameters (typically less than 3). \n",
    "- `ImportanceSamplingPosterior`: typically inaccurate, but can be very useful to improve the accuracy of a `VIPosterior` (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show how you can use these different sampling algorithms in the `sbi` toolbox. We begin with the `MCMCPosterior`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sbi.inference import MCMCPosterior, likelihood_estimator_based_potential\n",
    "\n",
    "trainer = NLE()\n",
    "likelihood_estimator = trainer.append_simulations(theta, x).train()\n",
    "\n",
    "potential_fn, parameter_transform = likelihood_estimator_based_potential(\n",
    "    likelihood_estimator, prior, x_o\n",
    ")\n",
    "posterior = MCMCPosterior(\n",
    "    potential_fn, proposal=prior, theta_transform=parameter_transform, warmup_steps=10\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use variational inference or rejection sampling, you have to replace the last line with `VIPosterior` or `RejectionPosterior`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sbi.inference import RejectionPosterior, VIPosterior\n",
    "\n",
    "# For VI, we have to train.\n",
    "posterior = VIPosterior(\n",
    "    potential_fn, prior=prior, theta_transform=parameter_transform\n",
    ").train()\n",
    "\n",
    "posterior = RejectionPosterior(\n",
    "    potential_fn, proposal=prior, theta_transform=parameter_transform\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is also possible to improve the accuracy of a `VIPosterior` with importance sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sbi.inference import ImportanceSamplingPosterior, VIPosterior\n",
    "\n",
    "vi_posterior = VIPosterior(\n",
    "    potential_fn, prior=prior, theta_transform=parameter_transform\n",
    ").train()\n",
    "refined_posterior = ImportancerSamplingPosterior(\n",
    "    potential_fn, vi_posterior, oversampling_factor=32\n",
    ")\n",
    "samples = refined_posterior.sample((1000,))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you could also plug the `potential_fn` into any sampler of your choice and not rely on any of the in-built `sbi`-samplers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further explanation\n",
    "\n",
    "The first lines are the same as always:\n",
    "```python\n",
    "inference = NLE()\n",
    "likelihood_estimator = inference.append_simulations(theta, x).train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we obtain the potential function. A potential function is a function of the parameter $f(\\theta)$. The posterior is proportional to the product of likelihood and prior: $p(\\theta | x_o) \\propto p(x_o | \\theta)p(\\theta)$. The potential function is the logarithm of the right-hand side of this equation: $f(\\theta) = \\log(p(x_o | \\theta)p(\\theta))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "potential_fn, parameter_transform = likelihood_estimator_based_potential(\n",
    "    likelihood_estimator, prior, x_o\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `potential_fn`, you can evaluate the potential:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Assuming that your parameters are 1D.\n",
    "potential = potential_fn(\n",
    "    torch.zeros(1, num_dim)\n",
    ")  # -> returns f(0) = log( p(x_o|0) p(0) )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other object that is returned by `likelihood_estimator_based_potential` is a `parameter_transform`. The `parameter_transform` is a [pytorch transform](https://github.com/pytorch/pytorch/blob/master/torch/distributions/transforms.py). The `parameter_transform` is a fixed transform that is can be applied to parameter `theta`. It transforms the parameters into unconstrained space (if the prior is bounded, e.g. `BoxUniform`), and standardizes the parameters (i.e. zero mean, one std). Using `parameter_transform` during sampling is optional, but it usually improves the performance of MCMC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "theta_tf = parameter_transform(torch.zeros(1, num_dim))\n",
    "theta_original = parameter_transform.inv(theta_tf)\n",
    "print(theta_original)  # -> tensor([[0.0]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having obtained the `potential_fn`, we can sample from the posterior with MCMC or rejection sampling:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "posterior = MCMCPosterior(\n",
    "    potential_fn, proposal=prior, theta_transform=parameter_transform\n",
    ")\n",
    "posterior = RejectionPosterior(potential_fn, proposal=prior)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the usage of NPE\n",
    "\n",
    "NPE usually does not require MCMC or rejection sampling (if you still need it, you can use the same syntax as above with the `posterior_estimator_based_potential` function). Instead, NPE samples from the neural network. If the support of the prior is bounded, some samples can lie outside of the support of the prior. The `DirectPosterior` class automatically rejects these samples:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sbi.inference import NPE, DirectPosterior\n",
    "\n",
    "inference = NPE()\n",
    "posterior_estimator = inference.append_simulations(theta, x).train()\n",
    "\n",
    "posterior = DirectPosterior(posterior_estimator, prior=prior)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
