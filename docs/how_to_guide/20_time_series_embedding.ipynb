{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42385334",
   "metadata": {},
   "source": [
    "# How to use time-series embeddings\n",
    "\n",
    "This short guide shows how to use **time-series embedding networks**:\n",
    "- `CausalCNNEmbedding`\n",
    "- `TransformerEmbedding`\n",
    "\n",
    "We’ll use a simple `torch.randn` simulator to demonstrate the workflow.\n",
    "\n",
    "> In real use, replace `torch.randn` with a dynamical simulator (e.g., SIR, Lotka–Volterra).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a670dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import embedding_nets, posterior_nn\n",
    "from sbi.utils import BoxUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c533f",
   "metadata": {},
   "source": [
    "Let's define a simple simulator that returns random time-series data to mimic sequential observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230dd3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "def simulator(theta):\n",
    "    return torch.randn(1, 100)\n",
    "prior = BoxUniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "thetas = prior.sample((50,))\n",
    "xs = simulator(thetas)\n",
    "x_o = simulator(torch.tensor([0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021377e2",
   "metadata": {},
   "source": [
    "## Using CausalCNNEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539c7dd",
   "metadata": {},
   "source": [
    "We use `CausalCNNEmbedding` to extract **local temporal patterns** from the sequence and train a Neural Posterior Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981016ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a causal CNN embedding for 1D time-series data\n",
    "embedding_cnn = embedding_nets.CausalCNNEmbedding(\n",
    "    input_shape=(100,),     # 1D time-series length\n",
    "    num_conv_layers=3,      # Number of CNN layers\n",
    "    pool_kernel_size=10,    # Pooling window for temporal downsampling\n",
    "    output_dim=16,          # Embedding size for NPE\n",
    ")\n",
    "\n",
    "\n",
    "# Build density estimator with embedding\n",
    "density_estimator_cnn = posterior_nn(\n",
    "    model=\"maf\",\n",
    "    embedding_net=embedding_cnn,\n",
    "    z_score_x=\"none\",\n",
    "    z_score_y=\"none\",\n",
    ")\n",
    "\n",
    "# Create and train NPE using the prior and simulated data\n",
    "inference_cnn = NPE(prior=prior, density_estimator=density_estimator_cnn)\n",
    "posterior_cnn = inference_cnn.append_simulations(thetas, xs).train()\n",
    "\n",
    "# Draw posterior samples given an observed time series\n",
    "samples_cnn = posterior_cnn.sample(torch.Size([10]), x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24905e86",
   "metadata": {},
   "source": [
    "## Using TransformerEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6aca03",
   "metadata": {},
   "source": [
    "Next, we define a `TransformerEmbedding` that models **global dependencies** via self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer configuration for sequence embedding\n",
    "cfg = dict(\n",
    "    vit=False,             # Use standard transformer, not ViT-style\n",
    "    feature_space_dim=192, # Internal embedding dimension (num_heads * head_dim)\n",
    "    sequence_length=100,   # Number of time points\n",
    "    output_dim=16,         # Output feature dimension for NPE\n",
    "    num_layers=3,          # Transformer depth\n",
    "    num_heads=12,          # Number of attention heads\n",
    "    head_dim=16,           # Size per attention head\n",
    "    d_model=192,           # ame as feature_space_dim\n",
    ")\n",
    "\n",
    "# Initialize the transformer embedding network\n",
    "base_trans = embedding_nets.TransformerEmbedding(cfg)\n",
    "\n",
    "# Project 1D inputs to match transformer feature dimension\n",
    "class ProjectedTransformer(nn.Module):\n",
    "    def __init__(self, transformer):\n",
    "        super().__init__()\n",
    "        self.proj, self.transformer = nn.Linear(1, 192), transformer\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        x = self.proj(x)\n",
    "        return self.transformer(x)\n",
    "\n",
    "embedding_trans = ProjectedTransformer(base_trans)\n",
    "\n",
    "# Build and train NPE with transformer embedding\n",
    "density_estimator_trans = posterior_nn(\n",
    "    model=\"maf\",\n",
    "    embedding_net=embedding_trans,\n",
    "    z_score_x=\"none\",\n",
    "    z_score_y=\"none\",\n",
    ")\n",
    "\n",
    "inference_trans = NPE(prior=prior, density_estimator=density_estimator_trans)\n",
    "posterior_trans = inference_trans.append_simulations(thetas, xs).train()\n",
    "\n",
    "# Sample from the learned posterior\n",
    "samples_trans = posterior_trans.sample(torch.Size([10]), x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee947ce",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- **CausalCNNEmbedding** uses temporal convolutions for local dependencies.  \n",
    "- **TransformerEmbedding** uses self-attention for global dependencies.  \n",
    "- The small wrapper `ProjectedTransformer` projects scalar time-series to match the transformer’s feature space.\n",
    "- Both embeddings integrate seamlessly into `posterior_nn`.\n",
    "> Use GPU for long sequences or large networks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
