{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3287170b",
   "metadata": {},
   "source": [
    "# How to specify a custom prior (e.g., multiple independent priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02257981",
   "metadata": {},
   "source": [
    "`sbi` works with torch distributions only, so we recommend using those whenever\n",
    "possible. If you want a different prior in each parameter dimension, then you\n",
    "can use the `sbi` utility `MultipleIndependent`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7fb48",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.distributions import MultivariateNormal, Exponential\n",
    "from sbi.utils import MultipleIndependent, BoxUniform\n",
    "\n",
    "prior = MultipleIndependent([\n",
    "    MultivariateNormal(torch.ones(2,), torch.eye(2,)),\n",
    "    BoxUniform(-torch.ones(3,), torch.ones(3,)),\n",
    "    Exponential(torch.ones(1,))\n",
    "])\n",
    "```\n",
    "This will create a prior for 6 parameters, of which the first\n",
    "two follow a Normal distribution, the second three follow a\n",
    "uniform distribution, and the last one follows an exponential\n",
    "distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d42d52",
   "metadata": {},
   "source": [
    "## Wrapping non-torch distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fa2dc",
   "metadata": {},
   "source": [
    "In case you want to use a custom prior that is not in the set of common\n",
    "distributions that's possible as well:\n",
    "You need to write a prior class that mimicks the behaviour of a\n",
    "[`torch.distributions.Distribution`](https://pytorch.org/docs/stable/_modules/torch/distributions/distribution.html#Distribution)\n",
    "class. `sbi` will wrap this class to make it a fully functional torch\n",
    "`Distribution`.\n",
    "\n",
    "Essentially, the class needs two methods:\n",
    "\n",
    "- `.sample(sample_shape)`, where sample_shape is a shape tuple, e.g., `(n,)`,\n",
    "  and returns a batch of n samples, e.g., of shape (n, 2)` for a two dimenional\n",
    "  prior.\n",
    "- `.log_prob(value)` method that returns the \"log probs\" of parameters under the\n",
    "  prior, e.g., for a batches of n parameters with shape `(n, ndims)` it should\n",
    "  return a log probs array of shape `(n,)`.\n",
    "\n",
    "For `sbi` > 0.17.2 this could look like the following:\n",
    "\n",
    "```python\n",
    "class CustomUniformPrior:\n",
    "    \"\"\"User defined numpy uniform prior.\n",
    "\n",
    "    Custom prior with user-defined valid .sample and .log_prob methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lower: Tensor, upper: Tensor, return_numpy: bool = False):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.dist = BoxUniform(lower, upper)\n",
    "        self.return_numpy = return_numpy\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        samples = self.dist.sample(sample_shape)\n",
    "        return samples.numpy() if self.return_numpy else samples\n",
    "\n",
    "    def log_prob(self, values):\n",
    "        if self.return_numpy:\n",
    "            values = torch.as_tensor(values)\n",
    "        log_probs = self.dist.log_prob(values)\n",
    "        return log_probs.numpy() if self.return_numpy else log_probs\n",
    "```\n",
    "\n",
    "Once you have such a class, you can wrap it into a `Distribution` using the\n",
    "`process_prior` function `sbi` provides:\n",
    "\n",
    "```python\n",
    "from sbi.utils import process_prior\n",
    "\n",
    "custom_prior = CustomUniformPrior(torch.zeros(2), torch.ones(2))\n",
    "prior, *_ = process_prior(custom_prior)  # Keeping only the first return.\n",
    "# use this wrapped prior in sbi...\n",
    "```\n",
    "\n",
    "In `sbi` it is sometimes required to check the support of the prior, e.g., when\n",
    "the prior support is bounded and one wants to reject samples from the posterior\n",
    "density estimator that lie outside the prior support. In torch `Distributions`\n",
    "this is handled automatically. However, when using a custom prior, it is not.\n",
    "Thus, if your prior has bounded support (like the one above), it makes sense to\n",
    "pass the bounds to the wrapper function such that `sbi` can pass them to torch\n",
    "`Distributions`:\n",
    "\n",
    "```python\n",
    "from sbi.utils import process_prior\n",
    "\n",
    "custom_prior = CustomUniformPrior(torch.zeros(2), torch.ones(2))\n",
    "prior = process_prior(custom_prior,\n",
    "                      custom_prior_wrapper_kwargs=dict(lower_bound=torch.zeros(2),\n",
    "                                                       upper_bound=torch.ones(2)))\n",
    "# use this wrapped prior in sbi...\n",
    "```\n",
    "\n",
    "Note that in `custom_prior_wrapper_kwargs` you can pass additinal arguments for\n",
    "the wrapper, e.g., `validate_args` or `arg_constraints` see the `Distribution`\n",
    "documentation for more details.\n",
    "\n",
    "If you are using `sbi` < 0.17.2 and use `NLE` the code above will produce a\n",
    "`NotImplementedError` (see [#581](https://github.com/mackelab/sbi/issues/581)).\n",
    "In this case, you need to update to a newer version of `sbi` or use `NPE`\n",
    "instead.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
