{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd7f17d",
   "metadata": {},
   "source": [
    "# API of implemented methods\n",
    "\n",
    "This notebook spells out the API for all algorithms implemented in the `sbi` toolbox:\n",
    "\n",
    "- [Neural Posterior estimation (NPE)](https://sbi.readthedocs.io/en/latest/tutorials/16_implemented_methods.html#neural-posterior-estimation-npe)\n",
    "\n",
    "- [Neural Likelihood estimation (NLE)](https://sbi.readthedocs.io/en/latest/tutorials/16_implemented_methods.html#neural-likelihood-estimation-nle)\n",
    "\n",
    "- [Neural Ratio estimation (NRE)](https://sbi.readthedocs.io/en/latest/tutorials/16_implemented_methods.html#neural-ratio-estimation-nre)\n",
    "\n",
    "- [Diagnostics and Utilities](https://sbi.readthedocs.io/en/latest/tutorials/16_implemented_methods.html#diagnostics-and-utilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31b3f4",
   "metadata": {},
   "source": [
    "For the examples below, we define a simple toy prior and simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac88848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example setup\n",
    "import torch\n",
    "\n",
    "from sbi.utils import BoxUniform\n",
    "\n",
    "# Define the prior\n",
    "num_dims = 2\n",
    "num_sims = 1000\n",
    "num_rounds = 2\n",
    "prior = BoxUniform(low=torch.zeros(num_dims), high=torch.ones(num_dims))\n",
    "simulator = lambda theta: theta + torch.randn_like(theta) * 0.1\n",
    "x_o = torch.tensor([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e608393",
   "metadata": {},
   "source": [
    "## Neural Posterior Estimation (NPE)\n",
    "\n",
    "### Default implementation\n",
    "\n",
    "The core idea of Neural Posterior Estimation (NPE) is to train a conditional generative\n",
    "model that directly predicts the posterior given observations. This idea was originally\n",
    "developed by Papamakarios & Murray (NeurIPS 2016). The default implementation of NPE in\n",
    "the `sbi` package follows Greenberg, Nonnenmacher & Macke (ICML 2019), who proposed NPE with\n",
    "normalizing flows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fac789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "\n",
    "inference = NPE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()\n",
    "samples = posterior.sample((1000,), x=x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b68da5",
   "metadata": {},
   "source": [
    "For the above default implementation of `NPE` in the `sbi` package, we recommend to\n",
    "cite:\n",
    "\n",
    "- _Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation_ by Papamakarios & Murray (NeurIPS 2016) [[PDF]](https://papers.nips.cc/paper/6084-fast-free-inference-of-simulation-models-with-bayesian-conditional-density-estimation.pdf)\n",
    "\n",
    "- _Automatic posterior transformation for likelihood-free inference_ by Greenberg, Nonnenmacher & Macke (ICML 2019) [[PDF]](http://proceedings.mlr.press/v97/greenberg19a/greenberg19a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bf467",
   "metadata": {},
   "source": [
    "Beyond this, the `sbi` package implements many modifications and extensions of these algorithms for Neural Posterior Estimation, which are outlined below. These methods include\n",
    "\n",
    "- various types of embedding networks (Lueckmann et al., Greenberg et al., Radev et al.,),\n",
    "- various generative models (Dax et al., Geffner et al., Sharrock et al.), and\n",
    "- various algorithms for sequential NPE (called _SNPE_) which perform inference across several rounds (Papamakarios et al., Lueckmann et al., Greenberg et al., Deistler et al.). Note that all SNPE methods use the same loss function in the first round and, thus, NPE is equivalent to single-round SNPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bc6b4",
   "metadata": {},
   "source": [
    "### Papamakarios & Murray, 2016\n",
    "\n",
    "_Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation_\n",
    "by Papamakarios & Murray (NeurIPS 2016) [[PDF]](https://papers.nips.cc/paper/6084-fast-free-inference-of-simulation-models-with-bayesian-conditional-density-estimation.pdf)\n",
    "\n",
    "Papamakarios et al. (2016) were the first to use neural networks to directly predict\n",
    "the posterior given observations. As density estimator, they used mixture density\n",
    "networks. In addition, they also proposed a sequential algorithm for neural posterior\n",
    "estimation. Their full algorithm can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE_A\n",
    "\n",
    "inference = NPE_A(prior)\n",
    "proposal = prior\n",
    "for r in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    # NPE-A trains a Gaussian density estimator in all but the last round. In the last round,\n",
    "    # it trains a mixture of Gaussians, which is why we have to pass the `final_round` flag.\n",
    "    final_round = r == num_rounds - 1\n",
    "    _ = inference.append_simulations(theta, x, proposal=proposal).train(final_round=final_round)\n",
    "    posterior = inference.build_posterior().set_default_x(x_o)\n",
    "    proposal = posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4d312",
   "metadata": {},
   "source": [
    "### Lueckmann et al., 2017\n",
    "\n",
    "_Flexible statistical inference for mechanistic models of neural dynamics_ by Lueckmann,\n",
    "Goncalves, Bassetto, Öcal, Nonnenmacher, Macke (NeurIPS 2017) [[PDF]](https://proceedings.neurips.cc/paper/2017/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html)\n",
    "\n",
    "Like Papamakarios et al. (2016), Lueckmann et al. (2017) used mixture density networks for\n",
    "NPE. In addition, they also proposed embedding networks for time series, and proposed a\n",
    "different loss function for the sequential version of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE_B\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding\n",
    "\n",
    "embedding = FCEmbedding(num_dims)\n",
    "density_estimator = posterior_nn(\"mdn\", embedding_net=embedding)\n",
    "inference = NPE_B(prior, density_estimator=density_estimator)\n",
    "\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x, proposal=proposal).train()\n",
    "    posterior = inference.build_posterior().set_default_x(x_o)\n",
    "    proposal = posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd7f43",
   "metadata": {},
   "source": [
    "### Greenberg et al., 2019\n",
    "\n",
    "_Automatic posterior transformation for likelihood-free inference_ by Greenberg, Nonnenmacher & Macke (ICML 2019) [[PDF]](http://proceedings.mlr.press/v97/greenberg19a/greenberg19a.pdf)\n",
    "\n",
    "Greenberg, Nonnenmacher & Macke were the first to use normalizing flows for neural\n",
    "posterior estimation (NPE), which is the default for the `NPE` class in the `sbi`\n",
    "package (see above). In addition, they also proposed embedding networks\n",
    "in combination with normalizing flows, and proposed a modified loss function for the\n",
    "sequential version of the neural posterior estimation. These additional contributions\n",
    "can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding\n",
    "\n",
    "embedding = FCEmbedding(num_dims)\n",
    "density_estimator = posterior_nn(\"maf\", embedding_net=embedding)\n",
    "inference = NPE(prior, density_estimator=density_estimator)\n",
    "\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x, proposal=proposal).train()\n",
    "    posterior = inference.build_posterior().set_default_x(x_o)\n",
    "    proposal = posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc71c3a1",
   "metadata": {},
   "source": [
    "### Radev et al., 2020\n",
    "\n",
    "_BayesFlow: Learning complex stochastic models with invertible neural networks_ by\n",
    "Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Köthe, U. (2020) (IEEE\n",
    "transactions on neural networks and learning systems 2020)\n",
    "[[Paper]](https://ieeexplore.ieee.org/abstract/document/9298920)\n",
    "\n",
    "The density estimation part of BayesFlow is equivalent to single-round NPE (Greenberg\n",
    "et al., 2019). The additional contribution of BayesFlow are permutation invariant\n",
    "embedding networks for iid data, which can be implemented in `sbi` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior estimation with BayesFlow is equivalent to single-round NPE.\n",
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding, PermutationInvariantEmbedding\n",
    "\n",
    "num_iid = 3\n",
    "simulator_iid = lambda theta: theta + torch.randn((num_iid, *theta.shape)) * 0.1\n",
    "\n",
    "trial_net = FCEmbedding(num_dims, 20)\n",
    "embedding = PermutationInvariantEmbedding(trial_net, 20)\n",
    "density_estimator = posterior_nn(\"maf\", embedding_net=embedding)\n",
    "inference = NPE(prior, density_estimator=density_estimator)\n",
    "\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator_iid(theta).permute(1, 0, 2)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()\n",
    "samples = posterior.sample((1000,), x=torch.zeros((1, num_iid, num_dims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef8b35",
   "metadata": {},
   "source": [
    "### Deistler et al., 2022\n",
    "\n",
    "_Truncated proposals for scalable and hassle-free simulation-based inference_ by\n",
    "Deistler, Goncalves & Macke (NeurIPS 2022) [[Paper]](https://arxiv.org/abs/2210.04815)\n",
    "\n",
    "Deistler et al. proposed a method for sequential inference. They propose to truncate\n",
    "the prior at every round. By doing this, the density estimator of NPE can be trained\n",
    "with the same loss (default) function at every round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93183126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "from sbi.utils import RestrictedPrior, get_density_thresholder\n",
    "\n",
    "inference = NPE(prior)\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x).train(force_first_round_loss=True)\n",
    "    posterior = inference.build_posterior().set_default_x(x_o)\n",
    "\n",
    "    accept_reject_fn = get_density_thresholder(posterior, quantile=1e-4)\n",
    "    proposal = RestrictedPrior(prior, accept_reject_fn, sample_with=\"rejection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2ea8c",
   "metadata": {},
   "source": [
    "### Dax et al., 2023\n",
    "\n",
    "_Flow Matching for Scalable Simulation-Based Inference_ by Dax, Wildberger, Buchholz,\n",
    "Green, Macke, Schölkopf (NeurIPS 2023) [[Paper]](https://arxiv.org/abs/2305.17161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import FMPE\n",
    "\n",
    "inference = FMPE(prior)\n",
    "# FMPE does not support multiple rounds of inference\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7beab32",
   "metadata": {},
   "source": [
    "### Geffner et al., Sharrock et al.,\n",
    "\n",
    "_Neural posterior score estimation_ based on:\n",
    "\n",
    "- **Compositional Score Modeling for Simulation-based Inference** by Geffner, T., Papamakarios, G., & Mnih, A. (ICML 2023) [[Paper]](https://proceedings.mlr.press/v202/geffner23a.html)  \n",
    "- **Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models** by Sharrock, L., Simons, J., Liu, S., & Beaumont, M. (ICML 2024) [[Paper]](https://arxiv.org/abs/2210.04872)  \n",
    "\n",
    "Note that currently only the single-round variant is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b68242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPSE\n",
    "\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "\n",
    "inference = NPSE(prior, sde_type=\"ve\")\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20cda82",
   "metadata": {},
   "source": [
    "## Neural Likelihood Estimation (NLE)\n",
    "\n",
    "### Default implementation\n",
    "\n",
    "The core idea of Neural Likelihood Estimation (NPE) is to train a conditional generative\n",
    "model that emulates the simulator and to then perform inference with traditional\n",
    "Bayesian methods such as MCMC or variational inference. This idea was originally\n",
    "developed by Papamakarios & Murray (AISTATS 2019). The default implementation of NLE in\n",
    "the `sbi` package follows Papamakarios & Murray, who proposed normalizing flows to\n",
    "emulate the simulator and MCMC to draw samples from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbeb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NLE\n",
    "\n",
    "inference = NLE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()\n",
    "samples = posterior.sample((1000,), x=x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8dae7",
   "metadata": {},
   "source": [
    "For the above default implementation of `NLE` in the `sbi` package, we recommend to\n",
    "cite:\n",
    "\n",
    "- _Sequential neural likelihood: Fast likelihood-free inference with autoregressive\n",
    "flows_ by Papamakarios, Sterratt & Murray (AISTATS 2019)\n",
    "[[PDF]](http://proceedings.mlr.press/v89/papamakarios19a/papamakarios19a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098b2ee",
   "metadata": {},
   "source": [
    "Beyond this, the `sbi` package implements many modifications and extensions of these algorithms for Neural Likelihood Estimation, which are outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90120ff8",
   "metadata": {},
   "source": [
    "### Papamakarios et al., 2019\n",
    "\n",
    "_Sequential neural likelihood: Fast likelihood-free inference with autoregressive\n",
    "flows_ by Papamakarios, Sterratt & Murray (AISTATS 2019)\n",
    "[[PDF]](http://proceedings.mlr.press/v89/papamakarios19a/papamakarios19a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301098a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NLE\n",
    "from sbi.inference.posteriors.posterior_parameters import MCMCPosteriorParameters\n",
    "\n",
    "inference = NLE(prior)\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x).train()\n",
    "    posterior = inference.build_posterior(posterior_parameters=MCMCPosteriorParameters(method=\"slice_np_vectorized\", num_chains=20,\n",
    "                                                           thin=5))\n",
    "\n",
    "    proposal = posterior.set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55404e7a",
   "metadata": {},
   "source": [
    "### Glöckler et al., 2022\n",
    "\n",
    "_Variational methods for simulation-based inference_ by Glöckler, Deistler, Macke\n",
    "(ICLR 2022) [[Paper]](https://arxiv.org/abs/2203.04176)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NLE\n",
    "from sbi.inference.posteriors.posterior_parameters import VIPosteriorParameters\n",
    "\n",
    "inference = NLE(prior)\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x).train()\n",
    "    posterior = inference.build_posterior(posterior_parameters=VIPosteriorParameters(vi_method=\"fKL\")).set_default_x(x_o)\n",
    "    proposal = posterior.train()  # Train VI posterior on given x_o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfc57c",
   "metadata": {},
   "source": [
    "### Boelts et al., 2022\n",
    "\n",
    "_Flexible and efficient simulation-based inference for models of decision-making_ by\n",
    "Boelts, Lueckmann, Gao, Macke (Elife 2022) [[Paper]](https://elifesciences.org/articles/77220)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c674f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import MNLE\n",
    "\n",
    "inference = MNLE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "# add a column of discrete data to x.\n",
    "x = torch.cat((simulator(theta), torch.bernoulli(theta[:, :1])), dim=1)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d163f1",
   "metadata": {},
   "source": [
    "## Neural Ratio Estimation (NRE)\n",
    "\n",
    "### Default implementation\n",
    "\n",
    "The core idea of Neural Ratio Estimation (NRE) is to train a classifier that\n",
    "distinguishes whether simulation outputs were generated from a particular parameter\n",
    "set or not. The classifier is then used to perform inference with traditional\n",
    "Bayesian methods such as MCMC or variational inference. This idea was originally\n",
    "developed by Hermans et al. (ICML 2020). The default implementation of NRE in\n",
    "the `sbi` package follows Durkan et al. (ICML 2020), who used a modified loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NRE\n",
    "\n",
    "inference = NRE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()\n",
    "samples = posterior.sample((1000,), x=x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55c938",
   "metadata": {},
   "source": [
    "\n",
    "For the above default implementation of `NRE` in the `sbi` package, we recommend to\n",
    "cite:\n",
    "\n",
    "- _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_ by\n",
    "Hermans, Begy & Louppe (ICML 2020)\n",
    "[[PDF]](http://proceedings.mlr.press/v119/hermans20a/hermans20a.pdf)\n",
    "\n",
    "- _On Contrastive Learning for Likelihood-free Inference_ by\n",
    "Durkan, Murray & Papamakarios (ICML 2020)\n",
    "[[PDF]](http://proceedings.mlr.press/v119/durkan20a/durkan20a.pdf)\n",
    "\n",
    "Beyond this, the `sbi` package implements many modifications and extensions of these algorithms for Neural Likelihood Estimation, which are outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f830ef",
   "metadata": {},
   "source": [
    "### Hermans et al., 2020\n",
    "\n",
    "_Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_ by Hermans,\n",
    "Begy & Louppe (ICML 2020) [[PDF]](http://proceedings.mlr.press/v119/hermans20a/hermans20a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646642e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NRE_A\n",
    "\n",
    "inference = NRE_A(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b1eb8",
   "metadata": {},
   "source": [
    "### Durkan et al., 2020\n",
    "\n",
    "_On Contrastive Learning for Likelihood-free Inference_ by Durkan, Murray &\n",
    "Papamakarios (ICML 2020) [[PDF]](http://proceedings.mlr.press/v119/durkan20a/durkan20a.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NRE\n",
    "from sbi.inference.posteriors.posterior_parameters import MCMCPosteriorParameters\n",
    "\n",
    "inference = NRE(prior)\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x).train()\n",
    "    posterior = inference.build_posterior(\n",
    "        posterior_parameters=MCMCPosteriorParameters(\n",
    "            method=\"slice_np_vectorized\", num_chains=20, thin=5\n",
    "        )\n",
    "    )\n",
    "\n",
    "    proposal = posterior.set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53de3dd",
   "metadata": {},
   "source": [
    "### Delaunoy et al., 2022\n",
    "\n",
    "_Towards Reliable Simulation-Based Inference with Balanced Neural Ratio Estimation_ by\n",
    "Delaunoy, Hermans, Rozet, Wehenkel & Louppe (NeurIPS 2022)\n",
    "[[PDF]](https://arxiv.org/pdf/2208.13624.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import BNRE\n",
    "\n",
    "inference = BNRE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "_ = inference.append_simulations(theta, x).train(regularization_strength=100.)\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80922d50",
   "metadata": {},
   "source": [
    "### Miller et al., 2022\n",
    "\n",
    "_Contrastive Neural Ratio Estimation_ by Benjamin Kurt Miller, Christoph Weniger,\n",
    "Patrick Forré (NeurIPS 2022) [[PDF]](https://arxiv.org/pdf/2210.06170.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main feature of NRE-C is producing an exact ratio of densities at optimum,\n",
    "# even when using multiple contrastive pairs (classes).\n",
    "\n",
    "from sbi.inference import NRE_C\n",
    "\n",
    "inference = NRE_C(prior)\n",
    "proposal = prior\n",
    "theta = proposal.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "_ = inference.append_simulations(theta, x).train(\n",
    "    num_classes=5,  # sees `2 * num_classes - 1` marginally drawn contrastive pairs.\n",
    "    gamma=1.0,  # controls the weight between terms in its loss function.\n",
    ")\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b5bace",
   "metadata": {},
   "source": [
    "## Diagnostics and utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e501b8d",
   "metadata": {},
   "source": [
    "### Talts et al., 2018\n",
    "\n",
    "_Simulation-based calibration_ by Talts, Betancourt, Simpson, Vehtari, Gelman\n",
    "(arxiv 2018) [[Paper]](https://arxiv.org/abs/1804.06788)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da329911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import sbc_rank_plot\n",
    "from sbi.diagnostics import run_sbc\n",
    "\n",
    "thetas = prior.sample((1000,))\n",
    "xs = simulator(thetas)\n",
    "\n",
    "# SBC is fast for fully amortized NPE.\n",
    "inference = NPE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()\n",
    "\n",
    "ranks, dap_samples = run_sbc(\n",
    "    thetas, xs, posterior, num_posterior_samples=1_000\n",
    ")\n",
    "\n",
    "fig, axes = sbc_rank_plot(\n",
    "    ranks=ranks,\n",
    "    num_posterior_samples=1000,\n",
    "    plot_type=\"hist\",\n",
    "    num_bins=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a391a",
   "metadata": {},
   "source": [
    "### Deistler et al., Rozet et al.,\n",
    "\n",
    "_Expected coverage (sample-based)_ as computed in Deistler, Goncalves, Macke\n",
    "(Neurips 2022) [[Paper]](https://arxiv.org/abs/2210.04815) and in Rozet, Louppe (2021)\n",
    "[[Paper]](https://matheo.uliege.be/handle/2268.2/12993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = prior.sample((100,))\n",
    "xs = simulator(thetas)\n",
    "\n",
    "ranks, dap_samples = run_sbc(\n",
    "    thetas,\n",
    "    xs,\n",
    "    posterior,\n",
    "    num_posterior_samples=1_000,\n",
    "    reduce_fns=posterior.log_prob  # Difference to SBC.\n",
    ")\n",
    "\n",
    "# NOTE: Here we obtain a single rank plot because ranks are calculated\n",
    "# for the entire posterior and not for each marginal like in SBC.\n",
    "fig, axes = sbc_rank_plot(\n",
    "    ranks=ranks,\n",
    "    num_posterior_samples=1000,\n",
    "    plot_type=\"hist\",\n",
    "    num_bins=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01a5b1",
   "metadata": {},
   "source": [
    "### Lemost et al., 2023\n",
    "\n",
    "_TARP: Sampling-Based Accuracy Testing of Posterior Estimators for General Inference_\n",
    "by Lemos, Coogan, Hezaveh & Perreault-Levasseur (ICML 2023) [[Paper]](https://arxiv.org/abs/2302.03026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d79e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import plot_tarp\n",
    "from sbi.diagnostics.tarp import run_tarp\n",
    "\n",
    "thetas = prior.sample((1000,))\n",
    "xs = simulator(thetas)\n",
    "\n",
    "expected_coverage, ideal_coverage = run_tarp(\n",
    "    thetas,\n",
    "    xs,\n",
    "    posterior,\n",
    "    references=None,  # optional, defaults to uniform samples across parameter space.\n",
    "    num_posterior_samples=1_000,\n",
    ")\n",
    "\n",
    "fix, axes = plot_tarp(expected_coverage, ideal_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dfeabc",
   "metadata": {},
   "source": [
    "### Deistler et al., 2022\n",
    "\n",
    "_Restriction estimator_ by Deistler, Macke & Goncalves (PNAS 2022)\n",
    "[[Paper]](https://www.pnas.org/doi/10.1073/pnas.2207632119)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "from sbi.utils import RestrictionEstimator\n",
    "\n",
    "restriction_estimator = RestrictionEstimator(prior=prior)\n",
    "proposal = prior\n",
    "\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    restriction_estimator.append_simulations(theta, x)\n",
    "    classifier = restriction_estimator.train()\n",
    "    proposal = restriction_estimator.restrict_prior()\n",
    "\n",
    "all_theta, all_x, _ = restriction_estimator.get_simulations()\n",
    "\n",
    "inference = NPE(prior)\n",
    "density_estimator = inference.append_simulations(all_theta, all_x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
