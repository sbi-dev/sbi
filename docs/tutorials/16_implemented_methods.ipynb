{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd7f17d",
   "metadata": {},
   "source": [
    "# API of implemented methods\n",
    "\n",
    "This notebook spells out the API for all algorithms implemented in the `sbi` toolbox:\n",
    "\n",
    "- Posterior estimation (NPE)\n",
    "\n",
    "- Likelihood estimation (NLE)\n",
    "\n",
    "- Likelihood-ratio estimation (NRE)\n",
    "\n",
    "- Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac88848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example setup\n",
    "import torch\n",
    "\n",
    "from sbi.utils import BoxUniform\n",
    "\n",
    "# Define the prior\n",
    "num_dims = 2\n",
    "num_sims = 1000\n",
    "num_rounds = 2\n",
    "prior = BoxUniform(low=torch.zeros(num_dims), high=torch.ones(num_dims))\n",
    "simulator = lambda theta: theta + torch.randn_like(theta) * 0.1\n",
    "x_o = torch.tensor([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e608393",
   "metadata": {},
   "source": [
    "## Posterior estimation (NPE)\n",
    "\n",
    "The core idea of Neural Posterior Estimation (NPE) is to train a conditional generative\n",
    "model that directly predicts the posterior given observations. This idea was originally\n",
    "developed by Papamakarios & Murray (NeurIPS 2016). The default implementation of NPE in\n",
    "the `sbi` package follows Greenberg, Nonnenmacher & Macke, who proposed NPE with\n",
    "normalizing flows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1aa30f",
   "metadata": {},
   "source": [
    "As such, for the default implementation of `NPE` in the `sbi` package, we recommend to\n",
    "cite:\n",
    "\n",
    "**Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation**<br> by Papamakarios & Murray (NeurIPS 2016) <br>[[PDF]](https://papers.nips.cc/paper/6084-fast-free-inference-of-simulation-models-with-bayesian-conditional-density-estimation.pdf)\n",
    "\n",
    "**Automatic posterior transformation for likelihood-free inference**<br>by Greenberg, Nonnenmacher & Macke (ICML 2019) <br>[[PDF]](http://proceedings.mlr.press/v97/greenberg19a/greenberg19a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "\n",
    "inference = NPE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()\n",
    "samples = posterior.sample((1000,), x=x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bf467",
   "metadata": {},
   "source": [
    "Beyond this, the `sbi` package implements many modifications and extensions of these algorithms for Neural Posterior Estimation, which are outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bc6b4",
   "metadata": {},
   "source": [
    "**Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation**<br> by Papamakarios & Murray (NeurIPS 2016) <br>[[PDF]](https://papers.nips.cc/paper/6084-fast-free-inference-of-simulation-models-with-bayesian-conditional-density-estimation.pdf)\n",
    "\n",
    "Papamakarios et al. (2016) were the first to use neural networks to directly predict\n",
    "the posterior given observations. As density estimator, they used mixture density\n",
    "networks. In addition, they also proposed a sequential algorithm for neural posterior\n",
    "estimation. Their full algorithm can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE_A\n",
    "\n",
    "inference = NPE_A(prior)\n",
    "proposal = prior\n",
    "for r in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    # NPE-A trains a Gaussian density estimator in all but the last round. In the last round,\n",
    "    # it trains a mixture of Gaussians, which is why we have to pass the `final_round` flag.\n",
    "    final_round = r == num_rounds - 1\n",
    "    _ = inference.append_simulations(theta, x, proposal=proposal).train(final_round=final_round)\n",
    "    posterior = inference.build_posterior().set_default_x(x_o)\n",
    "    proposal = posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4d312",
   "metadata": {},
   "source": [
    "**Flexible statistical inference for mechanistic models of neural dynamics**<br>by Lueckmann, Goncalves, Bassetto, Öcal, Nonnenmacher, Macke (NeurIPS 2017) <br>[[PDF]](https://proceedings.neurips.cc/paper/2017/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html)\n",
    "\n",
    "Like Papamakarios et al. (2016), Lueckmann et al. (2017) used mixture density networks for\n",
    "NPE. In addition, they also proposed embedding networks for time series, and proposed a\n",
    "different loss function for the sequential version of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE_B\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding\n",
    "\n",
    "embedding = FCEmbedding(num_dims)\n",
    "density_estimator = posterior_nn(\"mdn\", embedding_net=embedding)\n",
    "inference = NPE_B(prior, density_estimator=density_estimator)\n",
    "\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x, proposal=proposal).train()\n",
    "    posterior = inference.build_posterior().set_default_x(x_o)\n",
    "    proposal = posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd7f43",
   "metadata": {},
   "source": [
    "**Automatic posterior transformation for likelihood-free inference**<br>by Greenberg, Nonnenmacher & Macke (ICML 2019) <br>[[PDF]](http://proceedings.mlr.press/v97/greenberg19a/greenberg19a.pdf)\n",
    "\n",
    "Greenberg, Nonnenmacher & Macke were the first to use normalizing flows for neural\n",
    "posterior estimation (NPE), which is the default for the `NPE` class in the `sbi`\n",
    "package (see above). In addition, they also proposed embedding networks\n",
    "in combination with normalizing flows, and proposed a modified loss function for the\n",
    "sequential version of the neural posterior estimation. These additional contributions\n",
    "can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding\n",
    "\n",
    "embedding = FCEmbedding(num_dims)\n",
    "density_estimator = posterior_nn(\"maf\", embedding_net=embedding)\n",
    "inference = NPE(prior, density_estimator=density_estimator)\n",
    "\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x, proposal=proposal).train()\n",
    "    posterior = inference.build_posterior().set_default_x(x_o)\n",
    "    proposal = posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc71c3a1",
   "metadata": {},
   "source": [
    "**BayesFlow: Learning complex stochastic models with invertible neural\n",
    "networks**<br> by Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Köthe,\n",
    "U. (2020) (IEEE transactions on neural networks and learning systems 2020)<br>\n",
    "[Paper](https://ieeexplore.ieee.org/abstract/document/9298920)\n",
    "\n",
    "The density estimation part of BayesFlow is equivalent to single-round NPE (Greenberg\n",
    "et al., 2019). The additional contribution of BayesFlow are permutation invariant\n",
    "embedding networks for iid data, which can be implemented in `sbi` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior estimation with BayesFlow is equivalent to single-round NPE.\n",
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding, PermutationInvariantEmbedding\n",
    "\n",
    "num_iid = 3\n",
    "simulator_iid = lambda theta: theta + torch.randn((num_iid, *theta.shape)) * 0.1\n",
    "\n",
    "trial_net = FCEmbedding(num_dims, 20)\n",
    "embedding = PermutationInvariantEmbedding(trial_net, 20)\n",
    "density_estimator = posterior_nn(\"maf\", embedding_net=embedding)\n",
    "inference = NPE(prior, density_estimator=density_estimator)\n",
    "\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator_iid(theta).permute(1, 0, 2)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()\n",
    "samples = posterior.sample((1000,), x=torch.zeros((1, num_iid, num_dims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef8b35",
   "metadata": {},
   "source": [
    "**Truncated proposals for scalable and hassle-free simulation-based inference** <br> by Deistler, Goncalves & Macke (NeurIPS 2022) <br>[[Paper]](https://arxiv.org/abs/2210.04815)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93183126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "from sbi.utils import RestrictedPrior, get_density_thresholder\n",
    "\n",
    "inference = NPE(prior)\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x).train(force_first_round_loss=True)\n",
    "    posterior = inference.build_posterior().set_default_x(x_o)\n",
    "\n",
    "    accept_reject_fn = get_density_thresholder(posterior, quantile=1e-4)\n",
    "    proposal = RestrictedPrior(prior, accept_reject_fn, sample_with=\"rejection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2ea8c",
   "metadata": {},
   "source": [
    "**Flow Matching for Scalable Simulation-Based Inference** <br> by Dax, Wildberger, Buchholz, Green, Macke,\n",
    "Schölkopf (NeurIPS 2023) <br> [[Paper]](https://arxiv.org/abs/2305.17161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import FMPE\n",
    "\n",
    "inference = FMPE(prior)\n",
    "# FMPE does not support multiple rounds of inference\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7beab32",
   "metadata": {},
   "source": [
    "**Neural posterior score estimation**<br>\n",
    "\n",
    "based on:  \n",
    "\n",
    "- **Compositional Score Modeling for Simulation-based Inference** by Geffner, T., Papamakarios, G., & Mnih, A. (ICML 2023) [[Paper]](https://proceedings.mlr.press/v202/geffner23a.html)  \n",
    "- **Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models** by Sharrock, L., Simons, J., Liu, S., & Beaumont, M. (ICML 2024) [[Paper]](https://arxiv.org/abs/2210.04872)  \n",
    "\n",
    "Note that currently only the single-round variant is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b68242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPSE\n",
    "\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "\n",
    "inference = NPSE(prior, sde_type=\"ve\")\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20cda82",
   "metadata": {},
   "source": [
    "## Likelihood estimation (NLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90120ff8",
   "metadata": {},
   "source": [
    "**Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows**<br>by Papamakarios, Sterratt & Murray (AISTATS 2019) <br>[[PDF]](http://proceedings.mlr.press/v89/papamakarios19a/papamakarios19a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301098a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NLE\n",
    "from sbi.inference.posteriors.posterior_parameters import MCMCPosteriorParameters\n",
    "\n",
    "inference = NLE(prior)\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x).train()\n",
    "    posterior = inference.build_posterior(posterior_parameters=MCMCPosteriorParameters(method=\"slice_np_vectorized\", num_chains=20,\n",
    "                                                           thin=5))\n",
    "\n",
    "    proposal = posterior.set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55404e7a",
   "metadata": {},
   "source": [
    "**Variational methods for simulation-based inference** <br> by Glöckler, Deistler, Macke (ICLR 2022) <br>[[Paper]](https://arxiv.org/abs/2203.04176)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NLE\n",
    "from sbi.inference.posteriors.posterior_parameters import VIPosteriorParameters\n",
    "\n",
    "inference = NLE(prior)\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x).train()\n",
    "    posterior = inference.build_posterior(posterior_parameters=VIPosteriorParameters(vi_method=\"fKL\")).set_default_x(x_o)\n",
    "    proposal = posterior.train()  # Train VI posterior on given x_o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfc57c",
   "metadata": {},
   "source": [
    "**Flexible and efficient simulation-based inference for models of decision-making** <br> by Boelts, Lueckmann, Gao, Macke (Elife 2022) <br>[[Paper]](https://elifesciences.org/articles/77220)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c674f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import MNLE\n",
    "\n",
    "inference = MNLE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "# add a column of discrete data to x.\n",
    "x = torch.cat((simulator(theta), torch.bernoulli(theta[:, :1])), dim=1)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d163f1",
   "metadata": {},
   "source": [
    "## Likelihood-ratio estimation (NRE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f830ef",
   "metadata": {},
   "source": [
    "**Likelihood-free MCMC with Amortized Approximate Likelihood Ratios**<br>by Hermans, Begy & Louppe (ICML 2020) <br>[[PDF]](http://proceedings.mlr.press/v119/hermans20a/hermans20a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646642e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NRE_A\n",
    "\n",
    "inference = NRE_A(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b1eb8",
   "metadata": {},
   "source": [
    "**On Contrastive Learning for Likelihood-free Inference**<br>Durkan, Murray & Papamakarios (ICML 2020) <br>[[PDF]](http://proceedings.mlr.press/v119/durkan20a/durkan20a.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NRE\n",
    "from sbi.inference.posteriors.posterior_parameters import MCMCPosteriorParameters\n",
    "\n",
    "inference = NRE(prior)\n",
    "proposal = prior\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    _ = inference.append_simulations(theta, x).train()\n",
    "    posterior = inference.build_posterior(posterior_parameters=MCMCPosteriorParameters(method=\"slice_np_vectorized\", num_chains=20,\n",
    "                                                           thin=5))\n",
    "\n",
    "    proposal = posterior.set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53de3dd",
   "metadata": {},
   "source": [
    "**Towards Reliable Simulation-Based Inference with Balanced Neural Ratio Estimation**<br>by Delaunoy, Hermans, Rozet, Wehenkel & Louppe (NeurIPS 2022) <br>[[PDF]](https://arxiv.org/pdf/2208.13624.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import BNRE\n",
    "\n",
    "inference = BNRE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "_ = inference.append_simulations(theta, x).train(regularization_strength=100.)\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80922d50",
   "metadata": {},
   "source": [
    "**Contrastive Neural Ratio Estimation**<br>Benjamin Kurt Miller, Christoph Weniger, Patrick Forré (NeurIPS 2022) <br>[[PDF]](https://arxiv.org/pdf/2210.06170.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main feature of NRE-C is producing an exact ratio of densities at optimum,\n",
    "# even when using multiple contrastive pairs (classes).\n",
    "\n",
    "from sbi.inference import NRE_C\n",
    "\n",
    "inference = NRE_C(prior)\n",
    "proposal = prior\n",
    "theta = proposal.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "_ = inference.append_simulations(theta, x).train(\n",
    "    num_classes=5,  # sees `2 * num_classes - 1` marginally drawn contrastive pairs.\n",
    "    gamma=1.0,  # controls the weight between terms in its loss function.\n",
    ")\n",
    "posterior = inference.build_posterior().set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b5bace",
   "metadata": {},
   "source": [
    "## Diagnostics and utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e501b8d",
   "metadata": {},
   "source": [
    "**Simulation-based calibration**<br>by Talts, Betancourt, Simpson, Vehtari, Gelman (arxiv 2018) <br>[[Paper]](https://arxiv.org/abs/1804.06788)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da329911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import sbc_rank_plot\n",
    "from sbi.diagnostics import run_sbc\n",
    "\n",
    "thetas = prior.sample((1000,))\n",
    "xs = simulator(thetas)\n",
    "\n",
    "# SBC is fast for fully amortized NPE.\n",
    "inference = NPE(prior)\n",
    "theta = prior.sample((num_sims,))\n",
    "x = simulator(theta)\n",
    "inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()\n",
    "\n",
    "ranks, dap_samples = run_sbc(\n",
    "    thetas, xs, posterior, num_posterior_samples=1_000\n",
    ")\n",
    "\n",
    "fig, axes = sbc_rank_plot(\n",
    "    ranks=ranks,\n",
    "    num_posterior_samples=1000,\n",
    "    plot_type=\"hist\",\n",
    "    num_bins=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a391a",
   "metadata": {},
   "source": [
    "**Expected coverage (sample-based)**<br>as computed in Deistler, Goncalves, Macke (Neurips 2022) [[Paper]](https://arxiv.org/abs/2210.04815) and in Rozet, Louppe (2021) [[Paper]](https://matheo.uliege.be/handle/2268.2/12993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = prior.sample((100,))\n",
    "xs = simulator(thetas)\n",
    "\n",
    "ranks, dap_samples = run_sbc(\n",
    "    thetas,\n",
    "    xs,\n",
    "    posterior,\n",
    "    num_posterior_samples=1_000,\n",
    "    reduce_fns=posterior.log_prob  # Difference to SBC.\n",
    ")\n",
    "\n",
    "# NOTE: Here we obtain a single rank plot because ranks are calculated\n",
    "# for the entire posterior and not for each marginal like in SBC.\n",
    "fig, axes = sbc_rank_plot(\n",
    "    ranks=ranks,\n",
    "    num_posterior_samples=1000,\n",
    "    plot_type=\"hist\",\n",
    "    num_bins=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01a5b1",
   "metadata": {},
   "source": [
    "**TARP: Sampling-Based Accuracy Testing of Posterior Estimators for General Inference**\n",
    "\n",
    "Lemos, Coogan, Hezaveh & Perreault-Levasseur (ICML 2023)<br>[[Paper]](https://arxiv.org/abs/2302.03026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d79e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import plot_tarp\n",
    "from sbi.diagnostics.tarp import run_tarp\n",
    "\n",
    "thetas = prior.sample((1000,))\n",
    "xs = simulator(thetas)\n",
    "\n",
    "expected_coverage, ideal_coverage = run_tarp(\n",
    "    thetas,\n",
    "    xs,\n",
    "    posterior,\n",
    "    references=None,  # optional, defaults to uniform samples across parameter space.\n",
    "    num_posterior_samples=1_000,\n",
    ")\n",
    "\n",
    "fix, axes = plot_tarp(expected_coverage, ideal_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dfeabc",
   "metadata": {},
   "source": [
    "**Restriction estimator**<br>by Deistler, Macke & Goncalves (PNAS 2022) <br>[[Paper]](https://www.pnas.org/doi/10.1073/pnas.2207632119)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "from sbi.utils import RestrictionEstimator\n",
    "\n",
    "restriction_estimator = RestrictionEstimator(prior=prior)\n",
    "proposal = prior\n",
    "\n",
    "for _ in range(num_rounds):\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    x = simulator(theta)\n",
    "    restriction_estimator.append_simulations(theta, x)\n",
    "    classifier = restriction_estimator.train()\n",
    "    proposal = restriction_estimator.restrict_prior()\n",
    "\n",
    "all_theta, all_x, _ = restriction_estimator.get_simulations()\n",
    "\n",
    "inference = NPE(prior)\n",
    "density_estimator = inference.append_simulations(all_theta, all_x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
