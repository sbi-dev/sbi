{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive learning rate schedulers for improved training\n",
    "\n",
    "Note, you can find the original version of this notebook at [docs/advanced_tutorials/22_adaptive_learning_rates.ipynb](https://github.com/sbi-dev/sbi/blob/main/docs/advanced_tutorials/22_adaptive_learning_rates.ipynb) in the `sbi` repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate schedulers can significantly improve neural network training by automatically adjusting the learning rate during training. This tutorial demonstrates how to use PyTorch's learning rate schedulers with all `sbi` inference methods (NPE, NLE, and NRE).\n",
    "\n",
    "We'll show how different schedulers affect training dynamics and final performance using a simple but illustrative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.inference import NLE_A, NPE, NRE_A\n",
    "from sbi.simulators.linear_gaussian import linear_gaussian\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Linear Gaussian model\n",
    "\n",
    "We'll use a simple 2D linear Gaussian model where we can easily visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior and simulator\n",
    "num_dim = 2\n",
    "prior = MultivariateNormal(loc=torch.zeros(num_dim), covariance_matrix=torch.eye(num_dim))\n",
    "\n",
    "# Simulator parameters\n",
    "likelihood_shift = -0.5 * torch.ones(num_dim)\n",
    "likelihood_cov = 0.3 * torch.eye(num_dim)\n",
    "\n",
    "def simulator(theta):\n",
    "    return linear_gaussian(theta, likelihood_shift, likelihood_cov)\n",
    "\n",
    "# Generate training data\n",
    "num_simulations = 1000\n",
    "theta = prior.sample((num_simulations,))\n",
    "x = simulator(theta)\n",
    "\n",
    "print(f\"Generated {num_simulations} training samples\")\n",
    "print(f\"Parameter space dimensionality: {theta.shape[1]}\")\n",
    "print(f\"Data space dimensionality: {x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different learning rate schedulers\n",
    "\n",
    "Let's train the same NPE model with different schedulers and compare their training dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_scheduler(scheduler_name, scheduler_kwargs, learning_rate=1e-3, max_epochs=100):\n",
    "    \"\"\"Train NPE with a specific scheduler and return training history.\"\"\"\n",
    "\n",
    "    inference = NPE(prior=prior, show_progress_bars=False)\n",
    "    inference.append_simulations(theta, x)\n",
    "\n",
    "    # Train with scheduler\n",
    "    posterior = inference.train(\n",
    "        lr_scheduler=scheduler_name,\n",
    "        lr_scheduler_kwargs=scheduler_kwargs,\n",
    "        learning_rate=learning_rate,\n",
    "        max_num_epochs=max_epochs,\n",
    "        validation_fraction=0.1,\n",
    "        training_batch_size=50\n",
    "    )\n",
    "\n",
    "    # Extract training history\n",
    "    summary = inference._summary\n",
    "    return {\n",
    "        'epochs': summary['epochs_trained'],\n",
    "        'train_losses': summary['train_log_probs'],\n",
    "        'val_losses': summary['validation_log_probs'],\n",
    "        'learning_rates': summary['learning_rates'],\n",
    "        'posterior': posterior,\n",
    "        'inference': inference\n",
    "    }\n",
    "\n",
    "# Define scheduler configurations to test\n",
    "scheduler_configs = {\n",
    "    'No Scheduler': (None, {}),\n",
    "    'ReduceLROnPlateau': ('plateau', {'factor': 0.5, 'patience': 10}),\n",
    "    'ExponentialLR': ('exponential', {'gamma': 0.95}),\n",
    "    'CosineAnnealingLR': ('cosine', {'T_max': 50, 'eta_min': 1e-6}),\n",
    "    'StepLR': ('step', {'step_size': 25, 'gamma': 0.3})\n",
    "}\n",
    "\n",
    "# Train with each scheduler\n",
    "results = {}\n",
    "for name, (scheduler, kwargs) in scheduler_configs.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    results[name] = train_with_scheduler(scheduler, kwargs, learning_rate=5e-3, max_epochs=150)\n",
    "    final_epoch = results[name]['epochs'][-1]\n",
    "    final_val_loss = results[name]['val_losses'][-1]\n",
    "    print(f\"  Converged after {final_epoch} epochs, final validation loss: {final_val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training dynamics\n",
    "\n",
    "Let's compare how different schedulers affect training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "\n",
    "# Plot training loss\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    epochs = result['epochs']\n",
    "    train_losses = result['train_losses']\n",
    "    axes[0].plot(epochs, train_losses, label=name, color=colors[i], alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss vs Epoch')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation loss\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    epochs = result['epochs']\n",
    "    val_losses = result['val_losses']\n",
    "    axes[1].plot(epochs, val_losses, label=name, color=colors[i], alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss vs Epoch')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning rate schedules\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    if name != 'No Scheduler':  # Skip constant LR\n",
    "        epochs = result['epochs']\n",
    "        lrs = result['learning_rates']\n",
    "        axes[2].plot(epochs, lrs, label=name, color=colors[i], alpha=0.8)\n",
    "\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Training efficiency: epochs to convergence\n",
    "names = list(results.keys())\n",
    "epochs_to_converge = [results[name]['epochs'][-1] for name in names]\n",
    "final_val_losses = [results[name]['val_losses'][-1] for name in names]\n",
    "\n",
    "bars = axes[3].bar(range(len(names)), epochs_to_converge, color=colors[:len(names)], alpha=0.7)\n",
    "axes[3].set_xlabel('Scheduler Type')\n",
    "axes[3].set_ylabel('Epochs to Convergence')\n",
    "axes[3].set_title('Training Efficiency')\n",
    "axes[3].set_xticks(range(len(names)))\n",
    "axes[3].set_xticklabels(names, rotation=45, ha='right')\n",
    "\n",
    "# Add final validation loss as text on bars\n",
    "for bar, val_loss in zip(bars, final_val_losses, strict=False):\n",
    "    height = bar.get_height()\n",
    "    axes[3].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'Val: {val_loss:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining posterior quality\n",
    "\n",
    "Let's compare the posterior quality for different schedulers by sampling from the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set observation point\n",
    "x_o = torch.tensor([[0.5, -0.2]])\n",
    "\n",
    "# Sample from posteriors\n",
    "fig, axes = plt.subplots(1, len(results), figsize=(4*len(results), 4))\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    # Build posterior and sample\n",
    "    posterior = result['inference'].build_posterior()\n",
    "    samples = posterior.sample((1000,), x=x_o)\n",
    "\n",
    "    # Plot\n",
    "    pairplot(samples,\n",
    "             limits=[[-3, 3], [-3, 3]],\n",
    "             figsize=(4, 4),\n",
    "             fig=fig,\n",
    "             subplot_idx=(0, i),\n",
    "             points=x_o,\n",
    "             points_colors=['red'])\n",
    "\n",
    "    axes[i].set_title(f'{name}\\n(Final val loss: {result[\"val_losses\"][-1]:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced scheduler configuration\n",
    "\n",
    "### Using dictionary configuration for complex schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex scheduler configuration using dictionary\n",
    "advanced_scheduler_config = {\n",
    "    \"type\": \"plateau\",\n",
    "    \"factor\": 0.3,        # Reduce LR by 70%\n",
    "    \"patience\": 8,        # Wait 8 epochs\n",
    "    \"threshold\": 1e-4,    # Minimum improvement threshold\n",
    "    \"cooldown\": 5,        # Wait 5 epochs after reduction\n",
    "    \"min_lr\": 1e-6,       # Don't reduce below this\n",
    "}\n",
    "\n",
    "inference_advanced = NPE(prior=prior, show_progress_bars=False)\n",
    "inference_advanced.append_simulations(theta, x)\n",
    "\n",
    "posterior_advanced = inference_advanced.train(\n",
    "    lr_scheduler=advanced_scheduler_config,\n",
    "    learning_rate=1e-2,    # Higher initial LR\n",
    "    max_num_epochs=200,\n",
    "    validation_fraction=0.1,\n",
    "    min_lr_threshold=2e-6  # Stop if LR gets too small\n",
    ")\n",
    "\n",
    "print(f\"Advanced scheduler training completed after {inference_advanced._summary['epochs_trained'][-1]} epochs\")\n",
    "print(f\"Final learning rate: {inference_advanced._summary['learning_rates'][-1]:.2e}\")\n",
    "print(f\"Final validation loss: {inference_advanced._summary['validation_log_probs'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate early stopping based on minimum learning rate\n",
    "inference_early_stop = NPE(prior=prior, show_progress_bars=False)\n",
    "inference_early_stop.append_simulations(theta, x)\n",
    "\n",
    "posterior_early_stop = inference_early_stop.train(\n",
    "    lr_scheduler=\"exponential\",\n",
    "    lr_scheduler_kwargs={\"gamma\": 0.85},  # Fast decay\n",
    "    learning_rate=5e-3,\n",
    "    max_num_epochs=300,                   # High max epochs\n",
    "    min_lr_threshold=1e-4,                # Stop when LR < 1e-4\n",
    "    stop_after_epochs=100,                # Also stop if no improvement\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "# Plot the learning rate decay and stopping point\n",
    "epochs = inference_early_stop._summary['epochs_trained']\n",
    "lrs = inference_early_stop._summary['learning_rates']\n",
    "val_losses = inference_early_stop._summary['validation_log_probs']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Learning rate schedule\n",
    "ax1.plot(epochs, lrs, 'b-', linewidth=2)\n",
    "ax1.axhline(y=1e-4, color='r', linestyle='--', alpha=0.7, label='min_lr_threshold')\n",
    "ax1.set_ylabel('Learning Rate')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('Learning Rate Schedule with Early Stopping')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "ax2.plot(epochs, val_losses, 'g-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_title('Validation Loss During Training')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training stopped after {epochs[-1]} epochs due to learning rate threshold\")\n",
    "print(f\"Final learning rate: {lrs[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedulers with NLE and NRE\n",
    "\n",
    "Learning rate schedulers work identically across all `sbi` inference methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test schedulers with NLE and NRE\n",
    "methods = {\n",
    "    'NPE': NPE,\n",
    "    'NLE': NLE_A,\n",
    "    'NRE': NRE_A\n",
    "}\n",
    "\n",
    "scheduler_config = {\n",
    "    \"type\": \"plateau\",\n",
    "    \"factor\": 0.5,\n",
    "    \"patience\": 5\n",
    "}\n",
    "\n",
    "method_results = {}\n",
    "\n",
    "for name, Method in methods.items():\n",
    "    print(f\"Training {name} with plateau scheduler...\")\n",
    "\n",
    "    inference = Method(prior=prior, show_progress_bars=False)\n",
    "    inference.append_simulations(theta, x)\n",
    "\n",
    "    estimator = inference.train(\n",
    "        lr_scheduler=scheduler_config,\n",
    "        learning_rate=1e-3,\n",
    "        max_num_epochs=100,\n",
    "        validation_fraction=0.1,\n",
    "        training_batch_size=50\n",
    "    )\n",
    "\n",
    "    epochs_trained = inference._summary['epochs_trained'][-1]\n",
    "    final_lr = inference._summary['learning_rates'][-1]\n",
    "\n",
    "    method_results[name] = {\n",
    "        'epochs': epochs_trained,\n",
    "        'final_lr': final_lr,\n",
    "        'inference': inference\n",
    "    }\n",
    "\n",
    "    print(f\"  Converged after {epochs_trained} epochs, final LR: {final_lr:.2e}\")\n",
    "\n",
    "# Compare learning rate evolution across methods\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for name, result in method_results.items():\n",
    "    epochs = result['inference']._summary['epochs_trained']\n",
    "    lrs = result['inference']._summary['learning_rates']\n",
    "    plt.plot(epochs, lrs, label=f'{name}', linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule Across Different Inference Methods')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices and recommendations\n",
    "\n",
    "Based on the experiments above, here are key takeaways:\n",
    "\n",
    "1. **ReduceLROnPlateau is often the best choice**: It adapts to your specific training dynamics and typically provides good results across different problems.\n",
    "\n",
    "2. **Start with higher learning rates**: Schedulers allow you to begin with more aggressive learning rates that would otherwise cause instability.\n",
    "\n",
    "3. **Use min_lr_threshold**: This prevents wasted computation when learning rates become ineffectively small.\n",
    "\n",
    "4. **Monitor training curves**: The learning rate schedule should align with improvements in validation loss.\n",
    "\n",
    "5. **Scheduler choice depends on your problem**: \n",
    "   - Plateau: Good default choice, adapts to training progress\n",
    "   - Exponential: Simple and predictable decay\n",
    "   - Cosine: Good for longer training runs\n",
    "   - Cyclic: Can help escape local minima\n",
    "\n",
    "6. **Combine with early stopping**: Use both scheduler-based and patience-based stopping criteria for robust training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Learning rate schedulers provide a powerful way to improve `sbi` training:\n",
    "\n",
    "- **Easy to use**: Just add `lr_scheduler` and `lr_scheduler_kwargs` parameters to `.train()`\n",
    "- **Universal compatibility**: Works with NPE, NLE, NRE, and all their variants\n",
    "- **Flexible configuration**: Use string shortcuts or detailed dictionary configs\n",
    "- **Automatic monitoring**: Learning rates are tracked in training summaries\n",
    "- **Early stopping**: Prevent over-training with `min_lr_threshold`\n",
    "\n",
    "Experiment with different schedulers to find what works best for your specific inference problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
