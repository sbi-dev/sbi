{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with `sbi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sbi.utils as utils\n",
    "from sbi.inference.base import infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the inference procedure\n",
    "\n",
    "`sbi` provides a simple interface to run state-of-the-art algorithms for simulation-based inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference, you need to provide two things:\n",
    "\n",
    "1) a prior distribution that allows to sample parameter sets.  \n",
    "2) a simulator that takes parameter sets and produces simulation outputs.\n",
    "\n",
    "For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simulator that adds 1.0 and some Gaussian noise to the parameter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 3\n",
    "prior = utils.BoxUniform(-2*torch.ones(num_dim), 2*torch.ones(num_dim))\n",
    "\n",
    "def simulator(parameter_set):\n",
    "    return 1.0 + parameter_set + torch.randn(parameter_set.shape) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sbi` can then run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = infer('SNPE', simulator, prior, num_simulations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have made some observation $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = torch.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Given this observation, we can then sample from the posterior $p(\\theta|x)$, evaluate its log-probability, or plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample((10000,), x=observation)\n",
    "log_probability = posterior.log_prob(samples, x=observation)\n",
    "_ = utils.pairplot(samples, limits=[[-2,2],[-2,2],[-2,2]], fig_size=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements for the simulator, prior, and observation\n",
    "\n",
    "For all algorithms, all you need to provide are a prior and a simulator. Let's talk about what requirements they need to satisfy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prior\n",
    "A distribution that allows to sample parameter sets. Any datatype for the prior is allowed as long as it allows to call `prior.sample()` and `prior.log_prob()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulator\n",
    "A python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity.\n",
    "\n",
    "Allowed data types and shapes for input and output:\n",
    "- the input parameter set and the output have to be either a `np.ndarray` or a `torch.Tensor`. \n",
    "- the input parameter set should have either shape `(1,N)` or `(N)`, and the output must have shape `(1,M)` or `(M)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "An observation $x_o$ for which you want to infer the posterior $p(\\theta|x_o)$.\n",
    "\n",
    "Allowed data types and shapes:\n",
    "- either a `np.ndarray` or a `torch.Tensor`.\n",
    "- shape must be either `(1,N)` or `(N)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running different algorithms\n",
    "\n",
    "*sbi* implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNL, and SRE. You can try the different algorithms by simply swapping out the `method`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = infer('SNPE', simulator, prior, num_simulations=1000)\n",
    "posterior = infer('SNLE', simulator, prior, num_simulations=1000)\n",
    "posterior = infer('SNRE', simulator, prior, num_simulations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then infer, sample, evaluate, and plot the posterior as described above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "sbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
