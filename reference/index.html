
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://sbi-dev.github.io/sbi/reference/">
      
      
        <link rel="prev" href="../code_of_conduct/">
      
      
        <link rel="next" href="../faq/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.1.9">
    
    
      
        <title>API Reference - sbi</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.85bb2934.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.a6bdf11c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../static/global.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#api-reference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="sbi" class="md-header__button md-logo" aria-label="sbi" data-md-component="logo">
      
  <img src="../static/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            sbi
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API Reference
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="http://github.com/sbi-dev/sbi" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    sbi-dev/sbi
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="sbi" class="md-nav__button md-logo" aria-label="sbi" data-md-component="logo">
      
  <img src="../static/logo.svg" alt="logo">

    </a>
    sbi
  </label>
  
    <div class="md-nav__source">
      <a href="http://github.com/sbi-dev/sbi" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    sbi-dev/sbi
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../install/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Tutorials and Examples
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Tutorials and Examples
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          Introduction
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          Introduction
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/00_getting_started/" class="md-nav__link">
        Getting started
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/02_flexible_interface/" class="md-nav__link">
        Flexible interface
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/01_gaussian_amortized/" class="md-nav__link">
        Amortized inference
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/16_implemented_methods/" class="md-nav__link">
        Implemented algorithms
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          Advanced
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          Advanced
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/03_multiround_inference/" class="md-nav__link">
        Multi-round inference
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/11_sampler_interface/" class="md-nav__link">
        Sampling algorithms in sbi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/04_density_estimators/" class="md-nav__link">
        Custom density estimators
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/05_embedding_net/" class="md-nav__link">
        Learning summary statistics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/14_iid_data_and_permutation_invariant_embeddings/" class="md-nav__link">
        SBI with trial-based data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/08_restriction_estimator/" class="md-nav__link">
        Handling invalid simulations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/10_crafting_summary_statistics/" class="md-nav__link">
        Crafting summary statistics
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          Diagnostics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          Diagnostics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/12_diagnostics_posterior_predictive_check/" class="md-nav__link">
        Posterior predictive checks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/13_diagnostics_simulation_based_calibration/" class="md-nav__link">
        Simulation-based calibration
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/15_mcmc_diagnostics_with_arviz/" class="md-nav__link">
        Density plots and MCMC diagnostics with ArviZ
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          Analysis
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          Analysis
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/07_conditional_distributions/" class="md-nav__link">
        Conditional distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/09_sensitivity_analysis/" class="md-nav__link">
        Posterior sensitivity analysis
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          Examples
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          Examples
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/00_HH_simulator/" class="md-nav__link">
        Hodgkin-Huxley example
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/01_decision_making_model/" class="md-nav__link">
        Decision making model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          Contributing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Contributing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../contribute/" class="md-nav__link">
        Guide
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../code_of_conduct/" class="md-nav__link">
        Code of Conduct
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          API Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        API Reference
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    Inference
  </a>
  
    <nav class="md-nav" aria-label="Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.base.infer" class="md-nav__link">
    infer()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.user_input_checks.prepare_for_sbi" class="md-nav__link">
    prepare_for_sbi()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.base.simulate_for_sbi" class="md-nav__link">
    simulate_for_sbi()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snpe.snpe_a.SNPE_A" class="md-nav__link">
    SNPE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snpe.snpe_c.SNPE_C" class="md-nav__link">
    SNPE_C
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snle.snle_a.SNLE_A" class="md-nav__link">
    SNLE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_a.SNRE_A" class="md-nav__link">
    SNRE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_b.SNRE_B" class="md-nav__link">
    SNRE_B
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_c.SNRE_C" class="md-nav__link">
    SNRE_C
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.bnre.BNRE" class="md-nav__link">
    BNRE
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.mcabc.MCABC" class="md-nav__link">
    MCABC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC" class="md-nav__link">
    SMCABC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#posteriors" class="md-nav__link">
    Posteriors
  </a>
  
    <nav class="md-nav" aria-label="Posteriors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" class="md-nav__link">
    DirectPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior" class="md-nav__link">
    ImportanceSamplingPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="md-nav__link">
    MCMCPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="md-nav__link">
    RejectionPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior" class="md-nav__link">
    VIPosterior
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models" class="md-nav__link">
    Models
  </a>
  
    <nav class="md-nav" aria-label="Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.posterior_nn" class="md-nav__link">
    posterior_nn()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.likelihood_nn" class="md-nav__link">
    likelihood_nn()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.classifier_nn" class="md-nav__link">
    classifier_nn()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#potentials" class="md-nav__link">
    Potentials
  </a>
  
    <nav class="md-nav" aria-label="Potentials">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential" class="md-nav__link">
    posterior_estimator_based_potential()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential" class="md-nav__link">
    likelihood_estimator_based_potential()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential" class="md-nav__link">
    ratio_estimator_based_potential()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#analysis" class="md-nav__link">
    Analysis
  </a>
  
    <nav class="md-nav" aria-label="Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.pairplot" class="md-nav__link">
    pairplot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.marginal_plot" class="md-nav__link">
    marginal_plot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.conditional_pairplot" class="md-nav__link">
    conditional_pairplot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.conditional_density.conditional_corrcoeff" class="md-nav__link">
    conditional_corrcoeff()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../faq/" class="md-nav__link">
        FAQ
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../credits/" class="md-nav__link">
        Credits
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    Inference
  </a>
  
    <nav class="md-nav" aria-label="Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.base.infer" class="md-nav__link">
    infer()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.user_input_checks.prepare_for_sbi" class="md-nav__link">
    prepare_for_sbi()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.base.simulate_for_sbi" class="md-nav__link">
    simulate_for_sbi()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snpe.snpe_a.SNPE_A" class="md-nav__link">
    SNPE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snpe.snpe_c.SNPE_C" class="md-nav__link">
    SNPE_C
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snle.snle_a.SNLE_A" class="md-nav__link">
    SNLE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_a.SNRE_A" class="md-nav__link">
    SNRE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_b.SNRE_B" class="md-nav__link">
    SNRE_B
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_c.SNRE_C" class="md-nav__link">
    SNRE_C
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.bnre.BNRE" class="md-nav__link">
    BNRE
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.mcabc.MCABC" class="md-nav__link">
    MCABC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC" class="md-nav__link">
    SMCABC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#posteriors" class="md-nav__link">
    Posteriors
  </a>
  
    <nav class="md-nav" aria-label="Posteriors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" class="md-nav__link">
    DirectPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior" class="md-nav__link">
    ImportanceSamplingPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="md-nav__link">
    MCMCPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="md-nav__link">
    RejectionPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior" class="md-nav__link">
    VIPosterior
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models" class="md-nav__link">
    Models
  </a>
  
    <nav class="md-nav" aria-label="Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.posterior_nn" class="md-nav__link">
    posterior_nn()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.likelihood_nn" class="md-nav__link">
    likelihood_nn()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.classifier_nn" class="md-nav__link">
    classifier_nn()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#potentials" class="md-nav__link">
    Potentials
  </a>
  
    <nav class="md-nav" aria-label="Potentials">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential" class="md-nav__link">
    posterior_estimator_based_potential()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential" class="md-nav__link">
    likelihood_estimator_based_potential()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential" class="md-nav__link">
    ratio_estimator_based_potential()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#analysis" class="md-nav__link">
    Analysis
  </a>
  
    <nav class="md-nav" aria-label="Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.pairplot" class="md-nav__link">
    pairplot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.marginal_plot" class="md-nav__link">
    marginal_plot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.conditional_pairplot" class="md-nav__link">
    conditional_pairplot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.conditional_density.conditional_corrcoeff" class="md-nav__link">
    conditional_corrcoeff()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="api-reference">API Reference<a class="headerlink" href="#api-reference" title="Permanent link">&para;</a></h1>
<h2 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-function">



<h3 id="sbi.inference.base.infer" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#sbi.inference.base.infer" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Runs simulation-based inference and returns the posterior.</p>
<p>This function provides a simple interface to run sbi. Inference is run for a single
round and hence the returned posterior <span class="arithmatex">\(p(\theta|x)\)</span> can be sampled and evaluated
for any <span class="arithmatex">\(x\)</span> (i.e. it is amortized).</p>
<p>The scope of this function is limited to the most essential features of sbi. For
more flexibility (e.g. multi-round inference, different density estimators) please
use the flexible interface described here:
<a href="https://www.mackelab.org/sbi/tutorial/02_flexible_interface/">https://www.mackelab.org/sbi/tutorial/02_flexible_interface/</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>simulator</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\mathrm{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="torch.distributions.Distribution">Distribution</span></code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>method</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>What inference method to use. Either of SNPE, SNLE or SNRE.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_simulations</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of simulation calls. More simulations means a longer
runtime, but a better posterior estimate.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_workers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of parallel workers to use for simulations.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/base.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">infer</span><span class="p">(</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Runs simulation-based inference and returns the posterior.</span>

<span class="sd">    This function provides a simple interface to run sbi. Inference is run for a single</span>
<span class="sd">    round and hence the returned posterior $p(\theta|x)$ can be sampled and evaluated</span>
<span class="sd">    for any $x$ (i.e. it is amortized).</span>

<span class="sd">    The scope of this function is limited to the most essential features of sbi. For</span>
<span class="sd">    more flexibility (e.g. multi-round inference, different density estimators) please</span>
<span class="sd">    use the flexible interface described here:</span>
<span class="sd">    https://www.mackelab.org/sbi/tutorial/02_flexible_interface/</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        method: What inference method to use. Either of SNPE, SNLE or SNRE.</span>
<span class="sd">        num_simulations: Number of simulation calls. More simulations means a longer</span>
<span class="sd">            runtime, but a better posterior estimate.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>

<span class="sd">    Returns: Posterior over parameters conditional on observations (amortized).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">method_fun</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="p">,</span> <span class="n">method</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span>
            <span class="s2">&quot;Method not available. `method` must be one of &#39;SNPE&#39;, &#39;SNLE&#39;, &#39;SNRE&#39;.&quot;</span>
        <span class="p">)</span>

    <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span> <span class="o">=</span> <span class="n">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>

    <span class="n">inference</span> <span class="o">=</span> <span class="n">method_fun</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">)</span>
    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">simulate_for_sbi</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">num_simulations</span><span class="o">=</span><span class="n">num_simulations</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">inference</span><span class="o">.</span><span class="n">append_simulations</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">inference</span><span class="o">.</span><span class="n">build_posterior</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">posterior</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.utils.user_input_checks.prepare_for_sbi" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">user_input_checks</span><span class="o">.</span><span class="n">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span></code>

<a href="#sbi.utils.user_input_checks.prepare_for_sbi" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Prepare simulator and prior for usage in sbi.</p>
<p>NOTE: This is a wrapper around <code>process_prior</code> and <code>process_simulator</code> which can be
used in isolation as well.</p>
<p>Attempts to meet the following requirements by reshaping and type-casting:</p>
<ul>
<li>the simulator function receives as input and returns a Tensor.<br/></li>
<li>the simulator can simulate batches of parameters and return batches of data.<br/></li>
<li>the prior does not produce batches and samples and evaluates to Tensor.<br/></li>
<li>the output shape is a <code>torch.Size((1,N))</code> (i.e, has a leading batch dimension 1).</li>
</ul>
<p>If this is not possible, a suitable exception will be raised.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>simulator</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>Simulator as provided by the user.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
          </td>
          <td><p>Prior as provided by the user.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Tuple">Tuple</span>[<span title="typing.Callable">Callable</span>, <span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>Tuple (simulator, prior) checked and matching the requirements of sbi.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/utils/user_input_checks.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prepare simulator and prior for usage in sbi.</span>

<span class="sd">    NOTE: This is a wrapper around `process_prior` and `process_simulator` which can be</span>
<span class="sd">    used in isolation as well.</span>

<span class="sd">    Attempts to meet the following requirements by reshaping and type-casting:</span>

<span class="sd">    - the simulator function receives as input and returns a Tensor.&lt;br/&gt;</span>
<span class="sd">    - the simulator can simulate batches of parameters and return batches of data.&lt;br/&gt;</span>
<span class="sd">    - the prior does not produce batches and samples and evaluates to Tensor.&lt;br/&gt;</span>
<span class="sd">    - the output shape is a `torch.Size((1,N))` (i.e, has a leading batch dimension 1).</span>

<span class="sd">    If this is not possible, a suitable exception will be raised.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: Simulator as provided by the user.</span>
<span class="sd">        prior: Prior as provided by the user.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple (simulator, prior) checked and matching the requirements of sbi.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Check prior, return PyTorch prior.</span>
    <span class="n">prior</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">prior_returns_numpy</span> <span class="o">=</span> <span class="n">process_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="c1"># Check simulator, returns PyTorch simulator able to simulate batches.</span>
    <span class="n">simulator</span> <span class="o">=</span> <span class="n">process_simulator</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">prior_returns_numpy</span><span class="p">)</span>

    <span class="c1"># Consistency check after making ready for sbi.</span>
    <span class="n">check_sbi_inputs</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.inference.base.simulate_for_sbi" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">simulate_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.base.simulate_for_sbi" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Returns (<span class="arithmatex">\(\theta, x\)</span>) pairs obtained from sampling the proposal and simulating.</p>
<p>This function performs two steps:</p>
<ul>
<li>Sample parameters <span class="arithmatex">\(\theta\)</span> from the <code>proposal</code>.</li>
<li>Simulate these parameters to obtain <span class="arithmatex">\(x\)</span>.</li>
</ul>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>simulator</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\text{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>proposal</code></td>
          <td>
                <code><span title="typing.Any">Any</span></code>
          </td>
          <td><p>Probability distribution that the parameters <span class="arithmatex">\(\theta\)</span> are sampled
from.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_simulations</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of simulations that are run.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_workers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of parallel workers to use for simulations.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>simulation_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>seed</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[int]</code>
          </td>
          <td><p>Seed for reproducibility.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bar</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progress bar for simulating. This will not
affect whether there will be a progressbar while drawing samples from the
proposal.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/base.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">simulate_for_sbi</span><span class="p">(</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns ($\theta, x$) pairs obtained from sampling the proposal and simulating.</span>

<span class="sd">    This function performs two steps:</span>

<span class="sd">    - Sample parameters $\theta$ from the `proposal`.</span>
<span class="sd">    - Simulate these parameters to obtain $x$.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\text{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        proposal: Probability distribution that the parameters $\theta$ are sampled</span>
<span class="sd">            from.</span>
<span class="sd">        num_simulations: Number of simulations that are run.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        seed: Seed for reproducibility.</span>
<span class="sd">        show_progress_bar: Whether to show a progress bar for simulating. This will not</span>
<span class="sd">            affect whether there will be a progressbar while drawing samples from the</span>
<span class="sd">            proposal.</span>

<span class="sd">    Returns: Sampled parameters $\theta$ and simulation-outputs $x$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">proposal</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">simulate_in_batches</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span>
        <span class="n">sim_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bar</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.snpe.snpe_a.SNPE_A" class="doc doc-heading">
        <code>sbi.inference.snpe.snpe_a.SNPE_A</code>


<a href="#sbi.inference.snpe.snpe_a.SNPE_A" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.snpe.snpe_base.PosteriorEstimator">PosteriorEstimator</span></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SNPE_A</span><span class="p">(</span><span class="n">PosteriorEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">,</span>
        <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SNPE-A [1].</span>

<span class="sd">        [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">            Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">            https://arxiv.org/abs/1605.06376.</span>

<span class="sd">        This class implements SNPE-A. SNPE-A trains across multiple rounds with a</span>
<span class="sd">        maximum-likelihood-loss. This will make training converge to the proposal</span>
<span class="sd">        posterior instead of the true posterior. To correct for this, SNPE-A applies a</span>
<span class="sd">        post-hoc correction after training. This correction has to be performed</span>
<span class="sd">        analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the</span>
<span class="sd">        last round. In the last round, SNPE-A can use a Mixture of Gaussians.</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            density_estimator: If it is a string (only &quot;mdn_snpe_a&quot; is valid), use a</span>
<span class="sd">                pre-configured mixture of densities network. Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`. Note that until the last round only a</span>
<span class="sd">                single (multivariate) Gaussian component is used for training (see</span>
<span class="sd">                Algorithm 1 in [1]). In the last round, this component is replicated</span>
<span class="sd">                `num_components` times, its parameters are perturbed with a very small</span>
<span class="sd">                noise, and then the last training round is done with the expanded</span>
<span class="sd">                Gaussian mixture as estimator for the proposal posterior.</span>
<span class="sd">            num_components: Number of components of the mixture of Gaussians in the</span>
<span class="sd">                last round. This overrides the `num_components` value passed to</span>
<span class="sd">                `posterior_nn()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Catch invalid inputs.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">((</span><span class="n">density_estimator</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">callable</span><span class="p">(</span><span class="n">density_estimator</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;The `density_estimator` passed to SNPE_A needs to be a &quot;</span>
                <span class="s2">&quot;callable or the string &#39;mdn_snpe_a&#39;!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># `num_components` will be used to replicate the Gaussian in the last round.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="n">num_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
        <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
        <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
        <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span>
            <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_components&quot;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">final_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">component_perturbation</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-3</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the proposal posterior.</span>

<span class="sd">        [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">            Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">            https://arxiv.org/abs/1605.06376.</span>

<span class="sd">        Training is performed with maximum likelihood on samples from the latest round,</span>
<span class="sd">        which leads the algorithm to converge to the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            final_round: Whether we are in the last round of training or not. For all</span>
<span class="sd">                but the last round, Algorithm 1 from [1] is executed. In last the</span>
<span class="sd">                round, Algorithm 2 from [1] is executed once.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">                simulations `x`. See Lueckmann, Gonalves et al., NeurIPS 2017.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">                i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">                distribution different from the prior.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round. Not supported for</span>
<span class="sd">                SNPE-A.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">            component_perturbation: The standard deviation applied to all weights and</span>
<span class="sd">                biases when, in the last round, the Mixture of Gaussians is build from</span>
<span class="sd">                a single Gaussian. This value can be problem-specific and also depends</span>
<span class="sd">                on the number of mixture components.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="n">retrain_from_scratch</span><span class="p">,</span> <span class="s2">&quot;&quot;&quot;Retraining from scratch is not supported in</span>
<span class="s2">            SNPE-A yet. The reason for this is that, if we reininitialized the density</span>
<span class="s2">            estimator, the z-scoring would change, which would break the posthoc</span>
<span class="s2">            correction. This is a pure implementation issue.&quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span>
            <span class="n">entries</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;self&quot;</span><span class="p">,</span>
                <span class="s2">&quot;__class__&quot;</span><span class="p">,</span>
                <span class="s2">&quot;final_round&quot;</span><span class="p">,</span>
                <span class="s2">&quot;component_perturbation&quot;</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># SNPE-A always discards the prior samples.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;discard_prior_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;force_first_round_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
            <span class="c1"># If there is (will be) only one round, train with Algorithm 2 from [1].</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
                <span class="p">)</span>
            <span class="c1"># Run Algorithm 2 from [1].</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span><span class="p">:</span>
                <span class="c1"># Now switch to the specified number of components. This method will</span>
                <span class="c1"># only be used if `retrain_from_scratch=True`. Otherwise,</span>
                <span class="c1"># the MDN will be built from replicating the single-component net for</span>
                <span class="c1"># `num_component` times (via `_expand_mog()`).</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
                <span class="p">)</span>

                <span class="c1"># Extend the MDN to the originally desired number of components.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_expand_mog</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">component_perturbation</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;You have already run SNPE-A with `final_round=True`. Running it&quot;</span>
                    <span class="s2">&quot;again with this setting will not allow computing the posthoc&quot;</span>
                    <span class="s2">&quot;correction applied in SNPE-A. Thus, you will get an error when &quot;</span>
                    <span class="s2">&quot;calling `.build_posterior()` after training.&quot;</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Run Algorithm 1 from [1].</span>
            <span class="c1"># Wrap the function that builds the MDN such that we can make</span>
            <span class="c1"># sure that there is only one component when running.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">correct_for_proposal</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SNPE_A_MDN&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build mixture of Gaussians that approximates the posterior.</span>

<span class="sd">        Returns a `SNPE_A_MDN` object, which applies the posthoc-correction required in</span>
<span class="sd">        SNPE-A.</span>

<span class="sd">        Args:</span>
<span class="sd">            density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">                If `None`, use the latest neural density estimator that was trained.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">density_estimator</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
            <span class="p">)</span>  <span class="c1"># PosteriorEstimator.train() also returns a deepcopy, mimic this here</span>
            <span class="c1"># If internal net is used device is defined.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Set proposal of the density estimator.</span>
        <span class="c1"># This also evokes the z-scoring correction if necessary.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">proposal</span><span class="p">,</span> <span class="p">(</span><span class="n">MultivariateNormal</span><span class="p">,</span> <span class="n">utils</span><span class="o">.</span><span class="n">BoxUniform</span><span class="p">)</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Prior must be `torch.distributions.MultivariateNormal` or `sbi.utils.</span>
<span class="s2">                BoxUniform`&quot;&quot;&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">DirectPosterior</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;The proposal you passed to `append_simulations` is neither the prior</span>
<span class="s2">                nor a `DirectPosterior`. SNPE-A currently only supports these scenarios.</span>
<span class="s2">                &quot;&quot;&quot;</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Create the SNPE_A_MDN</span>
        <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="n">SNPE_A_MDN</span><span class="p">(</span>
            <span class="n">flow</span><span class="o">=</span><span class="n">density_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapped_density_estimator</span>

    <span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DirectPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">        This method first corrects the estimated density with `correct_for_proposal`</span>
<span class="sd">        and then returns a `DirectPosterior`.</span>

<span class="sd">        Args:</span>
<span class="sd">            density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">                If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">            prior: Prior distribution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">                initialization `inference = SNPE_A(prior)` or to `.build_posterior</span>
<span class="s2">                (prior=prior)`.&quot;&quot;&quot;</span>
            <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

        <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">correct_for_proposal</span><span class="p">(</span>
            <span class="n">density_estimator</span><span class="o">=</span><span class="n">density_estimator</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">DirectPosterior</span><span class="p">(</span>
            <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">wrapped_density_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the log-probability of the proposal posterior.</span>

<span class="sd">        For SNPE-A this is the same as `self._neural_net.log_prob(theta, x)` in</span>
<span class="sd">        `_loss()` to be found in `snpe_base.py`.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters .</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns: Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_expand_mog</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replicate a singe Gaussian trained with Algorithm 1 before continuing</span>
<span class="sd">        with Algorithm 2. The weights and biases of the associated MDN layers</span>
<span class="sd">        are repeated `num_components` times, slightly perturbed to break the</span>
<span class="sd">        symmetry such that the gradients in the subsequent training are not</span>
<span class="sd">        all identical.</span>

<span class="sd">        Args:</span>
<span class="sd">            eps: Standard deviation for the random perturbation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">MultivariateGaussianMDN</span><span class="p">)</span>

        <span class="c1"># Increase the number of components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>

        <span class="c1"># Expand the 1-dim Gaussian.</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                <span class="n">key</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="s2">&quot;means&quot;</span><span class="p">,</span> <span class="s2">&quot;unconstrained&quot;</span><span class="p">,</span> <span class="s2">&quot;upper&quot;</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># let autograd construct a new gradient</span>
                <span class="k">elif</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># let autograd construct a new gradient</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snpe.snpe_a.SNPE_A.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;mdn_snpe_a&#39;</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.snpe.snpe_a.SNPE_A.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>SNPE-A [1].</p>
<p>[1] <em>Fast epsilon-free Inference of Simulation Models with Bayesian Conditional
    Density Estimation</em>, Papamakarios et al., NeurIPS 2016,
    <a href="https://arxiv.org/abs/1605.06376">https://arxiv.org/abs/1605.06376</a>.</p>
<p>This class implements SNPE-A. SNPE-A trains across multiple rounds with a
maximum-likelihood-loss. This will make training converge to the proposal
posterior instead of the true posterior. To correct for this, SNPE-A applies a
post-hoc correction after training. This correction has to be performed
analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the
last round. In the last round, SNPE-A can use a Mixture of Gaussians.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>density_estimator</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>If it is a string (only &ldquo;mdn_snpe_a&rdquo; is valid), use a
pre-configured mixture of densities network. Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>. Note that until the last round only a
single (multivariate) Gaussian component is used for training (see
Algorithm 1 in [1]). In the last round, this component is replicated
<code>num_components</code> times, its parameters are perturbed with a very small
noise, and then the last training round is done with the expanded
Gaussian mixture as estimator for the proposal posterior.</p></td>
          <td>
                <code>&#39;mdn_snpe_a&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>num_components</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of components of the mixture of Gaussians in the
last round. This overrides the <code>num_components</code> value passed to
<code>posterior_nn()</code>.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
          <td>
                <code>&#39;cpu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>logging_level</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, str]</code>
          </td>
          <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
          <td>
                <code>&#39;WARNING&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>summary_writer</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
          </td>
          <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during training.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snpe/snpe_a.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">,</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SNPE-A [1].</span>

<span class="sd">    [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">        Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">        https://arxiv.org/abs/1605.06376.</span>

<span class="sd">    This class implements SNPE-A. SNPE-A trains across multiple rounds with a</span>
<span class="sd">    maximum-likelihood-loss. This will make training converge to the proposal</span>
<span class="sd">    posterior instead of the true posterior. To correct for this, SNPE-A applies a</span>
<span class="sd">    post-hoc correction after training. This correction has to be performed</span>
<span class="sd">    analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the</span>
<span class="sd">    last round. In the last round, SNPE-A can use a Mixture of Gaussians.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        density_estimator: If it is a string (only &quot;mdn_snpe_a&quot; is valid), use a</span>
<span class="sd">            pre-configured mixture of densities network. Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`. Note that until the last round only a</span>
<span class="sd">            single (multivariate) Gaussian component is used for training (see</span>
<span class="sd">            Algorithm 1 in [1]). In the last round, this component is replicated</span>
<span class="sd">            `num_components` times, its parameters are perturbed with a very small</span>
<span class="sd">            noise, and then the last training round is done with the expanded</span>
<span class="sd">            Gaussian mixture as estimator for the proposal posterior.</span>
<span class="sd">        num_components: Number of components of the mixture of Gaussians in the</span>
<span class="sd">            last round. This overrides the `num_components` value passed to</span>
<span class="sd">            `posterior_nn()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Catch invalid inputs.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">((</span><span class="n">density_estimator</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">callable</span><span class="p">(</span><span class="n">density_estimator</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;The `density_estimator` passed to SNPE_A needs to be a &quot;</span>
            <span class="s2">&quot;callable or the string &#39;mdn_snpe_a&#39;!&quot;</span>
        <span class="p">)</span>

    <span class="c1"># `num_components` will be used to replicate the Gaussian in the last round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="n">num_components</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
    <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
    <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
    <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span>
        <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_components&quot;</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snpe.snpe_a.SNPE_A.build_posterior" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.snpe.snpe_a.SNPE_A.build_posterior" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Build posterior from the neural density estimator.</p>
<p>This method first corrects the estimated density with <code>correct_for_proposal</code>
and then returns a <code>DirectPosterior</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>density_estimator</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchModule">TorchModule</span>]</code>
          </td>
          <td><p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>Prior distribution.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sbi.inference.posteriors.direct_posterior.DirectPosterior" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior">DirectPosterior</a></code>
          </td>
          <td><p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snpe/snpe_a.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DirectPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">    This method first corrects the estimated density with `correct_for_proposal`</span>
<span class="sd">    and then returns a `DirectPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">        prior: Prior distribution.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">            initialization `inference = SNPE_A(prior)` or to `.build_posterior</span>
<span class="s2">            (prior=prior)`.&quot;&quot;&quot;</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

    <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">correct_for_proposal</span><span class="p">(</span>
        <span class="n">density_estimator</span><span class="o">=</span><span class="n">density_estimator</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">DirectPosterior</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">wrapped_density_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snpe.snpe_a.SNPE_A.correct_for_proposal" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">correct_for_proposal</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.snpe.snpe_a.SNPE_A.correct_for_proposal" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Build mixture of Gaussians that approximates the posterior.</p>
<p>Returns a <code>SNPE_A_MDN</code> object, which applies the posthoc-correction required in
SNPE-A.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>density_estimator</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchModule">TorchModule</span>]</code>
          </td>
          <td><p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="sbi.inference.snpe.snpe_a.SNPE_A_MDN">SNPE_A_MDN</span></code>
          </td>
          <td><p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snpe/snpe_a.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">correct_for_proposal</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SNPE_A_MDN&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build mixture of Gaussians that approximates the posterior.</span>

<span class="sd">    Returns a `SNPE_A_MDN` object, which applies the posthoc-correction required in</span>
<span class="sd">    SNPE-A.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">density_estimator</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="p">)</span>  <span class="c1"># PosteriorEstimator.train() also returns a deepcopy, mimic this here</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Set proposal of the density estimator.</span>
    <span class="c1"># This also evokes the z-scoring correction if necessary.</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">proposal</span><span class="p">,</span> <span class="p">(</span><span class="n">MultivariateNormal</span><span class="p">,</span> <span class="n">utils</span><span class="o">.</span><span class="n">BoxUniform</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Prior must be `torch.distributions.MultivariateNormal` or `sbi.utils.</span>
<span class="s2">            BoxUniform`&quot;&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">DirectPosterior</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;The proposal you passed to `append_simulations` is neither the prior</span>
<span class="s2">            nor a `DirectPosterior`. SNPE-A currently only supports these scenarios.</span>
<span class="s2">            &quot;&quot;&quot;</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Create the SNPE_A_MDN</span>
    <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="n">SNPE_A_MDN</span><span class="p">(</span>
        <span class="n">flow</span><span class="o">=</span><span class="n">density_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">proposal</span><span class="o">=</span><span class="n">proposal</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">wrapped_density_estimator</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snpe.snpe_a.SNPE_A.train" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="n">final_round</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">component_perturbation</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span></code>

<a href="#sbi.inference.snpe.snpe_a.SNPE_A.train" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return density estimator that approximates the proposal posterior.</p>
<p>[1] <em>Fast epsilon-free Inference of Simulation Models with Bayesian Conditional
    Density Estimation</em>, Papamakarios et al., NeurIPS 2016,
    <a href="https://arxiv.org/abs/1605.06376">https://arxiv.org/abs/1605.06376</a>.</p>
<p>Training is performed with maximum likelihood on samples from the latest round,
which leads the algorithm to converge to the proposal posterior.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>final_round</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether we are in the last round of training or not. For all
but the last round, Algorithm 1 from [1] is executed. In last the
round, Algorithm 2 from [1] is executed once.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>training_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Training batch size.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate for Adam optimizer.</p></td>
          <td>
                <code>0.0005</code>
          </td>
        </tr>
        <tr>
          <td><code>validation_fraction</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The fraction of data to use for validation.</p></td>
          <td>
                <code>0.1</code>
          </td>
        </tr>
        <tr>
          <td><code>stop_after_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
          <td>
                <code>20</code>
          </td>
        </tr>
        <tr>
          <td><code>max_num_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
          <td>
                <code>2 ** 31 - 1</code>
          </td>
        </tr>
        <tr>
          <td><code>clip_max_norm</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
          <td>
                <code>5.0</code>
          </td>
        </tr>
        <tr>
          <td><code>calibration_kernel</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>A function to calibrate the loss with respect to the
simulations <code>x</code>. See Lueckmann, Gonalves et al., NeurIPS 2017.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>resume_training</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>force_first_round_loss</code></td>
          <td>
          </td>
          <td><p>If <code>True</code>, train with maximum likelihood,
i.e., potentially ignoring the correction for using a proposal
distribution different from the prior.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>retrain_from_scratch</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round. Not supported for
SNPE-A.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>show_train_summary</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>dataloader_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>component_perturbation</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The standard deviation applied to all weights and
biases when, in the last round, the Mixture of Gaussians is build from
a single Gaussian. This value can be problem-specific and also depends
on the number of mixture components.</p></td>
          <td>
                <code>0.005</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snpe/snpe_a.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">final_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">component_perturbation</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-3</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the proposal posterior.</span>

<span class="sd">    [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">        Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">        https://arxiv.org/abs/1605.06376.</span>

<span class="sd">    Training is performed with maximum likelihood on samples from the latest round,</span>
<span class="sd">    which leads the algorithm to converge to the proposal posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        final_round: Whether we are in the last round of training or not. For all</span>
<span class="sd">            but the last round, Algorithm 1 from [1] is executed. In last the</span>
<span class="sd">            round, Algorithm 2 from [1] is executed once.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">            simulations `x`. See Lueckmann, Gonalves et al., NeurIPS 2017.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">            i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">            distribution different from the prior.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round. Not supported for</span>
<span class="sd">            SNPE-A.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">        component_perturbation: The standard deviation applied to all weights and</span>
<span class="sd">            biases when, in the last round, the Mixture of Gaussians is build from</span>
<span class="sd">            a single Gaussian. This value can be problem-specific and also depends</span>
<span class="sd">            on the number of mixture components.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">retrain_from_scratch</span><span class="p">,</span> <span class="s2">&quot;&quot;&quot;Retraining from scratch is not supported in</span>
<span class="s2">        SNPE-A yet. The reason for this is that, if we reininitialized the density</span>
<span class="s2">        estimator, the z-scoring would change, which would break the posthoc</span>
<span class="s2">        correction. This is a pure implementation issue.&quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span>
        <span class="n">entries</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;self&quot;</span><span class="p">,</span>
            <span class="s2">&quot;__class__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;final_round&quot;</span><span class="p">,</span>
            <span class="s2">&quot;component_perturbation&quot;</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># SNPE-A always discards the prior samples.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;discard_prior_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;force_first_round_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
        <span class="c1"># If there is (will be) only one round, train with Algorithm 2 from [1].</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
            <span class="p">)</span>
        <span class="c1"># Run Algorithm 2 from [1].</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span><span class="p">:</span>
            <span class="c1"># Now switch to the specified number of components. This method will</span>
            <span class="c1"># only be used if `retrain_from_scratch=True`. Otherwise,</span>
            <span class="c1"># the MDN will be built from replicating the single-component net for</span>
            <span class="c1"># `num_component` times (via `_expand_mog()`).</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
            <span class="p">)</span>

            <span class="c1"># Extend the MDN to the originally desired number of components.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_expand_mog</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">component_perturbation</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You have already run SNPE-A with `final_round=True`. Running it&quot;</span>
                <span class="s2">&quot;again with this setting will not allow computing the posthoc&quot;</span>
                <span class="s2">&quot;correction applied in SNPE-A. Thus, you will get an error when &quot;</span>
                <span class="s2">&quot;calling `.build_posterior()` after training.&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Run Algorithm 1 from [1].</span>
        <span class="c1"># Wrap the function that builds the MDN such that we can make</span>
        <span class="c1"># sure that there is only one component when running.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.snpe.snpe_c.SNPE_C" class="doc doc-heading">
        <code>sbi.inference.snpe.snpe_c.SNPE_C</code>


<a href="#sbi.inference.snpe.snpe_c.SNPE_C" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.snpe.snpe_base.PosteriorEstimator">PosteriorEstimator</span></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snpe/snpe_c.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SNPE_C</span><span class="p">(</span><span class="n">PosteriorEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SNPE-C / APT [1].</span>

<span class="sd">        [1] _Automatic Posterior Transformation for Likelihood-free Inference_,</span>
<span class="sd">            Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</span>

<span class="sd">        This class implements two loss variants of SNPE-C: the non-atomic and the atomic</span>
<span class="sd">        version. The atomic loss of SNPE-C can be used for any density estimator,</span>
<span class="sd">        i.e. also for normalizing flows. However, it suffers from leakage issues. On</span>
<span class="sd">        the other hand, the non-atomic loss can only be used only if the proposal</span>
<span class="sd">        distribution is a mixture of Gaussians, the density estimator is a mixture of</span>
<span class="sd">        Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from</span>
<span class="sd">        leakage issues. At the beginning of each round, we print whether the non-atomic</span>
<span class="sd">        or the atomic version is used.</span>

<span class="sd">        In this codebase, we will automatically switch to the non-atomic loss if the</span>
<span class="sd">        following criteria are fulfilled:&lt;br/&gt;</span>
<span class="sd">        - proposal is a `DirectPosterior` with density_estimator `mdn`, as built</span>
<span class="sd">            with `utils.sbi.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">        - the density estimator is a `mdn`, as built with</span>
<span class="sd">            `utils.sbi.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">        - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or</span>
<span class="sd">            `isinstance(prior, sbi.utils.BoxUniform)`</span>

<span class="sd">        Note that custom implementations of any of these densities (or estimators) will</span>
<span class="sd">        not trigger the non-atomic loss, and the algorithm will fall back onto using</span>
<span class="sd">        the atomic loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them.</span>
<span class="sd">            density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">                provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_combined_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the distribution $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_atoms: Number of atoms to use for classification.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">                simulations `x`. See Lueckmann, Gonalves et al., NeurIPS 2017.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">                i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">                distribution different from the prior.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            use_combined_loss: Whether to train the neural net also on prior samples</span>
<span class="sd">                using maximum likelihood in addition to training it on all samples using</span>
<span class="sd">                atomic loss. The extra MLE loss helps prevent density leaking with</span>
<span class="sd">                bounded priors.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
        <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
        <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
        <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span> <span class="o">=</span> <span class="n">use_combined_loss</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span>
            <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="s2">&quot;use_combined_loss&quot;</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Set the proposal to the last proposal that was passed by the user. For</span>
            <span class="c1"># atomic SNPE, it does not matter what the proposal is. For non-atomic</span>
            <span class="c1"># SNPE, we only use the latest data that was passed, i.e. the one from the</span>
            <span class="c1"># last proposal.</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">DirectPosterior</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">check_dist_class</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">class_to_check</span><span class="o">=</span><span class="p">(</span><span class="n">Uniform</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">)</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;non-atomic&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="k">else</span> <span class="s2">&quot;atomic&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using SNPE-C with </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> loss&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
                <span class="c1"># Take care of z-scoring, pre-compute and store prior terms.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_state_for_mog_proposal</span><span class="p">()</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_state_for_mog_proposal</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set state variables that are used at each training step of non-atomic SNPE-C.</span>

<span class="sd">        Three things are computed:</span>
<span class="sd">        1) Check if z-scoring was requested. To do so, we check if the `_transform`</span>
<span class="sd">            argument of the net had been a `CompositeTransform`. See pyknos mdn.py.</span>
<span class="sd">        2) Define a (potentially standardized) prior. It&#39;s standardized if z-scoring</span>
<span class="sd">            had been requested.</span>
<span class="sd">        3) Compute (Precision * mean) for the prior. This quantity is used at every</span>
<span class="sd">            training step if the prior is Gaussian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_transform</span><span class="p">,</span> <span class="n">CompositeTransform</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_maybe_z_scored_prior</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prec_m_prod_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">precision_matrix</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_maybe_z_scored_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute and store potentially standardized prior (if z-scoring was done).</span>

<span class="sd">        The proposal posterior is:</span>
<span class="sd">        $pp(\theta|x) = 1/Z * q(\theta|x) * prop(\theta) / p(\theta)$</span>

<span class="sd">        Let&#39;s denote z-scored theta by `a`: a = (theta - mean) / std</span>
<span class="sd">        Then pp&#39;(a|x) = 1/Z_2 * q&#39;(a|x) * prop&#39;(a) / p&#39;(a)$</span>

<span class="sd">        The &#39; indicates that the evaluation occurs in standardized space. The constant</span>
<span class="sd">        scaling factor has been absorbed into Z_2.</span>
<span class="sd">        From the above equation, we see that we need to evaluate the prior **in</span>
<span class="sd">        standardized space**. We build the standardized prior in this function.</span>

<span class="sd">        The standardize transform that is applied to the samples theta does not use</span>
<span class="sd">        the exact prior mean and std (due to implementation issues). Hence, the z-scored</span>
<span class="sd">        prior will not be exactly have mean=0 and std=1.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">_transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_scale</span>
            <span class="n">shift</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">_transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_shift</span>

            <span class="c1"># Following the definintion of the linear transform in</span>
            <span class="c1"># `standardizing_transform` in `sbiutils.py`:</span>
            <span class="c1"># shift=-mean / std</span>
            <span class="c1"># scale=1 / std</span>
            <span class="c1"># Solving these equations for mean and std:</span>
            <span class="n">estim_prior_std</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">scale</span>
            <span class="n">estim_prior_mean</span> <span class="o">=</span> <span class="o">-</span><span class="n">shift</span> <span class="o">*</span> <span class="n">estim_prior_std</span>

            <span class="c1"># Compute the discrepancy of the true prior mean and std and the mean and</span>
            <span class="c1"># std that was empirically estimated from samples.</span>
            <span class="c1"># N(theta|m,s) = N((theta-m_e)/s_e|(m-m_e)/s_e, s/s_e)</span>
            <span class="c1"># Above: m,s are true prior mean and std. m_e,s_e are estimated prior mean</span>
            <span class="c1"># and std (estimated from samples and used to build standardize transform).</span>
            <span class="n">almost_zero_mean</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">mean</span> <span class="o">-</span> <span class="n">estim_prior_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">estim_prior_std</span>
            <span class="n">almost_one_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">variance</span><span class="p">)</span> <span class="o">/</span> <span class="n">estim_prior_std</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
                    <span class="n">almost_zero_mean</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">almost_one_std</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">range_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">almost_one_std</span> <span class="o">*</span> <span class="mf">3.0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">BoxUniform</span><span class="p">(</span>
                    <span class="n">almost_zero_mean</span> <span class="o">-</span> <span class="n">range_</span><span class="p">,</span> <span class="n">almost_zero_mean</span> <span class="o">+</span> <span class="n">range_</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">DirectPosterior</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the log-probability of the proposal posterior.</span>

<span class="sd">        If the proposal is a MoG, the density estimator is a MoG, and the prior is</span>
<span class="sd">        either Gaussian or uniform, we use non-atomic loss. Else, use atomic loss (which</span>
<span class="sd">        suffers from leakage).</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters .</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns: Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prob_proposal_posterior_mog</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">proposal</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prob_proposal_posterior_atomic</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior_atomic</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return log probability of the proposal posterior for atomic proposals.</span>

<span class="sd">        We have two main options when evaluating the proposal posterior.</span>
<span class="sd">            (1) Generate atoms from the proposal prior.</span>
<span class="sd">            (2) Generate atoms from a more targeted distribution, such as the most</span>
<span class="sd">                recent posterior.</span>
<span class="sd">        If we choose the latter, it is likely beneficial not to do this in the first</span>
<span class="sd">        round, since we would be sampling from a randomly-initialized neural density</span>
<span class="sd">        estimator.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters .</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">num_atoms</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">clamp_and_warn</span><span class="p">(</span><span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Each set of parameter atoms is evaluated using the same x,</span>
        <span class="c1"># so we repeat rows of the data x, e.g. [1, 2] -&gt; [1, 1, 2, 2]</span>
        <span class="n">repeated_x</span> <span class="o">=</span> <span class="n">repeat_rows</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># To generate the full set of atoms for a given item in the batch,</span>
        <span class="c1"># we sample without replacement num_atoms - 1 times from the rest</span>
        <span class="c1"># of the theta in the batch.</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eye</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">choices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_atoms</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">contrasting_theta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">choices</span><span class="p">]</span>

        <span class="c1"># We can now create our sets of atoms from the contrasting parameter sets</span>
        <span class="c1"># we have generated.</span>
        <span class="n">atomic_theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">theta</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">contrasting_theta</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_atoms</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="c1"># Evaluate large batch giving (batch_size * num_atoms) log prob posterior evals.</span>
        <span class="n">log_prob_posterior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">atomic_theta</span><span class="p">,</span> <span class="n">repeated_x</span><span class="p">)</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_posterior</span><span class="p">,</span> <span class="s2">&quot;posterior eval&quot;</span><span class="p">)</span>
        <span class="n">log_prob_posterior</span> <span class="o">=</span> <span class="n">log_prob_posterior</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># Get (batch_size * num_atoms) log prob prior evals.</span>
        <span class="n">log_prob_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">atomic_theta</span><span class="p">)</span>
        <span class="n">log_prob_prior</span> <span class="o">=</span> <span class="n">log_prob_prior</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_prior</span><span class="p">,</span> <span class="s2">&quot;prior eval&quot;</span><span class="p">)</span>

        <span class="c1"># Compute unnormalized proposal posterior.</span>
        <span class="n">unnormalized_log_prob</span> <span class="o">=</span> <span class="n">log_prob_posterior</span> <span class="o">-</span> <span class="n">log_prob_prior</span>

        <span class="c1"># Normalize proposal posterior across discrete set of atoms.</span>
        <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="n">unnormalized_log_prob</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
            <span class="n">unnormalized_log_prob</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_proposal_posterior</span><span class="p">,</span> <span class="s2">&quot;proposal posterior eval&quot;</span><span class="p">)</span>

        <span class="c1"># XXX This evaluates the posterior on _all_ prior samples</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span><span class="p">:</span>
            <span class="n">log_prob_posterior_non_atomic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">masks</span> <span class="o">*</span> <span class="n">log_prob_posterior_non_atomic</span> <span class="o">+</span> <span class="n">log_prob_proposal_posterior</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_proposal_posterior</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior_mog</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">proposal</span><span class="p">:</span> <span class="n">DirectPosterior</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return log-probability of the proposal posterior for MoG proposal.</span>

<span class="sd">        For MoG proposals and MoG density estimators, this can be done in closed form</span>
<span class="sd">        and does not require atomic loss (i.e. there will be no leakage issues).</span>

<span class="sd">        Notation:</span>

<span class="sd">        m are mean vectors.</span>
<span class="sd">        prec are precision matrices.</span>
<span class="sd">        cov are covariance matrices.</span>

<span class="sd">        _p at the end indicates that it is the proposal.</span>
<span class="sd">        _d indicates that it is the density estimator.</span>
<span class="sd">        _pp indicates the proposal posterior.</span>

<span class="sd">        All tensors will have shapes (batch_dim, num_components, ...)</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters .</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Evaluate the proposal. MDNs do not have functionality to run the embedding_net</span>
        <span class="c1"># and then get the mixture_components (**without** calling log_prob()). Hence,</span>
        <span class="c1"># we call them separately here.</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">_embedding_net</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">_distribution</span>
        <span class="p">)</span>  <span class="c1"># defined to avoid ugly black formatting.</span>
        <span class="n">logits_p</span><span class="p">,</span> <span class="n">m_p</span><span class="p">,</span> <span class="n">prec_p</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_mixture_components</span><span class="p">(</span><span class="n">encoded_x</span><span class="p">)</span>
        <span class="n">norm_logits_p</span> <span class="o">=</span> <span class="n">logits_p</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits_p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Evaluate the density estimator.</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_embedding_net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span>  <span class="c1"># defined to avoid black formatting.</span>
        <span class="n">logits_d</span><span class="p">,</span> <span class="n">m_d</span><span class="p">,</span> <span class="n">prec_d</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_mixture_components</span><span class="p">(</span><span class="n">encoded_x</span><span class="p">)</span>
        <span class="n">norm_logits_d</span> <span class="o">=</span> <span class="n">logits_d</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># z-score theta if it z-scoring had been requested.</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_score_theta</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Compute the MoG parameters of the proposal posterior.</span>
        <span class="p">(</span>
            <span class="n">logits_pp</span><span class="p">,</span>
            <span class="n">m_pp</span><span class="p">,</span>
            <span class="n">prec_pp</span><span class="p">,</span>
            <span class="n">cov_pp</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_posterior_transformation</span><span class="p">(</span>
            <span class="n">norm_logits_p</span><span class="p">,</span> <span class="n">m_p</span><span class="p">,</span> <span class="n">prec_p</span><span class="p">,</span> <span class="n">norm_logits_d</span><span class="p">,</span> <span class="n">m_d</span><span class="p">,</span> <span class="n">prec_d</span>
        <span class="p">)</span>

        <span class="c1"># Compute the log_prob of theta under the product.</span>
        <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">mog_log_prob</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">logits_pp</span><span class="p">,</span> <span class="n">m_pp</span><span class="p">,</span> <span class="n">prec_pp</span>
        <span class="p">)</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">assert_all_finite</span><span class="p">(</span>
            <span class="n">log_prob_proposal_posterior</span><span class="p">,</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;the evaluation of the MoG proposal posterior. This is likely due to a</span>
<span class="sd">            numerical instability in the training procedure. Please create an issue on</span>
<span class="sd">            Github.&quot;&quot;&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_proposal_posterior</span>

    <span class="k">def</span> <span class="nf">_automatic_posterior_transformation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">logits_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the MoG parameters of the proposal posterior.</span>

<span class="sd">        The proposal posterior is:</span>
<span class="sd">        $pp(\theta|x) = 1/Z * q(\theta|x) * prop(\theta) / p(\theta)$</span>
<span class="sd">        In words: proposal posterior = posterior estimate * proposal / prior.</span>

<span class="sd">        If the posterior estimate and the proposal are MoG and the prior is either</span>
<span class="sd">        Gaussian or uniform, we can solve this in closed-form. The is implemented in</span>
<span class="sd">        this function.</span>

<span class="sd">        This function implements Appendix A1 from Greenberg et al. 2019.</span>

<span class="sd">        We have to build L*K components. How do we do this?</span>
<span class="sd">        Example: proposal has two components, density estimator has three components.</span>
<span class="sd">        Let&#39;s call the two components of the proposal i,j and the three components</span>
<span class="sd">        of the density estimator x,y,z. We have to multiply every component of the</span>
<span class="sd">        proposal with every component of the density estimator. So, what we do is:</span>
<span class="sd">        1) for the proposal, build: i,i,i,j,j,j. Done with torch.repeat_interleave()</span>
<span class="sd">        2) for the density estimator, build: x,y,z,x,y,z. Done with torch.repeat()</span>
<span class="sd">        3) Multiply them with simple matrix operations.</span>

<span class="sd">        Args:</span>
<span class="sd">            logits_p: Component weight of each Gaussian of the proposal.</span>
<span class="sd">            means_p: Mean of each Gaussian of the proposal.</span>
<span class="sd">            precisions_p: Precision matrix of each Gaussian of the proposal.</span>
<span class="sd">            logits_d: Component weight for each Gaussian of the density estimator.</span>
<span class="sd">            means_d: Mean of each Gaussian of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrix of each Gaussian of the density estimator.</span>

<span class="sd">        Returns: (Component weight, mean, precision matrix, covariance matrix) of each</span>
<span class="sd">            Gaussian of the proposal posterior. Has L*K terms (proposal has L terms,</span>
<span class="sd">            density estimator has K terms).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_precisions_proposal_posterior</span><span class="p">(</span>
            <span class="n">precisions_p</span><span class="p">,</span> <span class="n">precisions_d</span>
        <span class="p">)</span>

        <span class="n">means_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_means_proposal_posterior</span><span class="p">(</span>
            <span class="n">covariances_pp</span><span class="p">,</span> <span class="n">means_p</span><span class="p">,</span> <span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_d</span><span class="p">,</span> <span class="n">precisions_d</span>
        <span class="p">)</span>

        <span class="n">logits_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logits_proposal_posterior</span><span class="p">(</span>
            <span class="n">means_pp</span><span class="p">,</span>
            <span class="n">precisions_pp</span><span class="p">,</span>
            <span class="n">covariances_pp</span><span class="p">,</span>
            <span class="n">logits_p</span><span class="p">,</span>
            <span class="n">means_p</span><span class="p">,</span>
            <span class="n">precisions_p</span><span class="p">,</span>
            <span class="n">logits_d</span><span class="p">,</span>
            <span class="n">means_d</span><span class="p">,</span>
            <span class="n">precisions_d</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">logits_pp</span><span class="p">,</span> <span class="n">means_pp</span><span class="p">,</span> <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span>

    <span class="k">def</span> <span class="nf">_precisions_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the precisions and covariances of the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: (Precisions, Covariances) of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">precisions_p_rep</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">precisions_d_rep</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">precisions_pp</span> <span class="o">=</span> <span class="n">precisions_p_rep</span> <span class="o">+</span> <span class="n">precisions_d_rep</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="n">precisions_pp</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">precision_matrix</span>

        <span class="n">covariances_pp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">precisions_pp</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span>

    <span class="k">def</span> <span class="nf">_means_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">covariances_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the means of the proposal posterior.</span>

<span class="sd">        means_pp = C_ix * (P_i * m_i + P_x * m_x - P_o * m_o).</span>

<span class="sd">        Args:</span>
<span class="sd">            covariances_pp: Covariance matrices of the proposal posterior.</span>
<span class="sd">            means_p: Means of the proposal distribution.</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            means_d: Means of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: Means of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># First, compute the product P_i * m_i and P_j * m_j</span>
        <span class="n">prec_m_prod_p</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_p</span><span class="p">)</span>
        <span class="n">prec_m_prod_d</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">,</span> <span class="n">means_d</span><span class="p">)</span>

        <span class="c1"># Repeat them to allow for matrix operations: same trick as for the precisions.</span>
        <span class="n">prec_m_prod_p_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">prec_m_prod_d_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Means = C_ij * (P_i * m_i + P_x * m_x - P_o * m_o).</span>
        <span class="n">summed_cov_m_prod_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_p_rep</span> <span class="o">+</span> <span class="n">prec_m_prod_d_rep</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="n">summed_cov_m_prod_rep</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prec_m_prod_prior</span>

        <span class="n">means_pp</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">covariances_pp</span><span class="p">,</span> <span class="n">summed_cov_m_prod_rep</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">means_pp</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_logits_proposal_posterior</span><span class="p">(</span>
        <span class="n">means_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">covariances_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the component weights (i.e. logits) of the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            means_pp: Means of the proposal posterior.</span>
<span class="sd">            precisions_pp: Precision matrices of the proposal posterior.</span>
<span class="sd">            covariances_pp: Covariance matrices of the proposal posterior.</span>
<span class="sd">            logits_p: Component weights (i.e. logits) of the proposal distribution.</span>
<span class="sd">            means_p: Means of the proposal distribution.</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            logits_d: Component weights (i.e. logits) of the density estimator.</span>
<span class="sd">            means_d: Means of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: Component weights of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Compute log(alpha_i * beta_j)</span>
        <span class="n">logits_p_rep</span> <span class="o">=</span> <span class="n">logits_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">logits_d_rep</span> <span class="o">=</span> <span class="n">logits_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>
        <span class="n">logit_factors</span> <span class="o">=</span> <span class="n">logits_p_rep</span> <span class="o">+</span> <span class="n">logits_d_rep</span>

        <span class="c1"># Compute sqrt(det()/(det()*det()))</span>
        <span class="n">logdet_covariances_pp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">covariances_pp</span><span class="p">)</span>
        <span class="n">logdet_covariances_p</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">)</span>
        <span class="n">logdet_covariances_d</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">)</span>

        <span class="c1"># Repeat the proposal and density estimator terms such that there are LK terms.</span>
        <span class="c1"># Same trick as has been used above.</span>
        <span class="n">logdet_covariances_p_rep</span> <span class="o">=</span> <span class="n">logdet_covariances_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
            <span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">logdet_covariances_d_rep</span> <span class="o">=</span> <span class="n">logdet_covariances_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>

        <span class="n">log_sqrt_det_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">logdet_covariances_pp</span>
            <span class="o">-</span> <span class="p">(</span><span class="n">logdet_covariances_p_rep</span> <span class="o">+</span> <span class="n">logdet_covariances_d_rep</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Compute for proposal, density estimator, and proposal posterior:</span>
        <span class="c1"># mu_i.T * P_i * mu_i</span>
        <span class="n">exponent_p</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_p</span><span class="p">)</span>
        <span class="n">exponent_d</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">,</span> <span class="n">means_d</span><span class="p">)</span>
        <span class="n">exponent_pp</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_pp</span><span class="p">,</span> <span class="n">means_pp</span><span class="p">)</span>

        <span class="c1"># Extend proposal and density estimator exponents to get LK terms.</span>
        <span class="n">exponent_p_rep</span> <span class="o">=</span> <span class="n">exponent_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">exponent_d_rep</span> <span class="o">=</span> <span class="n">exponent_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>
        <span class="n">exponent</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">exponent_p_rep</span> <span class="o">+</span> <span class="n">exponent_d_rep</span> <span class="o">-</span> <span class="n">exponent_pp</span><span class="p">)</span>

        <span class="n">logits_pp</span> <span class="o">=</span> <span class="n">logit_factors</span> <span class="o">+</span> <span class="n">log_sqrt_det_ratio</span> <span class="o">+</span> <span class="n">exponent</span>

        <span class="k">return</span> <span class="n">logits_pp</span>

    <span class="k">def</span> <span class="nf">_maybe_z_score_theta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return potentially standardized theta if z-scoring was requested.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span><span class="p">:</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">theta</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snpe.snpe_c.SNPE_C.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.snpe.snpe_c.SNPE_C.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>SNPE-C / APT [1].</p>
<p>[1] <em>Automatic Posterior Transformation for Likelihood-free Inference</em>,
    Greenberg et al., ICML 2019, <a href="https://arxiv.org/abs/1905.07488">https://arxiv.org/abs/1905.07488</a>.</p>
<p>This class implements two loss variants of SNPE-C: the non-atomic and the atomic
version. The atomic loss of SNPE-C can be used for any density estimator,
i.e. also for normalizing flows. However, it suffers from leakage issues. On
the other hand, the non-atomic loss can only be used only if the proposal
distribution is a mixture of Gaussians, the density estimator is a mixture of
Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from
leakage issues. At the beginning of each round, we print whether the non-atomic
or the atomic version is used.</p>
<p>In this codebase, we will automatically switch to the non-atomic loss if the
following criteria are fulfilled:<br/>
- proposal is a <code>DirectPosterior</code> with density_estimator <code>mdn</code>, as built
    with <code>utils.sbi.posterior_nn()</code>.<br/>
- the density estimator is a <code>mdn</code>, as built with
    <code>utils.sbi.posterior_nn()</code>.<br/>
- <code>isinstance(prior, MultivariateNormal)</code> (from <code>torch.distributions</code>) or
    <code>isinstance(prior, sbi.utils.BoxUniform)</code></p>
<p>Note that custom implementations of any of these densities (or estimators) will
not trigger the non-atomic loss, and the algorithm will fall back onto using
the atomic loss.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>density_estimator</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>If it is a string, use a pre-configured network of the
provided type (one of nsf, maf, mdn, made). Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>.</p></td>
          <td>
                <code>&#39;maf&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
          <td>
                <code>&#39;cpu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>logging_level</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, str]</code>
          </td>
          <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
          <td>
                <code>&#39;WARNING&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>summary_writer</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
          </td>
          <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during training.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snpe/snpe_c.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SNPE-C / APT [1].</span>

<span class="sd">    [1] _Automatic Posterior Transformation for Likelihood-free Inference_,</span>
<span class="sd">        Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</span>

<span class="sd">    This class implements two loss variants of SNPE-C: the non-atomic and the atomic</span>
<span class="sd">    version. The atomic loss of SNPE-C can be used for any density estimator,</span>
<span class="sd">    i.e. also for normalizing flows. However, it suffers from leakage issues. On</span>
<span class="sd">    the other hand, the non-atomic loss can only be used only if the proposal</span>
<span class="sd">    distribution is a mixture of Gaussians, the density estimator is a mixture of</span>
<span class="sd">    Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from</span>
<span class="sd">    leakage issues. At the beginning of each round, we print whether the non-atomic</span>
<span class="sd">    or the atomic version is used.</span>

<span class="sd">    In this codebase, we will automatically switch to the non-atomic loss if the</span>
<span class="sd">    following criteria are fulfilled:&lt;br/&gt;</span>
<span class="sd">    - proposal is a `DirectPosterior` with density_estimator `mdn`, as built</span>
<span class="sd">        with `utils.sbi.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">    - the density estimator is a `mdn`, as built with</span>
<span class="sd">        `utils.sbi.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">    - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or</span>
<span class="sd">        `isinstance(prior, sbi.utils.BoxUniform)`</span>

<span class="sd">    Note that custom implementations of any of these densities (or estimators) will</span>
<span class="sd">    not trigger the non-atomic loss, and the algorithm will fall back onto using</span>
<span class="sd">    the atomic loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them.</span>
<span class="sd">        density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">            provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snpe.snpe_c.SNPE_C.train" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_first_round_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_combined_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.snpe.snpe_c.SNPE_C.train" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>num_atoms</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of atoms to use for classification.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>training_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Training batch size.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate for Adam optimizer.</p></td>
          <td>
                <code>0.0005</code>
          </td>
        </tr>
        <tr>
          <td><code>validation_fraction</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The fraction of data to use for validation.</p></td>
          <td>
                <code>0.1</code>
          </td>
        </tr>
        <tr>
          <td><code>stop_after_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
          <td>
                <code>20</code>
          </td>
        </tr>
        <tr>
          <td><code>max_num_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
          <td>
                <code>2 ** 31 - 1</code>
          </td>
        </tr>
        <tr>
          <td><code>clip_max_norm</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
          <td>
                <code>5.0</code>
          </td>
        </tr>
        <tr>
          <td><code>calibration_kernel</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>A function to calibrate the loss with respect to the
simulations <code>x</code>. See Lueckmann, Gonalves et al., NeurIPS 2017.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>resume_training</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>force_first_round_loss</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>If <code>True</code>, train with maximum likelihood,
i.e., potentially ignoring the correction for using a proposal
distribution different from the prior.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>discard_prior_samples</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>use_combined_loss</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to train the neural net also on prior samples
using maximum likelihood in addition to training it on all samples using
atomic loss. The extra MLE loss helps prevent density leaking with
bounded priors.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>retrain_from_scratch</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>show_train_summary</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>dataloader_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snpe/snpe_c.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_combined_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the distribution $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_atoms: Number of atoms to use for classification.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">            simulations `x`. See Lueckmann, Gonalves et al., NeurIPS 2017.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">            i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">            distribution different from the prior.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        use_combined_loss: Whether to train the neural net also on prior samples</span>
<span class="sd">            using maximum likelihood in addition to training it on all samples using</span>
<span class="sd">            atomic loss. The extra MLE loss helps prevent density leaking with</span>
<span class="sd">            bounded priors.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
    <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
    <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
    <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span> <span class="o">=</span> <span class="n">use_combined_loss</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span>
        <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="s2">&quot;use_combined_loss&quot;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Set the proposal to the last proposal that was passed by the user. For</span>
        <span class="c1"># atomic SNPE, it does not matter what the proposal is. For non-atomic</span>
        <span class="c1"># SNPE, we only use the latest data that was passed, i.e. the one from the</span>
        <span class="c1"># last proposal.</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">DirectPosterior</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">check_dist_class</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">class_to_check</span><span class="o">=</span><span class="p">(</span><span class="n">Uniform</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">)</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;non-atomic&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="k">else</span> <span class="s2">&quot;atomic&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using SNPE-C with </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> loss&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
            <span class="c1"># Take care of z-scoring, pre-compute and store prior terms.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_state_for_mog_proposal</span><span class="p">()</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.snle.snle_a.SNLE_A" class="doc doc-heading">
        <code>sbi.inference.snle.snle_a.SNLE_A</code>


<a href="#sbi.inference.snle.snle_a.SNLE_A" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.snle.snle_base.LikelihoodEstimator">LikelihoodEstimator</span></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snle/snle_a.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SNLE_A</span><span class="p">(</span><span class="n">LikelihoodEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Neural Likelihood [1].</span>

<span class="sd">        [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with</span>
<span class="sd">        Autoregressive Flows_, Papamakarios et al., AISTATS 2019,</span>
<span class="sd">        https://arxiv.org/abs/1805.07226</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">                provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snle.snle_a.SNLE_A.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.snle.snle_a.SNLE_A.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Sequential Neural Likelihood [1].</p>
<p>[1] Sequential Neural Likelihood: Fast Likelihood-free Inference with
Autoregressive Flows_, Papamakarios et al., AISTATS 2019,
<a href="https://arxiv.org/abs/1805.07226">https://arxiv.org/abs/1805.07226</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>density_estimator</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>If it is a string, use a pre-configured network of the
provided type (one of nsf, maf, mdn, made). Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>.</p></td>
          <td>
                <code>&#39;maf&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
          <td>
                <code>&#39;cpu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>logging_level</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, str]</code>
          </td>
          <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
          <td>
                <code>&#39;WARNING&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>summary_writer</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
          </td>
          <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snle/snle_a.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Neural Likelihood [1].</span>

<span class="sd">    [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with</span>
<span class="sd">    Autoregressive Flows_, Papamakarios et al., AISTATS 2019,</span>
<span class="sd">    https://arxiv.org/abs/1805.07226</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">            provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.snre.snre_a.SNRE_A" class="doc doc-heading">
        <code>sbi.inference.snre.snre_a.SNRE_A</code>


<a href="#sbi.inference.snre.snre_a.SNRE_A" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.snre.snre_base.RatioEstimator">RatioEstimator</span></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_a.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SNRE_A</span><span class="p">(</span><span class="n">RatioEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;AALR[1], here known as SNRE_A.</span>

<span class="sd">        [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans</span>
<span class="sd">            et al., ICML 2020, https://arxiv.org/abs/1903.04057</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">                inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">                `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">loss_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">            loss_kwargs: Additional or updated kwargs to be passed to the self._loss fn.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># AALR is defined for `num_atoms=2`.</span>
        <span class="c1"># Proxy to `super().__call__` to ensure right parameter.</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the binary cross-entropy loss for the trained classifier.</span>

<span class="sd">        The classifier takes as input a $(\theta,x)$ pair. It is trained to predict 1</span>
<span class="sd">        if the pair was sampled from the joint $p(\theta,x)$, and to predict 0 if the</span>
<span class="sd">        pair was sampled from the marginals $p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># Alternating pairs where there is one sampled from the joint and one</span>
        <span class="c1"># sampled from the marginals. The first element is sampled from the</span>
        <span class="c1"># joint p(theta, x) and is labelled 1. The second element is sampled</span>
        <span class="c1"># from the marginals p(theta)p(x) and is labelled 0. And so on.</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># two atoms</span>
        <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Binary cross entropy to learn the likelihood (AALR-specific)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snre.snre_a.SNRE_A.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.snre.snre_a.SNRE_A.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>AALR[1], here known as SNRE_A.</p>
<p>[1] <em>Likelihood-free MCMC with Amortized Approximate Likelihood Ratios</em>, Hermans
    et al., ICML 2020, <a href="https://arxiv.org/abs/1903.04057">https://arxiv.org/abs/1903.04057</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>classifier</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p></td>
          <td>
                <code>&#39;resnet&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
          <td>
                <code>&#39;cpu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>logging_level</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, str]</code>
          </td>
          <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
          <td>
                <code>&#39;warning&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>summary_writer</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
          </td>
          <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_a.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;AALR[1], here known as SNRE_A.</span>

<span class="sd">    [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans</span>
<span class="sd">        et al., ICML 2020, https://arxiv.org/abs/1903.04057</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snre.snre_a.SNRE_A.train" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="p">{})</span></code>

<a href="#sbi.inference.snre.snre_a.SNRE_A.train" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>training_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Training batch size.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate for Adam optimizer.</p></td>
          <td>
                <code>0.0005</code>
          </td>
        </tr>
        <tr>
          <td><code>validation_fraction</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The fraction of data to use for validation.</p></td>
          <td>
                <code>0.1</code>
          </td>
        </tr>
        <tr>
          <td><code>stop_after_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
          <td>
                <code>20</code>
          </td>
        </tr>
        <tr>
          <td><code>max_num_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
          <td>
                <code>2 ** 31 - 1</code>
          </td>
        </tr>
        <tr>
          <td><code>clip_max_norm</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
          <td>
                <code>5.0</code>
          </td>
        </tr>
        <tr>
          <td><code>resume_training</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>discard_prior_samples</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>retrain_from_scratch</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>show_train_summary</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>dataloader_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>loss_kwargs</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td><p>Additional or updated kwargs to be passed to the self._loss fn.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_a.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">loss_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">        loss_kwargs: Additional or updated kwargs to be passed to the self._loss fn.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># AALR is defined for `num_atoms=2`.</span>
    <span class="c1"># Proxy to `super().__call__` to ensure right parameter.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.snre.snre_b.SNRE_B" class="doc doc-heading">
        <code>sbi.inference.snre.snre_b.SNRE_B</code>


<a href="#sbi.inference.snre.snre_b.SNRE_B" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.snre.snre_base.RatioEstimator">RatioEstimator</span></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_b.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SNRE_B</span><span class="p">(</span><span class="n">RatioEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SRE[1], here known as SNRE_B.</span>

<span class="sd">        [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,</span>
<span class="sd">            ICML 2020, https://arxiv.org/pdf/2002.03712</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">                inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">                `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_atoms: Number of atoms to use for classification.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return cross-entropy (via softmax activation) loss for 1-out-of-`num_atoms`</span>
<span class="sd">        classification.</span>

<span class="sd">        The classifier takes as input `num_atoms` $(\theta,x)$ pairs. Out of these</span>
<span class="sd">        pairs, one pair was sampled from the joint $p(\theta,x)$ and all others from the</span>
<span class="sd">        marginals $p(\theta)p(x)$. The classifier is trained to predict which of the</span>
<span class="sd">        pairs was sampled from the joint $p(\theta,x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># For 1-out-of-`num_atoms` classification each datapoint consists</span>
        <span class="c1"># of `num_atoms` points, with one of them being the correct one.</span>
        <span class="c1"># We have a batch of `batch_size` such datapoints.</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># Index 0 is the theta-x-pair sampled from the joint p(theta,x) and hence the</span>
        <span class="c1"># &quot;correct&quot; one for the 1-out-of-N classification.</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snre.snre_b.SNRE_B.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.snre.snre_b.SNRE_B.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>SRE[1], here known as SNRE_B.</p>
<p>[1] <em>On Contrastive Learning for Likelihood-free Inference</em>, Durkan et al.,
    ICML 2020, <a href="https://arxiv.org/pdf/2002.03712">https://arxiv.org/pdf/2002.03712</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>classifier</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p></td>
          <td>
                <code>&#39;resnet&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
          <td>
                <code>&#39;cpu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>logging_level</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, str]</code>
          </td>
          <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
          <td>
                <code>&#39;warning&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>summary_writer</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
          </td>
          <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_b.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SRE[1], here known as SNRE_B.</span>

<span class="sd">    [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,</span>
<span class="sd">        ICML 2020, https://arxiv.org/pdf/2002.03712</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snre.snre_b.SNRE_B.train" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.snre.snre_b.SNRE_B.train" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>num_atoms</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of atoms to use for classification.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>training_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Training batch size.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate for Adam optimizer.</p></td>
          <td>
                <code>0.0005</code>
          </td>
        </tr>
        <tr>
          <td><code>validation_fraction</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The fraction of data to use for validation.</p></td>
          <td>
                <code>0.1</code>
          </td>
        </tr>
        <tr>
          <td><code>stop_after_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
          <td>
                <code>20</code>
          </td>
        </tr>
        <tr>
          <td><code>max_num_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
          <td>
                <code>2 ** 31 - 1</code>
          </td>
        </tr>
        <tr>
          <td><code>clip_max_norm</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
          <td>
                <code>5.0</code>
          </td>
        </tr>
        <tr>
          <td><code>resume_training</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>discard_prior_samples</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>retrain_from_scratch</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>show_train_summary</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>dataloader_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_b.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_atoms: Number of atoms to use for classification.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.snre.snre_c.SNRE_C" class="doc doc-heading">
        <code>sbi.inference.snre.snre_c.SNRE_C</code>


<a href="#sbi.inference.snre.snre_c.SNRE_C" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.snre.snre_base.RatioEstimator">RatioEstimator</span></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_c.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SNRE_C</span><span class="p">(</span><span class="n">RatioEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;NRE-C[1] is a generalization of the non-sequential (amortized) versions of</span>
<span class="sd">        SNRE_A and SNRE_B. We call the algorithm SNRE_C within `sbi`.</span>

<span class="sd">        NRE-C:</span>
<span class="sd">        (1) like SNRE_B, features a &quot;multiclass&quot; loss function where several marginally</span>
<span class="sd">            drawn parameter-data pairs are contrasted against a jointly drawn pair.</span>
<span class="sd">        (2) like AALR/NRE_A, i.e., the non-sequential version of SNRE_A, it encourages</span>
<span class="sd">            the approximate ratio $p(\theta,x)/p(\theta)p(x)$, accessed through</span>
<span class="sd">            `.potential()` within `sbi`, to be exact at optimum. This addresses the</span>
<span class="sd">            issue that SNRE_B estimates this ratio only up to an arbitrary function</span>
<span class="sd">            (normalizing constant) of the data $x$.</span>

<span class="sd">        Just like for all ratio estimation algorithms, the sequential version of SNRE_C</span>
<span class="sd">        will be estimated only up to a function (normalizing constant) of the data $x$</span>
<span class="sd">        in rounds after the first.</span>

<span class="sd">        [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,</span>
<span class="sd">            NeurIPS 2022, https://arxiv.org/abs/2210.06170</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">                inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">                `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_classes: Number of theta to classify against, corresponds to $K$ in</span>
<span class="sd">                _Contrastive Neural Ratio Estimation_. Minimum value is 1. Similar to</span>
<span class="sd">                `num_atoms` for SNRE_B except SNRE_C has an additional independently</span>
<span class="sd">                drawn sample. The total number of alternative parameters `NRE-C` &quot;sees&quot;</span>
<span class="sd">                is $2K-1$ or `2 * num_classes - 1` divided between two loss terms.</span>
<span class="sd">            gamma: Determines the relative weight of the sum of all $K$ dependently</span>
<span class="sd">                drawn classes against the marginally drawn one. Specifically,</span>
<span class="sd">                $p(y=k) :=p_K$, $p(y=0) := p_0$, $p_0 = 1 - K p_K$, and finally</span>
<span class="sd">                $\gamma := K p_K / p_0$.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">                during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_atoms&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_classes&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;loss_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">)}</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return cross-entropy loss (via &#39;&#39;multi-class sigmoid&#39;&#39; activation) for</span>
<span class="sd">        1-out-of-`K + 1` classification.</span>

<span class="sd">        At optimum, this loss function returns the exact likelihood-to-evidence ratio</span>
<span class="sd">        in the first round.</span>
<span class="sd">        Details of loss computation are described in Contrastive Neural Ratio</span>
<span class="sd">        Estimation[1]. The paper does not discuss the sequential case.</span>

<span class="sd">        [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,</span>
<span class="sd">            NeurIPS 2022, https://arxiv.org/abs/2210.06170</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Reminder: K = num_classes</span>
        <span class="c1"># The algorithm is written with K, so we convert back to K format rather than</span>
        <span class="c1"># reasoning in num_atoms.</span>
        <span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_atoms</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="n">num_classes</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;num_classes = </span><span class="si">{</span><span class="n">num_classes</span><span class="si">}</span><span class="s2"> must be greater than 1.&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># We append an contrastive theta to the marginal case because we will remove</span>
        <span class="c1"># the jointly drawn</span>
        <span class="c1"># sample in the logits_marginal[:, 0] position. That makes the remaining sample</span>
        <span class="c1"># marginally drawn.</span>
        <span class="c1"># We have a batch of `batch_size` datapoints.</span>
        <span class="n">logits_marginal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">logits_joint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span>
        <span class="p">)</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="n">logits_marginal</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">logits_marginal</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># Index 0 is the theta-x-pair sampled from the joint p(theta,x) and hence</span>
        <span class="c1"># we remove the jointly drawn sample from the logits_marginal</span>
        <span class="n">logits_marginal</span> <span class="o">=</span> <span class="n">logits_marginal</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="c1"># ... and retain it in the logits_joint. Now we have two arrays with K choices.</span>

        <span class="c1"># To use logsumexp, we extend the denominator logits with loggamma</span>
        <span class="n">loggamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
        <span class="n">logK</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
        <span class="n">denominator_marginal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">loggamma</span> <span class="o">+</span> <span class="n">logits_marginal</span><span class="p">,</span> <span class="n">logK</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">denominator_joint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">loggamma</span> <span class="o">+</span> <span class="n">logits_joint</span><span class="p">,</span> <span class="n">logK</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Compute the contributions to the loss from each term in the classification.</span>
        <span class="n">log_prob_marginal</span> <span class="o">=</span> <span class="n">logK</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">denominator_marginal</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_prob_joint</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">loggamma</span> <span class="o">+</span> <span class="n">logits_joint</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">denominator_joint</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># relative weights. p_marginal := p_0, and p_joint := p_K * K from the notation.</span>
        <span class="n">p_marginal</span><span class="p">,</span> <span class="n">p_joint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_prior_probs_marginal_and_joint</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_marginal</span> <span class="o">*</span> <span class="n">log_prob_marginal</span> <span class="o">+</span> <span class="n">p_joint</span> <span class="o">*</span> <span class="n">log_prob_joint</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_prior_probs_marginal_and_joint</span><span class="p">(</span><span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return a tuple (p_marginal, p_joint) where `p_marginal := `$p_0$,</span>
<span class="sd">        `p_joint := `$p_K \cdot K$.</span>

<span class="sd">        We let the joint (dependently drawn) class to be equally likely across K</span>
<span class="sd">        options. The marginal class is therefore restricted to get the remaining</span>
<span class="sd">        probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p_joint</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">p_marginal</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p_marginal</span><span class="p">,</span> <span class="n">p_joint</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snre.snre_c.SNRE_C.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.snre.snre_c.SNRE_C.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>NRE-C[1] is a generalization of the non-sequential (amortized) versions of
SNRE_A and SNRE_B. We call the algorithm SNRE_C within <code>sbi</code>.</p>
<p>NRE-C:
(1) like SNRE_B, features a &ldquo;multiclass&rdquo; loss function where several marginally
    drawn parameter-data pairs are contrasted against a jointly drawn pair.
(2) like AALR/NRE_A, i.e., the non-sequential version of SNRE_A, it encourages
    the approximate ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>, accessed through
    <code>.potential()</code> within <code>sbi</code>, to be exact at optimum. This addresses the
    issue that SNRE_B estimates this ratio only up to an arbitrary function
    (normalizing constant) of the data <span class="arithmatex">\(x\)</span>.</p>
<p>Just like for all ratio estimation algorithms, the sequential version of SNRE_C
will be estimated only up to a function (normalizing constant) of the data <span class="arithmatex">\(x\)</span>
in rounds after the first.</p>
<p>[1] <em>Contrastive Neural Ratio Estimation</em>, Benajmin Kurt Miller, et. al.,
    NeurIPS 2022, <a href="https://arxiv.org/abs/2210.06170">https://arxiv.org/abs/2210.06170</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>classifier</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p></td>
          <td>
                <code>&#39;resnet&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
          <td>
                <code>&#39;cpu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>logging_level</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, str]</code>
          </td>
          <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
          <td>
                <code>&#39;warning&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>summary_writer</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
          </td>
          <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_c.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;NRE-C[1] is a generalization of the non-sequential (amortized) versions of</span>
<span class="sd">    SNRE_A and SNRE_B. We call the algorithm SNRE_C within `sbi`.</span>

<span class="sd">    NRE-C:</span>
<span class="sd">    (1) like SNRE_B, features a &quot;multiclass&quot; loss function where several marginally</span>
<span class="sd">        drawn parameter-data pairs are contrasted against a jointly drawn pair.</span>
<span class="sd">    (2) like AALR/NRE_A, i.e., the non-sequential version of SNRE_A, it encourages</span>
<span class="sd">        the approximate ratio $p(\theta,x)/p(\theta)p(x)$, accessed through</span>
<span class="sd">        `.potential()` within `sbi`, to be exact at optimum. This addresses the</span>
<span class="sd">        issue that SNRE_B estimates this ratio only up to an arbitrary function</span>
<span class="sd">        (normalizing constant) of the data $x$.</span>

<span class="sd">    Just like for all ratio estimation algorithms, the sequential version of SNRE_C</span>
<span class="sd">    will be estimated only up to a function (normalizing constant) of the data $x$</span>
<span class="sd">    in rounds after the first.</span>

<span class="sd">    [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,</span>
<span class="sd">        NeurIPS 2022, https://arxiv.org/abs/2210.06170</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snre.snre_c.SNRE_C.train" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.snre.snre_c.SNRE_C.train" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>num_classes</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of theta to classify against, corresponds to <span class="arithmatex">\(K\)</span> in
<em>Contrastive Neural Ratio Estimation</em>. Minimum value is 1. Similar to
<code>num_atoms</code> for SNRE_B except SNRE_C has an additional independently
drawn sample. The total number of alternative parameters <code>NRE-C</code> &ldquo;sees&rdquo;
is <span class="arithmatex">\(2K-1\)</span> or <code>2 * num_classes - 1</code> divided between two loss terms.</p></td>
          <td>
                <code>5</code>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Determines the relative weight of the sum of all <span class="arithmatex">\(K\)</span> dependently
drawn classes against the marginally drawn one. Specifically,
<span class="arithmatex">\(p(y=k) :=p_K\)</span>, <span class="arithmatex">\(p(y=0) := p_0\)</span>, <span class="arithmatex">\(p_0 = 1 - K p_K\)</span>, and finally
<span class="arithmatex">\(\gamma := K p_K / p_0\)</span>.</p></td>
          <td>
                <code>1.0</code>
          </td>
        </tr>
        <tr>
          <td><code>training_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Training batch size.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate for Adam optimizer.</p></td>
          <td>
                <code>0.0005</code>
          </td>
        </tr>
        <tr>
          <td><code>validation_fraction</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The fraction of data to use for validation.</p></td>
          <td>
                <code>0.1</code>
          </td>
        </tr>
        <tr>
          <td><code>stop_after_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
          <td>
                <code>20</code>
          </td>
        </tr>
        <tr>
          <td><code>max_num_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
          <td>
                <code>2 ** 31 - 1</code>
          </td>
        </tr>
        <tr>
          <td><code>clip_max_norm</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
          <td>
                <code>5.0</code>
          </td>
        </tr>
        <tr>
          <td><code>exclude_invalid_x</code></td>
          <td>
          </td>
          <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>resume_training</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>discard_prior_samples</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>retrain_from_scratch</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>show_train_summary</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>dataloader_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/snre_c.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_classes: Number of theta to classify against, corresponds to $K$ in</span>
<span class="sd">            _Contrastive Neural Ratio Estimation_. Minimum value is 1. Similar to</span>
<span class="sd">            `num_atoms` for SNRE_B except SNRE_C has an additional independently</span>
<span class="sd">            drawn sample. The total number of alternative parameters `NRE-C` &quot;sees&quot;</span>
<span class="sd">            is $2K-1$ or `2 * num_classes - 1` divided between two loss terms.</span>
<span class="sd">        gamma: Determines the relative weight of the sum of all $K$ dependently</span>
<span class="sd">            drawn classes against the marginally drawn one. Specifically,</span>
<span class="sd">            $p(y=k) :=p_K$, $p(y=0) := p_0$, $p_0 = 1 - K p_K$, and finally</span>
<span class="sd">            $\gamma := K p_K / p_0$.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_atoms&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_classes&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;loss_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">)}</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.snre.bnre.BNRE" class="doc doc-heading">
        <code>sbi.inference.snre.bnre.BNRE</code>


<a href="#sbi.inference.snre.bnre.BNRE" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="sbi.inference.snre.snre_a.SNRE_A" href="#sbi.inference.snre.snre_a.SNRE_A">SNRE_A</a></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/bnre.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BNRE</span><span class="p">(</span><span class="n">SNRE_A</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Balanced neural ratio estimation (BNRE)[1]. BNRE is a variation of NRE</span>
<span class="sd">        aiming to produce more conservative posterior approximations</span>

<span class="sd">        [1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G..</span>
<span class="sd">        Towards Reliable Simulation-Based Inference with Balanced Neural Ratio</span>
<span class="sd">        Estimation.</span>
<span class="sd">        NeurIPS 2022. https://arxiv.org/abs/2208.13624</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations $(\theta, x)$, which can thus be used for</span>
<span class="sd">                shape inference and potentially for z-scoring. It needs to return a</span>
<span class="sd">                PyTorch `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">100.0</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        Args:</span>

<span class="sd">            regularization_strength: The multiplicative coefficient applied to the</span>
<span class="sd">                balancing regularizer ($\lambda$).</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">                during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;loss_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;regularization_strength&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;regularization_strength&quot;</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the binary cross-entropy loss for the trained classifier.</span>

<span class="sd">        The classifier takes as input a $(\theta,x)$ pair. It is trained to predict 1</span>
<span class="sd">        if the pair was sampled from the joint $p(\theta,x)$, and to predict 0 if the</span>
<span class="sd">        pair was sampled from the marginals $p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># Alternating pairs where there is one sampled from the joint and one</span>
        <span class="c1"># sampled from the marginals. The first element is sampled from the</span>
        <span class="c1"># joint p(theta, x) and is labelled 1. The second element is sampled</span>
        <span class="c1"># from the marginals p(theta)p(x) and is labelled 0. And so on.</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># two atoms</span>
        <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Binary cross entropy to learn the likelihood (AALR-specific)</span>
        <span class="n">bce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Balancing regularizer</span>
        <span class="n">regularizer</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="o">.</span><span class="n">square</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">bce</span> <span class="o">+</span> <span class="n">regularization_strength</span> <span class="o">*</span> <span class="n">regularizer</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snre.bnre.BNRE.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.snre.bnre.BNRE.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Balanced neural ratio estimation (BNRE)[1]. BNRE is a variation of NRE
aiming to produce more conservative posterior approximations</p>
<p>[1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G..
Towards Reliable Simulation-Based Inference with Balanced Neural Ratio
Estimation.
NeurIPS 2022. <a href="https://arxiv.org/abs/2208.13624">https://arxiv.org/abs/2208.13624</a></p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>classifier</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations <span class="arithmatex">\((\theta, x)\)</span>, which can thus be used for
shape inference and potentially for z-scoring. It needs to return a
PyTorch <code>nn.Module</code> implementing the classifier.</p></td>
          <td>
                <code>&#39;resnet&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
          <td>
                <code>&#39;cpu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>logging_level</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, str]</code>
          </td>
          <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
          <td>
                <code>&#39;warning&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>summary_writer</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
          </td>
          <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/bnre.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Balanced neural ratio estimation (BNRE)[1]. BNRE is a variation of NRE</span>
<span class="sd">    aiming to produce more conservative posterior approximations</span>

<span class="sd">    [1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G..</span>
<span class="sd">    Towards Reliable Simulation-Based Inference with Balanced Neural Ratio</span>
<span class="sd">    Estimation.</span>
<span class="sd">    NeurIPS 2022. https://arxiv.org/abs/2208.13624</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations $(\theta, x)$, which can thus be used for</span>
<span class="sd">            shape inference and potentially for z-scoring. It needs to return a</span>
<span class="sd">            PyTorch `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.snre.bnre.BNRE.train" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="n">regularization_strength</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.snre.bnre.BNRE.train" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>regularization_strength</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The multiplicative coefficient applied to the
balancing regularizer (<span class="arithmatex">\(\lambda\)</span>).</p></td>
          <td>
                <code>100.0</code>
          </td>
        </tr>
        <tr>
          <td><code>training_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Training batch size.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate for Adam optimizer.</p></td>
          <td>
                <code>0.0005</code>
          </td>
        </tr>
        <tr>
          <td><code>validation_fraction</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>The fraction of data to use for validation.</p></td>
          <td>
                <code>0.1</code>
          </td>
        </tr>
        <tr>
          <td><code>stop_after_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
          <td>
                <code>20</code>
          </td>
        </tr>
        <tr>
          <td><code>max_num_epochs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
          <td>
                <code>2 ** 31 - 1</code>
          </td>
        </tr>
        <tr>
          <td><code>clip_max_norm</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
          <td>
                <code>5.0</code>
          </td>
        </tr>
        <tr>
          <td><code>exclude_invalid_x</code></td>
          <td>
          </td>
          <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>resume_training</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>discard_prior_samples</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>retrain_from_scratch</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>show_train_summary</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>dataloader_kwargs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
          </td>
          <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/snre/bnre.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">100.0</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    Args:</span>

<span class="sd">        regularization_strength: The multiplicative coefficient applied to the</span>
<span class="sd">            balancing regularizer ($\lambda$).</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;loss_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;regularization_strength&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;regularization_strength&quot;</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.abc.mcabc.MCABC" class="doc doc-heading">
        <code>sbi.inference.abc.mcabc.MCABC</code>


<a href="#sbi.inference.abc.mcabc.MCABC" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.abc.abc_base.ABCBASE">ABCBASE</span></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/mcabc.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MCABC</span><span class="p">(</span><span class="n">ABCBASE</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</span>

<span class="sd">        [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.</span>
<span class="sd">        (1999). Population growth of human Y chromosomes: a study of Y chromosome</span>
<span class="sd">        microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</span>

<span class="sd">        Args:</span>
<span class="sd">            simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">                simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">                regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">                can be used.</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">                a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">            num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">            simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">                maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">                same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">                (simulation_batch_size, parameter_dimension).</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
        <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantile</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run MCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">        Args:</span>
<span class="sd">            x_o: Observed data.</span>
<span class="sd">            num_simulations: Number of simulations to run.</span>
<span class="sd">            eps: Acceptance threshold $\epsilon$ for distance between observed and</span>
<span class="sd">                simulated data.</span>
<span class="sd">            quantile: Upper quantile of smallest distances for which the corresponding</span>
<span class="sd">                parameters are returned, e.g, q=0.01 will return the top 1%. Exactly</span>
<span class="sd">                one of quantile or `eps` have to be passed.</span>
<span class="sd">            lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">            sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">                Fearnhead &amp; Prangle 2012.</span>
<span class="sd">            sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">            sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">                sass regression, default 1 - no expansion.</span>
<span class="sd">            kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">                object from which one can sample.</span>
<span class="sd">            kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">                &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">                heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">                default &#39;cv&#39;.</span>
<span class="sd">                &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">                &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">                more details</span>
<span class="sd">            return_summary: Whether to return the distances and data corresponding to</span>
<span class="sd">                the accepted parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            theta (if kde False): accepted parameters</span>
<span class="sd">            kde (if kde True): KDE object based on accepted parameters from which one</span>
<span class="sd">                can .sample() and .log_prob().</span>
<span class="sd">            summary (if summary True): dictionary containing the accepted paramters (if</span>
<span class="sd">                kde True), distances and simulated data x.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Exactly one of eps or quantile need to be passed.</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span>
            <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Eps or quantile must be passed, but not both.&quot;</span>

        <span class="c1"># Run SASS and change the simulator and x_o accordingly.</span>
        <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
            <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Running SASS with </span><span class="si">{</span><span class="n">num_pilot_simulations</span><span class="si">}</span><span class="s2"> pilot samples.&quot;</span>
            <span class="p">)</span>
            <span class="n">num_simulations</span> <span class="o">-=</span> <span class="n">num_pilot_simulations</span>

            <span class="n">pilot_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_pilot_simulations</span><span class="p">,))</span>
            <span class="n">pilot_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">pilot_theta</span><span class="p">)</span>

            <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
                <span class="n">pilot_theta</span><span class="p">,</span> <span class="n">pilot_x</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
            <span class="p">)</span>

            <span class="n">simulator</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
            <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="n">x_o</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">simulator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span>

        <span class="c1"># Simulate and calculate distances.</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Infer shape of x to test and set x_o.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="c1"># Select based on acceptance threshold epsilon.</span>
        <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">&lt;</span> <span class="n">eps</span>
            <span class="n">num_accepted</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">assert</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;No parameters accepted, eps=</span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2"> too small&quot;</span>

            <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
            <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
            <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>

        <span class="c1"># Select based on quantile on sorted distances.</span>
        <span class="k">elif</span> <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_top_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_simulations</span> <span class="o">*</span> <span class="n">quantile</span><span class="p">)</span>
            <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
            <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
            <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;One of epsilon or quantile has to be passed.&quot;</span><span class="p">)</span>

        <span class="c1"># Maybe adjust theta with LRA.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
            <span class="n">final_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">theta_accepted</span><span class="p">,</span> <span class="n">x_accepted</span><span class="p">,</span> <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">final_theta</span> <span class="o">=</span> <span class="n">theta_accepted</span>

        <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&quot;&quot;KDE on </span><span class="si">{</span><span class="n">final_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples with bandwidth option</span>
<span class="s2">                </span><span class="si">{</span><span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">]</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="s2">&quot;bandwidth&quot;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">kde_kwargs</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">&quot;cv&quot;</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">                Beware that KDE can give unreliable results when used with too few</span>
<span class="s2">                samples and in high dimensions.&quot;&quot;&quot;</span>
            <span class="p">)</span>

            <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">(</span>
                    <span class="n">kde_dist</span><span class="p">,</span>
                    <span class="nb">dict</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">final_theta</span><span class="p">,</span> <span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">kde_dist</span>
        <span class="k">elif</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_theta</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_theta</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.mcabc.MCABC.__call__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quantile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_fraction</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.mcabc.MCABC.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Run MCABC and return accepted parameters or KDE object fitted on them.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_o</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="numpy.ndarray">ndarray</span>]</code>
          </td>
          <td><p>Observed data.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_simulations</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of simulations to run.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>eps</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Acceptance threshold <span class="arithmatex">\(\epsilon\)</span> for distance between observed and
simulated data.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>quantile</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Upper quantile of smallest distances for which the corresponding
parameters are returned, e.g, q=0.01 will return the top 1%. Exactly
one of quantile or <code>eps</code> have to be passed.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>lra</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to run linear regression adjustment as in Beaumont et al. 2002</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>sass</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to determine semi-automatic summary statistics as in
Fearnhead &amp; Prangle 2012.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>sass_fraction</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Fraction of simulation budget used for the initial sass run.</p></td>
          <td>
                <code>0.25</code>
          </td>
        </tr>
        <tr>
          <td><code>sass_expansion_degree</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Degree of the polynomial feature expansion for the
sass regression, default 1 - no expansion.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>kde</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to run KDE on the accepted parameters to return a KDE
object from which one can sample.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>kde_kwargs</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td><p>kwargs for performing KDE:
&lsquo;bandwidth=&rsquo;; either a float, or a string naming a bandwidth
heuristics, e.g., &lsquo;cv&rsquo; (cross validation), &lsquo;silvermann&rsquo; or &lsquo;scott&rsquo;,
default &lsquo;cv&rsquo;.
&lsquo;transform&rsquo;: transform applied to the parameters before doing KDE.
&lsquo;sample_weights&rsquo;: weights associated with samples. See &lsquo;get_kde&rsquo; for
more details</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
        <tr>
          <td><code>return_summary</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to return the distances and data corresponding to
the accepted parameters.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>theta</code></td>          <td>
                <code>if kde False</code>
          </td>
          <td><p>accepted parameters</p></td>
        </tr>
        <tr>
<td><code>kde</code></td>          <td>
                <code>if kde True</code>
          </td>
          <td><p>KDE object based on accepted parameters from which one
can .sample() and .log_prob().</p></td>
        </tr>
        <tr>
<td><code>summary</code></td>          <td>
                <code>if summary True</code>
          </td>
          <td><p>dictionary containing the accepted paramters (if
kde True), distances and simulated data x.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/mcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantile</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run MCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_o: Observed data.</span>
<span class="sd">        num_simulations: Number of simulations to run.</span>
<span class="sd">        eps: Acceptance threshold $\epsilon$ for distance between observed and</span>
<span class="sd">            simulated data.</span>
<span class="sd">        quantile: Upper quantile of smallest distances for which the corresponding</span>
<span class="sd">            parameters are returned, e.g, q=0.01 will return the top 1%. Exactly</span>
<span class="sd">            one of quantile or `eps` have to be passed.</span>
<span class="sd">        lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">        sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">            Fearnhead &amp; Prangle 2012.</span>
<span class="sd">        sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">        sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">            sass regression, default 1 - no expansion.</span>
<span class="sd">        kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">            object from which one can sample.</span>
<span class="sd">        kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">            &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">            heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">            default &#39;cv&#39;.</span>
<span class="sd">            &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">            &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">            more details</span>
<span class="sd">        return_summary: Whether to return the distances and data corresponding to</span>
<span class="sd">            the accepted parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        theta (if kde False): accepted parameters</span>
<span class="sd">        kde (if kde True): KDE object based on accepted parameters from which one</span>
<span class="sd">            can .sample() and .log_prob().</span>
<span class="sd">        summary (if summary True): dictionary containing the accepted paramters (if</span>
<span class="sd">            kde True), distances and simulated data x.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Exactly one of eps or quantile need to be passed.</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span>
        <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">&quot;Eps or quantile must be passed, but not both.&quot;</span>

    <span class="c1"># Run SASS and change the simulator and x_o accordingly.</span>
    <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
        <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Running SASS with </span><span class="si">{</span><span class="n">num_pilot_simulations</span><span class="si">}</span><span class="s2"> pilot samples.&quot;</span>
        <span class="p">)</span>
        <span class="n">num_simulations</span> <span class="o">-=</span> <span class="n">num_pilot_simulations</span>

        <span class="n">pilot_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_pilot_simulations</span><span class="p">,))</span>
        <span class="n">pilot_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">pilot_theta</span><span class="p">)</span>

        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
            <span class="n">pilot_theta</span><span class="p">,</span> <span class="n">pilot_x</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
        <span class="p">)</span>

        <span class="n">simulator</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="n">x_o</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">simulator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span>

    <span class="c1"># Simulate and calculate distances.</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="c1"># Infer shape of x to test and set x_o.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># Select based on acceptance threshold epsilon.</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">&lt;</span> <span class="n">eps</span>
        <span class="n">num_accepted</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;No parameters accepted, eps=</span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2"> too small&quot;</span>

        <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
        <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
        <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>

    <span class="c1"># Select based on quantile on sorted distances.</span>
    <span class="k">elif</span> <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_top_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_simulations</span> <span class="o">*</span> <span class="n">quantile</span><span class="p">)</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
        <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
        <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;One of epsilon or quantile has to be passed.&quot;</span><span class="p">)</span>

    <span class="c1"># Maybe adjust theta with LRA.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
        <span class="n">final_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">theta_accepted</span><span class="p">,</span> <span class="n">x_accepted</span><span class="p">,</span> <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">final_theta</span> <span class="o">=</span> <span class="n">theta_accepted</span>

    <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&quot;&quot;KDE on </span><span class="si">{</span><span class="n">final_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples with bandwidth option</span>
<span class="s2">            </span><span class="si">{</span><span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">]</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="s2">&quot;bandwidth&quot;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">kde_kwargs</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">&quot;cv&quot;</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">            Beware that KDE can give unreliable results when used with too few</span>
<span class="s2">            samples and in high dimensions.&quot;&quot;&quot;</span>
        <span class="p">)</span>

        <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">kde_dist</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">final_theta</span><span class="p">,</span> <span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kde_dist</span>
    <span class="k">elif</span> <span class="n">return_summary</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_theta</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_theta</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.mcabc.MCABC.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.mcabc.MCABC.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</p>
<p>[1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.
(1999). Population growth of human Y chromosomes: a study of Y chromosome
microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>simulator</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\mathrm{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>distance</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Distance function to compare observed and simulated data. Can be
a custom function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>.</p></td>
          <td>
                <code>&#39;l2&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>num_workers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of parallel workers to use for simulations.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>simulation_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/mcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</span>

<span class="sd">    [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.</span>
<span class="sd">    (1999). Population growth of human Y chromosomes: a study of Y chromosome</span>
<span class="sd">    microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">            a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.abc.smcabc.SMCABC" class="doc doc-heading">
        <code>sbi.inference.abc.smcabc.SMCABC</code>


<a href="#sbi.inference.abc.smcabc.SMCABC" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.abc.abc_base.ABCBASE">ABCBASE</span></code></p>



        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SMCABC</span><span class="p">(</span><span class="n">ABCBASE</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
        <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
        <span class="n">algorithm_variant</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Monte Carlo Approximate Bayesian Computation.</span>

<span class="sd">        We distinguish between three different SMC methods here:</span>
<span class="sd">            - A: Toni et al. 2010 (Phd Thesis)</span>
<span class="sd">            - B: Sisson et al. 2007 (with correction from 2009)</span>
<span class="sd">            - C: Beaumont et al. 2009</span>

<span class="sd">        In Toni et al. 2010 we find an overview of the differences on page 34:</span>
<span class="sd">            - B: same as A except for resampling of weights if the effective sampling</span>
<span class="sd">                size is too small.</span>
<span class="sd">            - C: same as A except for calculation of the covariance of the perturbation</span>
<span class="sd">                kernel: the kernel covariance is a scaled version of the covariance of</span>
<span class="sd">                the previous population.</span>

<span class="sd">        Args:</span>
<span class="sd">            simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">                simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">                regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">                can be used.</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">                a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">            num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">            simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">                maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">                same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">                (simulation_batch_size, parameter_dimension).</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">            kernel: Perturbation kernel.</span>
<span class="sd">            algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">kernels</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Kernel &#39;</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported. Choose one from </span><span class="si">{</span><span class="n">kernels</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

        <span class="n">algorithm_variants</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="n">algorithm_variants</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;SMCABC variant &#39;</span><span class="si">{</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported, choose one from&quot;</span>
            <span class="s2">&quot; </span><span class="si">{algorithm_variants}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">=</span> <span class="n">algorithm_variant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance_to_x0</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Define simulator that keeps track of budget.</span>
        <span class="k">def</span> <span class="nf">simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">simulate_with_budget</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">epsilon_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">distance_based_decay</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">ess_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">kde_sample_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run SMCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">        Args:</span>
<span class="sd">            x_o: Observed data.</span>
<span class="sd">            num_particles: Number of particles in each population.</span>
<span class="sd">            num_initial_pop: Number of simulations used for initial population.</span>
<span class="sd">            num_simulations: Total number of possible simulations.</span>
<span class="sd">            epsilon_decay: Factor with which the acceptance threshold $\epsilon$ decays.</span>
<span class="sd">            distance_based_decay: Whether the $\epsilon$ decay is constant over</span>
<span class="sd">                populations or calculated from the previous populations distribution of</span>
<span class="sd">                distances.</span>
<span class="sd">            ess_min: Threshold of effective sampling size for resampling weights. Not</span>
<span class="sd">                used when None (default).</span>
<span class="sd">            kernel_variance_scale: Factor for scaling the perturbation kernel variance.</span>
<span class="sd">            use_last_pop_samples: Whether to fill up the current population with</span>
<span class="sd">                samples from the previous population when the budget is used up. If</span>
<span class="sd">                False, the current population is discarded and the previous population</span>
<span class="sd">                is returned.</span>
<span class="sd">            lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">            lra_with_weights: Whether to run lra as weighted linear regression with SMC</span>
<span class="sd">                weights</span>
<span class="sd">            sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">                Fearnhead &amp; Prangle 2012.</span>
<span class="sd">            sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">            sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">                sass regression, default 1 - no expansion.</span>
<span class="sd">            kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">                object from which one can sample.</span>
<span class="sd">            kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">                &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">                heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">                default &#39;cv&#39;.</span>
<span class="sd">                &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">                &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">                more details</span>
<span class="sd">            kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw</span>
<span class="sd">                particles.</span>
<span class="sd">            return_summary: Whether to return a dictionary with all accepted particles,</span>
<span class="sd">                weights, etc. at the end.</span>

<span class="sd">        Returns:</span>
<span class="sd">            theta (if kde False): accepted parameters of the last population.</span>
<span class="sd">            kde (if kde True): KDE object fitted on accepted parameters, from which one</span>
<span class="sd">                can .sample() and .log_prob().</span>
<span class="sd">            summary (if return_summary True): dictionary containing the accepted</span>
<span class="sd">                paramters (if kde True), distances and simulated data x of all</span>
<span class="sd">                populations.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">pop_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="n">num_simulations</span>

        <span class="c1"># Pilot run for SASS.</span>
        <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
            <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Running SASS with </span><span class="si">{</span><span class="n">num_pilot_simulations</span><span class="si">}</span><span class="s2"> pilot samples.&quot;</span>
            <span class="p">)</span>
            <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_sass_set_xo</span><span class="p">(</span>
                <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">lra</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
            <span class="p">)</span>
            <span class="c1"># Udpate simulator and xo</span>
            <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">sass_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">sass_simulator</span>

        <span class="c1"># run initial population</span>
        <span class="n">particles</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
            <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span>
        <span class="p">)</span>
        <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">, ess=</span><span class="si">{</span><span class="mf">1.0</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;num_sims=</span><span class="si">{</span><span class="n">num_initial_pop</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="n">all_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
        <span class="n">all_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
        <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
        <span class="n">all_epsilons</span> <span class="o">=</span> <span class="p">[</span><span class="n">epsilon</span><span class="p">]</span>
        <span class="n">all_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span><span class="p">:</span>
            <span class="n">pop_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># Decay based on quantile of distances from previous pop.</span>
            <span class="k">if</span> <span class="n">distance_based_decay</span><span class="p">:</span>
                <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_epsilon</span><span class="p">(</span>
                    <span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">epsilon_decay</span>
                <span class="p">)</span>
            <span class="c1"># Constant decay.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epsilon</span> <span class="o">*=</span> <span class="n">epsilon_decay</span>

            <span class="c1"># Get kernel variance from previous pop.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_kernel_variance</span><span class="p">(</span>
                <span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span>
                <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="n">kernel_variance_scale</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_next_population</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="n">use_last_pop_samples</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Resample population if effective sampling size is too small.</span>
            <span class="k">if</span> <span class="n">ess_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_if_ess_too_small</span><span class="p">(</span>
                    <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2"> done: eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; num_sims=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># collect results</span>
            <span class="n">all_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">)</span>
            <span class="n">all_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
            <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">all_epsilons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">all_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Maybe run LRA and adjust weights.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
            <span class="n">adjusted_particles</span><span class="p">,</span> <span class="n">adjusted_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra_update_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">observation</span><span class="o">=</span><span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">),</span>
                <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">lra_with_weights</span><span class="o">=</span><span class="n">lra_with_weights</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">final_particles</span> <span class="o">=</span> <span class="n">adjusted_particles</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">final_particles</span> <span class="o">=</span> <span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&quot;&quot;KDE on </span><span class="si">{</span><span class="n">final_particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples with bandwidth option</span>
<span class="s2">                </span><span class="si">{</span><span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">]</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="s2">&quot;bandwidth&quot;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">kde_kwargs</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">&quot;cv&quot;</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">                Beware that KDE can give unreliable results when used with too few</span>
<span class="s2">                samples and in high dimensions.&quot;&quot;&quot;</span>
            <span class="p">)</span>
            <span class="c1"># Maybe get particles weights from last population for weighted KDE.</span>
            <span class="k">if</span> <span class="n">kde_sample_weights</span><span class="p">:</span>
                <span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;sample_weights&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>

            <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_particles</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">(</span>
                    <span class="n">kde_dist</span><span class="p">,</span>
                    <span class="nb">dict</span><span class="p">(</span>
                        <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                        <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                        <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                        <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                        <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">kde_dist</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">final_particles</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">(</span>
                    <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                    <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                    <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                    <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_particles</span>

    <span class="k">def</span> <span class="nf">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return particles, epsilon and distances of initial population.&quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">num_particles</span> <span class="o">&lt;=</span> <span class="n">num_initial_pop</span>
        <span class="p">),</span> <span class="s2">&quot;number of initial round simulations must be greater than population size&quot;</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_initial_pop</span><span class="p">,))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Infer x shape to test and set x_o.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">sortidx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">particles</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">]</span>
        <span class="c1"># Take last accepted distance as epsilon.</span>
        <span class="n">initial_epsilon</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][</span><span class="n">num_particles</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">initial_epsilon</span><span class="p">):</span>
            <span class="n">initial_epsilon</span> <span class="o">=</span> <span class="mf">1e8</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span>
            <span class="n">initial_epsilon</span><span class="p">,</span>
            <span class="n">distances</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">],</span>
            <span class="n">x</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sample_next_population</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">distances</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return particles, weights and distances of new population.&quot;&quot;&quot;</span>

        <span class="n">new_particles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_distances</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_x</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">num_accepted_particles</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">while</span> <span class="n">num_accepted_particles</span> <span class="o">&lt;</span> <span class="n">num_particles</span><span class="p">:</span>
            <span class="c1"># Upperbound for batch size to not exceed simulation budget.</span>
            <span class="n">num_batch</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="n">num_particles</span> <span class="o">-</span> <span class="n">num_accepted_particles</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Sample from previous population and perturb.</span>
            <span class="n">particle_candidates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_and_perturb</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_batch</span>
            <span class="p">)</span>
            <span class="c1"># Simulate and select based on distance.</span>
            <span class="n">x_candidates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span><span class="p">(</span><span class="n">particle_candidates</span><span class="p">)</span>
            <span class="n">dists</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x_candidates</span><span class="p">)</span>
            <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">dists</span> <span class="o">&lt;=</span> <span class="n">epsilon</span>
            <span class="n">num_accepted_batch</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">num_accepted_batch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">new_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particle_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">new_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
                        <span class="n">particle_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">],</span>
                        <span class="n">particles</span><span class="p">,</span>
                        <span class="n">log_weights</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">new_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dists</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">new_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">num_accepted_particles</span> <span class="o">+=</span> <span class="n">num_accepted_batch</span>

            <span class="c1"># If simulation budget was exceeded and we still need particles, take</span>
            <span class="c1"># previous population or fill up with previous population.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span>
                <span class="ow">and</span> <span class="n">num_accepted_particles</span> <span class="o">&lt;</span> <span class="n">num_particles</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">use_last_pop_samples</span><span class="p">:</span>
                    <span class="n">num_remaining</span> <span class="o">=</span> <span class="n">num_particles</span> <span class="o">-</span> <span class="n">num_accepted_particles</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Simulation Budget exceeded, filling up with </span><span class="si">{</span><span class="n">num_remaining</span><span class="si">}</span>
<span class="s2">                        samples from last population.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                    <span class="c1"># Some new particles have been accepted already, therefore</span>
                    <span class="c1"># fill up the remaining once with old particles and weights.</span>
                    <span class="n">new_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">,</span> <span class="p">:])</span>
                    <span class="c1"># Recalculate weights with new particles.</span>
                    <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_particles</span><span class="p">),</span>
                            <span class="n">particles</span><span class="p">,</span>
                            <span class="n">log_weights</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                    <span class="n">new_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">])</span>
                    <span class="n">new_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;Simulation Budget exceeded, returning previous population.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">new_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
                    <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
                    <span class="n">new_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
                    <span class="n">new_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

                <span class="k">break</span>

        <span class="c1"># collect lists of tensors into tensors</span>
        <span class="n">new_particles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_particles</span><span class="p">)</span>
        <span class="n">new_log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_log_weights</span><span class="p">)</span>
        <span class="n">new_distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_distances</span><span class="p">)</span>
        <span class="n">new_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>

        <span class="c1"># normalize the new weights</span>
        <span class="n">new_log_weights</span> <span class="o">-=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">new_log_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Return sorted wrt distances.</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">new_distances</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">new_particles</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_log_weights</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_next_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distances</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">quantile</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return epsilon for next round based on quantile of this round&#39;s distances.</span>

<span class="sd">        Note: distances are made unique to avoid repeated distances from simulations</span>
<span class="sd">        that result in the same observation.</span>

<span class="sd">        Args:</span>
<span class="sd">            distances: The distances accepted in this round.</span>
<span class="sd">            quantile: Quantile in the distance distribution to determine new epsilon.</span>

<span class="sd">        Returns:</span>
<span class="sd">            epsilon: Epsilon for the next population.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Take unique distances to skip same distances simulations (return is sorted).</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="c1"># Cumsum as cdf proxy.</span>
        <span class="n">distances_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">distances</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="c1"># Take the q quantile of distances.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">qidx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">distances_cdf</span> <span class="o">&gt;=</span> <span class="n">quantile</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Accepted unique distances=</span><span class="si">{</span><span class="n">distances</span><span class="si">}</span><span class="s2"> don&#39;t match &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;quantile=</span><span class="si">{</span><span class="n">quantile</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">. Selecting last distance.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">qidx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># The new epsilon is given by that distance.</span>
        <span class="k">return</span> <span class="n">distances</span><span class="p">[</span><span class="n">qidx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_calculate_new_log_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">new_particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">old_particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">old_log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return new log weights following formulas in publications A,B anc C.&quot;&quot;&quot;</span>

        <span class="c1"># Prior can be batched across new particles.</span>
        <span class="n">prior_log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">new_particles</span><span class="p">)</span>

        <span class="c1"># Contstruct function to get kernel log prob for given old particle.</span>
        <span class="c1"># The kernel is centered on each old particle as in all three variants (A,B,C).</span>
        <span class="k">def</span> <span class="nf">kernel_log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_new_kernel</span><span class="p">(</span><span class="n">old_particles</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">)</span>

        <span class="c1"># We still have to loop over particles here because</span>
        <span class="c1"># the kernel log probs are already batched across old particles.</span>
        <span class="n">log_weighted_sum</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">old_log_weights</span> <span class="o">+</span> <span class="n">kernel_log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">new_particle</span> <span class="ow">in</span> <span class="n">new_particles</span>
            <span class="p">],</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># new weights are prior probs over weighted sum:</span>
        <span class="k">return</span> <span class="n">prior_log_probs</span> <span class="o">-</span> <span class="n">log_weighted_sum</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">sample_from_population_with_weights</span><span class="p">(</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return samples from particles sampled with weights.&quot;&quot;&quot;</span>

        <span class="c1"># define multinomial with weights as probs</span>
        <span class="n">multi</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
        <span class="c1"># sample num samples, with replacement</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">multi</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,)))</span>
        <span class="c1"># get indices of success trials</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># return those indices from trace</span>
        <span class="k">return</span> <span class="n">particles</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_sample_and_perturb</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample and perturb batch of new parameters from trace.</span>

<span class="sd">        Reject sampled and perturbed parameters outside of prior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_accepted</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">num_accepted</span> <span class="o">&lt;</span> <span class="n">num_samples</span><span class="p">:</span>
            <span class="n">parms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span> <span class="o">-</span> <span class="n">num_accepted</span>
            <span class="p">)</span>

            <span class="c1"># Create kernel on params and perturb.</span>
            <span class="n">parms_perturbed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_new_kernel</span><span class="p">(</span><span class="n">parms</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

            <span class="n">is_within_prior</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">parms_perturbed</span><span class="p">)</span>
            <span class="n">num_accepted</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">is_within_prior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="k">if</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parms_perturbed</span><span class="p">[</span><span class="n">is_within_prior</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_kernel_variance</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="c1"># For variant C, Beaumont et al. 2009, the kernel variance comes from the</span>
            <span class="c1"># previous population.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">==</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span>
                <span class="c1"># Calculate weighted covariance of particles.</span>
                <span class="n">population_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">aweights</span><span class="o">=</span><span class="n">weights</span><span class="p">)),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># Make sure variance is nonsingular.</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">population_cov</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                        </span><span class="sd">&quot;&quot;&quot;&quot;Singular particle covariance, using unit covariance.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                    <span class="n">population_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">population_cov</span>
            <span class="c1"># While for Toni et al. and Sisson et al. it comes from the parameter</span>
            <span class="c1"># ranges.</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">):</span>
                <span class="n">particle_ranges</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_particle_ranges</span><span class="p">(</span>
                    <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="n">samples_per_dim</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">particle_ranges</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variant, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="c1"># Variance spans the range of parameters for every dimension.</span>
            <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_particle_ranges</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="n">samples_per_dim</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return new kernel distribution for a given set of paramters.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="k">return</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
                <span class="n">loc</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="n">low</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="n">high</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="c1"># Move batch shape to event shape to get Uniform that is multivariate in</span>
            <span class="c1"># parameter dimension.</span>
            <span class="k">return</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resample_if_ess_too_small</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">ess_min</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">pop_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return resampled particles and uniform weights if effectice sampling size is</span>
<span class="sd">        too small.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">log_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_particles</span>
        <span class="c1"># Resampling of weights for low ESS only for Sisson et al. 2007.</span>
        <span class="k">if</span> <span class="n">ess</span> <span class="o">&lt;</span> <span class="n">ess_min</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ESS=</span><span class="si">{</span><span class="n">ess</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> too low, resampling pop </span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
            <span class="c1"># First resample, then set to uniform weights as in Sisson et al. 2007.</span>
            <span class="n">particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_particles</span>
            <span class="p">)</span>
            <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span>

    <span class="k">def</span> <span class="nf">run_lra_update_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">observation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return particles and weights adjusted with LRA.</span>

<span class="sd">        Runs (weighted) linear regression from xs onto particles to adjust the</span>
<span class="sd">        particles.</span>

<span class="sd">        Updates the SMC weights according to the new particles.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">adjusted_particels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">xs</span><span class="p">,</span>
            <span class="n">observation</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">log_weights</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">if</span> <span class="n">lra_with_weights</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Update SMC weights with LRA adjusted weights</span>
        <span class="n">adjusted_log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
            <span class="n">new_particles</span><span class="o">=</span><span class="n">adjusted_particels</span><span class="p">,</span>
            <span class="n">old_particles</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
            <span class="n">old_log_weights</span><span class="o">=</span><span class="n">log_weights</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">adjusted_particels</span><span class="p">,</span> <span class="n">adjusted_log_weights</span>

    <span class="k">def</span> <span class="nf">run_sass_set_xo</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_pilot_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return transform for semi-automatic summary statistics.</span>

<span class="sd">        Runs an single round of rejection abc with fixed budget and accepts</span>
<span class="sd">        num_particles simulations to run the regression for sass.</span>

<span class="sd">        Sets self.x_o once the x_shape can be derived from simulations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="p">(</span>
            <span class="n">pilot_particles</span><span class="p">,</span>
            <span class="n">_</span><span class="p">,</span>
            <span class="n">_</span><span class="p">,</span>
            <span class="n">pilot_xs</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
            <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span>
        <span class="p">)</span>
        <span class="c1"># Adjust with LRA.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="n">pilot_particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">pilot_particles</span><span class="p">,</span> <span class="n">pilot_xs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
            <span class="n">pilot_particles</span><span class="p">,</span>
            <span class="n">pilot_xs</span><span class="p">,</span>
            <span class="n">expansion_degree</span><span class="o">=</span><span class="n">sass_expansion_degree</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sass_transform</span>

    <span class="k">def</span> <span class="nf">get_particle_ranges</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return range of particles in each parameter dimension.&quot;&quot;&quot;</span>

        <span class="c1"># get weighted samples</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span>
            <span class="n">weights</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">samples_per_dim</span> <span class="o">*</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1"># Variance spans the range of particles for every dimension.</span>
        <span class="n">particle_ranges</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
        <span class="k">assert</span> <span class="n">particle_ranges</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">particle_ranges</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.smcabc.SMCABC.__call__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">epsilon_decay</span><span class="p">,</span> <span class="n">distance_based_decay</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ess_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">kde_sample_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lra_with_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_fraction</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Run SMCABC and return accepted parameters or KDE object fitted on them.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_o</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="numpy.ndarray">ndarray</span>]</code>
          </td>
          <td><p>Observed data.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_particles</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of particles in each population.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_initial_pop</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of simulations used for initial population.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_simulations</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Total number of possible simulations.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>epsilon_decay</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Factor with which the acceptance threshold <span class="arithmatex">\(\epsilon\)</span> decays.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>distance_based_decay</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the <span class="arithmatex">\(\epsilon\)</span> decay is constant over
populations or calculated from the previous populations distribution of
distances.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>ess_min</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td><p>Threshold of effective sampling size for resampling weights. Not
used when None (default).</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>kernel_variance_scale</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Factor for scaling the perturbation kernel variance.</p></td>
          <td>
                <code>1.0</code>
          </td>
        </tr>
        <tr>
          <td><code>use_last_pop_samples</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to fill up the current population with
samples from the previous population when the budget is used up. If
False, the current population is discarded and the previous population
is returned.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>lra</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to run linear regression adjustment as in Beaumont et al. 2002</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>lra_with_weights</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to run lra as weighted linear regression with SMC
weights</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>sass</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to determine semi-automatic summary statistics as in
Fearnhead &amp; Prangle 2012.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>sass_fraction</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Fraction of simulation budget used for the initial sass run.</p></td>
          <td>
                <code>0.25</code>
          </td>
        </tr>
        <tr>
          <td><code>sass_expansion_degree</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Degree of the polynomial feature expansion for the
sass regression, default 1 - no expansion.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>kde</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to run KDE on the accepted parameters to return a KDE
object from which one can sample.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>kde_kwargs</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td><p>kwargs for performing KDE:
&lsquo;bandwidth=&rsquo;; either a float, or a string naming a bandwidth
heuristics, e.g., &lsquo;cv&rsquo; (cross validation), &lsquo;silvermann&rsquo; or &lsquo;scott&rsquo;,
default &lsquo;cv&rsquo;.
&lsquo;transform&rsquo;: transform applied to the parameters before doing KDE.
&lsquo;sample_weights&rsquo;: weights associated with samples. See &lsquo;get_kde&rsquo; for
more details</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
        <tr>
          <td><code>kde_sample_weights</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether perform weighted KDE with SMC weights or on raw
particles.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>return_summary</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to return a dictionary with all accepted particles,
weights, etc. at the end.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>theta</code></td>          <td>
                <code>if kde False</code>
          </td>
          <td><p>accepted parameters of the last population.</p></td>
        </tr>
        <tr>
<td><code>kde</code></td>          <td>
                <code>if kde True</code>
          </td>
          <td><p>KDE object fitted on accepted parameters, from which one
can .sample() and .log_prob().</p></td>
        </tr>
        <tr>
<td><code>summary</code></td>          <td>
                <code>if return_summary True</code>
          </td>
          <td><p>dictionary containing the accepted
paramters (if kde True), distances and simulated data x of all
populations.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
    <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">epsilon_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">distance_based_decay</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">ess_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">kde_sample_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run SMCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_o: Observed data.</span>
<span class="sd">        num_particles: Number of particles in each population.</span>
<span class="sd">        num_initial_pop: Number of simulations used for initial population.</span>
<span class="sd">        num_simulations: Total number of possible simulations.</span>
<span class="sd">        epsilon_decay: Factor with which the acceptance threshold $\epsilon$ decays.</span>
<span class="sd">        distance_based_decay: Whether the $\epsilon$ decay is constant over</span>
<span class="sd">            populations or calculated from the previous populations distribution of</span>
<span class="sd">            distances.</span>
<span class="sd">        ess_min: Threshold of effective sampling size for resampling weights. Not</span>
<span class="sd">            used when None (default).</span>
<span class="sd">        kernel_variance_scale: Factor for scaling the perturbation kernel variance.</span>
<span class="sd">        use_last_pop_samples: Whether to fill up the current population with</span>
<span class="sd">            samples from the previous population when the budget is used up. If</span>
<span class="sd">            False, the current population is discarded and the previous population</span>
<span class="sd">            is returned.</span>
<span class="sd">        lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">        lra_with_weights: Whether to run lra as weighted linear regression with SMC</span>
<span class="sd">            weights</span>
<span class="sd">        sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">            Fearnhead &amp; Prangle 2012.</span>
<span class="sd">        sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">        sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">            sass regression, default 1 - no expansion.</span>
<span class="sd">        kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">            object from which one can sample.</span>
<span class="sd">        kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">            &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">            heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">            default &#39;cv&#39;.</span>
<span class="sd">            &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">            &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">            more details</span>
<span class="sd">        kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw</span>
<span class="sd">            particles.</span>
<span class="sd">        return_summary: Whether to return a dictionary with all accepted particles,</span>
<span class="sd">            weights, etc. at the end.</span>

<span class="sd">    Returns:</span>
<span class="sd">        theta (if kde False): accepted parameters of the last population.</span>
<span class="sd">        kde (if kde True): KDE object fitted on accepted parameters, from which one</span>
<span class="sd">            can .sample() and .log_prob().</span>
<span class="sd">        summary (if return_summary True): dictionary containing the accepted</span>
<span class="sd">            paramters (if kde True), distances and simulated data x of all</span>
<span class="sd">            populations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">pop_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="n">num_simulations</span>

    <span class="c1"># Pilot run for SASS.</span>
    <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
        <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Running SASS with </span><span class="si">{</span><span class="n">num_pilot_simulations</span><span class="si">}</span><span class="s2"> pilot samples.&quot;</span>
        <span class="p">)</span>
        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_sass_set_xo</span><span class="p">(</span>
            <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">lra</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
        <span class="p">)</span>
        <span class="c1"># Udpate simulator and xo</span>
        <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">sass_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">sass_simulator</span>

    <span class="c1"># run initial population</span>
    <span class="n">particles</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span>
    <span class="p">)</span>
    <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">, ess=</span><span class="si">{</span><span class="mf">1.0</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;num_sims=</span><span class="si">{</span><span class="n">num_initial_pop</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="n">all_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
    <span class="n">all_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
    <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
    <span class="n">all_epsilons</span> <span class="o">=</span> <span class="p">[</span><span class="n">epsilon</span><span class="p">]</span>
    <span class="n">all_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span><span class="p">:</span>
        <span class="n">pop_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Decay based on quantile of distances from previous pop.</span>
        <span class="k">if</span> <span class="n">distance_based_decay</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_epsilon</span><span class="p">(</span>
                <span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">epsilon_decay</span>
            <span class="p">)</span>
        <span class="c1"># Constant decay.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">*=</span> <span class="n">epsilon_decay</span>

        <span class="c1"># Get kernel variance from previous pop.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_kernel_variance</span><span class="p">(</span>
            <span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="n">kernel_variance_scale</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_next_population</span><span class="p">(</span>
            <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="n">use_last_pop_samples</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Resample population if effective sampling size is too small.</span>
        <span class="k">if</span> <span class="n">ess_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_if_ess_too_small</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2"> done: eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; num_sims=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># collect results</span>
        <span class="n">all_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">)</span>
        <span class="n">all_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
        <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">all_epsilons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="n">all_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Maybe run LRA and adjust weights.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
        <span class="n">adjusted_particles</span><span class="p">,</span> <span class="n">adjusted_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra_update_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">observation</span><span class="o">=</span><span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">),</span>
            <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">lra_with_weights</span><span class="o">=</span><span class="n">lra_with_weights</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">final_particles</span> <span class="o">=</span> <span class="n">adjusted_particles</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">final_particles</span> <span class="o">=</span> <span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&quot;&quot;KDE on </span><span class="si">{</span><span class="n">final_particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples with bandwidth option</span>
<span class="s2">            </span><span class="si">{</span><span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">]</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="s2">&quot;bandwidth&quot;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">kde_kwargs</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">&quot;cv&quot;</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">            Beware that KDE can give unreliable results when used with too few</span>
<span class="s2">            samples and in high dimensions.&quot;&quot;&quot;</span>
        <span class="p">)</span>
        <span class="c1"># Maybe get particles weights from last population for weighted KDE.</span>
        <span class="k">if</span> <span class="n">kde_sample_weights</span><span class="p">:</span>
            <span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;sample_weights&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>

        <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_particles</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">kde_dist</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">(</span>
                    <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                    <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                    <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                    <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kde_dist</span>

    <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">final_particles</span><span class="p">,</span>
            <span class="nb">dict</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_particles</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.smcabc.SMCABC.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">algorithm_variant</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Sequential Monte Carlo Approximate Bayesian Computation.</p>

<details class="we-distinguish-between-three-different-smc-methods-here" open>
  <summary>We distinguish between three different SMC methods here</summary>
  <ul>
<li>A: Toni et al. 2010 (Phd Thesis)</li>
<li>B: Sisson et al. 2007 (with correction from 2009)</li>
<li>C: Beaumont et al. 2009</li>
</ul>
</details>      <p>In Toni et al. 2010 we find an overview of the differences on page 34:
    - B: same as A except for resampling of weights if the effective sampling
        size is too small.
    - C: same as A except for calculation of the covariance of the perturbation
        kernel: the kernel covariance is a scaled version of the covariance of
        the previous population.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>simulator</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\mathrm{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="torch.distributions.Distribution">Distribution</span></code>
          </td>
          <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>distance</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Distance function to compare observed and simulated data. Can be
a custom function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>.</p></td>
          <td>
                <code>&#39;l2&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>num_workers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of parallel workers to use for simulations.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>simulation_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>kernel</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Perturbation kernel.</p></td>
          <td>
                <code>&#39;gaussian&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>algorithm_variant</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Indicating the choice of algorithm variant, A, B, or C.</p></td>
          <td>
                <code>&#39;C&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="n">algorithm_variant</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Monte Carlo Approximate Bayesian Computation.</span>

<span class="sd">    We distinguish between three different SMC methods here:</span>
<span class="sd">        - A: Toni et al. 2010 (Phd Thesis)</span>
<span class="sd">        - B: Sisson et al. 2007 (with correction from 2009)</span>
<span class="sd">        - C: Beaumont et al. 2009</span>

<span class="sd">    In Toni et al. 2010 we find an overview of the differences on page 34:</span>
<span class="sd">        - B: same as A except for resampling of weights if the effective sampling</span>
<span class="sd">            size is too small.</span>
<span class="sd">        - C: same as A except for calculation of the covariance of the perturbation</span>
<span class="sd">            kernel: the kernel covariance is a scaled version of the covariance of</span>
<span class="sd">            the previous population.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">            a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        kernel: Perturbation kernel.</span>
<span class="sd">        algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">kernels</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Kernel &#39;</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported. Choose one from </span><span class="si">{</span><span class="n">kernels</span><span class="si">}</span><span class="s2">.&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

    <span class="n">algorithm_variants</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="n">algorithm_variants</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;SMCABC variant &#39;</span><span class="si">{</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported, choose one from&quot;</span>
        <span class="s2">&quot; </span><span class="si">{algorithm_variants}</span><span class="s2">.&quot;</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">=</span> <span class="n">algorithm_variant</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">distance_to_x0</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Define simulator that keeps track of budget.</span>
    <span class="k">def</span> <span class="nf">simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">simulate_with_budget</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.smcabc.SMCABC.get_new_kernel" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">get_new_kernel</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.get_new_kernel" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return new kernel distribution for a given set of paramters.</p>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return new kernel distribution for a given set of paramters.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
        <span class="n">low</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="c1"># Move batch shape to event shape to get Uniform that is multivariate in</span>
        <span class="c1"># parameter dimension.</span>
        <span class="k">return</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.smcabc.SMCABC.get_particle_ranges" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">get_particle_ranges</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.get_particle_ranges" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return range of particles in each parameter dimension.</p>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_particle_ranges</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return range of particles in each parameter dimension.&quot;&quot;&quot;</span>

    <span class="c1"># get weighted samples</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
        <span class="n">particles</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">samples_per_dim</span> <span class="o">*</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="c1"># Variance spans the range of particles for every dimension.</span>
    <span class="n">particle_ranges</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="k">assert</span> <span class="n">particle_ranges</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">particle_ranges</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">resample_if_ess_too_small</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return resampled particles and uniform weights if effectice sampling size is
too small.</p>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">resample_if_ess_too_small</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">ess_min</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">pop_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return resampled particles and uniform weights if effectice sampling size is</span>
<span class="sd">    too small.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">log_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_particles</span>
    <span class="c1"># Resampling of weights for low ESS only for Sisson et al. 2007.</span>
    <span class="k">if</span> <span class="n">ess</span> <span class="o">&lt;</span> <span class="n">ess_min</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ESS=</span><span class="si">{</span><span class="n">ess</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> too low, resampling pop </span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="c1"># First resample, then set to uniform weights as in Sisson et al. 2007.</span>
        <span class="n">particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_particles</span>
        <span class="p">)</span>
        <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.smcabc.SMCABC.run_lra_update_weights" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">run_lra_update_weights</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">observation</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">lra_with_weights</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.run_lra_update_weights" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return particles and weights adjusted with LRA.</p>
<p>Runs (weighted) linear regression from xs onto particles to adjust the
particles.</p>
<p>Updates the SMC weights according to the new particles.</p>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">run_lra_update_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">observation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return particles and weights adjusted with LRA.</span>

<span class="sd">    Runs (weighted) linear regression from xs onto particles to adjust the</span>
<span class="sd">    particles.</span>

<span class="sd">    Updates the SMC weights according to the new particles.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">adjusted_particels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="n">xs</span><span class="p">,</span>
        <span class="n">observation</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">log_weights</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">if</span> <span class="n">lra_with_weights</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Update SMC weights with LRA adjusted weights</span>
    <span class="n">adjusted_log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
        <span class="n">new_particles</span><span class="o">=</span><span class="n">adjusted_particels</span><span class="p">,</span>
        <span class="n">old_particles</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
        <span class="n">old_log_weights</span><span class="o">=</span><span class="n">log_weights</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">adjusted_particels</span><span class="p">,</span> <span class="n">adjusted_log_weights</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.smcabc.SMCABC.run_sass_set_xo" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">run_sass_set_xo</span><span class="p">(</span><span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.run_sass_set_xo" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return transform for semi-automatic summary statistics.</p>
<p>Runs an single round of rejection abc with fixed budget and accepts
num_particles simulations to run the regression for sass.</p>
<p>Sets self.x_o once the x_shape can be derived from simulations.</p>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">run_sass_set_xo</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_pilot_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return transform for semi-automatic summary statistics.</span>

<span class="sd">    Runs an single round of rejection abc with fixed budget and accepts</span>
<span class="sd">    num_particles simulations to run the regression for sass.</span>

<span class="sd">    Sets self.x_o once the x_shape can be derived from simulations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="p">(</span>
        <span class="n">pilot_particles</span><span class="p">,</span>
        <span class="n">_</span><span class="p">,</span>
        <span class="n">_</span><span class="p">,</span>
        <span class="n">pilot_xs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span>
    <span class="p">)</span>
    <span class="c1"># Adjust with LRA.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="n">pilot_particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">pilot_particles</span><span class="p">,</span> <span class="n">pilot_xs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
    <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
        <span class="n">pilot_particles</span><span class="p">,</span>
        <span class="n">pilot_xs</span><span class="p">,</span>
        <span class="n">expansion_degree</span><span class="o">=</span><span class="n">sass_expansion_degree</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">sass_transform</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sample_from_population_with_weights</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return samples from particles sampled with weights.</p>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/abc/smcabc.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">sample_from_population_with_weights</span><span class="p">(</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return samples from particles sampled with weights.&quot;&quot;&quot;</span>

    <span class="c1"># define multinomial with weights as probs</span>
    <span class="n">multi</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="c1"># sample num samples, with replacement</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">multi</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,)))</span>
    <span class="c1"># get indices of success trials</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># return those indices from trace</span>
    <span class="k">return</span> <span class="n">particles</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="posteriors">Posteriors<a class="headerlink" href="#posteriors" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-class">



<h3 id="sbi.inference.posteriors.direct_posterior.DirectPosterior" class="doc doc-heading">
        <code>sbi.inference.posteriors.direct_posterior.DirectPosterior</code>


<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>

  
      <p>Posterior <span class="arithmatex">\(p(\theta|x_o)\)</span> with <code>log_prob()</code> and <code>sample()</code> methods, only
applicable to SNPE.<br/><br/>
SNPE trains a neural network to directly approximate the posterior distribution.
However, for bounded priors, the neural network can have leakage: it puts non-zero
mass in regions where the prior is zero. The <code>DirectPosterior</code> class wraps the
trained network to deal with these cases.<br/><br/>
Specifically, this class offers the following functionality:<br/>
- correct the calculation of the log probability such that it compensates for the
  leakage.<br/>
- reject samples that lie outside of the prior bounds.<br/><br/>
This class can not be used in combination with SNLE or SNRE.</p>


        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DirectPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Posterior $p(\theta|x_o)$ with `log_prob()` and `sample()` methods, only</span>
<span class="sd">    applicable to SNPE.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNPE trains a neural network to directly approximate the posterior distribution.</span>
<span class="sd">    However, for bounded priors, the neural network can have leakage: it puts non-zero</span>
<span class="sd">    mass in regions where the prior is zero. The `DirectPosterior` class wraps the</span>
<span class="sd">    trained network to deal with these cases.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    Specifically, this class offers the following functionality:&lt;br/&gt;</span>
<span class="sd">    - correct the calculation of the log probability such that it compensates for the</span>
<span class="sd">      leakage.&lt;br/&gt;</span>
<span class="sd">    - reject samples that lie outside of the prior bounds.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    This class can not be used in combination with SNLE or SNRE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">posterior_estimator</span><span class="p">:</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">            posterior_estimator: The trained neural posterior.</span>
<span class="sd">            max_sampling_batch_size: Batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">            enable_transform: Whether to transform parameters to unconstrained space</span>
<span class="sd">                during MAP optimization. When False, an identity transform will be</span>
<span class="sd">                returned for `theta_transform`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Because `DirectPosterior` does not take the `potential_fn` as input, it</span>
        <span class="c1"># builds it itself. The `potential_fn` and `theta_transform` are used only for</span>
        <span class="c1"># obtaining the MAP.</span>
        <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
        <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">posterior_estimator_based_potential</span><span class="p">(</span>
            <span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">prior</span><span class="p">,</span>
            <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">posterior_estimator</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;It samples the posterior network and rejects samples that</span>
<span class="s2">            lie outside of the prior bounds.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">accept_reject_sample</span><span class="p">(</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
            <span class="n">alternative_method</span><span class="o">=</span><span class="s2">&quot;build_posterior(..., sample_with=&#39;mcmc&#39;)&quot;</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">samples</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">leakage_correction_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of the posterior $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">                Renormalization of the posterior is useful when some</span>
<span class="sd">                probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">                The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">                need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">                `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">                - outside of the prior support regardless of this setting.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>
<span class="sd">            leakage_correction_params: A `dict` of keyword arguments to override the</span>
<span class="sd">                default values of `leakage_correction()`. Possible options are:</span>
<span class="sd">                `num_rejection_samples`, `force_update`, `show_progress_bars`, and</span>
<span class="sd">                `rejection_sampling_batch_size`.</span>
<span class="sd">                These parameters only have an effect if `norm_posterior=True`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `(len(),)`-shaped log posterior probability $\log p(\theta|x)$ for  in the</span>
<span class="sd">            support of the prior, - (corresponding to 0 probability) outside.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># TODO Train exited here, entered after sampling?</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="n">theta_repeated</span><span class="p">,</span> <span class="n">x_repeated</span> <span class="o">=</span> <span class="n">match_theta_and_x_batch_shapes</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="c1"># Evaluate on device, move back to cpu for comparison with prior.</span>
            <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
                <span class="n">theta_repeated</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">x_repeated</span>
            <span class="p">)</span>

            <span class="c1"># Force probability to be zero outside prior support.</span>
            <span class="n">in_prior_support</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta_repeated</span><span class="p">)</span>

            <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">in_prior_support</span><span class="p">,</span>
                <span class="n">unnorm_log_prob</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">leakage_correction_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">leakage_correction_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
            <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">leakage_correction_params</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">norm_posterior</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">leakage_correction</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">num_rejection_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">rejection_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return leakage correction factor for a leaky posterior density estimate.</span>

<span class="sd">        The factor is estimated from the acceptance probability during rejection</span>
<span class="sd">        sampling from the posterior.</span>

<span class="sd">        This is to avoid re-estimating the acceptance probability from scratch</span>
<span class="sd">        whenever `log_prob` is called and `norm_posterior=True`. Here, it</span>
<span class="sd">        is estimated only once for `self.default_x` and saved for later. We</span>
<span class="sd">        re-evaluate only whenever a new `x` is passed.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            num_rejection_samples: Number of samples used to estimate correction factor.</span>
<span class="sd">            show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">            rejection_sampling_batch_size: Batch size for rejection sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Saved or newly-estimated correction factor (as a scalar `Tensor`).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">accept_reject_sample</span><span class="p">(</span>
                <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
                <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_rejection_samples</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="n">sample_for_correction_factor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">rejection_sampling_batch_size</span><span class="p">,</span>
                <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
            <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
        <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
            <span class="k">return</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;posterior&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from the</span>
<span class="sd">                posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="torch.distributions.Distribution">Distribution</span></code>
          </td>
          <td><p>Prior distribution with <code>.log_prob()</code> and <code>.sample()</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>posterior_estimator</code></td>
          <td>
                <code><span title="pyknos.nflows.flows">flows</span>.<span title="pyknos.nflows.flows.Flow">Flow</span></code>
          </td>
          <td><p>The trained neural posterior.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>max_sampling_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Batchsize of samples being drawn from
the proposal at every iteration.</p></td>
          <td>
                <code>10000</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>x_shape</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[torch.<span title="torch.Size">Size</span>]</code>
          </td>
          <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>enable_transform</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to transform parameters to unconstrained space
during MAP optimization. When False, an identity transform will be
returned for <code>theta_transform</code>.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/direct_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">posterior_estimator</span><span class="p">:</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">        posterior_estimator: The trained neural posterior.</span>
<span class="sd">        max_sampling_batch_size: Batchsize of samples being drawn from</span>
<span class="sd">            the proposal at every iteration.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">        enable_transform: Whether to transform parameters to unconstrained space</span>
<span class="sd">            during MAP optimization. When False, an identity transform will be</span>
<span class="sd">            returned for `theta_transform`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Because `DirectPosterior` does not take the `potential_fn` as input, it</span>
    <span class="c1"># builds it itself. The `potential_fn` and `theta_transform` are used only for</span>
    <span class="c1"># obtaining the MAP.</span>
    <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
    <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">posterior_estimator_based_potential</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">posterior_estimator</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;It samples the posterior network and rejects samples that</span>
<span class="s2">        lie outside of the prior bounds.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_rejection_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rejection_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return leakage correction factor for a leaky posterior density estimate.</p>
<p>The factor is estimated from the acceptance probability during rejection
sampling from the posterior.</p>
<p>This is to avoid re-estimating the acceptance probability from scratch
whenever <code>log_prob</code> is called and <code>norm_posterior=True</code>. Here, it
is estimated only once for <code>self.default_x</code> and saved for later. We
re-evaluate only whenever a new <code>x</code> is passed.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>num_rejection_samples</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of samples used to estimate correction factor.</p></td>
          <td>
                <code>10000</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progress bar during sampling.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>rejection_sampling_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Batch size for rejection sampling.</p></td>
          <td>
                <code>10000</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Saved or newly-estimated correction factor (as a scalar <code>Tensor</code>).</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/direct_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">leakage_correction</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_rejection_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">rejection_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return leakage correction factor for a leaky posterior density estimate.</span>

<span class="sd">    The factor is estimated from the acceptance probability during rejection</span>
<span class="sd">    sampling from the posterior.</span>

<span class="sd">    This is to avoid re-estimating the acceptance probability from scratch</span>
<span class="sd">    whenever `log_prob` is called and `norm_posterior=True`. Here, it</span>
<span class="sd">    is estimated only once for `self.default_x` and saved for later. We</span>
<span class="sd">    re-evaluate only whenever a new `x` is passed.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        num_rejection_samples: Number of samples used to estimate correction factor.</span>
<span class="sd">        show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">        rejection_sampling_batch_size: Batch size for rejection sampling.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Saved or newly-estimated correction factor (as a scalar `Tensor`).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">accept_reject_sample</span><span class="p">(</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_rejection_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">sample_for_correction_factor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">rejection_sampling_batch_size</span><span class="p">,</span>
            <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
        <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
    <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
        <span class="k">return</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">leakage_correction_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the log-probability of the posterior <span class="arithmatex">\(p(\theta|x)\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>theta</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>norm_posterior</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to enforce a normalized posterior density.
Renormalization of the posterior is useful when some
probability falls out or leaks out of the prescribed prior support.
The normalizing factor is calculated via rejection sampling, so if you
need speedier but unnormalized log posterior estimates set here
<code>norm_posterior=False</code>. The returned log posterior is set to
- outside of the prior support regardless of this setting.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>track_gradients</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>leakage_correction_params</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[dict]</code>
          </td>
          <td><p>A <code>dict</code> of keyword arguments to override the
default values of <code>leakage_correction()</code>. Possible options are:
<code>num_rejection_samples</code>, <code>force_update</code>, <code>show_progress_bars</code>, and
<code>rejection_sampling_batch_size</code>.
These parameters only have an effect if <code>norm_posterior=True</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>(len(),)</code>-shaped log posterior probability <span class="arithmatex">\(\log p(\theta|x)\)</span> for  in the</p></td>
        </tr>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>support of the prior, - (corresponding to 0 probability) outside.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/direct_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">leakage_correction_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of the posterior $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">            Renormalization of the posterior is useful when some</span>
<span class="sd">            probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">            The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">            need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">            `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">            - outside of the prior support regardless of this setting.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">        leakage_correction_params: A `dict` of keyword arguments to override the</span>
<span class="sd">            default values of `leakage_correction()`. Possible options are:</span>
<span class="sd">            `num_rejection_samples`, `force_update`, `show_progress_bars`, and</span>
<span class="sd">            `rejection_sampling_batch_size`.</span>
<span class="sd">            These parameters only have an effect if `norm_posterior=True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(),)`-shaped log posterior probability $\log p(\theta|x)$ for  in the</span>
<span class="sd">        support of the prior, - (corresponding to 0 probability) outside.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># TODO Train exited here, entered after sampling?</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="n">theta_repeated</span><span class="p">,</span> <span class="n">x_repeated</span> <span class="o">=</span> <span class="n">match_theta_and_x_batch_shapes</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="c1"># Evaluate on device, move back to cpu for comparison with prior.</span>
        <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="n">theta_repeated</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">x_repeated</span>
        <span class="p">)</span>

        <span class="c1"># Force probability to be zero outside prior support.</span>
        <span class="n">in_prior_support</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta_repeated</span><span class="p">)</span>

        <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">in_prior_support</span><span class="p">,</span>
            <span class="n">unnorm_log_prob</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">leakage_correction_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">leakage_correction_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
        <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">leakage_correction_params</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">norm_posterior</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.map" class="doc doc-heading">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.map" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_iter</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate of the optimizer.</p></td>
          <td>
                <code>0.01</code>
          </td>
        </tr>
        <tr>
          <td><code>init_method</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
          <td>
                <code>&#39;posterior&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>num_init_samples</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>num_to_optimize</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>save_best_every</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during sampling from the
posterior.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>force_update</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>log_prob_kwargs</code></td>
          <td>
          </td>
          <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The MAP estimate.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/direct_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;posterior&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from the</span>
<span class="sd">            posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.sample" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return samples from posterior distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sample_shape</code></td>
          <td>
                <code><span title="sbi.types.Shape">Shape</span></code>
          </td>
          <td><p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p></td>
          <td>
                <code>torch.Size()</code>
          </td>
        </tr>
        <tr>
          <td><code>sample_with</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show sampling progress monitor.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/direct_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
        <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="n">accept_reject_sample</span><span class="p">(</span>
        <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
        <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
        <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
        <span class="n">alternative_method</span><span class="o">=</span><span class="s2">&quot;build_posterior(..., sample_with=&#39;mcmc&#39;)&quot;</span><span class="p">,</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior" class="doc doc-heading">
        <code>sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior</code>


<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>

  
      <p>Provides importance sampling to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>ImportanceSamplingPosterior</code> allows to estimate the posterior log-probability by
estimating the normlalization constant with importance sampling. It also allows to
perform importance sampling (with <code>.sample()</code>) and to draw approximate samples with
sampling-importance-resampling (SIR) (with <code>.sir_sample()</code>)</p>


        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/importance_posterior.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ImportanceSamplingPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides importance sampling to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `ImportanceSamplingPosterior` allows to estimate the posterior log-probability by</span>
<span class="sd">    estimating the normlalization constant with importance sampling. It also allows to</span>
<span class="sd">    perform importance sampling (with `.sample()`) and to draw approximate samples with</span>
<span class="sd">    sampling-importance-resampling (SIR) (with `.sir_sample()`)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sir&quot;</span><span class="p">,</span>
        <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples.</span>
<span class="sd">            proposal: The proposal distribution.</span>
<span class="sd">            theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">                during but only when calling `.map()`.</span>
<span class="sd">            method: Either of [`sir`|`importance`]. This sets the behavior of the</span>
<span class="sd">                `.sample()` method. With `sir`, approximate posterior samples are</span>
<span class="sd">                generated with sampling importance resampling (SIR). With</span>
<span class="sd">                `importance`, the `.sample()` method returns a tuple of samples and</span>
<span class="sd">                corresponding importance weights.</span>
<span class="sd">            oversampling_factor: Number of proposed samples from which only one is</span>
<span class="sd">                selected based on its importance weight.</span>
<span class="sd">            max_sampling_batch_size: The batch size of samples being drawn from the</span>
<span class="sd">                proposal at every iteration.</span>
<span class="sd">            device: Device on which to sample, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If</span>
<span class="sd">                None, `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oversampling_factor</span> <span class="o">=</span> <span class="n">oversampling_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides sampling-importance resampling (SIR) to .sample() from the &quot;</span>
            <span class="s2">&quot;posterior and can evaluate the _unnormalized_ posterior density with &quot;</span>
            <span class="s2">&quot;.log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">normalization_constant_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">        The normalization constant is estimated with importance sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>
<span class="sd">            normalization_constant_params: Parameters passed on to</span>
<span class="sd">                `estimate_normalization_constant()`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="n">potential_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
                <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">normalization_constant_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">normalization_constant_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
            <span class="n">normalization_constant</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimate_normalization_constant</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">normalization_constant_params</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="p">(</span><span class="n">potential_values</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">normalization_constant</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
            <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">estimate_normalization_constant</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span> <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the normalization constant via importance sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Number of importance samples used for the estimate.</span>
<span class="sd">            force_update: Whether to re-calculate the normlization constant when x is</span>
<span class="sd">                unchanged and have a cached value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
        <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_importance_weights</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_importance_weights</span><span class="p">))</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return samples from the approximate posterior distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: _description_</span>
<span class="sd">            x: _description_</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sir_sample</span><span class="p">(</span>
                <span class="n">sample_shape</span><span class="p">,</span>
                <span class="n">oversampling_factor</span><span class="o">=</span><span class="n">oversampling_factor</span><span class="p">,</span>
                <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;importance&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_importance_sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NameError</span>

    <span class="k">def</span> <span class="nf">_importance_sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns samples from the proposal and log of their importance weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples and logarithm of corresponding importance weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">samples</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span><span class="p">,</span> <span class="n">log_importance_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sir_sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns approximate samples from posterior $p(\theta|x)$ via SIR.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            x: Observed data.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            oversampling_factor: Number of proposed samples form which only one is</span>
<span class="sd">                selected based on its importance weight.</span>
<span class="sd">            max_sampling_batch_size: The batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration. Used only in `sir_sample()`.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">oversampling_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oversampling_factor</span>
            <span class="k">if</span> <span class="n">oversampling_factor</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">oversampling_factor</span>
        <span class="p">)</span>
        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>

        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">sampling_importance_resampling</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">oversampling_factor</span><span class="o">=</span><span class="n">oversampling_factor</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from the</span>
<span class="sd">                posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;sir&#39;</span><span class="p">,</span> <span class="n">oversampling_factor</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>potential_fn</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>The potential function from which to draw samples.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>proposal</code></td>
          <td>
                <code><span title="typing.Any">Any</span></code>
          </td>
          <td><p>The proposal distribution.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>theta_transform</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchTransform">TorchTransform</span>]</code>
          </td>
          <td><p>Transformation that is applied to parameters. Is not used
during but only when calling <code>.map()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>method</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Either of [<code>sir</code>|<code>importance</code>]. This sets the behavior of the
<code>.sample()</code> method. With <code>sir</code>, approximate posterior samples are
generated with sampling importance resampling (SIR). With
<code>importance</code>, the <code>.sample()</code> method returns a tuple of samples and
corresponding importance weights.</p></td>
          <td>
                <code>&#39;sir&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>oversampling_factor</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of proposed samples from which only one is
selected based on its importance weight.</p></td>
          <td>
                <code>32</code>
          </td>
        </tr>
        <tr>
          <td><code>max_sampling_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The batch size of samples being drawn from the
proposal at every iteration.</p></td>
          <td>
                <code>10000</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Device on which to sample, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If
None, <code>potential_fn.device</code> is used.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>x_shape</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[torch.<span title="torch.Size">Size</span>]</code>
          </td>
          <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/importance_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sir&quot;</span><span class="p">,</span>
    <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples.</span>
<span class="sd">        proposal: The proposal distribution.</span>
<span class="sd">        theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">            during but only when calling `.map()`.</span>
<span class="sd">        method: Either of [`sir`|`importance`]. This sets the behavior of the</span>
<span class="sd">            `.sample()` method. With `sir`, approximate posterior samples are</span>
<span class="sd">            generated with sampling importance resampling (SIR). With</span>
<span class="sd">            `importance`, the `.sample()` method returns a tuple of samples and</span>
<span class="sd">            corresponding importance weights.</span>
<span class="sd">        oversampling_factor: Number of proposed samples from which only one is</span>
<span class="sd">            selected based on its importance weight.</span>
<span class="sd">        max_sampling_batch_size: The batch size of samples being drawn from the</span>
<span class="sd">            proposal at every iteration.</span>
<span class="sd">        device: Device on which to sample, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If</span>
<span class="sd">            None, `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">oversampling_factor</span> <span class="o">=</span> <span class="n">oversampling_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides sampling-importance resampling (SIR) to .sample() from the &quot;</span>
        <span class="s2">&quot;posterior and can evaluate the _unnormalized_ posterior density with &quot;</span>
        <span class="s2">&quot;.log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.estimate_normalization_constant" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">estimate_normalization_constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.estimate_normalization_constant" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the normalization constant via importance sampling.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>num_samples</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of importance samples used for the estimate.</p></td>
          <td>
                <code>10000</code>
          </td>
        </tr>
        <tr>
          <td><code>force_update</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to re-calculate the normlization constant when x is
unchanged and have a cached value.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/importance_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">estimate_normalization_constant</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span> <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the normalization constant via importance sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_samples: Number of importance samples used for the estimate.</span>
<span class="sd">        force_update: Whether to re-calculate the normlization constant when x is</span>
<span class="sd">            unchanged and have a cached value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
    <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_importance_weights</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_importance_weights</span><span class="p">))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.log_prob" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">normalization_constant_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the log-probability of theta under the posterior.</p>
<p>The normalization constant is estimated with importance sampling.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>theta</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>track_gradients</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>normalization_constant_params</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[dict]</code>
          </td>
          <td><p>Parameters passed on to
<code>estimate_normalization_constant()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>len($\theta$)</code>-shaped log-probability.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/importance_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">normalization_constant_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">    The normalization constant is estimated with importance sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">        normalization_constant_params: Parameters passed on to</span>
<span class="sd">            `estimate_normalization_constant()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="n">potential_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">normalization_constant_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">normalization_constant_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
        <span class="n">normalization_constant</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimate_normalization_constant</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">normalization_constant_params</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">potential_values</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">normalization_constant</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.map" class="doc doc-heading">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.map" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_iter</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate of the optimizer.</p></td>
          <td>
                <code>0.01</code>
          </td>
        </tr>
        <tr>
          <td><code>init_method</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
          <td>
                <code>&#39;proposal&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>num_init_samples</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>num_to_optimize</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>save_best_every</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during sampling from the
posterior.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>force_update</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>log_prob_kwargs</code></td>
          <td>
          </td>
          <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The MAP estimate.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/importance_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from the</span>
<span class="sd">            posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.sample" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">oversampling_factor</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return samples from the approximate posterior distribution.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sample_shape</code></td>
          <td>
                <code><span title="sbi.types.Shape">Shape</span></code>
          </td>
          <td><p><em>description</em></p></td>
          <td>
                <code>torch.Size()</code>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p><em>description</em></p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/importance_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return samples from the approximate posterior distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: _description_</span>
<span class="sd">        x: _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sir_sample</span><span class="p">(</span>
            <span class="n">sample_shape</span><span class="p">,</span>
            <span class="n">oversampling_factor</span><span class="o">=</span><span class="n">oversampling_factor</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;importance&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_importance_sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NameError</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="doc doc-heading">
        <code>sbi.inference.posteriors.mcmc_posterior.MCMCPosterior</code>


<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>

  
      <p>Provides MCMC to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>MCMCPosterior</code> allows to sample from the posterior with MCMC.</p>


        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MCMCPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides MCMC to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `MCMCPosterior` allows to sample from the posterior with MCMC.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;resample&quot;</span><span class="p">,</span>
        <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples.</span>
<span class="sd">            proposal: Proposal distribution that is used to initialize the MCMC chain.</span>
<span class="sd">            theta_transform: Transformation that will be applied during sampling.</span>
<span class="sd">                Allows to perform MCMC in unconstrained space.</span>
<span class="sd">            method: Method used for MCMC sampling, one of `slice_np`,</span>
<span class="sd">                `slice_np_vectorized`, `slice`, `hmc`, `nuts`. `slice_np` is a custom</span>
<span class="sd">                numpy implementation of slice sampling. `slice_np_vectorized` is</span>
<span class="sd">                identical to `slice_np`, but if `num_chains&gt;1`, the chains are</span>
<span class="sd">                vectorized for `slice_np_vectorized` whereas they are run sequentially</span>
<span class="sd">                for `slice_np`. The samplers `hmc`, `nuts` or `slice` sample with Pyro.</span>
<span class="sd">            thin: The thinning factor for the chain.</span>
<span class="sd">            warmup_steps: The initial number of samples to discard.</span>
<span class="sd">            num_chains: The number of chains.</span>
<span class="sd">            init_strategy: The initialisation strategy for chains; `proposal` will draw</span>
<span class="sd">                init locations from `proposal`, whereas `sir` will use Sequential-</span>
<span class="sd">                Importance-Resampling (SIR). SIR initially samples</span>
<span class="sd">                `init_strategy_num_candidates` from the `proposal`, evaluates all of</span>
<span class="sd">                them under the `potential_fn` and `proposal`, and then resamples the</span>
<span class="sd">                initial locations with weights proportional to `exp(potential_fn -</span>
<span class="sd">                proposal.log_prob`. `resample` is the same as `sir` but</span>
<span class="sd">                uses `exp(potential_fn)` as weights.</span>
<span class="sd">            init_strategy_parameters: Dictionary of keyword arguments passed to the</span>
<span class="sd">                init strategy, e.g., for `init_strategy=sir` this could be</span>
<span class="sd">                `num_candidate_samples`, i.e., the number of candidates to to find init</span>
<span class="sd">                locations (internal default is `1000`), or `device`.</span>
<span class="sd">            init_strategy_num_candidates: Number of candidates to to find init</span>
<span class="sd">                 locations in `init_strategy=sir` (deprecated, use</span>
<span class="sd">                 init_strategy_parameters instead).</span>
<span class="sd">            num_workers: number of cpu cores used to parallelize mcmc</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="n">thin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="o">=</span> <span class="n">num_chains</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="o">=</span> <span class="n">init_strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="n">init_strategy_parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Hardcode parameter name to reduce clutter kwargs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_name</span> <span class="o">=</span> <span class="s2">&quot;theta&quot;</span>

        <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;Passing `init_strategy_num_candidates` is deprecated as of sbi</span>
<span class="sd">                v0.19.0. Instead, use e.g.,</span>
<span class="sd">                `init_strategy_parameters={&quot;num_candidate_samples&quot;: 1000}`&quot;&quot;&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span><span class="p">[</span>
                <span class="s2">&quot;num_candidate_samples&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">init_strategy_num_candidates</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides MCMC to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns MCMC method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span>

    <span class="nd">@mcmc_method</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;See `set_mcmc_method`.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_mcmc_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">posterior_sampler</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns sampler created by `sample`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span>

    <span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: Method to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `NeuralPosterior` for chainable calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warn</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;`.log_prob()` is deprecated for methods that can only evaluate the</span>
<span class="sd">            log-probability up to a normalizing constant. Use `.potential()` instead.&quot;&quot;&quot;</span>
        <span class="p">)</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$ with MCMC.</span>

<span class="sd">        Check the `__init__()` method for a description of all arguments as well as</span>
<span class="sd">        their default values.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            mcmc_parameters: Dictionary that is passed only to support the API of</span>
<span class="sd">                `sbi` v0.17.2 or older.</span>
<span class="sd">            mcmc_method: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. Please use `method` instead.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">thin</span>
        <span class="n">warmup_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="k">if</span> <span class="n">warmup_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">warmup_steps</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
        <span class="n">init_strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="k">if</span> <span class="n">init_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">init_strategy</span>
        <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="k">if</span> <span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_workers</span>
        <span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span>
            <span class="k">if</span> <span class="n">init_strategy_parameters</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">init_strategy_parameters</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;Passing `init_strategy_num_candidates` is deprecated as of sbi</span>
<span class="sd">                v0.19.0. Instead, use e.g.,</span>
<span class="sd">                `init_strategy_parameters={&quot;num_candidate_samples&quot;: 1000}`&quot;&quot;&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span><span class="p">[</span>
                <span class="s2">&quot;num_candidate_samples&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">init_strategy_num_candidates</span>
        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">mcmc_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
                <span class="s2">&quot;is deprecated and will be removed in a future release. Use `method` &quot;</span>
                <span class="s2">&quot;instead of `mcmc_method`.&quot;</span>
            <span class="p">)</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">mcmc_method</span>
        <span class="k">if</span> <span class="n">mcmc_parameters</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
                <span class="s2">&quot;is deprecated and will be removed in a future release. Instead, pass &quot;</span>
                <span class="s2">&quot;the variable to `.sample()` directly, e.g. &quot;</span>
                <span class="s2">&quot;`posterior.sample((1,), num_chains=5)`.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># The following lines are only for backwards compatibility with sbi v0.17.2 or</span>
        <span class="c1"># older.</span>
        <span class="n">m_p</span> <span class="o">=</span> <span class="n">mcmc_parameters</span>  <span class="c1"># define to shorten the variable name</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="s2">&quot;mcmc_method&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">thin</span><span class="p">,</span> <span class="s2">&quot;thin&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="s2">&quot;warmup_steps&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="s2">&quot;num_chains&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">init_strategy</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">init_strategy</span><span class="p">,</span> <span class="s2">&quot;init_strategy&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="n">initial_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_params</span><span class="p">(</span>
            <span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">num_chains</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">num_workers</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="o">**</span><span class="n">init_strategy_parameters</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="n">track_gradients</span> <span class="o">=</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;slice_np&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_np_mcmc</span><span class="p">(</span>
                    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                    <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                    <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                    <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">vectorized</span><span class="o">=</span><span class="p">(</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">),</span>
                    <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                    <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">,</span> <span class="s2">&quot;slice&quot;</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pyro_mcmc</span><span class="p">(</span>
                    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                    <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                    <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                    <span class="n">mcmc_method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                    <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">_build_mcmc_init_fn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">Transform</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return function that, when called, creates an initial parameter set for MCMC.</span>

<span class="sd">        Args:</span>
<span class="sd">            proposal: Proposal distribution.</span>
<span class="sd">            potential_fn: Potential function that the candidate samples are weighted</span>
<span class="sd">                with.</span>
<span class="sd">            init_strategy: Specifies the initialization method. Either of</span>
<span class="sd">                [`proposal`|`sir`|`resample`|`latest_sample`].</span>
<span class="sd">            kwargs: Passed on to init function. This way, init specific keywords can</span>
<span class="sd">                be set through `mcmc_parameters`. Unused arguments will be absorbed by</span>
<span class="sd">                the intitialization method.</span>

<span class="sd">        Returns: Initialization function.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;proposal&quot;</span> <span class="ow">or</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;prior&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;prior&quot;</span><span class="p">:</span>
                <span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;You set `init_strategy=prior`. As of sbi v0.18.0, this is &quot;</span>
                    <span class="s2">&quot;deprecated and it will be removed in a future release. Use &quot;</span>
                    <span class="s2">&quot;`init_strategy=proposal` instead.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">proposal_init</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;As of sbi v0.19.0, the behavior of the SIR initialization for MCMC &quot;</span>
                <span class="s2">&quot;has changed. If you wish to restore the behavior of sbi v0.18.0, set &quot;</span>
                <span class="s2">&quot;`init_strategy=&#39;resample&#39;.`&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">sir_init</span><span class="p">(</span>
                <span class="n">proposal</span><span class="p">,</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;resample&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">resample_given_potential_fn</span><span class="p">(</span>
                <span class="n">proposal</span><span class="p">,</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;latest_sample&quot;</span><span class="p">:</span>
            <span class="n">latest_sample</span> <span class="o">=</span> <span class="n">IterateParameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_init_params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">latest_sample</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_get_initial_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return initial parameters for MCMC obtained with given init strategy.</span>

<span class="sd">        Parallelizes across CPU cores only for SIR.</span>

<span class="sd">        Args:</span>
<span class="sd">            init_strategy: Specifies the initialization method. Either of</span>
<span class="sd">                [`proposal`|`sir`|`resample`|`latest_sample`].</span>
<span class="sd">            num_chains: number of MCMC chains, generates initial params for each</span>
<span class="sd">            num_workers: number of CPU cores for parallization</span>
<span class="sd">            show_progress_bars: whether to show progress bars for SIR init</span>
<span class="sd">            kwargs: Passed on to `_build_mcmc_init_fn`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: initial parameters, one for each chain</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Build init function</span>
        <span class="n">init_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_mcmc_init_fn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">init_strategy</span><span class="o">=</span><span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Parallelize inits for resampling only.</span>
        <span class="k">if</span> <span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;resample&quot;</span> <span class="ow">or</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">):</span>

            <span class="k">def</span> <span class="nf">seeded_init_fn</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">init_fn</span><span class="p">()</span>

            <span class="n">seeds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,))</span>

            <span class="c1"># Generate initial params parallelized over num_workers.</span>
            <span class="k">with</span> <span class="n">tqdm_joblib</span><span class="p">(</span>
                <span class="n">tqdm</span><span class="p">(</span>
                    <span class="nb">range</span><span class="p">(</span><span class="n">num_chains</span><span class="p">),</span>  <span class="c1"># type: ignore</span>
                    <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bars</span><span class="p">,</span>
                    <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Generating </span><span class="si">{</span><span class="n">num_chains</span><span class="si">}</span><span class="s2"> MCMC inits with </span><span class="si">{</span><span class="n">num_workers</span><span class="si">}</span>
<span class="s2">                         workers.&quot;&quot;&quot;</span><span class="p">,</span>
                    <span class="n">total</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">):</span>
                <span class="n">initial_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)(</span>
                        <span class="n">delayed</span><span class="p">(</span><span class="n">seeded_init_fn</span><span class="p">)(</span><span class="n">seed</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span>
                    <span class="p">)</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">initial_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">init_fn</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chains</span><span class="p">)]</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">initial_params</span>

    <span class="k">def</span> <span class="nf">_slice_np_mcmc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">potential_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">initial_params</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">vectorized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">init_width</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom implementation of slice sampling using Numpy.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Desired number of samples.</span>
<span class="sd">            potential_function: A callable **class**.</span>
<span class="sd">            initial_params: Initial parameters for MCMC chain.</span>
<span class="sd">            thin: Thinning (subsampling) factor.</span>
<span class="sd">            warmup_steps: Initial number of samples to discard.</span>
<span class="sd">            vectorized: Whether to use a vectorized implementation of the Slice sampler.</span>
<span class="sd">            num_workers: Number of CPU cores to use.</span>
<span class="sd">            init_width: Inital width of brackets.</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling;</span>
<span class="sd">                can only be turned off for vectorized sampler.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor of shape (num_samples, shape_of_single_theta).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_chains</span><span class="p">,</span> <span class="n">dim_samples</span> <span class="o">=</span> <span class="n">initial_params</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">vectorized</span><span class="p">:</span>
            <span class="n">SliceSamplerMultiChain</span> <span class="o">=</span> <span class="n">SliceSamplerSerial</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">SliceSamplerMultiChain</span> <span class="o">=</span> <span class="n">SliceSamplerVectorized</span>

        <span class="n">posterior_sampler</span> <span class="o">=</span> <span class="n">SliceSamplerMultiChain</span><span class="p">(</span>
            <span class="n">init_params</span><span class="o">=</span><span class="n">tensor2numpy</span><span class="p">(</span><span class="n">initial_params</span><span class="p">),</span>
            <span class="n">log_prob_fn</span><span class="o">=</span><span class="n">potential_function</span><span class="p">,</span>
            <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
            <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">init_width</span><span class="o">=</span><span class="n">init_width</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">warmup_</span> <span class="o">=</span> <span class="n">warmup_steps</span> <span class="o">*</span> <span class="n">thin</span>
        <span class="n">num_samples_</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">((</span><span class="n">num_samples</span> <span class="o">*</span> <span class="n">thin</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_chains</span><span class="p">)</span>
        <span class="c1"># Run mcmc including warmup</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">posterior_sampler</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">warmup_</span> <span class="o">+</span> <span class="n">num_samples_</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[:,</span> <span class="n">warmup_steps</span><span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># discard warmup steps</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>  <span class="c1"># chains x samples x dim</span>

        <span class="c1"># Save posterior sampler.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="n">posterior_sampler</span>

        <span class="c1"># Save sample as potential next init (if init_strategy == &#39;latest_sample&#39;).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_init_params</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="n">dim_samples</span><span class="p">)</span>

        <span class="c1"># Collect samples from all chains.</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim_samples</span><span class="p">)[:</span><span class="n">num_samples</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">assert</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">num_samples</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_pyro_mcmc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">potential_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">initial_params</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice&quot;</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples obtained using Pyro HMC, NUTS for slice kernels.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Desired number of samples.</span>
<span class="sd">            potential_function: A callable **class**. A class, but not a function,</span>
<span class="sd">                is picklable for Pyro MCMC to use it across chains in parallel,</span>
<span class="sd">                even when the potential function requires evaluating a neural network.</span>
<span class="sd">            mcmc_method: One of `hmc`, `nuts` or `slice`.</span>
<span class="sd">            thin: Thinning (subsampling) factor.</span>
<span class="sd">            warmup_steps: Initial number of samples to discard.</span>
<span class="sd">            num_chains: Whether to sample in parallel. If None, use all but one CPU.</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor of shape (num_samples, shape_of_single_theta).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>

        <span class="n">kernels</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">slice</span><span class="o">=</span><span class="n">Slice</span><span class="p">,</span> <span class="n">hmc</span><span class="o">=</span><span class="n">HMC</span><span class="p">,</span> <span class="n">nuts</span><span class="o">=</span><span class="n">NUTS</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">kernels</span><span class="p">[</span><span class="n">mcmc_method</span><span class="p">](</span><span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_function</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="p">(</span><span class="n">thin</span> <span class="o">*</span> <span class="n">num_samples</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_chains</span> <span class="o">+</span> <span class="n">num_chains</span><span class="p">,</span>
            <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
            <span class="n">initial_params</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">param_name</span><span class="p">:</span> <span class="n">initial_params</span><span class="p">},</span>
            <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
            <span class="n">mp_context</span><span class="o">=</span><span class="s2">&quot;spawn&quot;</span><span class="p">,</span>
            <span class="n">disable_progbar</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">transforms</span><span class="o">=</span><span class="p">{},</span>
        <span class="p">)</span>
        <span class="n">sampler</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">initial_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># .shape[1] = dim of theta</span>
        <span class="p">)</span>

        <span class="c1"># Save posterior sampler.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="n">sampler</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[::</span><span class="n">thin</span><span class="p">][:</span><span class="n">num_samples</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">num_samples</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_prepare_potential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Combines potential and transform and takes care of gradients and pyro.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: Which MCMC method to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A potential function that is ready to be used in MCMC.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice&quot;</span><span class="p">:</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">):</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="s2">&quot;slice_np&quot;</span> <span class="ow">in</span> <span class="n">method</span><span class="p">:</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="n">prepared_potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">transformed_potential</span><span class="p">,</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">pyro</span><span class="p">:</span>
            <span class="n">prepared_potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">pyro_potential_wrapper</span><span class="p">,</span> <span class="n">potential</span><span class="o">=</span><span class="n">prepared_potential</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">prepared_potential</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_arviz_inference_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InferenceData</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns arviz InferenceData object constructed most recent samples.</span>

<span class="sd">        Note: the InferenceData is constructed using the posterior samples generated in</span>
<span class="sd">        most recent call to `.sample(...)`.</span>

<span class="sd">        For Pyro HMC and NUTS kernels InferenceData will contain diagnostics, for Pyro</span>
<span class="sd">        Slice or sbi slice sampling samples, only the samples are added.</span>

<span class="sd">        Returns:</span>
<span class="sd">            inference_data: Arviz InferenceData object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;No samples have been generated, call .sample() first.&quot;&quot;&quot;</span>

        <span class="n">sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">MCMC</span><span class="p">,</span> <span class="n">SliceSamplerSerial</span><span class="p">,</span> <span class="n">SliceSamplerVectorized</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span>

        <span class="c1"># If Pyro sampler and samples not transformed, use arviz&#39; from_pyro.</span>
        <span class="c1"># Exclude &#39;slice&#39; kernel as it lacks the &#39;divergence&#39; diagnostics key.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span><span class="p">,</span> <span class="p">(</span><span class="n">HMC</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">IndependentTransform</span>
        <span class="p">):</span>
            <span class="n">inference_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pyro</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>

        <span class="c1"># otherwise get samples from sampler and transform to original space.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_samples</span><span class="p">(</span><span class="n">group_by_chain</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Pyro samplers returns dicts, get values.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                <span class="c1"># popitem gets last items, [1] get the values as tensor.</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">transformed_samples</span><span class="o">.</span><span class="n">popitem</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># Our slice samplers return numpy arrays.</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
                <span class="p">)</span>
            <span class="c1"># For MultipleIndependent priors transforms first dim must be batch dim.</span>
            <span class="c1"># thus, reshape back and forth to have batch dim in front.</span>
            <span class="n">samples_shape</span> <span class="o">=</span> <span class="n">transformed_samples</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">transformed_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">samples_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="o">*</span><span class="n">samples_shape</span>
            <span class="p">)</span>

            <span class="n">inference_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">convert_to_inference_data</span><span class="p">(</span>
                <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">samples</span><span class="p">}</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">inference_data</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.mcmc_method" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.mcmc_method" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns MCMC method.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.posterior_sampler" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">posterior_sampler</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.posterior_sampler" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns sampler created by <code>sample</code>.</p>
  </div>

</div>



<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;slice_np&#39;</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">init_strategy</span><span class="o">=</span><span class="s1">&#39;resample&#39;</span><span class="p">,</span> <span class="n">init_strategy_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">init_strategy_num_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>potential_fn</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>The potential function from which to draw samples.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>proposal</code></td>
          <td>
                <code><span title="typing.Any">Any</span></code>
          </td>
          <td><p>Proposal distribution that is used to initialize the MCMC chain.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>theta_transform</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchTransform">TorchTransform</span>]</code>
          </td>
          <td><p>Transformation that will be applied during sampling.
Allows to perform MCMC in unconstrained space.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>method</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Method used for MCMC sampling, one of <code>slice_np</code>,
<code>slice_np_vectorized</code>, <code>slice</code>, <code>hmc</code>, <code>nuts</code>. <code>slice_np</code> is a custom
numpy implementation of slice sampling. <code>slice_np_vectorized</code> is
identical to <code>slice_np</code>, but if <code>num_chains&gt;1</code>, the chains are
vectorized for <code>slice_np_vectorized</code> whereas they are run sequentially
for <code>slice_np</code>. The samplers <code>hmc</code>, <code>nuts</code> or <code>slice</code> sample with Pyro.</p></td>
          <td>
                <code>&#39;slice_np&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>thin</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The thinning factor for the chain.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>warmup_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The initial number of samples to discard.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>num_chains</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of chains.</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>init_strategy</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>The initialisation strategy for chains; <code>proposal</code> will draw
init locations from <code>proposal</code>, whereas <code>sir</code> will use Sequential-
Importance-Resampling (SIR). SIR initially samples
<code>init_strategy_num_candidates</code> from the <code>proposal</code>, evaluates all of
them under the <code>potential_fn</code> and <code>proposal</code>, and then resamples the
initial locations with weights proportional to <code>exp(potential_fn -
proposal.log_prob</code>. <code>resample</code> is the same as <code>sir</code> but
uses <code>exp(potential_fn)</code> as weights.</p></td>
          <td>
                <code>&#39;resample&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>init_strategy_parameters</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td><p>Dictionary of keyword arguments passed to the
init strategy, e.g., for <code>init_strategy=sir</code> this could be
<code>num_candidate_samples</code>, i.e., the number of candidates to to find init
locations (internal default is <code>1000</code>), or <code>device</code>.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
        <tr>
          <td><code>init_strategy_num_candidates</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[int]</code>
          </td>
          <td><p>Number of candidates to to find init
 locations in <code>init_strategy=sir</code> (deprecated, use
 init_strategy_parameters instead).</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_workers</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>number of cpu cores used to parallelize mcmc</p></td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>x_shape</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[torch.<span title="torch.Size">Size</span>]</code>
          </td>
          <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/mcmc_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span><span class="p">,</span>
    <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;resample&quot;</span><span class="p">,</span>
    <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples.</span>
<span class="sd">        proposal: Proposal distribution that is used to initialize the MCMC chain.</span>
<span class="sd">        theta_transform: Transformation that will be applied during sampling.</span>
<span class="sd">            Allows to perform MCMC in unconstrained space.</span>
<span class="sd">        method: Method used for MCMC sampling, one of `slice_np`,</span>
<span class="sd">            `slice_np_vectorized`, `slice`, `hmc`, `nuts`. `slice_np` is a custom</span>
<span class="sd">            numpy implementation of slice sampling. `slice_np_vectorized` is</span>
<span class="sd">            identical to `slice_np`, but if `num_chains&gt;1`, the chains are</span>
<span class="sd">            vectorized for `slice_np_vectorized` whereas they are run sequentially</span>
<span class="sd">            for `slice_np`. The samplers `hmc`, `nuts` or `slice` sample with Pyro.</span>
<span class="sd">        thin: The thinning factor for the chain.</span>
<span class="sd">        warmup_steps: The initial number of samples to discard.</span>
<span class="sd">        num_chains: The number of chains.</span>
<span class="sd">        init_strategy: The initialisation strategy for chains; `proposal` will draw</span>
<span class="sd">            init locations from `proposal`, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling (SIR). SIR initially samples</span>
<span class="sd">            `init_strategy_num_candidates` from the `proposal`, evaluates all of</span>
<span class="sd">            them under the `potential_fn` and `proposal`, and then resamples the</span>
<span class="sd">            initial locations with weights proportional to `exp(potential_fn -</span>
<span class="sd">            proposal.log_prob`. `resample` is the same as `sir` but</span>
<span class="sd">            uses `exp(potential_fn)` as weights.</span>
<span class="sd">        init_strategy_parameters: Dictionary of keyword arguments passed to the</span>
<span class="sd">            init strategy, e.g., for `init_strategy=sir` this could be</span>
<span class="sd">            `num_candidate_samples`, i.e., the number of candidates to to find init</span>
<span class="sd">            locations (internal default is `1000`), or `device`.</span>
<span class="sd">        init_strategy_num_candidates: Number of candidates to to find init</span>
<span class="sd">             locations in `init_strategy=sir` (deprecated, use</span>
<span class="sd">             init_strategy_parameters instead).</span>
<span class="sd">        num_workers: number of cpu cores used to parallelize mcmc</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="n">thin</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="o">=</span> <span class="n">num_chains</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="o">=</span> <span class="n">init_strategy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="n">init_strategy_parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Hardcode parameter name to reduce clutter kwargs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">param_name</span> <span class="o">=</span> <span class="s2">&quot;theta&quot;</span>

    <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Passing `init_strategy_num_candidates` is deprecated as of sbi</span>
<span class="sd">            v0.19.0. Instead, use e.g.,</span>
<span class="sd">            `init_strategy_parameters={&quot;num_candidate_samples&quot;: 1000}`&quot;&quot;&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span><span class="p">[</span>
            <span class="s2">&quot;num_candidate_samples&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">init_strategy_num_candidates</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides MCMC to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.get_arviz_inference_data" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">get_arviz_inference_data</span><span class="p">()</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.get_arviz_inference_data" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns arviz InferenceData object constructed most recent samples.</p>
<p>Note: the InferenceData is constructed using the posterior samples generated in
most recent call to <code>.sample(...)</code>.</p>
<p>For Pyro HMC and NUTS kernels InferenceData will contain diagnostics, for Pyro
Slice or sbi slice sampling samples, only the samples are added.</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>inference_data</code></td>          <td>
                <code><span title="arviz.data.InferenceData">InferenceData</span></code>
          </td>
          <td><p>Arviz InferenceData object.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/mcmc_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_arviz_inference_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InferenceData</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns arviz InferenceData object constructed most recent samples.</span>

<span class="sd">    Note: the InferenceData is constructed using the posterior samples generated in</span>
<span class="sd">    most recent call to `.sample(...)`.</span>

<span class="sd">    For Pyro HMC and NUTS kernels InferenceData will contain diagnostics, for Pyro</span>
<span class="sd">    Slice or sbi slice sampling samples, only the samples are added.</span>

<span class="sd">    Returns:</span>
<span class="sd">        inference_data: Arviz InferenceData object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">&quot;&quot;&quot;No samples have been generated, call .sample() first.&quot;&quot;&quot;</span>

    <span class="n">sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">MCMC</span><span class="p">,</span> <span class="n">SliceSamplerSerial</span><span class="p">,</span> <span class="n">SliceSamplerVectorized</span>
    <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span>

    <span class="c1"># If Pyro sampler and samples not transformed, use arviz&#39; from_pyro.</span>
    <span class="c1"># Exclude &#39;slice&#39; kernel as it lacks the &#39;divergence&#39; diagnostics key.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span><span class="p">,</span> <span class="p">(</span><span class="n">HMC</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">IndependentTransform</span>
    <span class="p">):</span>
        <span class="n">inference_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pyro</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>

    <span class="c1"># otherwise get samples from sampler and transform to original space.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_samples</span><span class="p">(</span><span class="n">group_by_chain</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Pyro samplers returns dicts, get values.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="c1"># popitem gets last items, [1] get the values as tensor.</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">transformed_samples</span><span class="o">.</span><span class="n">popitem</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Our slice samplers return numpy arrays.</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">)</span>
        <span class="c1"># For MultipleIndependent priors transforms first dim must be batch dim.</span>
        <span class="c1"># thus, reshape back and forth to have batch dim in front.</span>
        <span class="n">samples_shape</span> <span class="o">=</span> <span class="n">transformed_samples</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="n">transformed_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">samples_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="o">*</span><span class="n">samples_shape</span>
        <span class="p">)</span>

        <span class="n">inference_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">convert_to_inference_data</span><span class="p">(</span>
            <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">samples</span><span class="p">}</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">inference_data</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.log_prob" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the log-probability of theta under the posterior.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>theta</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>track_gradients</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>len($\theta$)</code>-shaped log-probability.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/mcmc_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warn</span><span class="p">(</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;`.log_prob()` is deprecated for methods that can only evaluate the</span>
<span class="sd">        log-probability up to a normalizing constant. Use `.potential()` instead.&quot;&quot;&quot;</span>
    <span class="p">)</span>
    <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.map" class="doc doc-heading">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.map" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_iter</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate of the optimizer.</p></td>
          <td>
                <code>0.01</code>
          </td>
        </tr>
        <tr>
          <td><code>init_method</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
          <td>
                <code>&#39;proposal&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>num_init_samples</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>num_to_optimize</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>save_best_every</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during sampling from
the posterior.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>force_update</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>log_prob_kwargs</code></td>
          <td>
          </td>
          <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The MAP estimate.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/mcmc_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy_num_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return samples from posterior distribution <span class="arithmatex">\(p(\theta|x)\)</span> with MCMC.</p>
<p>Check the <code>__init__()</code> method for a description of all arguments as well as
their default values.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sample_shape</code></td>
          <td>
                <code><span title="sbi.types.Shape">Shape</span></code>
          </td>
          <td><p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p></td>
          <td>
                <code>torch.Size()</code>
          </td>
        </tr>
        <tr>
          <td><code>mcmc_parameters</code></td>
          <td>
                <code><span title="typing.Dict">Dict</span></code>
          </td>
          <td><p>Dictionary that is passed only to support the API of
<code>sbi</code> v0.17.2 or older.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
        <tr>
          <td><code>mcmc_method</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. Please use <code>method</code> instead.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>sample_with</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show sampling progress monitor.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Samples from posterior.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/mcmc_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">thin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$ with MCMC.</span>

<span class="sd">    Check the `__init__()` method for a description of all arguments as well as</span>
<span class="sd">    their default values.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        mcmc_parameters: Dictionary that is passed only to support the API of</span>
<span class="sd">            `sbi` v0.17.2 or older.</span>
<span class="sd">        mcmc_method: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. Please use `method` instead.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># Replace arguments that were not passed with their default.</span>
    <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>
    <span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">thin</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="k">if</span> <span class="n">warmup_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">warmup_steps</span>
    <span class="n">num_chains</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
    <span class="n">init_strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="k">if</span> <span class="n">init_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">init_strategy</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="k">if</span> <span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_workers</span>
    <span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span>
        <span class="k">if</span> <span class="n">init_strategy_parameters</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">init_strategy_parameters</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Passing `init_strategy_num_candidates` is deprecated as of sbi</span>
<span class="sd">            v0.19.0. Instead, use e.g.,</span>
<span class="sd">            `init_strategy_parameters={&quot;num_candidate_samples&quot;: 1000}`&quot;&quot;&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span><span class="p">[</span>
            <span class="s2">&quot;num_candidate_samples&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">init_strategy_num_candidates</span>
    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">mcmc_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
            <span class="s2">&quot;is deprecated and will be removed in a future release. Use `method` &quot;</span>
            <span class="s2">&quot;instead of `mcmc_method`.&quot;</span>
        <span class="p">)</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">mcmc_method</span>
    <span class="k">if</span> <span class="n">mcmc_parameters</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
            <span class="s2">&quot;is deprecated and will be removed in a future release. Instead, pass &quot;</span>
            <span class="s2">&quot;the variable to `.sample()` directly, e.g. &quot;</span>
            <span class="s2">&quot;`posterior.sample((1,), num_chains=5)`.&quot;</span>
        <span class="p">)</span>
    <span class="c1"># The following lines are only for backwards compatibility with sbi v0.17.2 or</span>
    <span class="c1"># older.</span>
    <span class="n">m_p</span> <span class="o">=</span> <span class="n">mcmc_parameters</span>  <span class="c1"># define to shorten the variable name</span>
    <span class="n">method</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="s2">&quot;mcmc_method&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">thin</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">thin</span><span class="p">,</span> <span class="s2">&quot;thin&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="s2">&quot;warmup_steps&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">num_chains</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="s2">&quot;num_chains&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">init_strategy</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">init_strategy</span><span class="p">,</span> <span class="s2">&quot;init_strategy&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="n">initial_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_params</span><span class="p">(</span>
        <span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">num_chains</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">num_workers</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="o">**</span><span class="n">init_strategy_parameters</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="n">track_gradients</span> <span class="o">=</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;slice_np&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_np_mcmc</span><span class="p">(</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">vectorized</span><span class="o">=</span><span class="p">(</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">),</span>
                <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">,</span> <span class="s2">&quot;slice&quot;</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pyro_mcmc</span><span class="p">(</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                <span class="n">mcmc_method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NameError</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_mcmc_method" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">set_mcmc_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_mcmc_method" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Sets sampling method to for MCMC and returns <code>NeuralPosterior</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>method</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Method to use.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code>
          </td>
          <td><p><code>NeuralPosterior</code> for chainable calls.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/mcmc_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: Method to use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="doc doc-heading">
        <code>sbi.inference.posteriors.rejection_posterior.RejectionPosterior</code>


<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>

  
      <p>Provides rejection sampling to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>RejectionPosterior</code> allows to sample from the posterior with rejection sampling.</p>


        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/rejection_posterior.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">RejectionPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides rejection sampling to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `RejectionPosterior` allows to sample from the posterior with rejection sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples.</span>
<span class="sd">            proposal: The proposal distribution.</span>
<span class="sd">            theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">                during but only when calling `.map()`.</span>
<span class="sd">            max_sampling_batch_size: The batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration.</span>
<span class="sd">            num_samples_to_find_max: The number of samples that are used to find the</span>
<span class="sd">                maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">            num_iter_to_find_max: The number of gradient ascent iterations to find the</span>
<span class="sd">                maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">            m: Multiplier to the `potential_fn / proposal` ratio.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="n">num_samples_to_find_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="n">num_iter_to_find_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides rejection sampling to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`.log_prob()` is deprecated for methods that can only evaluate the log-probability up to a normalizing constant. Use `.potential()` instead.&quot;</span>
        <span class="p">)</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior $p(\theta|x)$ via rejection sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>
        <span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span>
            <span class="k">if</span> <span class="n">num_samples_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">num_samples_to_find_max</span>
        <span class="p">)</span>
        <span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span>
            <span class="k">if</span> <span class="n">num_iter_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">num_iter_to_find_max</span>
        <span class="p">)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">m</span>

        <span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rejection_sample</span><span class="p">(</span>
            <span class="n">potential</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">warn_acceptance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="n">num_samples_to_find_max</span><span class="p">,</span>
            <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="n">num_iter_to_find_max</span><span class="p">,</span>
            <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>potential_fn</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>The potential function from which to draw samples.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>proposal</code></td>
          <td>
                <code><span title="typing.Any">Any</span></code>
          </td>
          <td><p>The proposal distribution.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>theta_transform</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchTransform">TorchTransform</span>]</code>
          </td>
          <td><p>Transformation that is applied to parameters. Is not used
during but only when calling <code>.map()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_sampling_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The batchsize of samples being drawn from
the proposal at every iteration.</p></td>
          <td>
                <code>10000</code>
          </td>
        </tr>
        <tr>
          <td><code>num_samples_to_find_max</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of samples that are used to find the
maximum of the <code>potential_fn / proposal</code> ratio.</p></td>
          <td>
                <code>10000</code>
          </td>
        </tr>
        <tr>
          <td><code>num_iter_to_find_max</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The number of gradient ascent iterations to find the
maximum of the <code>potential_fn / proposal</code> ratio.</p></td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>m</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Multiplier to the <code>potential_fn / proposal</code> ratio.</p></td>
          <td>
                <code>1.2</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>x_shape</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[torch.<span title="torch.Size">Size</span>]</code>
          </td>
          <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/rejection_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">m</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples.</span>
<span class="sd">        proposal: The proposal distribution.</span>
<span class="sd">        theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">            during but only when calling `.map()`.</span>
<span class="sd">        max_sampling_batch_size: The batchsize of samples being drawn from</span>
<span class="sd">            the proposal at every iteration.</span>
<span class="sd">        num_samples_to_find_max: The number of samples that are used to find the</span>
<span class="sd">            maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">        num_iter_to_find_max: The number of gradient ascent iterations to find the</span>
<span class="sd">            maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">        m: Multiplier to the `potential_fn / proposal` ratio.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="n">num_samples_to_find_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="n">num_iter_to_find_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides rejection sampling to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.log_prob" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the log-probability of theta under the posterior.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>theta</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>track_gradients</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>len($\theta$)</code>-shaped log-probability.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/rejection_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;`.log_prob()` is deprecated for methods that can only evaluate the log-probability up to a normalizing constant. Use `.potential()` instead.&quot;</span>
    <span class="p">)</span>
    <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.map" class="doc doc-heading">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.map" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_iter</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate of the optimizer.</p></td>
          <td>
                <code>0.01</code>
          </td>
        </tr>
        <tr>
          <td><code>init_method</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
          <td>
                <code>&#39;proposal&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>num_init_samples</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>num_to_optimize</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>save_best_every</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during sampling from
the posterior.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>force_update</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>log_prob_kwargs</code></td>
          <td>
          </td>
          <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The MAP estimate.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/rejection_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.sample" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Return samples from posterior <span class="arithmatex">\(p(\theta|x)\)</span> via rejection sampling.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sample_shape</code></td>
          <td>
                <code><span title="sbi.types.Shape">Shape</span></code>
          </td>
          <td><p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p></td>
          <td>
                <code>torch.Size()</code>
          </td>
        </tr>
        <tr>
          <td><code>sample_with</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show sampling progress monitor.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
          </td>
          <td><p>Samples from posterior.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/rejection_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">m</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior $p(\theta|x)$ via rejection sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>
    <span class="c1"># Replace arguments that were not passed with their default.</span>
    <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
        <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
    <span class="p">)</span>
    <span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span>
        <span class="k">if</span> <span class="n">num_samples_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">num_samples_to_find_max</span>
    <span class="p">)</span>
    <span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span>
        <span class="k">if</span> <span class="n">num_iter_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">num_iter_to_find_max</span>
    <span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">m</span>

    <span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rejection_sample</span><span class="p">(</span>
        <span class="n">potential</span><span class="p">,</span>
        <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">warn_acceptance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="n">num_samples_to_find_max</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="n">num_iter_to_find_max</span><span class="p">,</span>
        <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior" class="doc doc-heading">
        <code>sbi.inference.posteriors.vi_posterior.VIPosterior</code>


<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>

  
      <p>Provides VI (Variational Inference) to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>VIPosterior</code> allows to learn a tractable variational posterior <span class="arithmatex">\(q(\theta)\)</span> which
approximates the true posterior <span class="arithmatex">\(p(\theta|x_o)\)</span>. After this second training stage,
we can produce approximate posterior samples, by just sampling from q with no
additional cost. For additional information see [1] and [2].<br/><br/>
References:<br/>
[1] Variational methods for simulation-based inference, Manuel Glckler, Michael
Deistler, Jakob Macke, 2022, <a href="https://openreview.net/forum?id=kZ0UYdhqkNY">https://openreview.net/forum?id=kZ0UYdhqkNY</a><br/>
[2] Sequential Neural Posterior and Likelihood Approximation, Samuel Wiqvist, Jes
Frellsen, Umberto Picchini, 2021, <a href="https://arxiv.org/abs/2102.06522">https://arxiv.org/abs/2102.06522</a></p>


        <details class="quote">
          <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">VIPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides VI (Variational Inference) to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `VIPosterior` allows to learn a tractable variational posterior $q(\theta)$ which</span>
<span class="sd">    approximates the true posterior $p(\theta|x_o)$. After this second training stage,</span>
<span class="sd">    we can produce approximate posterior samples, by just sampling from q with no</span>
<span class="sd">    additional cost. For additional information see [1] and [2].&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    References:&lt;br/&gt;</span>
<span class="sd">    [1] Variational methods for simulation-based inference, Manuel Glckler, Michael</span>
<span class="sd">    Deistler, Jakob Macke, 2022, https://openreview.net/forum?id=kZ0UYdhqkNY&lt;br/&gt;</span>
<span class="sd">    [2] Sequential Neural Posterior and Likelihood Approximation, Samuel Wiqvist, Jes</span>
<span class="sd">    Frellsen, Umberto Picchini, 2021, https://arxiv.org/abs/2102.06522</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples.</span>
<span class="sd">            prior: This is the prior distribution. Note that this is only</span>
<span class="sd">                used to check/construct the variational distribution or within some</span>
<span class="sd">                quality metrics. Please make sure that this matches with the prior</span>
<span class="sd">                within the potential_fn. If `None` is given, we will try to infer it</span>
<span class="sd">                from potential_fn or q, if this fails we raise an Error.</span>
<span class="sd">            q: Variational distribution, either string, `TransformedDistribution`, or a</span>
<span class="sd">                `VIPosterior` object. This specifies a parametric class of distribution</span>
<span class="sd">                over which the best possible posterior approximation is searched. For</span>
<span class="sd">                string input, we currently support [nsf, scf, maf, mcf, gaussian,</span>
<span class="sd">                gaussian_diag]. You can also specify your own variational family by</span>
<span class="sd">                passing a pyro `TransformedDistribution`.</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns a distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms` within the</span>
<span class="sd">                `get_flow_builder` method specifying the number of transformations</span>
<span class="sd">                within a normalizing flow. If q is already a `VIPosterior`, then the</span>
<span class="sd">                arguments will be copied from it (relevant for multi-round training).</span>
<span class="sd">            theta_transform: Maps form prior support to unconstrained space. The</span>
<span class="sd">                inverse is used here to ensure that the posterior support is equal to</span>
<span class="sd">                that of the prior.</span>
<span class="sd">            vi_method: This specifies the variational methods which are used to fit q to</span>
<span class="sd">                the posterior. We currently support [rKL, fKL, IW, alpha]. Note that</span>
<span class="sd">                some of the divergences are `mode seeking` i.e. they underestimate</span>
<span class="sd">                variance and collapse on multimodal targets (`rKL`, `alpha` for alpha &gt;</span>
<span class="sd">                1) and some are `mass covering` i.e. they overestimate variance but</span>
<span class="sd">                typically cover all modes (`fKL`, `IW`, `alpha` for alpha &lt; 1).</span>
<span class="sd">            device: Training device, e.g., `cpu`, `cuda` or `cuda:0`. We will ensure</span>
<span class="sd">                that all other objects are also on this device.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">            parameters: List of parameters of the variational posterior. This is only</span>
<span class="sd">                required for user-defined q i.e. if q does not have a `parameters`</span>
<span class="sd">                attribute.</span>
<span class="sd">            modules: List of modules of the variational posterior. This is only</span>
<span class="sd">                required for user-defined q i.e. if q does not have a `modules`</span>
<span class="sd">                attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="c1"># Especially the prior may be on another device -&gt; move it...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="c1"># Get prior and previous builds</span>
        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">prior</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="s2">&quot;prior&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Distribution</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;We could not find a suitable prior distribution within `potential_fn`&quot;</span>
                <span class="s2">&quot;or `q` (if a VIPosterior is given). Please explicitly specify a prior.&quot;</span>
            <span class="p">)</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># In contrast to MCMC we want to project into constrained space.</span>
        <span class="k">if</span> <span class="n">theta_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span><span class="o">.</span><span class="n">inv</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span>

        <span class="c1"># This will set the variational distribution and VI method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">vi_method</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides Variational inference to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _normalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">q</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the variational posterior.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q</span>

    <span class="nd">@q</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">q</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the variational distribution. If the distribution does not admit access</span>
<span class="sd">        through `parameters` and `modules` function, please use `set_q` if you want to</span>
<span class="sd">        explicitly specify the parameters and modules.</span>


<span class="sd">        Args:</span>
<span class="sd">            q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">                object. This specifies a parametric class of distribution over which</span>
<span class="sd">                the best possible posterior approximation is searched. For string input,</span>
<span class="sd">                we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">                course, you can also specify your own variational family by passing a</span>
<span class="sd">                `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">                Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">                parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">                using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">                is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">                (relevant for multi-round training).</span>


<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_q</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Defines the variational family.</span>

<span class="sd">        You can specify over which parameters/modules we optimize. This is required for</span>
<span class="sd">        custom distributions which e.g. do not inherit nn.Modules or has the function</span>
<span class="sd">        `parameters` or `modules` to give direct access to trainable parameters.</span>
<span class="sd">        Further, you can pass a function, which constructs a variational distribution</span>
<span class="sd">        if called.</span>

<span class="sd">        Args:</span>
<span class="sd">            q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">                object. This specifies a parametric class of distribution over which</span>
<span class="sd">                the best possible posterior approximation is searched. For string input,</span>
<span class="sd">                we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">                course, you can also specify your own variational family by passing a</span>
<span class="sd">                `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">                Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">                parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">                using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">                is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">                (relevant for multi-round training).</span>
<span class="sd">            parameters: List of parameters associated with the distribution object.</span>
<span class="sd">            modules: List of modules associated with the distribution object.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">adapt_variational_distribution</span><span class="p">(</span>
                <span class="n">q</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
                <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="n">self_custom_q_init_cache</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">self_custom_q_init_cache</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Callable</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">get_flow_builder</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span>

            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">event_shape</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_build_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_trained_on</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vi_method</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">vi_method</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_device</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_x</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_arg</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Something went wrong when initializing the variational distribution.</span>
<span class="s2">            Please create an issue on github https://github.com/mackelab/sbi/issues&quot;&quot;&quot;</span>
        <span class="n">check_variational_distribution</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">q</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Variational inference method e.g. one of [rKL, fKL, IW, alpha].&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span>

    <span class="nd">@vi_method</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;See `set_vi_method`.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets variational inference method.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: One of [rKL, fKL, IW, alpha].</span>

<span class="sd">        Returns:</span>
<span class="sd">            `VIPosterior` for chainable calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span> <span class="o">=</span> <span class="n">get_VI_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Samples from the variational posterior distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Shape of samples</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit on the specified `default_x` &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">. Please train using `posterior.train()`.&quot;</span>
            <span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the variational posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit using observation </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.</span><span class="se">\</span>
<span class="s2">                     Please train.&quot;</span>
            <span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_particles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
        <span class="n">max_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
        <span class="n">min_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
        <span class="n">warm_up_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">reset_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">check_for_convergence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">quality_control</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This method trains the variational posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: The observation.</span>
<span class="sd">            n_particles: Number of samples to approximate expectations within the</span>
<span class="sd">                variational bounds. The larger the more accurate are gradient</span>
<span class="sd">                estimates, but the computational cost per iteration increases.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            gamma: Learning rate decay per iteration. We use an exponential decay</span>
<span class="sd">                scheduler.</span>
<span class="sd">            max_num_iters: Maximum number of iterations.</span>
<span class="sd">            min_num_iters: Minimum number of iterations.</span>
<span class="sd">            clip_value: Gradient clipping value, decreasing may help if you see invalid</span>
<span class="sd">                values.</span>
<span class="sd">            warm_up_rounds: Initialize the posterior as the prior.</span>
<span class="sd">            retrain_from_scratch: Retrain the variational distributions from scratch.</span>
<span class="sd">            reset_optimizer: Reset the divergence optimizer</span>
<span class="sd">            show_progress_bar: If any progress report should be displayed.</span>
<span class="sd">            quality_control: If False quality control is skipped.</span>
<span class="sd">            quality_control_metric: Which metric to use for evaluating the quality.</span>
<span class="sd">            kwargs: Hyperparameters check corresponding `DivergenceOptimizer` for detail</span>
<span class="sd">                eps: Determines sensitivity of convergence check.</span>
<span class="sd">                retain_graph: Boolean which decides whether to retain the computation</span>
<span class="sd">                    graph. This may be required for some `exotic` user-specified q&#39;s.</span>
<span class="sd">                optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See</span>
<span class="sd">                    `DivergenceOptimizer` for details.</span>
<span class="sd">                scheduler: A PyTorch learning rate scheduler. See</span>
<span class="sd">                    `DivergenceOptimizer` for details.</span>
<span class="sd">                alpha: Only used if vi_method=`alpha`. Determines the alpha divergence.</span>
<span class="sd">                K: Only used if vi_method=`IW`. Determines the number of importance</span>
<span class="sd">                    weighted particles.</span>
<span class="sd">                stick_the_landing: If one should use the STL estimator (only for rKL,</span>
<span class="sd">                    IW, alpha).</span>
<span class="sd">                dreg: If one should use the DREG estimator (only for rKL, IW, alpha).</span>
<span class="sd">                weight_transform: Callable applied to importance weights (only for fKL)</span>
<span class="sd">        Returns:</span>
<span class="sd">            VIPosterior: `VIPosterior` (can be used to chain calls).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update optimizer with current arguments.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="o">**</span><span class="nb">locals</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">})</span>

        <span class="c1"># Init q and the optimizer if necessary</span>
        <span class="k">if</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
                <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">reset_optimizer</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
                <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Check context</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_2d_float32_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="p">)</span>

        <span class="n">already_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

        <span class="c1"># Optimize</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">reset_loss_stats</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="n">iters</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">iters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">)</span>

        <span class="c1"># Warmup before training</span>
        <span class="k">if</span> <span class="n">reset_optimizer</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up_was_done</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">already_trained</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                    <span class="s2">&quot;Warmup phase, this may take a few seconds...&quot;</span>
                <span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up</span><span class="p">(</span><span class="n">warm_up_rounds</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iters</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">mean_loss</span><span class="p">,</span> <span class="n">std_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_loss_stats</span><span class="p">()</span>
            <span class="c1"># Update progress bar</span>
            <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">tqdm</span><span class="p">)</span>
                <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                    <span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">std_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Check for convergence</span>
            <span class="k">if</span> <span class="n">check_for_convergence</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">min_num_iters</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">converged</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged with loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">break</span>
        <span class="c1"># Training finished:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">x</span>

        <span class="c1"># Evaluate quality</span>
        <span class="k">if</span> <span class="n">quality_control</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">quality_control_metric</span><span class="o">=</span><span class="n">quality_control_metric</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Quality control did not work, we reset the variational </span><span class="se">\</span>
<span class="s2">                        posterior,please check your setting. </span><span class="se">\</span>
<span class="s2">                        </span><span class="se">\n</span><span class="s2"> Following error occured </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
                    <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This function will evaluate the quality of the variational posterior</span>
<span class="sd">        distribution. We currently support two different metrics of type `psis`, which</span>
<span class="sd">        checks the quality based on the tails of importance weights (there should not be</span>
<span class="sd">        much with a large one), or `prop` which checks the proportionality between q</span>
<span class="sd">        and potential_fn.</span>

<span class="sd">        NOTE: In our experience `prop` is sensitive to distinguish ``good`` from ``ok``</span>
<span class="sd">        whereas `psis` is more sensitive in distinguishing `very bad` from `ok`.</span>

<span class="sd">        Args:</span>
<span class="sd">            quality_control_metric: The metric of choice, we currently support [psis,</span>
<span class="sd">                prop, prop_prior].</span>
<span class="sd">            N: Number of samples which is used to evaluate the metric.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">quality_control_fn</span><span class="p">,</span> <span class="n">quality_control_msg</span> <span class="o">=</span> <span class="n">get_quality_metric</span><span class="p">(</span>
            <span class="n">quality_control_metric</span>
        <span class="p">)</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quality_control_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)),</span> <span class="mi">3</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quality Score: </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="o">+</span> <span class="n">quality_control_msg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.q" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">q</span><span class="p">:</span> <span class="n">Distribution</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.q" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the variational posterior.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.vi_method" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.vi_method" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Variational inference method e.g. one of [rKL, fKL, IW, alpha].</p>
  </div>

</div>



<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.__init__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vi_method</span><span class="o">=</span><span class="s1">&#39;rKL&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[],</span> <span class="n">modules</span><span class="o">=</span><span class="p">[])</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>potential_fn</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>The potential function from which to draw samples.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchDistribution">TorchDistribution</span>]</code>
          </td>
          <td><p>This is the prior distribution. Note that this is only
used to check/construct the variational distribution or within some
quality metrics. Please make sure that this matches with the prior
within the potential_fn. If <code>None</code> is given, we will try to infer it
from potential_fn or q, if this fails we raise an Error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>q</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="sbi.types.PyroTransformedDistribution">PyroTransformedDistribution</span>, <a class="autorefs autorefs-internal" title="sbi.inference.posteriors.vi_posterior.VIPosterior" href="#sbi.inference.posteriors.vi_posterior.VIPosterior">VIPosterior</a>, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Variational distribution, either string, <code>TransformedDistribution</code>, or a
<code>VIPosterior</code> object. This specifies a parametric class of distribution
over which the best possible posterior approximation is searched. For
string input, we currently support [nsf, scf, maf, mcf, gaussian,
gaussian_diag]. You can also specify your own variational family by
passing a pyro <code>TransformedDistribution</code>.
Additionally, we allow a <code>Callable</code>, which allows you the pass a
<code>builder</code> function, which if called returns a distribution. This may be
useful for setting the hyperparameters e.g. <code>num_transfroms</code> within the
<code>get_flow_builder</code> method specifying the number of transformations
within a normalizing flow. If q is already a <code>VIPosterior</code>, then the
arguments will be copied from it (relevant for multi-round training).</p></td>
          <td>
                <code>&#39;maf&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>theta_transform</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchTransform">TorchTransform</span>]</code>
          </td>
          <td><p>Maps form prior support to unconstrained space. The
inverse is used here to ensure that the posterior support is equal to
that of the prior.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>vi_method</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>This specifies the variational methods which are used to fit q to
the posterior. We currently support [rKL, fKL, IW, alpha]. Note that
some of the divergences are <code>mode seeking</code> i.e. they underestimate
variance and collapse on multimodal targets (<code>rKL</code>, <code>alpha</code> for alpha &gt;
1) and some are <code>mass covering</code> i.e. they overestimate variance but
typically cover all modes (<code>fKL</code>, <code>IW</code>, <code>alpha</code> for alpha &lt; 1).</p></td>
          <td>
                <code>&#39;rKL&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Training device, e.g., <code>cpu</code>, <code>cuda</code> or <code>cuda:0</code>. We will ensure
that all other objects are also on this device.</p></td>
          <td>
                <code>&#39;cpu&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>x_shape</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[torch.<span title="torch.Size">Size</span>]</code>
          </td>
          <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>parameters</code></td>
          <td>
                <code><span title="typing.Iterable">Iterable</span></code>
          </td>
          <td><p>List of parameters of the variational posterior. This is only
required for user-defined q i.e. if q does not have a <code>parameters</code>
attribute.</p></td>
          <td>
                <code>[]</code>
          </td>
        </tr>
        <tr>
          <td><code>modules</code></td>
          <td>
                <code><span title="typing.Iterable">Iterable</span></code>
          </td>
          <td><p>List of modules of the variational posterior. This is only
required for user-defined q i.e. if q does not have a <code>modules</code>
attribute.</p></td>
          <td>
                <code>[]</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples.</span>
<span class="sd">        prior: This is the prior distribution. Note that this is only</span>
<span class="sd">            used to check/construct the variational distribution or within some</span>
<span class="sd">            quality metrics. Please make sure that this matches with the prior</span>
<span class="sd">            within the potential_fn. If `None` is given, we will try to infer it</span>
<span class="sd">            from potential_fn or q, if this fails we raise an Error.</span>
<span class="sd">        q: Variational distribution, either string, `TransformedDistribution`, or a</span>
<span class="sd">            `VIPosterior` object. This specifies a parametric class of distribution</span>
<span class="sd">            over which the best possible posterior approximation is searched. For</span>
<span class="sd">            string input, we currently support [nsf, scf, maf, mcf, gaussian,</span>
<span class="sd">            gaussian_diag]. You can also specify your own variational family by</span>
<span class="sd">            passing a pyro `TransformedDistribution`.</span>
<span class="sd">            Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">            `builder` function, which if called returns a distribution. This may be</span>
<span class="sd">            useful for setting the hyperparameters e.g. `num_transfroms` within the</span>
<span class="sd">            `get_flow_builder` method specifying the number of transformations</span>
<span class="sd">            within a normalizing flow. If q is already a `VIPosterior`, then the</span>
<span class="sd">            arguments will be copied from it (relevant for multi-round training).</span>
<span class="sd">        theta_transform: Maps form prior support to unconstrained space. The</span>
<span class="sd">            inverse is used here to ensure that the posterior support is equal to</span>
<span class="sd">            that of the prior.</span>
<span class="sd">        vi_method: This specifies the variational methods which are used to fit q to</span>
<span class="sd">            the posterior. We currently support [rKL, fKL, IW, alpha]. Note that</span>
<span class="sd">            some of the divergences are `mode seeking` i.e. they underestimate</span>
<span class="sd">            variance and collapse on multimodal targets (`rKL`, `alpha` for alpha &gt;</span>
<span class="sd">            1) and some are `mass covering` i.e. they overestimate variance but</span>
<span class="sd">            typically cover all modes (`fKL`, `IW`, `alpha` for alpha &lt; 1).</span>
<span class="sd">        device: Training device, e.g., `cpu`, `cuda` or `cuda:0`. We will ensure</span>
<span class="sd">            that all other objects are also on this device.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">        parameters: List of parameters of the variational posterior. This is only</span>
<span class="sd">            required for user-defined q i.e. if q does not have a `parameters`</span>
<span class="sd">            attribute.</span>
<span class="sd">        modules: List of modules of the variational posterior. This is only</span>
<span class="sd">            required for user-defined q i.e. if q does not have a `modules`</span>
<span class="sd">            attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="c1"># Especially the prior may be on another device -&gt; move it...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

    <span class="c1"># Get prior and previous builds</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="s2">&quot;prior&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Distribution</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;We could not find a suitable prior distribution within `potential_fn`&quot;</span>
            <span class="s2">&quot;or `q` (if a VIPosterior is given). Please explicitly specify a prior.&quot;</span>
        <span class="p">)</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># In contrast to MCMC we want to project into constrained space.</span>
    <span class="k">if</span> <span class="n">theta_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span><span class="o">.</span><span class="n">inv</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span>

    <span class="c1"># This will set the variational distribution and VI method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">vi_method</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides Variational inference to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _normalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.evaluate" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">evaluate</span><span class="p">(</span><span class="n">quality_control_metric</span><span class="o">=</span><span class="s1">&#39;psis&#39;</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">50000.0</span><span class="p">))</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.evaluate" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>This function will evaluate the quality of the variational posterior
distribution. We currently support two different metrics of type <code>psis</code>, which
checks the quality based on the tails of importance weights (there should not be
much with a large one), or <code>prop</code> which checks the proportionality between q
and potential_fn.</p>
<p>NOTE: In our experience <code>prop</code> is sensitive to distinguish <code>good</code> from <code>ok</code>
whereas <code>psis</code> is more sensitive in distinguishing <code>very bad</code> from <code>ok</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>quality_control_metric</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>The metric of choice, we currently support [psis,
prop, prop_prior].</p></td>
          <td>
                <code>&#39;psis&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>N</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of samples which is used to evaluate the metric.</p></td>
          <td>
                <code>int(50000.0)</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function will evaluate the quality of the variational posterior</span>
<span class="sd">    distribution. We currently support two different metrics of type `psis`, which</span>
<span class="sd">    checks the quality based on the tails of importance weights (there should not be</span>
<span class="sd">    much with a large one), or `prop` which checks the proportionality between q</span>
<span class="sd">    and potential_fn.</span>

<span class="sd">    NOTE: In our experience `prop` is sensitive to distinguish ``good`` from ``ok``</span>
<span class="sd">    whereas `psis` is more sensitive in distinguishing `very bad` from `ok`.</span>

<span class="sd">    Args:</span>
<span class="sd">        quality_control_metric: The metric of choice, we currently support [psis,</span>
<span class="sd">            prop, prop_prior].</span>
<span class="sd">        N: Number of samples which is used to evaluate the metric.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quality_control_fn</span><span class="p">,</span> <span class="n">quality_control_msg</span> <span class="o">=</span> <span class="n">get_quality_metric</span><span class="p">(</span>
        <span class="n">quality_control_metric</span>
    <span class="p">)</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quality_control_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)),</span> <span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quality Score: </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="o">+</span> <span class="n">quality_control_msg</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.log_prob" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the log-probability of theta under the variational posterior.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>theta</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Parameters</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>track_gradients</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis but increases memory
consumption.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p><code>len($\theta$)</code>-shaped log-probability.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the variational posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit using observation </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.</span><span class="se">\</span>
<span class="s2">                 Please train.&quot;</span>
        <span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.map" class="doc doc-heading">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.map" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchTensor">TorchTensor</span>]</code>
          </td>
          <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>num_iter</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate of the optimizer.</p></td>
          <td>
                <code>0.01</code>
          </td>
        </tr>
        <tr>
          <td><code>init_method</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="sbi.types.TorchTensor">TorchTensor</span>]</code>
          </td>
          <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
          <td>
                <code>&#39;proposal&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>num_init_samples</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
          <td>
                <code>10000</code>
          </td>
        </tr>
        <tr>
          <td><code>num_to_optimize</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>save_best_every</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bars</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to show a progressbar during sampling from
the posterior.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>force_update</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>log_prob_kwargs</code></td>
          <td>
          </td>
          <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>The MAP estimate.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.sample" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Samples from the variational posterior distribution.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sample_shape</code></td>
          <td>
                <code><span title="sbi.types.Shape">Shape</span></code>
          </td>
          <td><p>Shape of samples</p></td>
          <td>
                <code>torch.Size()</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Samples from posterior.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples from the variational posterior distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Shape of samples</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit on the specified `default_x` &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">. Please train using `posterior.train()`.&quot;</span>
        <span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.set_q" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[],</span> <span class="n">modules</span><span class="o">=</span><span class="p">[])</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_q" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Defines the variational family.</p>
<p>You can specify over which parameters/modules we optimize. This is required for
custom distributions which e.g. do not inherit nn.Modules or has the function
<code>parameters</code> or <code>modules</code> to give direct access to trainable parameters.
Further, you can pass a function, which constructs a variational distribution
if called.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>q</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[str, <span title="sbi.types.PyroTransformedDistribution">PyroTransformedDistribution</span>, <a class="autorefs autorefs-internal" title="sbi.inference.posteriors.vi_posterior.VIPosterior" href="#sbi.inference.posteriors.vi_posterior.VIPosterior">VIPosterior</a>, <span title="typing.Callable">Callable</span>]</code>
          </td>
          <td><p>Variational distribution, either string, distribution, or a VIPosterior
object. This specifies a parametric class of distribution over which
the best possible posterior approximation is searched. For string input,
we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of
course, you can also specify your own variational family by passing a
<code>parameterized</code> distribution object i.e. a torch.distributions
Distribution with methods <code>parameters</code> returning an iterable of all
parameters (you can pass them within the paramters/modules attribute).
Additionally, we allow a <code>Callable</code>, which allows you the pass a
<code>builder</code> function, which if called returns an distribution. This may be
useful for setting the hyperparameters e.g. <code>num_transfroms:int</code> by
using the <code>get_flow_builder</code> method specifying the hyperparameters. If q
is already a <code>VIPosterior</code>, then the arguments will be copied from it
(relevant for multi-round training).</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>parameters</code></td>
          <td>
                <code><span title="typing.Iterable">Iterable</span></code>
          </td>
          <td><p>List of parameters associated with the distribution object.</p></td>
          <td>
                <code>[]</code>
          </td>
        </tr>
        <tr>
          <td><code>modules</code></td>
          <td>
                <code><span title="typing.Iterable">Iterable</span></code>
          </td>
          <td><p>List of modules associated with the distribution object.</p></td>
          <td>
                <code>[]</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_q</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Defines the variational family.</span>

<span class="sd">    You can specify over which parameters/modules we optimize. This is required for</span>
<span class="sd">    custom distributions which e.g. do not inherit nn.Modules or has the function</span>
<span class="sd">    `parameters` or `modules` to give direct access to trainable parameters.</span>
<span class="sd">    Further, you can pass a function, which constructs a variational distribution</span>
<span class="sd">    if called.</span>

<span class="sd">    Args:</span>
<span class="sd">        q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">            object. This specifies a parametric class of distribution over which</span>
<span class="sd">            the best possible posterior approximation is searched. For string input,</span>
<span class="sd">            we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">            course, you can also specify your own variational family by passing a</span>
<span class="sd">            `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">            Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">            parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">            Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">            `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">            useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">            using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">            is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">            (relevant for multi-round training).</span>
<span class="sd">        parameters: List of parameters associated with the distribution object.</span>
<span class="sd">        modules: List of modules associated with the distribution object.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">adapt_variational_distribution</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
            <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="n">self_custom_q_init_cache</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">self_custom_q_init_cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Callable</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">get_flow_builder</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">event_shape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_build_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_trained_on</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vi_method</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">vi_method</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_arg</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span>
    <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Something went wrong when initializing the variational distribution.</span>
<span class="s2">        Please create an issue on github https://github.com/mackelab/sbi/issues&quot;&quot;&quot;</span>
    <span class="n">check_variational_distribution</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">q</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.set_vi_method" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">set_vi_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_vi_method" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Sets variational inference method.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>method</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>One of [rKL, fKL, IW, alpha].</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sbi.inference.posteriors.vi_posterior.VIPosterior" href="#sbi.inference.posteriors.vi_posterior.VIPosterior">VIPosterior</a></code>
          </td>
          <td><p><code>VIPosterior</code> for chainable calls.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets variational inference method.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: One of [rKL, fKL, IW, alpha].</span>

<span class="sd">    Returns:</span>
<span class="sd">        `VIPosterior` for chainable calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span> <span class="o">=</span> <span class="n">get_VI_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="sbi.inference.posteriors.vi_posterior.VIPosterior.train" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_particles</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">max_num_iters</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">min_num_iters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">clip_value</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">warm_up_rounds</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check_for_convergence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">quality_control</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="o">=</span><span class="s1">&#39;psis&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.train" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>This method trains the variational posterior.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sbi.types.TorchTensor">TorchTensor</span>]</code>
          </td>
          <td><p>The observation.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>n_particles</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of samples to approximate expectations within the
variational bounds. The larger the more accurate are gradient
estimates, but the computational cost per iteration increases.</p></td>
          <td>
                <code>256</code>
          </td>
        </tr>
        <tr>
          <td><code>learning_rate</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate of the optimizer.</p></td>
          <td>
                <code>0.001</code>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Learning rate decay per iteration. We use an exponential decay
scheduler.</p></td>
          <td>
                <code>0.999</code>
          </td>
        </tr>
        <tr>
          <td><code>max_num_iters</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Maximum number of iterations.</p></td>
          <td>
                <code>2000</code>
          </td>
        </tr>
        <tr>
          <td><code>min_num_iters</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Minimum number of iterations.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>clip_value</code></td>
          <td>
                <code>float</code>
          </td>
          <td><p>Gradient clipping value, decreasing may help if you see invalid
values.</p></td>
          <td>
                <code>10.0</code>
          </td>
        </tr>
        <tr>
          <td><code>warm_up_rounds</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Initialize the posterior as the prior.</p></td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>retrain_from_scratch</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Retrain the variational distributions from scratch.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>reset_optimizer</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Reset the divergence optimizer</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>show_progress_bar</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>If any progress report should be displayed.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>quality_control</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>If False quality control is skipped.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>quality_control_metric</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>Which metric to use for evaluating the quality.</p></td>
          <td>
                <code>&#39;psis&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>kwargs</code></td>
          <td>
          </td>
          <td><p>Hyperparameters check corresponding <code>DivergenceOptimizer</code> for detail
eps: Determines sensitivity of convergence check.
retain_graph: Boolean which decides whether to retain the computation
    graph. This may be required for some <code>exotic</code> user-specified q&rsquo;s.
optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See
    <code>DivergenceOptimizer</code> for details.
scheduler: A PyTorch learning rate scheduler. See
    <code>DivergenceOptimizer</code> for details.
alpha: Only used if vi_method=<code>alpha</code>. Determines the alpha divergence.
K: Only used if vi_method=<code>IW</code>. Determines the number of importance
    weighted particles.
stick_the_landing: If one should use the STL estimator (only for rKL,
    IW, alpha).
dreg: If one should use the DREG estimator (only for rKL, IW, alpha).
weight_transform: Callable applied to importance weights (only for fKL)</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>VIPosterior</code></td>          <td>
                <code><a class="autorefs autorefs-internal" title="sbi.inference.posteriors.vi_posterior.VIPosterior" href="#sbi.inference.posteriors.vi_posterior.VIPosterior">VIPosterior</a></code>
          </td>
          <td><p><code>VIPosterior</code> (can be used to chain calls).</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/posteriors/vi_posterior.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_particles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
    <span class="n">max_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
    <span class="n">min_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
    <span class="n">warm_up_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">reset_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_for_convergence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">quality_control</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This method trains the variational posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The observation.</span>
<span class="sd">        n_particles: Number of samples to approximate expectations within the</span>
<span class="sd">            variational bounds. The larger the more accurate are gradient</span>
<span class="sd">            estimates, but the computational cost per iteration increases.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        gamma: Learning rate decay per iteration. We use an exponential decay</span>
<span class="sd">            scheduler.</span>
<span class="sd">        max_num_iters: Maximum number of iterations.</span>
<span class="sd">        min_num_iters: Minimum number of iterations.</span>
<span class="sd">        clip_value: Gradient clipping value, decreasing may help if you see invalid</span>
<span class="sd">            values.</span>
<span class="sd">        warm_up_rounds: Initialize the posterior as the prior.</span>
<span class="sd">        retrain_from_scratch: Retrain the variational distributions from scratch.</span>
<span class="sd">        reset_optimizer: Reset the divergence optimizer</span>
<span class="sd">        show_progress_bar: If any progress report should be displayed.</span>
<span class="sd">        quality_control: If False quality control is skipped.</span>
<span class="sd">        quality_control_metric: Which metric to use for evaluating the quality.</span>
<span class="sd">        kwargs: Hyperparameters check corresponding `DivergenceOptimizer` for detail</span>
<span class="sd">            eps: Determines sensitivity of convergence check.</span>
<span class="sd">            retain_graph: Boolean which decides whether to retain the computation</span>
<span class="sd">                graph. This may be required for some `exotic` user-specified q&#39;s.</span>
<span class="sd">            optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See</span>
<span class="sd">                `DivergenceOptimizer` for details.</span>
<span class="sd">            scheduler: A PyTorch learning rate scheduler. See</span>
<span class="sd">                `DivergenceOptimizer` for details.</span>
<span class="sd">            alpha: Only used if vi_method=`alpha`. Determines the alpha divergence.</span>
<span class="sd">            K: Only used if vi_method=`IW`. Determines the number of importance</span>
<span class="sd">                weighted particles.</span>
<span class="sd">            stick_the_landing: If one should use the STL estimator (only for rKL,</span>
<span class="sd">                IW, alpha).</span>
<span class="sd">            dreg: If one should use the DREG estimator (only for rKL, IW, alpha).</span>
<span class="sd">            weight_transform: Callable applied to importance weights (only for fKL)</span>
<span class="sd">    Returns:</span>
<span class="sd">        VIPosterior: `VIPosterior` (can be used to chain calls).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Update optimizer with current arguments.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="o">**</span><span class="nb">locals</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">})</span>

    <span class="c1"># Init q and the optimizer if necessary</span>
    <span class="k">if</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">reset_optimizer</span>
        <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Check context</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_2d_float32_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="p">)</span>

    <span class="n">already_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

    <span class="c1"># Optimize</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">reset_loss_stats</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
        <span class="n">iters</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">iters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">)</span>

    <span class="c1"># Warmup before training</span>
    <span class="k">if</span> <span class="n">reset_optimizer</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up_was_done</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">already_trained</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="s2">&quot;Warmup phase, this may take a few seconds...&quot;</span>
            <span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up</span><span class="p">(</span><span class="n">warm_up_rounds</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iters</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">mean_loss</span><span class="p">,</span> <span class="n">std_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_loss_stats</span><span class="p">()</span>
        <span class="c1"># Update progress bar</span>
        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">tqdm</span><span class="p">)</span>
            <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">std_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Check for convergence</span>
        <span class="k">if</span> <span class="n">check_for_convergence</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">min_num_iters</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">converged</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged with loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
    <span class="c1"># Training finished:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Evaluate quality</span>
    <span class="k">if</span> <span class="n">quality_control</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">quality_control_metric</span><span class="o">=</span><span class="n">quality_control_metric</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Quality control did not work, we reset the variational </span><span class="se">\</span>
<span class="s2">                    posterior,please check your setting. </span><span class="se">\</span>
<span class="s2">                    </span><span class="se">\n</span><span class="s2"> Following error occured </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
                <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div><h2 id="models">Models<a class="headerlink" href="#models" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-function">



<h3 id="sbi.utils.get_nn_models.posterior_nn" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_nn_models</span><span class="o">.</span><span class="n">posterior_nn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_transforms</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embedding_net</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.utils.get_nn_models.posterior_nn" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Returns a function that builds a density estimator for learning the posterior.</p>
<p>This function will usually be used for SNPE. The returned function is to be passed
to the inference class when using the flexible interface.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>The type of density estimator that will be created. One of [<code>mdn</code>,
<code>made</code>, <code>maf</code>, <code>maf_rqs</code>, <code>nsf</code>].</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>z_score_theta</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Whether to z-score parameters <span class="arithmatex">\(\theta\)</span> before passing them into
the network, can take one of the following:
- <code>none</code>, or None: do not z-score.
- <code>independent</code>: z-score each dimension independently.
- <code>structured</code>: treat dimensions as related, therefore compute mean and std
over the entire batch, instead of per-dimension. Should be used when each
sample is, for example, a time series or an image.</p></td>
          <td>
                <code>&#39;independent&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>z_score_x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Whether to z-score simulation outputs <span class="arithmatex">\(x\)</span> before passing them into
the network, same options as z_score_theta.</p></td>
          <td>
                <code>&#39;independent&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>hidden_features</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of hidden features.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>num_transforms</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of transforms when a flow is used. Only relevant if
density estimator is a normalizing flow (i.e. currently either a <code>maf</code> or a
<code>nsf</code>). Ignored if density estimator is a <code>mdn</code> or <code>made</code>.</p></td>
          <td>
                <code>5</code>
          </td>
        </tr>
        <tr>
          <td><code>num_bins</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of bins used for the splines in <code>nsf</code>. Ignored if density
estimator not <code>nsf</code>.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>embedding_net</code></td>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Optional embedding network for simulation outputs <span class="arithmatex">\(x\)</span>. This
embedding net allows to learn features from potentially high-dimensional
simulation outputs.</p></td>
          <td>
                <code>nn.Identity()</code>
          </td>
        </tr>
        <tr>
          <td><code>num_components</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of mixture components for a mixture of Gaussians.
Ignored if density estimator is not an mdn.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>kwargs</code></td>
          <td>
          </td>
          <td><p>additional custom arguments passed to downstream build functions.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/utils/get_nn_models.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">posterior_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">num_transforms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">num_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function that builds a density estimator for learning the posterior.</span>

<span class="sd">    This function will usually be used for SNPE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of density estimator that will be created. One of [`mdn`,</span>
<span class="sd">            `made`, `maf`, `maf_rqs`, `nsf`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network, can take one of the following:</span>
<span class="sd">            - `none`, or None: do not z-score.</span>
<span class="sd">            - `independent`: z-score each dimension independently.</span>
<span class="sd">            - `structured`: treat dimensions as related, therefore compute mean and std</span>
<span class="sd">            over the entire batch, instead of per-dimension. Should be used when each</span>
<span class="sd">            sample is, for example, a time series or an image.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network, same options as z_score_theta.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        num_transforms: Number of transforms when a flow is used. Only relevant if</span>
<span class="sd">            density estimator is a normalizing flow (i.e. currently either a `maf` or a</span>
<span class="sd">            `nsf`). Ignored if density estimator is a `mdn` or `made`.</span>
<span class="sd">        num_bins: Number of bins used for the splines in `nsf`. Ignored if density</span>
<span class="sd">            estimator not `nsf`.</span>
<span class="sd">        embedding_net: Optional embedding network for simulation outputs $x$. This</span>
<span class="sd">            embedding net allows to learn features from potentially high-dimensional</span>
<span class="sd">            simulation outputs.</span>
<span class="sd">        num_components: Number of mixture components for a mixture of Gaussians.</span>
<span class="sd">            Ignored if density estimator is not an mdn.</span>
<span class="sd">        kwargs: additional custom arguments passed to downstream build functions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">&quot;z_score_x&quot;</span><span class="p">,</span>
                <span class="s2">&quot;z_score_y&quot;</span><span class="p">,</span>
                <span class="s2">&quot;hidden_features&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_transforms&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_bins&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embedding_net&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_components&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">num_transforms</span><span class="p">,</span>
                <span class="n">num_bins</span><span class="p">,</span>
                <span class="n">embedding_net</span><span class="p">,</span>
                <span class="n">num_components</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn_snpe_a</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">num_components</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build function for SNPE-A</span>

<span class="sd">        Extract the number of components from the kwargs, such that they are exposed as</span>
<span class="sd">        a kwargs, offering the possibility to later override this kwarg with</span>
<span class="sd">        `functools.partial`. This is necessary in order to make sure that the MDN in</span>
<span class="sd">        SNPE-A only has one component when running the Algorithm 1 part.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">build_mdn</span><span class="p">(</span>
            <span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span>
            <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span>
            <span class="n">num_components</span><span class="o">=</span><span class="n">num_components</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mdn&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mdn</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;made&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_made</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;maf&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_maf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;maf_rqs&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_maf_rqs</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;nsf&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_nsf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_components</span> <span class="o">!=</span> <span class="mi">10</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You set `num_components`. For SNPE-A, this has to be done at &quot;</span>
                <span class="s2">&quot;instantiation of the inference object, i.e. &quot;</span>
                <span class="s2">&quot;`inference = SNPE_A(..., num_components=20)`&quot;</span>
            <span class="p">)</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_components&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">build_fn_snpe_a</span> <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span> <span class="k">else</span> <span class="n">build_fn</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.utils.get_nn_models.likelihood_nn" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_nn_models</span><span class="o">.</span><span class="n">likelihood_nn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_transforms</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embedding_net</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.utils.get_nn_models.likelihood_nn" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Returns a function that builds a density estimator for learning the likelihood.</p>
<p>This function will usually be used for SNLE. The returned function is to be passed
to the inference class when using the flexible interface.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>The type of density estimator that will be created. One of [<code>mdn</code>,
<code>made</code>, <code>maf</code>, <code>maf_rqs</code>, <code>nsf</code>].</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>z_score_theta</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Whether to z-score parameters <span class="arithmatex">\(\theta\)</span> before passing them into
the network, can take one of the following:
- <code>none</code>, or None: do not z-score.
- <code>independent</code>: z-score each dimension independently.
- <code>structured</code>: treat dimensions as related, therefore compute mean and std
over the entire batch, instead of per-dimension. Should be used when each
sample is, for example, a time series or an image.</p></td>
          <td>
                <code>&#39;independent&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>z_score_x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Whether to z-score simulation outputs <span class="arithmatex">\(x\)</span> before passing them into
the network, same options as z_score_theta.</p></td>
          <td>
                <code>&#39;independent&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>hidden_features</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of hidden features.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>num_transforms</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of transforms when a flow is used. Only relevant if
density estimator is a normalizing flow (i.e. currently either a <code>maf</code> or a
<code>nsf</code>). Ignored if density estimator is a <code>mdn</code> or <code>made</code>.</p></td>
          <td>
                <code>5</code>
          </td>
        </tr>
        <tr>
          <td><code>num_bins</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of bins used for the splines in <code>nsf</code>. Ignored if density
estimator not <code>nsf</code>.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>embedding_net</code></td>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Optional embedding network for parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
          <td>
                <code>nn.Identity()</code>
          </td>
        </tr>
        <tr>
          <td><code>num_components</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of mixture components for a mixture of Gaussians.
Ignored if density estimator is not an mdn.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>kwargs</code></td>
          <td>
          </td>
          <td><p>additional custom arguments passed to downstream build functions.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/utils/get_nn_models.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">likelihood_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">num_transforms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">num_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function that builds a density estimator for learning the likelihood.</span>

<span class="sd">    This function will usually be used for SNLE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of density estimator that will be created. One of [`mdn`,</span>
<span class="sd">            `made`, `maf`, `maf_rqs`, `nsf`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network, can take one of the following:</span>
<span class="sd">            - `none`, or None: do not z-score.</span>
<span class="sd">            - `independent`: z-score each dimension independently.</span>
<span class="sd">            - `structured`: treat dimensions as related, therefore compute mean and std</span>
<span class="sd">            over the entire batch, instead of per-dimension. Should be used when each</span>
<span class="sd">            sample is, for example, a time series or an image.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network, same options as z_score_theta.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        num_transforms: Number of transforms when a flow is used. Only relevant if</span>
<span class="sd">            density estimator is a normalizing flow (i.e. currently either a `maf` or a</span>
<span class="sd">            `nsf`). Ignored if density estimator is a `mdn` or `made`.</span>
<span class="sd">        num_bins: Number of bins used for the splines in `nsf`. Ignored if density</span>
<span class="sd">            estimator not `nsf`.</span>
<span class="sd">        embedding_net: Optional embedding network for parameters $\theta$.</span>
<span class="sd">        num_components: Number of mixture components for a mixture of Gaussians.</span>
<span class="sd">            Ignored if density estimator is not an mdn.</span>
<span class="sd">        kwargs: additional custom arguments passed to downstream build functions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">&quot;z_score_x&quot;</span><span class="p">,</span>
                <span class="s2">&quot;z_score_y&quot;</span><span class="p">,</span>
                <span class="s2">&quot;hidden_features&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_transforms&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_bins&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embedding_net&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_components&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">num_transforms</span><span class="p">,</span>
                <span class="n">num_bins</span><span class="p">,</span>
                <span class="n">embedding_net</span><span class="p">,</span>
                <span class="n">num_components</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mdn&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mdn</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;made&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_made</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;maf&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_maf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;maf_rqs&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_maf_rqs</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;nsf&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_nsf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mnle&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mnle</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">return</span> <span class="n">build_fn</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.utils.get_nn_models.classifier_nn" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_nn_models</span><span class="o">.</span><span class="n">classifier_nn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">embedding_net_theta</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">embedding_net_x</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.utils.get_nn_models.classifier_nn" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Returns a function that builds a classifier for learning density ratios.</p>
<p>This function will usually be used for SNRE. The returned function is to be passed
to the inference class when using the flexible interface.</p>
<p>Note that in the view of the SNRE classifier we build below, x=theta and y=x.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td>
                <code>str</code>
          </td>
          <td><p>The type of classifier that will be created. One of [<code>linear</code>, <code>mlp</code>,
<code>resnet</code>].</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>z_score_theta</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Whether to z-score parameters <span class="arithmatex">\(\theta\)</span> before passing them into
the network, can take one of the following:
- <code>none</code>, or None: do not z-score.
- <code>independent</code>: z-score each dimension independently.
- <code>structured</code>: treat dimensions as related, therefore compute mean and std
over the entire batch, instead of per-dimension. Should be used when each
sample is, for example, a time series or an image.</p></td>
          <td>
                <code>&#39;independent&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>z_score_x</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Whether to z-score simulation outputs <span class="arithmatex">\(x\)</span> before passing them into
the network, same options as z_score_theta.</p></td>
          <td>
                <code>&#39;independent&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>hidden_features</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of hidden features.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>embedding_net_theta</code></td>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Optional embedding network for parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
          <td>
                <code>nn.Identity()</code>
          </td>
        </tr>
        <tr>
          <td><code>embedding_net_x</code></td>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>Optional embedding network for simulation outputs <span class="arithmatex">\(x\)</span>. This
embedding net allows to learn features from potentially high-dimensional
simulation outputs.</p></td>
          <td>
                <code>nn.Identity()</code>
          </td>
        </tr>
        <tr>
          <td><code>kwargs</code></td>
          <td>
          </td>
          <td><p>additional custom arguments passed to downstream build functions.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/utils/get_nn_models.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">classifier_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">embedding_net_theta</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">embedding_net_x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function that builds a classifier for learning density ratios.</span>

<span class="sd">    This function will usually be used for SNRE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Note that in the view of the SNRE classifier we build below, x=theta and y=x.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of classifier that will be created. One of [`linear`, `mlp`,</span>
<span class="sd">            `resnet`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network, can take one of the following:</span>
<span class="sd">            - `none`, or None: do not z-score.</span>
<span class="sd">            - `independent`: z-score each dimension independently.</span>
<span class="sd">            - `structured`: treat dimensions as related, therefore compute mean and std</span>
<span class="sd">            over the entire batch, instead of per-dimension. Should be used when each</span>
<span class="sd">            sample is, for example, a time series or an image.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network, same options as z_score_theta.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        embedding_net_theta:  Optional embedding network for parameters $\theta$.</span>
<span class="sd">        embedding_net_x:  Optional embedding network for simulation outputs $x$. This</span>
<span class="sd">            embedding net allows to learn features from potentially high-dimensional</span>
<span class="sd">            simulation outputs.</span>
<span class="sd">        kwargs: additional custom arguments passed to downstream build functions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">&quot;z_score_x&quot;</span><span class="p">,</span>
                <span class="s2">&quot;z_score_y&quot;</span><span class="p">,</span>
                <span class="s2">&quot;hidden_features&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embedding_net_x&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embedding_net_y&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">embedding_net_theta</span><span class="p">,</span>
                <span class="n">embedding_net_x</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_linear_classifier</span><span class="p">(</span>
                <span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mlp&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mlp_classifier</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;resnet&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_resnet_classifier</span><span class="p">(</span>
                <span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">return</span> <span class="n">build_fn</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div><h2 id="potentials">Potentials<a class="headerlink" href="#potentials" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-function">



<h3 id="sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">potentials</span><span class="o">.</span><span class="n">posterior_based_potential</span><span class="o">.</span><span class="n">posterior_estimator_based_potential</span><span class="p">(</span><span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Returns the potential for posterior-based methods.</p>
<p>It also returns a transformation that can be used to transform the potential into
unconstrained space.</p>
<p>The potential is the same as the log-probability of the <code>posterior_estimator</code>, but
it is set to <span class="arithmatex">\(-\inf\)</span> outside of the prior bounds.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>posterior_estimator</code></td>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>The neural network modelling the posterior.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="torch.distributions.Distribution">Distribution</span></code>
          </td>
          <td><p>The prior distribution.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>x_o</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>The observed data at which to evaluate the posterior.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>enable_transform</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to transform parameters to unconstrained space.
When False, an identity transform will be returned for <code>theta_transform</code>.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>The potential function and a transformation that maps</p></td>
        </tr>
        <tr>
          <td>
                <code><span title="sbi.types.TorchTransform">TorchTransform</span></code>
          </td>
          <td><p>to unconstrained space.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/potentials/posterior_based_potential.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">posterior_estimator_based_potential</span><span class="p">(</span>
    <span class="n">posterior_estimator</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">TorchTransform</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the potential for posterior-based methods.</span>

<span class="sd">    It also returns a transformation that can be used to transform the potential into</span>
<span class="sd">    unconstrained space.</span>

<span class="sd">    The potential is the same as the log-probability of the `posterior_estimator`, but</span>
<span class="sd">    it is set to $-\inf$ outside of the prior bounds.</span>

<span class="sd">    Args:</span>
<span class="sd">        posterior_estimator: The neural network modelling the posterior.</span>
<span class="sd">        prior: The prior distribution.</span>
<span class="sd">        x_o: The observed data at which to evaluate the posterior.</span>
<span class="sd">        enable_transform: Whether to transform parameters to unconstrained space.</span>
<span class="sd">            When False, an identity transform will be returned for `theta_transform`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The potential function and a transformation that maps</span>
<span class="sd">        to unconstrained space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">potential_fn</span> <span class="o">=</span> <span class="n">PosteriorBasedPotential</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>

    <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span>
        <span class="n">prior</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">potentials</span><span class="o">.</span><span class="n">likelihood_based_potential</span><span class="o">.</span><span class="n">likelihood_estimator_based_potential</span><span class="p">(</span><span class="n">likelihood_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Returns potential <span class="arithmatex">\(\log(p(x_o|\theta)p(\theta))\)</span> for likelihood-based methods.</p>
<p>It also returns a transformation that can be used to transform the potential into
unconstrained space.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>likelihood_estimator</code></td>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>The neural network modelling the likelihood.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="torch.distributions.Distribution">Distribution</span></code>
          </td>
          <td><p>The prior distribution.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>x_o</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>The observed data at which to evaluate the likelihood.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>enable_transform</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to transform parameters to unconstrained space.
 When False, an identity transform will be returned for <code>theta_transform</code>.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>The potential function <span class="arithmatex">\(p(x_o|\theta)p(\theta)\)</span> and a transformation that maps</p></td>
        </tr>
        <tr>
          <td>
                <code><span title="sbi.types.TorchTransform">TorchTransform</span></code>
          </td>
          <td><p>to unconstrained space.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/potentials/likelihood_based_potential.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">likelihood_estimator_based_potential</span><span class="p">(</span>
    <span class="n">likelihood_estimator</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">TorchTransform</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns potential $\log(p(x_o|\theta)p(\theta))$ for likelihood-based methods.</span>

<span class="sd">    It also returns a transformation that can be used to transform the potential into</span>
<span class="sd">    unconstrained space.</span>

<span class="sd">    Args:</span>
<span class="sd">        likelihood_estimator: The neural network modelling the likelihood.</span>
<span class="sd">        prior: The prior distribution.</span>
<span class="sd">        x_o: The observed data at which to evaluate the likelihood.</span>
<span class="sd">        enable_transform: Whether to transform parameters to unconstrained space.</span>
<span class="sd">             When False, an identity transform will be returned for `theta_transform`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The potential function $p(x_o|\theta)p(\theta)$ and a transformation that maps</span>
<span class="sd">        to unconstrained space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">likelihood_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">potential_fn</span> <span class="o">=</span> <span class="n">LikelihoodBasedPotential</span><span class="p">(</span>
        <span class="n">likelihood_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span>
        <span class="n">prior</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">potentials</span><span class="o">.</span><span class="n">ratio_based_potential</span><span class="o">.</span><span class="n">ratio_estimator_based_potential</span><span class="p">(</span><span class="n">ratio_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Returns the potential for ratio-based methods.</p>
<p>It also returns a transformation that can be used to transform the potential into
unconstrained space.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>ratio_estimator</code></td>
          <td>
                <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>
          </td>
          <td><p>The neural network modelling likelihood-to-evidence ratio.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>prior</code></td>
          <td>
                <code><span title="torch.distributions.Distribution">Distribution</span></code>
          </td>
          <td><p>The prior distribution.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>x_o</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>The observed data at which to evaluate the likelihood-to-evidence ratio.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>enable_transform</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>Whether to transform parameters to unconstrained space.
When False, an identity transform will be returned for <code>theta_transform</code>.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Callable">Callable</span></code>
          </td>
          <td><p>The potential function and a transformation that maps</p></td>
        </tr>
        <tr>
          <td>
                <code><span title="sbi.types.TorchTransform">TorchTransform</span></code>
          </td>
          <td><p>to unconstrained space.</p></td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/inference/potentials/ratio_based_potential.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ratio_estimator_based_potential</span><span class="p">(</span>
    <span class="n">ratio_estimator</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">TorchTransform</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the potential for ratio-based methods.</span>

<span class="sd">    It also returns a transformation that can be used to transform the potential into</span>
<span class="sd">    unconstrained space.</span>

<span class="sd">    Args:</span>
<span class="sd">        ratio_estimator: The neural network modelling likelihood-to-evidence ratio.</span>
<span class="sd">        prior: The prior distribution.</span>
<span class="sd">        x_o: The observed data at which to evaluate the likelihood-to-evidence ratio.</span>
<span class="sd">        enable_transform: Whether to transform parameters to unconstrained space.</span>
<span class="sd">            When False, an identity transform will be returned for `theta_transform`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The potential function and a transformation that maps</span>
<span class="sd">        to unconstrained space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">ratio_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">potential_fn</span> <span class="o">=</span> <span class="n">RatioBasedPotential</span><span class="p">(</span><span class="n">ratio_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span>
        <span class="n">prior</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div><h2 id="analysis">Analysis<a class="headerlink" href="#analysis" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-function">



<h3 id="sbi.analysis.plot.pairplot" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offdiag</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">upper</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.analysis.plot.pairplot" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Plot samples in a 2D grid showing marginals and pairwise marginals.</p>
<p>Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution
that the samples were drawn from. Each upper-diagonal plot can be interpreted as a
2D-marginal of the distribution.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>samples</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[<span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>], <span title="typing.List">List</span>[torch.<span title="torch.Tensor">Tensor</span>], <span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>, torch.<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Samples used to build the histogram.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>points</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.List">List</span>[<span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>], <span title="typing.List">List</span>[torch.<span title="torch.Tensor">Tensor</span>], <span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>, torch.<span title="torch.Tensor">Tensor</span>]]</code>
          </td>
          <td><p>List of additional points to scatter.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>limits</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.List">List</span>, torch.<span title="torch.Tensor">Tensor</span>]]</code>
          </td>
          <td><p>Array containing the plot xlim for each parameter dimension. If None,
just use the min and max of the passed samples</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>subset</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[int]]</code>
          </td>
          <td><p>List containing the dimensions to plot. E.g. subset=[1,3] will plot
plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and,
if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on).</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>offdiag</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.List">List</span>[str], str]]</code>
          </td>
          <td><p>Plotting style for upper diagonal, {hist, scatter, contour, cond,
None}.</p></td>
          <td>
                <code>&#39;hist&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>upper</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>deprecated, use offdiag instead.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>diag</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.List">List</span>[str], str]]</code>
          </td>
          <td><p>Plotting style for diagonal, {hist, cond, None}.</p></td>
          <td>
                <code>&#39;hist&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>figsize</code></td>
          <td>
                <code><span title="typing.Tuple">Tuple</span></code>
          </td>
          <td><p>Size of the entire figure.</p></td>
          <td>
                <code>(10, 10)</code>
          </td>
        </tr>
        <tr>
          <td><code>labels</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
          </td>
          <td><p>List of strings specifying the names of the parameters.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>ticks</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>, torch.<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Position of the ticks.</p></td>
          <td>
                <code>[]</code>
          </td>
        </tr>
        <tr>
          <td><code>fig</code></td>
          <td>
          </td>
          <td><p>matplotlib figure to plot on.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>axes</code></td>
          <td>
          </td>
          <td><p>matplotlib axes corresponding to fig.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
          </td>
          <td><p>Additional arguments to adjust the plot, e.g., <code>samples_colors</code>,
<code>points_colors</code> and many more, see the source code in <code>_get_default_opts()</code>
in <code>sbi.analysis.plot</code> for details.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/analysis/plot.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">pairplot</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">limits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">offdiag</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">,</span>
    <span class="n">diag</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">,</span>
    <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ticks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">upper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot samples in a 2D grid showing marginals and pairwise marginals.</span>

<span class="sd">    Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution</span>
<span class="sd">    that the samples were drawn from. Each upper-diagonal plot can be interpreted as a</span>
<span class="sd">    2D-marginal of the distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        samples: Samples used to build the histogram.</span>
<span class="sd">        points: List of additional points to scatter.</span>
<span class="sd">        limits: Array containing the plot xlim for each parameter dimension. If None,</span>
<span class="sd">            just use the min and max of the passed samples</span>
<span class="sd">        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot</span>
<span class="sd">            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,</span>
<span class="sd">            if they exist, the 4th, 5th and so on).</span>
<span class="sd">        offdiag: Plotting style for upper diagonal, {hist, scatter, contour, cond,</span>
<span class="sd">            None}.</span>
<span class="sd">        upper: deprecated, use offdiag instead.</span>
<span class="sd">        diag: Plotting style for diagonal, {hist, cond, None}.</span>
<span class="sd">        figsize: Size of the entire figure.</span>
<span class="sd">        labels: List of strings specifying the names of the parameters.</span>
<span class="sd">        ticks: Position of the ticks.</span>
<span class="sd">        fig: matplotlib figure to plot on.</span>
<span class="sd">        axes: matplotlib axes corresponding to fig.</span>
<span class="sd">        **kwargs: Additional arguments to adjust the plot, e.g., `samples_colors`,</span>
<span class="sd">            `points_colors` and many more, see the source code in `_get_default_opts()`</span>
<span class="sd">            in `sbi.analysis.plot` for details.</span>

<span class="sd">    Returns: figure and axis of posterior distribution plot</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># TODO: add color map support</span>
    <span class="c1"># TODO: automatically determine good bin sizes for histograms</span>
    <span class="c1"># TODO: add legend (if legend is True)</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_get_default_opts</span><span class="p">()</span>
    <span class="c1"># update the defaults dictionary by the current values of the variables (passed by</span>
    <span class="c1"># the user)</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="n">prepare_for_plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">limits</span><span class="p">)</span>

    <span class="c1"># checks.</span>
    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;legend&quot;</span><span class="p">]:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;samples_labels&quot;</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">samples</span>
        <span class="p">),</span> <span class="s2">&quot;Provide at least as many labels as samples.&quot;</span>
    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;upper is deprecated, use offdiag instead.&quot;</span><span class="p">)</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">]</span>

    <span class="c1"># Prepare diag/upper/lower</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>
    <span class="c1"># if type(opts[&#39;lower&#39;]) is not list:</span>
    <span class="c1">#    opts[&#39;lower&#39;] = [opts[&#39;lower&#39;] for _ in range(len(samples))]</span>
    <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;lower&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">diag_func</span> <span class="o">=</span> <span class="n">get_diag_func</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">offdiag_func</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;hist&quot;</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;hist2d&quot;</span><span class="p">:</span>
                    <span class="n">hist</span><span class="p">,</span> <span class="n">xedges</span><span class="p">,</span> <span class="n">yedges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram2d</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                        <span class="nb">range</span><span class="o">=</span><span class="p">[</span>
                            <span class="p">[</span><span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span>
                            <span class="p">[</span><span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span>
                        <span class="p">],</span>
                        <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;hist_offdiag&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
                        <span class="n">hist</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                        <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span>
                        <span class="n">extent</span><span class="o">=</span><span class="p">(</span>
                            <span class="n">xedges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">xedges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">yedges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">yedges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="p">),</span>
                        <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="s2">&quot;kde&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;kde2d&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;contour&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;contourf&quot;</span><span class="p">,</span>
                <span class="p">]:</span>
                    <span class="n">density</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="n">row</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                        <span class="n">bw_method</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;kde_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;bw_method&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                            <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;kde_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;bins&quot;</span><span class="p">],</span>
                        <span class="p">),</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                            <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;kde_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;bins&quot;</span><span class="p">],</span>
                        <span class="p">),</span>
                    <span class="p">)</span>
                    <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
                    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">density</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;kde&quot;</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;kde2d&quot;</span><span class="p">:</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
                            <span class="n">Z</span><span class="p">,</span>
                            <span class="n">extent</span><span class="o">=</span><span class="p">(</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="p">),</span>
                            <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span>
                            <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;contour&quot;</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;contour_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;percentile&quot;</span><span class="p">]:</span>
                            <span class="n">Z</span> <span class="o">=</span> <span class="n">probs2contours</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;contour_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;levels&quot;</span><span class="p">])</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span>
                            <span class="n">X</span><span class="p">,</span>
                            <span class="n">Y</span><span class="p">,</span>
                            <span class="n">Z</span><span class="p">,</span>
                            <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span>
                            <span class="n">extent</span><span class="o">=</span><span class="p">[</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="p">],</span>
                            <span class="n">colors</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;samples_colors&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                            <span class="n">levels</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;contour_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;levels&quot;</span><span class="p">],</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;scatter&quot;</span><span class="p">:</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;samples_colors&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                        <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;scatter_offdiag&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;offdiag&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;plot&quot;</span><span class="p">:</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;samples_colors&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                        <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;plot_offdiag&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">pass</span>

    <span class="k">return</span> <span class="n">_arrange_plots</span><span class="p">(</span>
        <span class="n">diag_func</span><span class="p">,</span> <span class="n">offdiag_func</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.analysis.plot.marginal_plot" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">marginal_plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.analysis.plot.marginal_plot" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Plot samples in a row showing 1D marginals of selected dimensions.</p>
<p>Each of the plots can be interpreted as a 1D-marginal of the distribution
that the samples were drawn from.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>samples</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[<span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>], <span title="typing.List">List</span>[torch.<span title="torch.Tensor">Tensor</span>], <span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>, torch.<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Samples used to build the histogram.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>points</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.List">List</span>[<span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>], <span title="typing.List">List</span>[torch.<span title="torch.Tensor">Tensor</span>], <span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>, torch.<span title="torch.Tensor">Tensor</span>]]</code>
          </td>
          <td><p>List of additional points to scatter.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>limits</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.List">List</span>, torch.<span title="torch.Tensor">Tensor</span>]]</code>
          </td>
          <td><p>Array containing the plot xlim for each parameter dimension. If None,
just use the min and max of the passed samples</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>subset</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[int]]</code>
          </td>
          <td><p>List containing the dimensions to plot. E.g. subset=[1,3] will plot
plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and,
if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on).</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>diag</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>Plotting style for 1D marginals, {hist, kde cond, None}.</p></td>
          <td>
                <code>&#39;hist&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>figsize</code></td>
          <td>
                <code><span title="typing.Tuple">Tuple</span></code>
          </td>
          <td><p>Size of the entire figure.</p></td>
          <td>
                <code>(10, 10)</code>
          </td>
        </tr>
        <tr>
          <td><code>labels</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
          </td>
          <td><p>List of strings specifying the names of the parameters.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>ticks</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>, torch.<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Position of the ticks.</p></td>
          <td>
                <code>[]</code>
          </td>
        </tr>
        <tr>
          <td><code>points_colors</code></td>
          <td>
          </td>
          <td><p>Colors of the <code>points</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>fig</code></td>
          <td>
          </td>
          <td><p>matplotlib figure to plot on.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>axes</code></td>
          <td>
          </td>
          <td><p>matplotlib axes corresponding to fig.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
          </td>
          <td><p>Additional arguments to adjust the plot, e.g., <code>samples_colors</code>,
<code>points_colors</code> and many more, see the source code in <code>_get_default_opts()</code>
in <code>sbi.analysis.plot</code> for details.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/analysis/plot.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">marginal_plot</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">limits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">diag</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">,</span>
    <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ticks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot samples in a row showing 1D marginals of selected dimensions.</span>

<span class="sd">    Each of the plots can be interpreted as a 1D-marginal of the distribution</span>
<span class="sd">    that the samples were drawn from.</span>

<span class="sd">    Args:</span>
<span class="sd">        samples: Samples used to build the histogram.</span>
<span class="sd">        points: List of additional points to scatter.</span>
<span class="sd">        limits: Array containing the plot xlim for each parameter dimension. If None,</span>
<span class="sd">            just use the min and max of the passed samples</span>
<span class="sd">        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot</span>
<span class="sd">            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,</span>
<span class="sd">            if they exist, the 4th, 5th and so on).</span>
<span class="sd">        diag: Plotting style for 1D marginals, {hist, kde cond, None}.</span>
<span class="sd">        figsize: Size of the entire figure.</span>
<span class="sd">        labels: List of strings specifying the names of the parameters.</span>
<span class="sd">        ticks: Position of the ticks.</span>
<span class="sd">        points_colors: Colors of the `points`.</span>
<span class="sd">        fig: matplotlib figure to plot on.</span>
<span class="sd">        axes: matplotlib axes corresponding to fig.</span>
<span class="sd">        **kwargs: Additional arguments to adjust the plot, e.g., `samples_colors`,</span>
<span class="sd">            `points_colors` and many more, see the source code in `_get_default_opts()`</span>
<span class="sd">            in `sbi.analysis.plot` for details.</span>

<span class="sd">    Returns: figure and axis of posterior distribution plot</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_get_default_opts</span><span class="p">()</span>
    <span class="c1"># update the defaults dictionary by the current values of the variables (passed by</span>
    <span class="c1"># the user)</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="n">prepare_for_plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">limits</span><span class="p">)</span>

    <span class="c1"># Prepare diag/upper/lower</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>

    <span class="n">diag_func</span> <span class="o">=</span> <span class="n">get_diag_func</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_arrange_plots</span><span class="p">(</span>
        <span class="n">diag_func</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.analysis.plot.conditional_pairplot" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">conditional_pairplot</span><span class="p">(</span><span class="n">density</span><span class="p">,</span> <span class="n">condition</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.analysis.plot.conditional_pairplot" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Plot conditional distribution given all other parameters.</p>
<p>The conditionals can be interpreted as slices through the <code>density</code> at a location
given by <code>condition</code>.</p>
<p>For example:
Say we have a 3D density with parameters <span class="arithmatex">\(\theta_0\)</span>, <span class="arithmatex">\(\theta_1\)</span>, <span class="arithmatex">\(\theta_2\)</span> and
a condition <span class="arithmatex">\(c\)</span> passed by the user in the <code>condition</code> argument.
For the plot of <span class="arithmatex">\(\theta_0\)</span> on the diagonal, this will plot the conditional
<span class="arithmatex">\(p(\theta_0 | \theta_1=c[1], \theta_2=c[2])\)</span>. For the upper
diagonal of <span class="arithmatex">\(\theta_1\)</span> and <span class="arithmatex">\(\theta_2\)</span>, it will plot
<span class="arithmatex">\(p(\theta_1, \theta_2 | \theta_0=c[0])\)</span>. All other diagonals and upper-diagonals
are built in the corresponding way.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>density</code></td>
          <td>
                <code><span title="typing.Any">Any</span></code>
          </td>
          <td><p>Probability density with a <code>log_prob()</code> method.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>condition</code></td>
          <td>
                <code>torch.<span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Condition that all but the one/two regarded parameters are fixed to.
The condition should be of shape (1, dim_theta), i.e. it could e.g. be
a sample from the posterior distribution.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>limits</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>, torch.<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Limits in between which each parameter will be evaluated.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>points</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.List">List</span>[<span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>], <span title="typing.List">List</span>[torch.<span title="torch.Tensor">Tensor</span>], <span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>, torch.<span title="torch.Tensor">Tensor</span>]]</code>
          </td>
          <td><p>Additional points to scatter.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>subset</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[int]]</code>
          </td>
          <td><p>List containing the dimensions to plot. E.g. subset=[1,3] will plot
plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and,
if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on)</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>resolution</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Resolution of the grid at which we evaluate the <code>pdf</code>.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
        <tr>
          <td><code>figsize</code></td>
          <td>
                <code><span title="typing.Tuple">Tuple</span></code>
          </td>
          <td><p>Size of the entire figure.</p></td>
          <td>
                <code>(10, 10)</code>
          </td>
        </tr>
        <tr>
          <td><code>labels</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
          </td>
          <td><p>List of strings specifying the names of the parameters.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>ticks</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>, torch.<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>Position of the ticks.</p></td>
          <td>
                <code>[]</code>
          </td>
        </tr>
        <tr>
          <td><code>points_colors</code></td>
          <td>
          </td>
          <td><p>Colors of the <code>points</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>fig</code></td>
          <td>
          </td>
          <td><p>matplotlib figure to plot on.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>axes</code></td>
          <td>
          </td>
          <td><p>matplotlib axes corresponding to fig.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
          </td>
          <td><p>Additional arguments to adjust the plot, e.g., <code>samples_colors</code>,
<code>points_colors</code> and many more, see the source code in <code>_get_default_opts()</code>
in <code>sbi.analysis.plot</code> for details.</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/analysis/plot.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">conditional_pairplot</span><span class="p">(</span>
    <span class="n">density</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">condition</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">limits</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">resolution</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ticks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot conditional distribution given all other parameters.</span>

<span class="sd">    The conditionals can be interpreted as slices through the `density` at a location</span>
<span class="sd">    given by `condition`.</span>

<span class="sd">    For example:</span>
<span class="sd">    Say we have a 3D density with parameters $\theta_0$, $\theta_1$, $\theta_2$ and</span>
<span class="sd">    a condition $c$ passed by the user in the `condition` argument.</span>
<span class="sd">    For the plot of $\theta_0$ on the diagonal, this will plot the conditional</span>
<span class="sd">    $p(\theta_0 | \theta_1=c[1], \theta_2=c[2])$. For the upper</span>
<span class="sd">    diagonal of $\theta_1$ and $\theta_2$, it will plot</span>
<span class="sd">    $p(\theta_1, \theta_2 | \theta_0=c[0])$. All other diagonals and upper-diagonals</span>
<span class="sd">    are built in the corresponding way.</span>

<span class="sd">    Args:</span>
<span class="sd">        density: Probability density with a `log_prob()` method.</span>
<span class="sd">        condition: Condition that all but the one/two regarded parameters are fixed to.</span>
<span class="sd">            The condition should be of shape (1, dim_theta), i.e. it could e.g. be</span>
<span class="sd">            a sample from the posterior distribution.</span>
<span class="sd">        limits: Limits in between which each parameter will be evaluated.</span>
<span class="sd">        points: Additional points to scatter.</span>
<span class="sd">        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot</span>
<span class="sd">            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,</span>
<span class="sd">            if they exist, the 4th, 5th and so on)</span>
<span class="sd">        resolution: Resolution of the grid at which we evaluate the `pdf`.</span>
<span class="sd">        figsize: Size of the entire figure.</span>
<span class="sd">        labels: List of strings specifying the names of the parameters.</span>
<span class="sd">        ticks: Position of the ticks.</span>
<span class="sd">        points_colors: Colors of the `points`.</span>

<span class="sd">        fig: matplotlib figure to plot on.</span>
<span class="sd">        axes: matplotlib axes corresponding to fig.</span>
<span class="sd">        **kwargs: Additional arguments to adjust the plot, e.g., `samples_colors`,</span>
<span class="sd">            `points_colors` and many more, see the source code in `_get_default_opts()`</span>
<span class="sd">            in `sbi.analysis.plot` for details.</span>

<span class="sd">    Returns: figure and axis of posterior distribution plot</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">density</span><span class="o">.</span><span class="n">_device</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">density</span><span class="p">,</span> <span class="s2">&quot;_device&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="c1"># Setting these is required because _pairplot_scaffold will check if opts[&#39;diag&#39;] is</span>
    <span class="c1"># `None`. This would break if opts has no key &#39;diag&#39;. Same for &#39;upper&#39;.</span>
    <span class="n">diag</span> <span class="o">=</span> <span class="s2">&quot;cond&quot;</span>
    <span class="n">offdiag</span> <span class="o">=</span> <span class="s2">&quot;cond&quot;</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_get_default_opts</span><span class="p">()</span>
    <span class="c1"># update the defaults dictionary by the current values of the variables (passed by</span>
    <span class="c1"># the user)</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;lower&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">eps_margins</span> <span class="o">=</span> <span class="n">prepare_for_conditional_plot</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="n">diag_func</span> <span class="o">=</span> <span class="n">get_conditional_diag_func</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">eps_margins</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">offdiag_func</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">p_image</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">eval_conditional_density</span><span class="p">(</span>
                <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;density&quot;</span><span class="p">],</span>
                <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;condition&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                <span class="n">limits</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                <span class="n">row</span><span class="p">,</span>
                <span class="n">col</span><span class="p">,</span>
                <span class="n">resolution</span><span class="o">=</span><span class="n">resolution</span><span class="p">,</span>
                <span class="n">eps_margins1</span><span class="o">=</span><span class="n">eps_margins</span><span class="p">[</span><span class="n">row</span><span class="p">],</span>
                <span class="n">eps_margins2</span><span class="o">=</span><span class="n">eps_margins</span><span class="p">[</span><span class="n">col</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
            <span class="n">p_image</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
            <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span>
            <span class="n">extent</span><span class="o">=</span><span class="p">(</span>
                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="p">),</span>
            <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">_arrange_plots</span><span class="p">(</span>
        <span class="n">diag_func</span><span class="p">,</span> <span class="n">offdiag_func</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="sbi.analysis.conditional_density.conditional_corrcoeff" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">conditional_density</span><span class="o">.</span><span class="n">conditional_corrcoeff</span><span class="p">(</span><span class="n">density</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">condition</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

<a href="#sbi.analysis.conditional_density.conditional_corrcoeff" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents first">
  
      <p>Returns the conditional correlation matrix of a distribution.</p>
<p>To compute the conditional distribution, we condition all but two parameters to
values from <code>condition</code>, and then compute the Pearson correlation
coefficient <span class="arithmatex">\(\rho\)</span> between the remaining two parameters under the distribution
<code>density</code>. We do so for any pair of parameters specified in <code>subset</code>, thus
creating a matrix containing conditional correlations between any pair of
parameters.</p>
<p>If <code>condition</code> is a batch of conditions, this function computes the conditional
correlation matrix for each one of them and returns the mean.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>density</code></td>
          <td>
                <code><span title="typing.Any">Any</span></code>
          </td>
          <td><p>Probability density function with <code>.log_prob()</code> function.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>limits</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Limits within which to evaluate the <code>density</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>condition</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td><p>Values to condition the <code>density</code> on. If a batch of conditions is
passed, we compute the conditional correlation matrix for each of them and
return the average conditional correlation matrix.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>subset</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[int]]</code>
          </td>
          <td><p>Evaluate the conditional distribution only on a subset of dimensions.
If <code>None</code> this function uses all dimensions.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>resolution</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>Number of grid points on which the conditional distribution is
evaluated. A higher value increases the accuracy of the estimated
correlation but also increases the computational cost.</p></td>
          <td>
                <code>50</code>
          </td>
        </tr>
    </tbody>
  </table>
      <p>or <code>(len(subset), len(subset))</code> if <code>subset</code> was specified.</p>

      <details class="quote">
        <summary>Source code in <code>/home/michael/Documents/sbi/sbi/analysis/conditional_density.py</code></summary>
        <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">conditional_corrcoeff</span><span class="p">(</span>
    <span class="n">density</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">limits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">condition</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">resolution</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the conditional correlation matrix of a distribution.</span>

<span class="sd">    To compute the conditional distribution, we condition all but two parameters to</span>
<span class="sd">    values from `condition`, and then compute the Pearson correlation</span>
<span class="sd">    coefficient $\rho$ between the remaining two parameters under the distribution</span>
<span class="sd">    `density`. We do so for any pair of parameters specified in `subset`, thus</span>
<span class="sd">    creating a matrix containing conditional correlations between any pair of</span>
<span class="sd">    parameters.</span>

<span class="sd">    If `condition` is a batch of conditions, this function computes the conditional</span>
<span class="sd">    correlation matrix for each one of them and returns the mean.</span>

<span class="sd">    Args:</span>
<span class="sd">        density: Probability density function with `.log_prob()` function.</span>
<span class="sd">        limits: Limits within which to evaluate the `density`.</span>
<span class="sd">        condition: Values to condition the `density` on. If a batch of conditions is</span>
<span class="sd">            passed, we compute the conditional correlation matrix for each of them and</span>
<span class="sd">            return the average conditional correlation matrix.</span>
<span class="sd">        subset: Evaluate the conditional distribution only on a subset of dimensions.</span>
<span class="sd">            If `None` this function uses all dimensions.</span>
<span class="sd">        resolution: Number of grid points on which the conditional distribution is</span>
<span class="sd">            evaluated. A higher value increases the accuracy of the estimated</span>
<span class="sd">            correlation but also increases the computational cost.</span>

<span class="sd">    Returns: Average conditional correlation matrix of shape either `(num_dim, num_dim)`</span>
<span class="sd">    or `(len(subset), len(subset))` if `subset` was specified.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">density</span><span class="o">.</span><span class="n">_device</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">density</span><span class="p">,</span> <span class="s2">&quot;_device&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="n">subset_</span> <span class="o">=</span> <span class="n">subset</span> <span class="k">if</span> <span class="n">subset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">range</span><span class="p">(</span><span class="n">condition</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">correlation_matrices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cond</span> <span class="ow">in</span> <span class="n">condition</span><span class="p">:</span>
        <span class="n">correlation_matrices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">compute_corrcoeff</span><span class="p">(</span>
                        <span class="n">eval_conditional_density</span><span class="p">(</span>
                            <span class="n">density</span><span class="p">,</span>
                            <span class="n">cond</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                            <span class="n">limits</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                            <span class="n">dim1</span><span class="o">=</span><span class="n">dim1</span><span class="p">,</span>
                            <span class="n">dim2</span><span class="o">=</span><span class="n">dim2</span><span class="p">,</span>
                            <span class="n">resolution</span><span class="o">=</span><span class="n">resolution</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="n">limits</span><span class="p">[[</span><span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">]]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">dim1</span> <span class="ow">in</span> <span class="n">subset_</span>
                    <span class="k">for</span> <span class="n">dim2</span> <span class="ow">in</span> <span class="n">subset_</span>
                    <span class="k">if</span> <span class="n">dim1</span> <span class="o">&lt;</span> <span class="n">dim2</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">average_correlations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">correlation_matrices</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># `average_correlations` is still a vector containing the upper triangular entries.</span>
    <span class="c1"># Below, assemble them into a matrix:</span>
    <span class="n">av_correlation_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset_</span><span class="p">)),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">triu_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span>
        <span class="n">row</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_</span><span class="p">),</span> <span class="n">offset</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="n">av_correlation_matrix</span><span class="p">[</span><span class="n">triu_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">triu_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">average_correlations</span>

    <span class="c1"># Make the matrix symmetric by copying upper diagonal to lower diagonal.</span>
    <span class="n">av_correlation_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">av_correlation_matrix</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span>
        <span class="n">av_correlation_matrix</span><span class="o">.</span><span class="n">T</span>
    <span class="p">)</span>

    <span class="n">av_correlation_matrix</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">av_correlation_matrix</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/sbi-dev/sbi" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.fac441b0.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>