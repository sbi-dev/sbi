
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ie=edge" http-equiv="x-ua-compatible"/>
<link href="https://mackelab.org/sbi/reference/" rel="canonical"/>
<meta content="Copy to clipboard" name="lang:clipboard.copy"/>
<meta content="Copied to clipboard" name="lang:clipboard.copied"/>
<meta content="en" name="lang:search.language"/>
<meta content="True" name="lang:search.pipeline.stopwords"/>
<meta content="True" name="lang:search.pipeline.trimmer"/>
<meta content="No matching documents" name="lang:search.result.none"/>
<meta content="1 matching document" name="lang:search.result.one"/>
<meta content="# matching documents" name="lang:search.result.other"/>
<meta content="[\s\-]+" name="lang:search.tokenizer"/>
<link href="../assets/images/favicon.png" rel="shortcut icon"/>
<meta content="mkdocs-1.1, mkdocs-material-4.6.3" name="generator"/>
<title>API Reference - sbi</title>
<link href="../assets/stylesheets/application.adb8469c.css" rel="stylesheet"/>
<link href="../assets/stylesheets/application-palette.a8b3c06d.css" rel="stylesheet"/>
<meta content="#3f51b5" name="theme-color"/>
<script src="../assets/javascripts/modernizr.86422ebf.js"></script>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&amp;display=fallback" rel="stylesheet"/>
<style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
<link href="../assets/fonts/material-icons.css" rel="stylesheet"/>
<link href="../static/global.css" rel="stylesheet"/>
<link href="../css/ansi-colours.css" rel="stylesheet"/>
<link href="../css/jupyter-cells.css" rel="stylesheet"/>
<link href="../css/pandas-dataframe.css" rel="stylesheet"/>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" dir="ltr">
<svg class="md-svg">
<defs>
<svg height="448" id="__github" viewbox="0 0 416 448" width="416" xmlns="http://www.w3.org/2000/svg"><path d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor"></path></svg>
</defs>
</svg>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
<a class="md-skip" href="#api-reference" tabindex="0">
        Skip to content
      </a>
<header class="md-header" data-md-component="header">
<nav class="md-header-nav md-grid">
<div class="md-flex">
<div class="md-flex__cell md-flex__cell--shrink">
<a aria-label="sbi" class="md-header-nav__button md-logo" href="https://mackelab.org/sbi/" title="sbi">
<img alt="logo" height="24" src="../static/logo.svg" width="24"/>
</a>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
</div>
<div class="md-flex__cell md-flex__cell--stretch">
<div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
<span class="md-header-nav__topic">
              sbi
            </span>
<span class="md-header-nav__topic">
              
                API Reference
              
            </span>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="query" data-md-state="active" name="query" placeholder="Search" spellcheck="false" type="text"/>
<label class="md-icon md-search__icon" for="__search"></label>
<button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset">
        
      </button>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="result">
<div class="md-search-result__meta">
            Type to start searching
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<div class="md-header-nav__source">
<a class="md-source" data-md-source="github" href="http://github.com/mackelab/sbi/" title="Go to repository">
<div class="md-source__icon">
<svg height="24" viewbox="0 0 24 24" width="24">
<use height="24" width="24" xlink:href="#__github"></use>
</svg>
</div>
<div class="md-source__repository">
    mackelab/sbi
  </div>
</a>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container">
<main class="md-main" role="main">
<div class="md-main__inner md-grid" data-md-component="container">
<div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title md-nav__title--site" for="__drawer">
<a class="md-nav__button md-logo" href="https://mackelab.org/sbi/" title="sbi">
<img alt="logo" height="48" src="../static/logo.svg" width="48"/>
</a>
    sbi
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-source="github" href="http://github.com/mackelab/sbi/" title="Go to repository">
<div class="md-source__icon">
<svg height="24" viewbox="0 0 24 24" width="24">
<use height="24" width="24" xlink:href="#__github"></use>
</svg>
</div>
<div class="md-source__repository">
    mackelab/sbi
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href=".." title="Home">
      Home
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../install/" title="Installation">
      Installation
    </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" id="nav-3" type="checkbox"/>
<label class="md-nav__link" for="nav-3">
      Tutorial
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-3">
        Tutorial
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/markdown_files/00_getting_started/" title="Getting started">
      Getting started
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/markdown_files/01_gaussian_amortized/" title="Amortized inference">
      Amortized inference
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/markdown_files/02_HH_simulator/" title="Hodgkin-Huxley example">
      Hodgkin-Huxley example
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/markdown_files/03_flexible_interface/" title="Flexible interface">
      Flexible interface
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../contribute/" title="Contribute">
      Contribute
    </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-toggle md-nav__toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
        API Reference
      </label>
<a class="md-nav__link md-nav__link--active" href="./" title="API Reference">
      API Reference
    </a>
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">Table of contents</label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#inference">
    Inference
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.base.infer">
    infer()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.user_input.user_input_checks.prepare_for_sbi">
    prepare_for_sbi()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snpe.snpe_c.SNPE_C">
    SNPE_C
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snle.snle_a.SNLE_A">
    SNLE_A
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snre.snre_a.SNRE_A">
    SNRE_A
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snre.snre_b.SNRE_B">
    SNRE_B
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#posterior">
    Posterior
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.posterior.NeuralPosterior">
    NeuralPosterior
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#models">
    Models
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.posterior_nn">
    posterior_nn()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.likelihood_nn">
    likelihood_nn()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.classifier_nn">
    classifier_nn()
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#utils">
    Utils
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.plot.pairplot">
    pairplot()
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../credits/" title="Credits">
      Credits
    </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">Table of contents</label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#inference">
    Inference
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.base.infer">
    infer()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.user_input.user_input_checks.prepare_for_sbi">
    prepare_for_sbi()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snpe.snpe_c.SNPE_C">
    SNPE_C
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snle.snle_a.SNLE_A">
    SNLE_A
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snre.snre_a.SNRE_A">
    SNRE_A
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.snre.snre_b.SNRE_B">
    SNRE_B
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#posterior">
    Posterior
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.inference.posterior.NeuralPosterior">
    NeuralPosterior
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#models">
    Models
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.posterior_nn">
    posterior_nn()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.likelihood_nn">
    likelihood_nn()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.get_nn_models.classifier_nn">
    classifier_nn()
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#utils">
    Utils
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#sbi.utils.plot.pairplot">
    pairplot()
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content">
<article class="md-content__inner md-typeset">
<a class="md-icon md-content__icon" href="http://github.com/mackelab/sbi/edit/master/docs/reference.md" title="Edit this page"></a>
<h1 id="api-reference">API Reference<a class="headerlink" href="#api-reference" title="Permanent link">¶</a></h1>
<h2 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">¶</a></h2>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.inference.base.infer">
<code class="highlight language-python">
sbi.inference.base.infer<span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.inference.base.infer" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Return posterior distribution by running simulation-based inference.</p>
<p>This function provides a simple interface to run sbi. Inference is run for a single
round and hence the returned posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> can be sampled and evaluated
for any <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> (i.e. it is amortized).</p>
<p>The scope of this function is limited to the most essential features of sbi. For
more flexibility (e.g. multi-round inference, different density estimators) please
use the flexible interface described here:
<a href="https://www.mackelab.org/sbi/tutorial/markdown_files/03_flexible_interface/">https://www.mackelab.org/sbi/tutorial/markdown_files/03_flexible_interface/</a></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to simulations, or observations, <code class="codehilite"><span class="err">x</span></code>, <span><span class="MathJax_Preview">\mathrm{sim}(\theta)\to x</span><script type="math/tex">\mathrm{sim}(\theta)\to x</script></span>. Any regular Python callable (i.e. function or class with <code class="codehilite"><span class="err">__call__</span></code> method) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code>and <code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code> (for example, a PyTorch distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>method</code></td>
<td><code>str</code></td>
<td>
<p>What inference method to use. Either of SNPE, SNLE or SNRE.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_simulations</code></td>
<td><code>int</code></td>
<td>
<p>Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p>Returns: Posterior over parameters conditional on observations (amortized).</p>
<details class="quote">
<summary>Source code in <code>sbi/inference/base.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">infer</span><span class="p">(</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Return posterior distribution by running simulation-based inference.</span>

<span class="sd">    This function provides a simple interface to run sbi. Inference is run for a single</span>
<span class="sd">    round and hence the returned posterior $p(\theta|x)$ can be sampled and evaluated</span>
<span class="sd">    for any $x$ (i.e. it is amortized).</span>

<span class="sd">    The scope of this function is limited to the most essential features of sbi. For</span>
<span class="sd">    more flexibility (e.g. multi-round inference, different density estimators) please</span>
<span class="sd">    use the flexible interface described here:</span>
<span class="sd">    https://www.mackelab.org/sbi/tutorial/markdown_files/03_flexible_interface/</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        method: What inference method to use. Either of SNPE, SNLE or SNRE.</span>
<span class="sd">        num_simulations: Number of simulation calls. More simulations means a longer</span>
<span class="sd">            runtime, but a better posterior estimate.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>

<span class="sd">    Returns: Posterior over parameters conditional on observations (amortized).</span>
<span class="sd">    """</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">method_fun</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="p">,</span> <span class="n">method</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span>
            <span class="s2">"Method not available. `method` must be one of 'SNPE', 'SNLE', 'SNRE'."</span>
        <span class="p">)</span>

    <span class="c1"># Note this typically simulates once to find out the right `x_shape`.</span>
    <span class="n">prior</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">x_shape</span> <span class="o">=</span> <span class="n">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">infer_</span> <span class="o">=</span> <span class="n">method_fun</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">infer_</span><span class="p">(</span><span class="n">num_rounds</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_simulations_per_round</span><span class="o">=</span><span class="n">num_simulations</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">posterior</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.user_input.user_input_checks.prepare_for_sbi">
<code class="highlight language-python">
sbi.user_input.user_input_checks.prepare_for_sbi<span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.user_input.user_input_checks.prepare_for_sbi" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Prepare simulator, prior and shape of the observed data for usage in sbi.</p>
<p>One of the goals is to allow you to use sbi with inputs computed in numpy.</p>
<p>Attempts to meet the following requirements by reshaping and type-casting:
- the simulator function receives as input and returns a Tensor.
- the simulator can simulate batches of parameters and return batches of data.
- the prior does not produce batches and samples and evaluates to Tensor.
- the output shape is a <code class="codehilite"><span class="err">torch.Size((1,N))</span></code> (i.e, has a leading batch dimension 1).</p>
<p>If this is not possible, a suitable exception will be raised.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>Simulator as provided by the user.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>Prior as provided by the user.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_shape</code></td>
<td><code>Optional[Shape]</code></td>
<td>
<p>Shape of a single simulation output, either (1,N) or (N).</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[Callable, Distribution, Optional[torch.Size]]</code></td>
<td>
<p>Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/user_input/user_input_checks.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">prepare_for_sbi</span><span class="p">(</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Shape</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]]:</span>
    <span class="sd">"""Prepare simulator, prior and shape of the observed data for usage in sbi.</span>

<span class="sd">    One of the goals is to allow you to use sbi with inputs computed in numpy.</span>

<span class="sd">    Attempts to meet the following requirements by reshaping and type-casting:</span>
<span class="sd">    - the simulator function receives as input and returns a Tensor.</span>
<span class="sd">    - the simulator can simulate batches of parameters and return batches of data.</span>
<span class="sd">    - the prior does not produce batches and samples and evaluates to Tensor.</span>
<span class="sd">    - the output shape is a `torch.Size((1,N))` (i.e, has a leading batch dimension 1).</span>

<span class="sd">    If this is not possible, a suitable exception will be raised.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: Simulator as provided by the user.</span>
<span class="sd">        prior: Prior as provided by the user.</span>
<span class="sd">        x_shape: Shape of a single simulation output, either (1,N) or (N).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi.</span>
<span class="sd">    """</span>

    <span class="c1"># Check prior, return PyTorch prior.</span>
    <span class="n">prior</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">prior_returns_numpy</span> <span class="o">=</span> <span class="n">process_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="c1"># Check simulator, returns PyTorch simulator able to simulate batches.</span>
    <span class="n">simulator</span> <span class="o">=</span> <span class="n">process_simulator</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">prior_returns_numpy</span><span class="p">)</span>

    <span class="c1"># Check data shape, returns shape with leading batch dimension.</span>
    <span class="n">x_shape</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">process_x_shape</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

    <span class="c1"># Consistency check after making ready for sbi.</span>
    <span class="n">check_sbi_inputs</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C">
<code>sbi.inference.snpe.snpe_c.SNPE_C</code>
<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.snpe.snpe_c.SNPE_C.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_rounds</span><span class="p">,</span> <span class="n">num_simulations_per_round</span><span class="p">,</span> <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">z_score_min_std</span><span class="o">=</span><span class="mf">1e-07</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch_each_round</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run SNPE.</p>
<p>Return posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> after inference (possibly over several rounds).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_rounds</code></td>
<td><code>int</code></td>
<td>
<p>Number of rounds to run. Each round consists of a simulation and training phase. <code class="codehilite"><span class="err">num_rounds=1</span></code> leads to a posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> valid for <em>any</em> <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> (amortized), but requires many simulations. Alternatively, with <code class="codehilite"><span class="err">num_rounds&gt;1</span></code> the inference returns a posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span> focused on a specific observation <code class="codehilite"><span class="err">x_o</span></code>, potentially requiring less simulations.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_simulations_per_round</code></td>
<td><code>OneOrMore[int]</code></td>
<td>
<p>Number of simulator calls per round.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_o</code></td>
<td><code>Optional[Tensor]</code></td>
<td>
<p>An observation that is only required when doing inference over multiple rounds. After the first round, <code class="codehilite"><span class="err">x_o</span></code> is used to guide the sampling so that the simulator is run with parameters that are likely for that <code class="codehilite"><span class="err">x_o</span></code>, i.e. they are sampled from the posterior obtained in the previous round <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>num_atoms</code></td>
<td><code>int</code></td>
<td>
<p>Number of atoms to use for classification.</p>
</td>
<td><code>10</code></td>
</tr>
<tr>
<td><code>training_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Training batch size.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code></td>
<td>
<p>Learning rate for Adam optimizer.</p>
</td>
<td><code>0.0005</code></td>
</tr>
<tr>
<td><code>validation_fraction</code></td>
<td><code>float</code></td>
<td>
<p>The fraction of data to use for validation.</p>
</td>
<td><code>0.1</code></td>
</tr>
<tr>
<td><code>stop_after_epochs</code></td>
<td><code>int</code></td>
<td>
<p>The number of epochs to wait for improvement on the validation set before terminating training.</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>max_num_epochs</code></td>
<td><code>Optional[int]</code></td>
<td>
<p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also <code class="codehilite"><span class="err">stop_after_epochs</span></code>).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>clip_max_norm</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p>
</td>
<td><code>5.0</code></td>
</tr>
<tr>
<td><code>calibration_kernel</code></td>
<td><code>Optional[Callable]</code></td>
<td>
<p>A function to calibrate the loss with respect to the simulations <code class="codehilite"><span class="err">x</span></code>. See Lueckmann, Gonçalves et al., NeurIPS 2017.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>exclude_invalid_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to exclude simulation outputs <code class="codehilite"><span class="err">x=NaN</span></code> or <code class="codehilite"><span class="err">x=±∞</span></code> during training. Expect errors, silent or explicit, when <code class="codehilite"><span class="err">False</span></code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>z_score_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to z-score simulations <code class="codehilite"><span class="err">x</span></code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>z_score_min_std</code></td>
<td><code>float</code></td>
<td>
<p>Minimum value of the standard deviation to use when z-scoring <code class="codehilite"><span class="err">x</span></code>. This is typically needed when some simulator outputs are constant or nearly so.</p>
</td>
<td><code>1e-07</code></td>
</tr>
<tr>
<td><code>discard_prior_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>retrain_from_scratch_each_round</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to retrain the conditional density estimator for the posterior from scratch each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> that can be sampled and evaluated.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_rounds</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_simulations_per_round</span><span class="p">:</span> <span class="n">OneOrMore</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">z_score_min_std</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-7</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch_each_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Run SNPE.</span>

<span class="sd">    Return posterior $p(\theta|x)$ after inference (possibly over several rounds).</span>

<span class="sd">    Args:</span>
<span class="sd">        num_rounds: Number of rounds to run. Each round consists of a simulation and</span>
<span class="sd">            training phase. `num_rounds=1` leads to a posterior $p(\theta|x)$ valid</span>
<span class="sd">            for _any_ $x$ (amortized), but requires many simulations.</span>
<span class="sd">            Alternatively, with `num_rounds&gt;1` the inference returns a posterior</span>
<span class="sd">            $p(\theta|x_o)$ focused on a specific observation `x_o`, potentially</span>
<span class="sd">            requiring less simulations.</span>
<span class="sd">        num_simulations_per_round: Number of simulator calls per round.</span>
<span class="sd">        x_o: An observation that is only required when doing inference</span>
<span class="sd">            over multiple rounds. After the first round, `x_o` is used to guide the</span>
<span class="sd">            sampling so that the simulator is run with parameters that are likely</span>
<span class="sd">            for that `x_o`, i.e. they are sampled from the posterior obtained in the</span>
<span class="sd">            previous round $p(\theta|x_o)$.</span>
<span class="sd">        num_atoms: Number of atoms to use for classification.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. If None, we</span>
<span class="sd">            train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">            simulations `x`. See Lueckmann, Gonçalves et al., NeurIPS 2017.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        z_score_x: Whether to z-score simulations `x`.</span>
<span class="sd">        z_score_min_std: Minimum value of the standard deviation to use when</span>
<span class="sd">            z-scoring `x`. This is typically needed when some simulator outputs are</span>
<span class="sd">            constant or nearly so.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch_each_round: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$ that can be sampled and evaluated.</span>
<span class="sd">    """</span>

    <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent's `__call__` here,</span>
    <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
    <span class="c1"># continue. It's sneaky because we are using the object (self) as a namespace</span>
    <span class="c1"># to pass arguments between functions, and that's implicit state management.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">,</span> <span class="s2">"num_atoms"</span><span class="p">))</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.snpe.snpe_c.SNPE_C.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">'maf'</span><span class="p">,</span> <span class="n">sample_with_mcmc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">use_combined_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">'WARNING'</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_round_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>SNPE-C / APT [1].</p>
<p>[1] <em>Automatic Posterior Transformation for Likelihood-free Inference</em>,
    Greenberg et al., ICML 2019, <a href="https://arxiv.org/abs/1905.07488">https://arxiv.org/abs/1905.07488</a>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to simulations, or observations, <code class="codehilite"><span class="err">x</span></code>, <span><span class="MathJax_Preview">\mathrm{sim}(\theta)\to x</span><script type="math/tex">\mathrm{sim}(\theta)\to x</script></span>. Any regular Python callable (i.e. function or class with <code class="codehilite"><span class="err">__call__</span></code> method) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code>and <code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code> (for example, a PyTorch distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_shape</code></td>
<td><code>Optional[torch.Size]</code></td>
<td>
<p>Shape of a single simulation output <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, has to be (1,N).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If &gt;= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>density_estimator</code></td>
<td><code>Union[str, nn.Module]</code></td>
<td>
<p>Either a string or a density estimation neural network that can <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code> and <code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code>. If it is a string, use a pre- configured network of the provided type (one of nsf, maf, mdn, made).</p>
</td>
<td><code>'maf'</code></td>
</tr>
<tr>
<td><code>sample_with_mcmc</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to sample with MCMC. MCMC can be used to deal with high leakage.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>If MCMC sampling is used, specify the method here: either of slice_np, slice, hmc, nuts.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>use_combined_loss</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td><code>str</code></td>
<td>
<p>torch device on which to compute, e.g. gpu, cpu.</p>
</td>
<td><code>'cpu'</code></td>
</tr>
<tr>
<td><code>logging_level</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
</td>
<td><code>'WARNING'</code></td>
</tr>
<tr>
<td><code>summary_writer</code></td>
<td><code>Optional[SummaryWriter]</code></td>
<td>
<p>A <code class="codehilite"><span class="err">SummaryWriter</span></code> to control, among others, log file location (default is <code class="codehilite"><span class="err">&lt;current working directory&gt;/logs</span></code>.)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>show_round_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show the validation loss and leakage after each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"maf"</span><span class="p">,</span>
    <span class="n">sample_with_mcmc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">use_combined_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cpu"</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"WARNING"</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">show_round_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">"""SNPE-C / APT [1].</span>

<span class="sd">    [1] _Automatic Posterior Transformation for Likelihood-free Inference_,</span>
<span class="sd">        Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        x_shape: Shape of a single simulation output $x$, has to be (1,N).</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        density_estimator: Either a string or a density estimation neural network</span>
<span class="sd">            that can `.log_prob()` and `.sample()`. If it is a string, use a pre-</span>
<span class="sd">            configured network of the provided type (one of nsf, maf, mdn, made).</span>
<span class="sd">        sample_with_mcmc: Whether to sample with MCMC. MCMC can be used to deal</span>
<span class="sd">            with high leakage.</span>
<span class="sd">        mcmc_method: If MCMC sampling is used, specify the method here: either of</span>
<span class="sd">            slice_np, slice, hmc, nuts.</span>
<span class="sd">        use_combined_loss: Whether to train the neural net also on prior samples</span>
<span class="sd">            using maximum likelihood in addition to training it on all samples using</span>
<span class="sd">            atomic loss. The extra MLE loss helps prevent density leaking with</span>
<span class="sd">            bounded priors.</span>
<span class="sd">        device: torch device on which to compute, e.g. gpu, cpu.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        show_round_summary: Whether to show the validation loss and leakage after</span>
<span class="sd">            each round.</span>
<span class="sd">    """</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span> <span class="o">=</span> <span class="n">use_combined_loss</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">,</span> <span class="s2">"use_combined_loss"</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A">
<code>sbi.inference.snle.snle_a.SNLE_A</code>
<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.snle.snle_a.SNLE_A.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_rounds</span><span class="p">,</span> <span class="n">num_simulations_per_round</span><span class="p">,</span> <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch_each_round</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run SNLE.</p>
<p>Return posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> after inference (possibly over several rounds).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_rounds</code></td>
<td><code>int</code></td>
<td>
<p>Number of rounds to run. Each round consists of a simulation and training phase. <code class="codehilite"><span class="err">num_rounds=1</span></code> leads to a posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> valid for <em>any</em> <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> (amortized), but requires many simulations. Alternatively, with <code class="codehilite"><span class="err">num_rounds&gt;1</span></code> the inference returns a posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span> focused on a specific observation <code class="codehilite"><span class="err">x_o</span></code>, potentially requiring less simulations.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_simulations_per_round</code></td>
<td><code>OneOrMore[int]</code></td>
<td>
<p>Number of simulator calls per round.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_o</code></td>
<td><code>Optional[Tensor]</code></td>
<td>
<p>An observation that is only required when doing inference over multiple rounds. After the first round, <code class="codehilite"><span class="err">x_o</span></code> is used to guide the sampling so that the simulator is run with parameters that are likely for that <code class="codehilite"><span class="err">x_o</span></code>, i.e. they are sampled from the posterior obtained in the previous round <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>training_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Training batch size.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code></td>
<td>
<p>Learning rate for Adam optimizer.</p>
</td>
<td><code>0.0005</code></td>
</tr>
<tr>
<td><code>validation_fraction</code></td>
<td><code>float</code></td>
<td>
<p>The fraction of data to use for validation.</p>
</td>
<td><code>0.1</code></td>
</tr>
<tr>
<td><code>stop_after_epochs</code></td>
<td><code>int</code></td>
<td>
<p>The number of epochs to wait for improvement on the validation set before terminating training.</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>max_num_epochs</code></td>
<td><code>Optional[int]</code></td>
<td>
<p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also <code class="codehilite"><span class="err">stop_after_epochs</span></code>).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>clip_max_norm</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p>
</td>
<td><code>5.0</code></td>
</tr>
<tr>
<td><code>exclude_invalid_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to exclude simulation outputs <code class="codehilite"><span class="err">x=NaN</span></code> or <code class="codehilite"><span class="err">x=±∞</span></code> during training. Expect errors, silent or explicit, when <code class="codehilite"><span class="err">False</span></code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>discard_prior_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>retrain_from_scratch_each_round</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to retrain the conditional density estimator for the posterior from scratch each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span> that can be sampled and evaluated.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_rounds</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_simulations_per_round</span><span class="p">:</span> <span class="n">OneOrMore</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch_each_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Run SNLE.</span>

<span class="sd">    Return posterior $p(\theta|x)$ after inference (possibly over several rounds).</span>

<span class="sd">    Args:</span>
<span class="sd">        num_rounds: Number of rounds to run. Each round consists of a simulation and</span>
<span class="sd">            training phase. `num_rounds=1` leads to a posterior $p(\theta|x)$ valid</span>
<span class="sd">            for _any_ $x$ (amortized), but requires many simulations.</span>
<span class="sd">            Alternatively, with `num_rounds&gt;1` the inference returns a posterior</span>
<span class="sd">            $p(\theta|x_o)$ focused on a specific observation `x_o`, potentially</span>
<span class="sd">            requiring less simulations.</span>
<span class="sd">        num_simulations_per_round: Number of simulator calls per round.</span>
<span class="sd">        x_o: An observation that is only required when doing inference</span>
<span class="sd">            over multiple rounds. After the first round, `x_o` is used to guide the</span>
<span class="sd">            sampling so that the simulator is run with parameters that are likely</span>
<span class="sd">            for that `x_o`, i.e. they are sampled from the posterior obtained in the</span>
<span class="sd">            previous round $p(\theta|x_o)$.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. If None, we</span>
<span class="sd">            train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch_each_round: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x_o)$ that can be sampled and evaluated.</span>
<span class="sd">    """</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.snle.snle_a.SNLE_A.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">'maf'</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">'WARNING'</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_round_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Sequential Neural Likelihood [1].</p>
<p>[1] Sequential Neural Likelihood: Fast Likelihood-free Inference with
Autoregressive Flows_, Papamakarios et al., AISTATS 2019,
<a href="https://arxiv.org/abs/1805.07226">https://arxiv.org/abs/1805.07226</a></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and maps them to simulations, or observations, <code class="codehilite"><span class="err">x</span></code>, <span><span class="MathJax_Preview">\text{sim}(\theta)\to x</span><script type="math/tex">\text{sim}(\theta)\to x</script></span>. Any regular Python callable (i.e. function or class with <code class="codehilite"><span class="err">__call__</span></code> method) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code>and <code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code> (for example, a PyTorch distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_shape</code></td>
<td><code>Optional[torch.Size]</code></td>
<td>
<p>Shape of a single simulation output <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, has to be (1,N).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If &gt;= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>density_estimator</code></td>
<td><code>Union[str, nn.Module]</code></td>
<td>
<p>Either a string or a density estimation neural network that can <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code> and <code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code>. If it is a string, use a pre- configured network of the provided type (one of nsf, maf, mdn, made).</p>
</td>
<td><code>'maf'</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>If MCMC sampling is used, specify the method here: either of slice_np, slice, hmc, nuts.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td><code>str</code></td>
<td>
<p>torch device on which to compute, e.g. gpu, cpu.</p>
</td>
<td><code>'cpu'</code></td>
</tr>
<tr>
<td><code>logging_level</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
</td>
<td><code>'WARNING'</code></td>
</tr>
<tr>
<td><code>summary_writer</code></td>
<td><code>Optional[SummaryWriter]</code></td>
<td>
<p>A <code class="codehilite"><span class="err">SummaryWriter</span></code> to control, among others, log file location (default is <code class="codehilite"><span class="err">&lt;current working directory&gt;/logs</span></code>.)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>show_round_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show the validation loss and leakage after each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"maf"</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cpu"</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"WARNING"</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">show_round_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">"""Sequential Neural Likelihood [1].</span>

<span class="sd">    [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with</span>
<span class="sd">    Autoregressive Flows_, Papamakarios et al., AISTATS 2019,</span>
<span class="sd">    https://arxiv.org/abs/1805.07226</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\text{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        x_shape: Shape of a single simulation output $x$, has to be (1,N).</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        density_estimator: Either a string or a density estimation neural network</span>
<span class="sd">            that can `.log_prob()` and `.sample()`. If it is a string, use a pre-</span>
<span class="sd">            configured network of the provided type (one of nsf, maf, mdn, made).</span>
<span class="sd">        mcmc_method: If MCMC sampling is used, specify the method here: either of</span>
<span class="sd">            slice_np, slice, hmc, nuts.</span>
<span class="sd">        device: torch device on which to compute, e.g. gpu, cpu.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        show_round_summary: Whether to show the validation loss and leakage after</span>
<span class="sd">            each round.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A">
<code>sbi.inference.snre.snre_a.SNRE_A</code>
<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.snre.snre_a.SNRE_A.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_rounds</span><span class="p">,</span> <span class="n">num_simulations_per_round</span><span class="p">,</span> <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch_each_round</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run AALR / SNRE_A.</p>
<p>Return posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> after inference (possibly over several rounds).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_rounds</code></td>
<td><code>int</code></td>
<td>
<p>Number of rounds to run. Each round consists of a simulation and training phase. <code class="codehilite"><span class="err">num_rounds=1</span></code> leads to a posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> valid for <em>any</em> <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> (amortized), but requires many simulations. Alternatively, with <code class="codehilite"><span class="err">num_rounds&gt;1</span></code> the inference returns a posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span> focused on a specific observation <code class="codehilite"><span class="err">x_o</span></code>, potentially requiring less simulations.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_simulations_per_round</code></td>
<td><code>OneOrMore[int]</code></td>
<td>
<p>Number of simulator calls per round.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_o</code></td>
<td><code>Optional[Tensor]</code></td>
<td>
<p>An observation that is only required when doing inference over multiple rounds. After the first round, <code class="codehilite"><span class="err">x_o</span></code> is used to guide the sampling so that the simulator is run with parameters that are likely for that <code class="codehilite"><span class="err">x_o</span></code>, i.e. they are sampled from the posterior obtained in the previous round <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>training_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Training batch size.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code></td>
<td>
<p>Learning rate for Adam optimizer.</p>
</td>
<td><code>0.0005</code></td>
</tr>
<tr>
<td><code>validation_fraction</code></td>
<td><code>float</code></td>
<td>
<p>The fraction of data to use for validation.</p>
</td>
<td><code>0.1</code></td>
</tr>
<tr>
<td><code>stop_after_epochs</code></td>
<td><code>int</code></td>
<td>
<p>The number of epochs to wait for improvement on the validation set before terminating training.</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>max_num_epochs</code></td>
<td><code>Optional[int]</code></td>
<td>
<p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also <code class="codehilite"><span class="err">stop_after_epochs</span></code>).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>clip_max_norm</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p>
</td>
<td><code>5.0</code></td>
</tr>
<tr>
<td><code>exclude_invalid_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to exclude simulation outputs <code class="codehilite"><span class="err">x=NaN</span></code> or <code class="codehilite"><span class="err">x=±∞</span></code> during training. Expect errors, silent or explicit, when <code class="codehilite"><span class="err">False</span></code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>discard_prior_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>retrain_from_scratch_each_round</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to retrain the conditional density estimator for the posterior from scratch each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> that can be sampled and evaluated.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_rounds</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_simulations_per_round</span><span class="p">:</span> <span class="n">OneOrMore</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch_each_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Run AALR / SNRE_A.</span>

<span class="sd">    Return posterior $p(\theta|x)$ after inference (possibly over several rounds).</span>

<span class="sd">    Args:</span>
<span class="sd">        num_rounds: Number of rounds to run. Each round consists of a simulation and</span>
<span class="sd">            training phase. `num_rounds=1` leads to a posterior $p(\theta|x)$ valid</span>
<span class="sd">            for _any_ $x$ (amortized), but requires many simulations.</span>
<span class="sd">            Alternatively, with `num_rounds&gt;1` the inference returns a posterior</span>
<span class="sd">            $p(\theta|x_o)$ focused on a specific observation `x_o`, potentially</span>
<span class="sd">            requiring less simulations.</span>
<span class="sd">        num_simulations_per_round: Number of simulator calls per round.</span>
<span class="sd">        x_o: An observation that is only required when doing inference</span>
<span class="sd">            over multiple rounds. After the first round, `x_o` is used to guide the</span>
<span class="sd">            sampling so that the simulator is run with parameters that are likely</span>
<span class="sd">            for that `x_o`, i.e. they are sampled from the posterior obtained in the</span>
<span class="sd">            previous round $p(\theta|x_o)$.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. If None, we</span>
<span class="sd">            train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch_each_round: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$ that can be sampled and evaluated.</span>
<span class="sd">    """</span>

    <span class="c1"># AALR is defined for `num_atoms=2`.</span>
    <span class="c1"># Proxy to `super().__call__` to ensure right parameter.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.snre.snre_a.SNRE_A.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_net</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">'resnet'</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">'warning'</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_round_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>AALR[1], here known as SNRE_A.</p>
<p>[1] <em>Likelihood-free MCMC with Amortized Approximate Likelihood Ratios</em>, Hermans
    et al., ICML 2020, <a href="https://arxiv.org/abs/1903.04057">https://arxiv.org/abs/1903.04057</a></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters $       heta$ and maps them to simulations, or observations, <code class="codehilite"><span class="err">x</span></code>, <span><span class="MathJax_Preview">\mathrm{sim}(       heta)   o x</span><script type="math/tex">\mathrm{sim}(       heta)   o x</script></span>. Any regular Python callable (i.e. function or class with <code class="codehilite"><span class="err">__call__</span></code> method) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code>and <code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code> (for example, a PyTorch distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_shape</code></td>
<td><code>Optional[torch.Size]</code></td>
<td>
<p>Shape of a single simulation output <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, has to be (1,N).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If &gt;= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>embedding_net</code></td>
<td><code>nn.Module</code></td>
<td>
<p>Can be used to encode observations <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>. Currently not implemented.</p>
</td>
<td><code>Identity()</code></td>
</tr>
<tr>
<td><code>classifier</code></td>
<td><code>Union[str, nn.Module]</code></td>
<td>
<p>Classifier trained to approximate likelihood rations. If str, use a pre-configured neural network.</p>
</td>
<td><code>'resnet'</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>If MCMC sampling is used, specify the method here: either of slice_np, slice, hmc, nuts.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td><code>str</code></td>
<td>
<p>torch device on which to compute, e.g. gpu, cpu.</p>
</td>
<td><code>'cpu'</code></td>
</tr>
<tr>
<td><code>logging_level</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
</td>
<td><code>'warning'</code></td>
</tr>
<tr>
<td><code>summary_writer</code></td>
<td><code>Optional[SummaryWriter]</code></td>
<td>
<p>A <code class="codehilite"><span class="err">SummaryWriter</span></code> to control, among others, log file location (default is <code class="codehilite"><span class="err">&lt;current working directory&gt;/logs</span></code>.)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>show_round_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show the validation loss and leakage after each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"resnet"</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cpu"</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"warning"</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">show_round_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">"""AALR[1], here known as SNRE_A.</span>

<span class="sd">    [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans</span>
<span class="sd">        et al., ICML 2020, https://arxiv.org/abs/1903.04057</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        x_shape: Shape of a single simulation output $x$, has to be (1,N).</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        embedding_net: Can be used to encode observations $x$. Currently not</span>
<span class="sd">            implemented.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood rations. If str,</span>
<span class="sd">            use a pre-configured neural network.</span>
<span class="sd">        mcmc_method: If MCMC sampling is used, specify the method here: either of</span>
<span class="sd">            slice_np, slice, hmc, nuts.</span>
<span class="sd">        device: torch device on which to compute, e.g. gpu, cpu.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        show_round_summary: Whether to show the validation loss and leakage after</span>
<span class="sd">            each round.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B">
<code>sbi.inference.snre.snre_b.SNRE_B</code>
<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__call__()" id="sbi.inference.snre.snre_b.SNRE_B.__call__">
<code class="highlight language-python">
__call__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_rounds</span><span class="p">,</span> <span class="n">num_simulations_per_round</span><span class="p">,</span> <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch_each_round</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Run SRE / SNRE_B.</p>
<p>Return posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> after inference (possibly over several rounds).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>num_rounds</code></td>
<td><code>int</code></td>
<td>
<p>Number of rounds to run. Each round consists of a simulation and training phase. <code class="codehilite"><span class="err">num_rounds=1</span></code> leads to a posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> valid for <em>any</em> <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> (amortized), but requires many simulations. Alternatively, with <code class="codehilite"><span class="err">num_rounds&gt;1</span></code> the inference returns a posterior <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span> focused on a specific observation <code class="codehilite"><span class="err">x_o</span></code>, potentially requiring less simulations.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_simulations_per_round</code></td>
<td><code>OneOrMore[int]</code></td>
<td>
<p>Number of simulator calls per round.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_o</code></td>
<td><code>Optional[Tensor]</code></td>
<td>
<p>An observation that is only required when doing inference over multiple rounds. After the first round, <code class="codehilite"><span class="err">x_o</span></code> is used to guide the sampling so that the simulator is run with parameters that are likely for that <code class="codehilite"><span class="err">x_o</span></code>, i.e. they are sampled from the posterior obtained in the previous round <span><span class="MathJax_Preview">p(\theta|x_o)</span><script type="math/tex">p(\theta|x_o)</script></span>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>training_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Training batch size.</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code></td>
<td>
<p>Learning rate for Adam optimizer.</p>
</td>
<td><code>0.0005</code></td>
</tr>
<tr>
<td><code>validation_fraction</code></td>
<td><code>float</code></td>
<td>
<p>The fraction of data to use for validation.</p>
</td>
<td><code>0.1</code></td>
</tr>
<tr>
<td><code>stop_after_epochs</code></td>
<td><code>int</code></td>
<td>
<p>The number of epochs to wait for improvement on the validation set before terminating training.</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>max_num_epochs</code></td>
<td><code>Optional[int]</code></td>
<td>
<p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. If None, we train until validation loss increases (see also <code class="codehilite"><span class="err">stop_after_epochs</span></code>).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>clip_max_norm</code></td>
<td><code>Optional[float]</code></td>
<td>
<p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p>
</td>
<td><code>5.0</code></td>
</tr>
<tr>
<td><code>exclude_invalid_x</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to exclude simulation outputs <code class="codehilite"><span class="err">x=NaN</span></code> or <code class="codehilite"><span class="err">x=±∞</span></code> during training. Expect errors, silent or explicit, when <code class="codehilite"><span class="err">False</span></code>.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>discard_prior_samples</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>retrain_from_scratch_each_round</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to retrain the conditional density estimator for the posterior from scratch each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralPosterior</code></td>
<td>
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> that can be sampled and evaluated.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_rounds</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_simulations_per_round</span><span class="p">:</span> <span class="n">OneOrMore</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch_each_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Run SRE / SNRE_B.</span>

<span class="sd">    Return posterior $p(\theta|x)$ after inference (possibly over several rounds).</span>

<span class="sd">    Args:</span>
<span class="sd">        num_rounds: Number of rounds to run. Each round consists of a simulation and</span>
<span class="sd">            training phase. `num_rounds=1` leads to a posterior $p(\theta|x)$ valid</span>
<span class="sd">            for _any_ $x$ (amortized), but requires many simulations.</span>
<span class="sd">            Alternatively, with `num_rounds&gt;1` the inference returns a posterior</span>
<span class="sd">            $p(\theta|x_o)$ focused on a specific observation `x_o`, potentially</span>
<span class="sd">            requiring less simulations.</span>
<span class="sd">        num_simulations_per_round: Number of simulator calls per round.</span>
<span class="sd">        x_o: An observation that is only required when doing inference</span>
<span class="sd">            over multiple rounds. After the first round, `x_o` is used to guide the</span>
<span class="sd">            sampling so that the simulator is run with parameters that are likely</span>
<span class="sd">            for that `x_o`, i.e. they are sampled from the posterior obtained in the</span>
<span class="sd">            previous round $p(\theta|x_o)$.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. If None, we</span>
<span class="sd">            train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch_each_round: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$ that can be sampled and evaluated.</span>
<span class="sd">    """</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.snre.snre_b.SNRE_B.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_net</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">'resnet'</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">'warning'</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_round_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>SRE[1], here known as SNRE_B.</p>
<p>[1] <em>On Contrastive Learning for Likelihood-free Inference</em>, Durkan et al.,
    ICML 2020, <a href="https://arxiv.org/pdf/2002.03712">https://arxiv.org/pdf/2002.03712</a></p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>simulator</code></td>
<td><code>Callable</code></td>
<td>
<p>A function that takes parameters $       heta$ and maps them to simulations, or observations, <code class="codehilite"><span class="err">x</span></code>, <span><span class="MathJax_Preview">\mathrm{sim}(       heta)   o x</span><script type="math/tex">\mathrm{sim}(       heta)   o x</script></span>. Any regular Python callable (i.e. function or class with <code class="codehilite"><span class="err">__call__</span></code> method) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code>and <code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code> (for example, a PyTorch distribution) can be used.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_shape</code></td>
<td><code>Optional[torch.Size]</code></td>
<td>
<p>Shape of a single simulation output <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, has to be (1,N).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>num_workers</code></td>
<td><code>int</code></td>
<td>
<p>Number of parallel workers to use for simulations.</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>simulation_batch_size</code></td>
<td><code>int</code></td>
<td>
<p>Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If &gt;= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension).</p>
</td>
<td><code>1</code></td>
</tr>
<tr>
<td><code>embedding_net</code></td>
<td><code>nn.Module</code></td>
<td>
<p>Can be used to encode observations <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>. Currently not implemented.</p>
</td>
<td><code>Identity()</code></td>
</tr>
<tr>
<td><code>classifier</code></td>
<td><code>Union[str, nn.Module]</code></td>
<td>
<p>Classifier trained to approximate likelihood rations. If str, use a pre-configured neural network.</p>
</td>
<td><code>'resnet'</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>If MCMC sampling is used, specify the method here: either of slice_np, slice, hmc, nuts.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td><code>str</code></td>
<td>
<p>torch device on which to compute, e.g. gpu, cpu.</p>
</td>
<td><code>'cpu'</code></td>
</tr>
<tr>
<td><code>logging_level</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
</td>
<td><code>'warning'</code></td>
</tr>
<tr>
<td><code>summary_writer</code></td>
<td><code>Optional[SummaryWriter]</code></td>
<td>
<p>A <code class="codehilite"><span class="err">SummaryWriter</span></code> to control, among others, log file location (default is <code class="codehilite"><span class="err">&lt;current working directory&gt;/logs</span></code>.)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progressbar during simulation and sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>show_round_summary</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show the validation loss and leakage after each round.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"resnet"</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cpu"</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"warning"</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">show_round_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">"""SRE[1], here known as SNRE_B.</span>

<span class="sd">    [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,</span>
<span class="sd">        ICML 2020, https://arxiv.org/pdf/2002.03712</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        x_shape: Shape of a single simulation output $x$, has to be (1,N).</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        embedding_net: Can be used to encode observations $x$. Currently not</span>
<span class="sd">            implemented.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood rations. If str,</span>
<span class="sd">            use a pre-configured neural network.</span>
<span class="sd">        mcmc_method: If MCMC sampling is used, specify the method here: either of</span>
<span class="sd">            slice_np, slice, hmc, nuts.</span>
<span class="sd">        device: torch device on which to compute, e.g. gpu, cpu.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        show_round_summary: Whether to show the validation loss and leakage after</span>
<span class="sd">            each round.</span>
<span class="sd">    """</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">"self"</span><span class="p">,</span> <span class="s2">"__class__"</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<h2 id="posterior">Posterior<a class="headerlink" href="#posterior" title="Permanent link">¶</a></h2>
<div class="doc doc-object doc-class">
<h3 class="doc doc-heading" id="sbi.inference.posterior.NeuralPosterior">
<code>sbi.inference.posterior.NeuralPosterior</code>
<a class="headerlink" href="#sbi.inference.posterior.NeuralPosterior" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span> with <code class="codehilite"><span class="err">log_prob()</span></code> and <code class="codehilite"><span class="err">sample()</span></code> methods.<br/>
<br/>

    All inference methods in sbi train a neural network which is then used to obtain
    the posterior distribution. The <code class="codehilite"><span class="err">NeuralPosterior</span></code> class wraps the trained network
    such that one can directly evaluate the log probability and draw samples from the
    posterior. The neural network itself can be accessed via the <code class="codehilite"><span class="na">.net</span></code> attribute.
    <br/>
<br/>

    Specifically, this class offers the following functionality:<br/>

    - Correction of leakage (applicable only to SNPE): If the prior is bounded, the
      posterior resulting from SNPE can generate samples that lie outside of the prior
      support (i.e. the posterior leaks). This class rejects these samples or,
      alternatively, allows to sample from the posterior with MCMC. It also corrects the
      calculation of the log probability such that it compensates for the leakage.<br/>

    - Posterior inference from likelihood (SNL) and likelihood ratio (SRE): SNL and SRE
      learn to approximate the likelihood and likelihood ratio, which in turn can be
      used to generate samples from the posterior. This class provides the needed MCMC
      methods to sample from the posterior and to evaluate the log probability.</p>
<div class="doc doc-children">
<div class="doc doc-object doc-attribute">
<h4 class="doc doc-heading" data-toc-label="default_x" id="sbi.inference.posterior.NeuralPosterior.default_x">
<code class="highlight">
default_x: <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-property"><code>property</code></small>
<small class="doc doc-property doc-property-writable"><code>writable</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p>Return default x used by <code class="codehilite"><span class="na">.sample</span><span class="p">(),</span> <span class="no">.log_prob</span></code> as conditioning context.</p>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="__init__()" id="sbi.inference.posterior.NeuralPosterior.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method_family</span><span class="p">,</span> <span class="n">neural_net</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with_mcmc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">'slice_np'</span><span class="p">,</span> <span class="n">get_potential_function</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
</h4>
<div class="doc doc-contents">
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>method_family</code></td>
<td><code>str</code></td>
<td>
<p>One of snpe, snl, snre_a or snre_b.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>neural_net</code></td>
<td><code>nn.Module</code></td>
<td>
<p>A classifier for SNRE, a density estimator for SNPE and SNL.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>Prior distribution with <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code> and <code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code>.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_shape</code></td>
<td><code>Optional[torch.Size]</code></td>
<td>
<p>Shape of a single simulator output.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>sample_with_mcmc</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to sample with MCMC. Will always be <code class="codehilite"><span class="err">True</span></code> for SRE and SNL, but can also be set to <code class="codehilite"><span class="err">True</span></code> for SNPE if MCMC is preferred to deal with leakage over rejection sampling.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>mcmc_method</code></td>
<td><code>str</code></td>
<td>
<p>If MCMC sampling is used, specify the method here: either of slice_np, slice, hmc, nuts.</p>
</td>
<td><code>'slice_np'</code></td>
</tr>
<tr>
<td><code>get_potential_function</code></td>
<td><code>Optional[Callable]</code></td>
<td>
<p>Callable that returns the potential function used for MCMC sampling.</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">method_family</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">neural_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with_mcmc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"slice_np"</span><span class="p">,</span>
    <span class="n">get_potential_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Args:</span>
<span class="sd">        method_family: One of snpe, snl, snre_a or snre_b.</span>
<span class="sd">        neural_net: A classifier for SNRE, a density estimator for SNPE and SNL.</span>
<span class="sd">        prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">        x_shape: Shape of a single simulator output.</span>
<span class="sd">        sample_with_mcmc: Whether to sample with MCMC. Will always be `True` for SRE</span>
<span class="sd">            and SNL, but can also be set to `True` for SNPE if MCMC is preferred to</span>
<span class="sd">            deal with leakage over rejection sampling.</span>
<span class="sd">        mcmc_method: If MCMC sampling is used, specify the method here: either of</span>
<span class="sd">            slice_np, slice, hmc, nuts.</span>
<span class="sd">        get_potential_function: Callable that returns the potential function used</span>
<span class="sd">            for MCMC sampling.</span>
<span class="sd">    """</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">neural_net</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="c1"># This can be changed via `.set_default_x() below.`</span>
    <span class="c1"># TODO: set via default_x directly here? would require process_x to accept None.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x_o_training_focused_on</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_sample_with_mcmc</span> <span class="o">=</span> <span class="n">sample_with_mcmc</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">mcmc_method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_init_params</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_get_potential_function</span> <span class="o">=</span> <span class="n">get_potential_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span> <span class="o">=</span> <span class="n">x_shape</span>

    <span class="k">if</span> <span class="n">method_family</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"snpe"</span><span class="p">,</span> <span class="s2">"snle_a"</span><span class="p">,</span> <span class="s2">"snre_a"</span><span class="p">,</span> <span class="s2">"snre_b"</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_method_family</span> <span class="o">=</span> <span class="n">method_family</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Method family unsupported."</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_num_trained_rounds</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Correction factor for leakage, only applicable to SNPE-family methods.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="leakage_correction()" id="sbi.inference.posterior.NeuralPosterior.leakage_correction">
<code class="highlight language-python">
leakage_correction<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_rejection_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return leakage correction factor for a leaky posterior density estimate.</p>
<p>The factor is estimated from the acceptance probability during rejection
sampling from the posterior.</p>
<div class="admonition note">
<p class="admonition-title">This is to avoid re-estimating the acceptance probability from scratch</p>
<p>whenever <code class="codehilite"><span class="err">log_prob</span></code> is called and <code class="codehilite"><span class="err">norm_posterior_snpe=True</span></code>. Here, it
  is estimated only once for <code class="codehilite"><span class="err">self.default_x</span></code> and saved for later. We
  re-evaluate only whenever a new <code class="codehilite"><span class="err">x</span></code> is passed.</p>
</div>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>num_rejection_samples</code></td>
<td><code>int</code></td>
<td>
<p>Number of samples used to estimate correction factor.</p>
</td>
<td><code>10000</code></td>
</tr>
<tr>
<td><code>force_update</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to force a reevaluation of the leakage correction even if the context <code class="codehilite"><span class="err">x</span></code> is the same as <code class="codehilite"><span class="err">self.default_x</span></code>. This is useful to enforce a new leakage estimate for rounds after the first (2, 3,..).</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show a progress bar during sampling.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p>Saved or newly-estimated correction factor (as a scalar <code class="codehilite"><span class="err">Tensor</span></code>).</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">leakage_correction</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_rejection_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Return leakage correction factor for a leaky posterior density estimate.</span>

<span class="sd">    The factor is estimated from the acceptance probability during rejection</span>
<span class="sd">    sampling from the posterior.</span>

<span class="sd">    NOTE: This is to avoid re-estimating the acceptance probability from scratch</span>
<span class="sd">          whenever `log_prob` is called and `norm_posterior_snpe=True`. Here, it</span>
<span class="sd">          is estimated only once for `self.default_x` and saved for later. We</span>
<span class="sd">          re-evaluate only whenever a new `x` is passed.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$.</span>
<span class="sd">        num_rejection_samples: Number of samples used to estimate correction factor.</span>
<span class="sd">        force_update: Whether to force a reevaluation of the leakage correction even</span>
<span class="sd">            if the context `x` is the same as `self.default_x`. This is useful to</span>
<span class="sd">            enforce a new leakage estimate for rounds after the first (2, 3,..).</span>
<span class="sd">        show_progress_bars: Whether to show a progress bar during sampling.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Saved or newly-estimated correction factor (as a scalar `Tensor`).</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="nf">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">utils</span><span class="o">.</span><span class="n">sample_posterior_within_prior</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_rejection_samples</span><span class="p">,</span> <span class="n">show_progress_bars</span>
        <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
    <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don't save.</span>
        <span class="k">return</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span>  <span class="c1"># type:ignore</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="log_prob()" id="sbi.inference.posterior.NeuralPosterior.log_prob">
<code class="highlight language-python">
log_prob<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_posterior_snpe</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return posterior log probability  <span><span class="MathJax_Preview">\log p(\theta|x)</span><script type="math/tex">\log p(\theta|x)</script></span>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td><code>Tensor</code></td>
<td>
<p>Parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Optional[Tensor]</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>. If not provided, fall back onto an <code class="codehilite"><span class="err">x_o</span></code> if previously provided for multi-round training, or to another default if set later for convenience, see <code class="codehilite"><span class="na">.set_default_x</span><span class="p">()</span></code>.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>norm_posterior_snpe</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to enforce a normalized posterior density when using SNPE. Renormalization of the posterior is useful when some probability falls out or leaks out of the prescribed prior support. The normalizing factor is calculated via rejection sampling, so if you need speedier but unnormalized log posterior estimates set here <code class="codehilite"><span class="err">norm_posterior_snpe=False</span></code>. The returned log posterior is set to -∞ outside of the prior support regardless of this setting.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>track_gradients</code></td>
<td><code>bool</code></td>
<td>
<p>Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tensor</code></td>
<td>
<p><code class="codehilite"><span class="err">(len(θ),)</span></code>-shaped log posterior probability <span><span class="MathJax_Preview">\log p(\theta|x)</span><script type="math/tex">\log p(\theta|x)</script></span> for θ in the
support of the prior, -∞ (corresponding to 0 probability) outside.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_posterior_snpe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""Return posterior log probability  $\log p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$. If not provided, fall</span>
<span class="sd">            back onto an `x_o` if previously provided for multi-round training, or</span>
<span class="sd">            to another default if set later for convenience, see `.set_default_x()`.</span>
<span class="sd">        norm_posterior_snpe: Whether to enforce a normalized posterior density when</span>
<span class="sd">            using SNPE. Renormalization of the posterior is useful when some</span>
<span class="sd">            probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">            The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">            need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">            `norm_posterior_snpe=False`. The returned log posterior is set to</span>
<span class="sd">            -∞ outside of the prior support regardless of this setting.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(θ),)`-shaped log posterior probability $\log p(\theta|x)$ for θ in the</span>
<span class="sd">        support of the prior, -∞ (corresponding to 0 probability) outside.</span>
<span class="sd">    """</span>

    <span class="c1"># TODO Train exited here, entered after sampling?</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

    <span class="c1"># Select and check x to condition on.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_2d_float32_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_single_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_x_consistent_with_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_warn_if_posterior_was_focused_on_different_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Repeat `x` in case of evaluation on multiple `theta`. This is needed below in</span>
    <span class="c1"># when calling nflows in order to have matching shapes of theta and context x</span>
    <span class="c1"># at neural network evaluation time.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_match_x_with_theta_batch_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">log_prob_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"_log_prob_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_method_family</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_method_family</span><span class="si">}</span><span class="s2"> cannot evaluate probabilities."</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_method_family</span> <span class="o">==</span> <span class="s2">"snpe"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">log_prob_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">norm_posterior</span><span class="o">=</span><span class="n">norm_posterior_snpe</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">log_prob_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="sample()" id="sbi.inference.posterior.NeuralPosterior.sample">
<code class="highlight language-python">
sample<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return samples from posterior distribution <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>.</p>
<p>Samples are obtained either with rejection sampling or MCMC. SNPE can use
rejection sampling and MCMC (which can help to deal with strong leakage). SNL
and SRE are restricted to sampling with MCMC.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sample_shape</code></td>
<td><code>Shape</code></td>
<td>
<p>Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw <code class="codehilite"><span class="err">sample_shape.numel()</span></code> samples and then reshape into the desired shape.</p>
</td>
<td><code>torch.Size([])</code></td>
</tr>
<tr>
<td><code>x</code></td>
<td><code>Optional[Tensor]</code></td>
<td>
<p>Conditioning context for posterior <span><span class="MathJax_Preview">p(\theta|x)</span><script type="math/tex">p(\theta|x)</script></span>. If not provided, fall back onto <code class="codehilite"><span class="err">x_o</span></code> if previously provided for multiround training, or to a set default (see <code class="codehilite"><span class="err">set_default_x()</span></code> method).</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>show_progress_bars</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to show sampling progress monitor.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>**kwargs</code></td>
<td><code></code></td>
<td>
<p>Additional parameters to be passed to the MCMC sampler, such as <code class="codehilite"><span class="err">thin</span></code> and <code class="codehilite"><span class="err">warmup_steps</span></code>.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p>Returns: Samples from posterior.</p>
<details class="quote">
<summary>Source code in <code>sbi/inference/posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">    Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">    Samples are obtained either with rejection sampling or MCMC. SNPE can use</span>
<span class="sd">    rejection sampling and MCMC (which can help to deal with strong leakage). SNL</span>
<span class="sd">    and SRE are restricted to sampling with MCMC.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        x: Conditioning context for posterior $p(\theta|x)$. If not provided,</span>
<span class="sd">            fall back onto `x_o` if previously provided for multiround training, or</span>
<span class="sd">            to a set default (see `set_default_x()` method).</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">        **kwargs: Additional parameters to be passed to the MCMC sampler, such as</span>
<span class="sd">            `thin` and `warmup_steps`.</span>

<span class="sd">    Returns: Samples from posterior.</span>
<span class="sd">    """</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_2d_float32_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_single_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_x_consistent_with_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_warn_if_posterior_was_focused_on_different_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_with_mcmc</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_posterior_mcmc</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">mcmc_method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_method_family</span> <span class="o">==</span> <span class="s2">"snpe"</span><span class="p">:</span>
        <span class="c1"># Rejection sampling.</span>
        <span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">sample_posterior_within_prior</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"Only SNPE can use rejection sampling. All other"</span>
            <span class="s2">"methods require MCMC."</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_default_x()" id="sbi.inference.posterior.NeuralPosterior.set_default_x">
<code class="highlight language-python">
set_default_x<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Return <code class="codehilite"><span class="err">NeuralPosterior</span></code> object with default conditioning context set to <code class="codehilite"><span class="err">x</span></code>.</p>
<p>This is a pure convenience to avoid having to repeatedly specify <code class="codehilite"><span class="err">x</span></code> in calls to
<code class="codehilite"><span class="na">.sample</span><span class="p">()</span></code> and <code class="codehilite"><span class="na">.log_prob</span><span class="p">()</span></code> - only θ needs to be passed.</p>
<p>This convenience is particularly useful when the posterior is focused, i.e.
has been trained over multiple rounds to be accurate in the vicinity of a
particular <code class="codehilite"><span class="err">x=x_o</span></code> (you can check if your posterior object is focused by
printing it).</p>
<p>NOTE: this method is chainable, i.e. will return the NeuralPosterior object so
that calls like <code class="codehilite"><span class="err">posterior.set_default_x(my_x).sample(mytheta)</span></code> are possible.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x</code></td>
<td><code>Tensor</code></td>
<td>
<p>The default observation to set for the posterior <span><span class="MathJax_Preview">p(theta|x)</span><script type="math/tex">p(theta|x)</script></span>.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>'NeuralPosterior'</code></td>
<td>
<p><code class="codehilite"><span class="err">NeuralPosterior</span></code> that will use a default <code class="codehilite"><span class="err">x</span></code> when not explicitly passed.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">"NeuralPosterior"</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Return `NeuralPosterior` object with default conditioning context set to `x`.</span>

<span class="sd">    This is a pure convenience to avoid having to repeatedly specify `x` in calls to</span>
<span class="sd">    `.sample()` and `.log_prob()` - only θ needs to be passed.</span>

<span class="sd">    This convenience is particularly useful when the posterior is focused, i.e.</span>
<span class="sd">    has been trained over multiple rounds to be accurate in the vicinity of a</span>
<span class="sd">    particular `x=x_o` (you can check if your posterior object is focused by</span>
<span class="sd">    printing it).</span>

<span class="sd">    NOTE: this method is chainable, i.e. will return the NeuralPosterior object so</span>
<span class="sd">    that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The default observation to set for the posterior $p(theta|x)$.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` that will use a default `x` when not explicitly passed.</span>
<span class="sd">    """</span>
    <span class="n">processed_x</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_warn_if_posterior_was_focused_on_different_x</span><span class="p">(</span><span class="n">processed_x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">processed_x</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h4 class="doc doc-heading" data-toc-label="set_embedding_net()" id="sbi.inference.posterior.NeuralPosterior.set_embedding_net">
<code class="highlight language-python">
set_embedding_net<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_net</span><span class="p">)</span> </code>
</h4>
<div class="doc doc-contents">
<p>Set the <code class="codehilite"><span class="err">embedding_net</span></code> as an attribute of the <code class="codehilite"><span class="err">neural_net</span></code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>embedding_net</code></td>
<td><code>nn.Module</code></td>
<td>
<p>Neural net to encode <code class="codehilite"><span class="err">x</span></code>.</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/inference/posterior.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>564
565
566
567
568
569
570
571
572
573
574
575
576
577</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">set_embedding_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Set the `embedding_net` as an attribute of the `neural_net`.</span>

<span class="sd">    Args:</span>
<span class="sd">        embedding_net: Neural net to encode `x`.</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embedding_net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">"`embedding_net`is not a `nn.Module`. "</span>
        <span class="s2">"If you want to use hard-coded summary features, "</span>
        <span class="s2">"please simply pass the already encoded summary features as input and pass "</span>
        <span class="s2">"`embedding_net=None`."</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_embedding_net</span> <span class="o">=</span> <span class="n">embedding_net</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</div>
</div>
</div>
<h2 id="models">Models<a class="headerlink" href="#models" title="Permanent link">¶</a></h2>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.posterior_nn">
<code class="highlight language-python">
sbi.utils.get_nn_models.posterior_nn<span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o_shape</span><span class="p">,</span> <span class="n">embedding</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">mdn_num_components</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">made_num_mixture_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">made_num_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">flow_num_transforms</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.utils.get_nn_models.posterior_nn" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Neural posterior density estimator</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model</code></td>
<td><code>str</code></td>
<td>
<p>Model, one of maf / mdn / made / nsf</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>prior</code></td>
<td><code></code></td>
<td>
<p>Prior distribution.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_o_shape</code></td>
<td><code>torch.Size</code></td>
<td>
<p>Shape of a single observation. Used as input size to the NN.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embedding</code></td>
<td><code>nn.Module</code></td>
<td>
<p>Embedding network</p>
</td>
<td><code>Identity()</code></td>
</tr>
<tr>
<td><code>hidden_features</code></td>
<td><code>int</code></td>
<td>
<p>For all, number of hidden features</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>mdn_num_components</code></td>
<td><code>int</code></td>
<td>
<p>For MDNs only, number of components</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>made_num_mixture_components</code></td>
<td><code>int</code></td>
<td>
<p>For MADEs only, number of mixture components</p>
</td>
<td><code>10</code></td>
</tr>
<tr>
<td><code>made_num_blocks</code></td>
<td><code>int</code></td>
<td>
<p>For MADEs only, number of blocks</p>
</td>
<td><code>4</code></td>
</tr>
<tr>
<td><code>flow_num_transforms</code></td>
<td><code>int</code></td>
<td>
<p>For flows only, number of transforms</p>
</td>
<td><code>5</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>nn.Module</code></td>
<td>
<p>Neural network</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span> 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">posterior_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">x_o_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="n">embedding</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">mdn_num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">made_num_mixture_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">made_num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">flow_num_transforms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">"""Neural posterior density estimator</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Model, one of maf / mdn / made / nsf</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        x_o_shape: Shape of a single observation. Used as input size to the NN.</span>
<span class="sd">        embedding: Embedding network</span>
<span class="sd">        hidden_features: For all, number of hidden features</span>
<span class="sd">        mdn_num_components: For MDNs only, number of components</span>
<span class="sd">        made_num_mixture_components: For MADEs only, number of mixture components</span>
<span class="sd">        made_num_blocks: For MADEs only, number of blocks</span>
<span class="sd">        flow_num_transforms: For flows only, number of transforms</span>

<span class="sd">    Returns:</span>
<span class="sd">        Neural network</span>
<span class="sd">    """</span>

    <span class="c1"># We need these asserts because mean and std can be defined outside, prior to user</span>
    <span class="c1"># input checks.</span>
    <span class="n">prior_mean</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">mean</span>
    <span class="n">prior_std</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">stddev</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">prior_mean</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">float32</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">"Prior mean must have dtype float32, is </span><span class="si">{</span><span class="n">prior_mean</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">."</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">prior_std</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">float32</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">"Prior std must have dtype float32, is </span><span class="si">{</span><span class="n">prior_std</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">."</span>

    <span class="n">standardizing_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">AffineTransform</span><span class="p">(</span>
        <span class="n">shift</span><span class="o">=-</span><span class="n">prior_mean</span> <span class="o">/</span> <span class="n">prior_std</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="n">prior_std</span>
    <span class="p">)</span>

    <span class="n">theta_numel</span> <span class="o">=</span> <span class="n">prior_mean</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">x_o_numel</span> <span class="o">=</span> <span class="n">x_o_shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">theta_numel</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_check_1d_flow_limitations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"parameter"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"mdn"</span><span class="p">:</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">MultivariateGaussianMDN</span><span class="p">(</span>
            <span class="n">features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">,</span>
            <span class="n">context_features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">,</span>
            <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
            <span class="n">hidden_net</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">x_o_numel</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="p">),</span>
            <span class="n">num_components</span><span class="o">=</span><span class="n">mdn_num_components</span><span class="p">,</span>
            <span class="n">custom_initialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"made"</span><span class="p">:</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">standardizing_transform</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">distributions_</span><span class="o">.</span><span class="n">MADEMoG</span><span class="p">(</span>
            <span class="n">features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">,</span>
            <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
            <span class="n">context_features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">,</span>
            <span class="n">num_blocks</span><span class="o">=</span><span class="n">made_num_blocks</span><span class="p">,</span>
            <span class="n">num_mixture_components</span><span class="o">=</span><span class="n">made_num_mixture_components</span><span class="p">,</span>
            <span class="n">use_residual_blocks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">random_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">,</span>
            <span class="n">dropout_probability</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">custom_initialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">distribution</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"maf"</span><span class="p">:</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">transforms</span><span class="o">.</span><span class="n">MaskedAffineAutoregressiveTransform</span><span class="p">(</span>
                            <span class="n">features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">,</span>
                            <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
                            <span class="n">context_features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">,</span>
                            <span class="n">num_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">use_residual_blocks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">random_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">activation</span><span class="o">=</span><span class="n">tanh</span><span class="p">,</span>
                            <span class="n">dropout_probability</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                            <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomPermutation</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_num_transforms</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">([</span><span class="n">standardizing_transform</span><span class="p">,</span> <span class="n">transform</span><span class="p">,])</span>

        <span class="n">distribution</span> <span class="o">=</span> <span class="n">distributions_</span><span class="o">.</span><span class="n">StandardNormal</span><span class="p">((</span><span class="n">theta_numel</span><span class="p">,))</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">distribution</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"nsf"</span><span class="p">:</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">transforms</span><span class="o">.</span><span class="n">PiecewiseRationalQuadraticCouplingTransform</span><span class="p">(</span>
                            <span class="n">mask</span><span class="o">=</span><span class="n">create_alternating_binary_mask</span><span class="p">(</span>
                                <span class="n">features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">,</span> <span class="n">even</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
                            <span class="p">),</span>
                            <span class="n">transform_net_create_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="n">nets</span><span class="o">.</span><span class="n">ResidualNet</span><span class="p">(</span>
                                <span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span>
                                <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">,</span>
                                <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
                                <span class="n">context_features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">,</span>
                                <span class="n">num_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">,</span>
                                <span class="n">dropout_probability</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="p">),</span>
                            <span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                            <span class="n">tails</span><span class="o">=</span><span class="s2">"linear"</span><span class="p">,</span>
                            <span class="n">tail_bound</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span>
                            <span class="n">apply_unconditional_transform</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="n">transforms</span><span class="o">.</span><span class="n">LULinear</span><span class="p">(</span><span class="n">theta_numel</span><span class="p">,</span> <span class="n">identity_init</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_num_transforms</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">([</span><span class="n">standardizing_transform</span><span class="p">,</span> <span class="n">transform</span><span class="p">,])</span>

        <span class="n">distribution</span> <span class="o">=</span> <span class="n">distributions_</span><span class="o">.</span><span class="n">StandardNormal</span><span class="p">((</span><span class="n">theta_numel</span><span class="p">,))</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">distribution</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span>

    <span class="k">return</span> <span class="n">neural_net</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.likelihood_nn">
<code class="highlight language-python">
sbi.utils.get_nn_models.likelihood_nn<span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">theta_shape</span><span class="p">,</span> <span class="n">x_o_shape</span><span class="p">,</span> <span class="n">embedding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">mdn_num_components</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">made_num_mixture_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">made_num_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">flow_num_transforms</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.utils.get_nn_models.likelihood_nn" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Neural likelihood density estimator</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model</code></td>
<td><code>str</code></td>
<td>
<p>Model, one of maf / mdn / made / nsf</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>theta_shape</code></td>
<td><code>torch.Size</code></td>
<td>
<p>event shape of the prior, number of parameters.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_o_shape</code></td>
<td><code>torch.Size</code></td>
<td>
<p>number of elements in a single data point.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embedding</code></td>
<td><code>Optional[nn.Module]</code></td>
<td>
<p>Embedding network</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>hidden_features</code></td>
<td><code>int</code></td>
<td>
<p>For all, number of hidden features</p>
</td>
<td><code>50</code></td>
</tr>
<tr>
<td><code>mdn_num_components</code></td>
<td><code>int</code></td>
<td>
<p>For MDNs only, number of components</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>made_num_mixture_components</code></td>
<td><code>int</code></td>
<td>
<p>For MADEs only, number of mixture components</p>
</td>
<td><code>10</code></td>
</tr>
<tr>
<td><code>made_num_blocks</code></td>
<td><code>int</code></td>
<td>
<p>For MADEs only, number of blocks</p>
</td>
<td><code>4</code></td>
</tr>
<tr>
<td><code>flow_num_transforms</code></td>
<td><code>int</code></td>
<td>
<p>For flows only, number of transforms</p>
</td>
<td><code>5</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>nn.Module</code></td>
<td>
<p>Neural network</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">likelihood_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">theta_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="n">x_o_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="n">embedding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">mdn_num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">made_num_mixture_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">made_num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">flow_num_transforms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">"""Neural likelihood density estimator</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Model, one of maf / mdn / made / nsf</span>
<span class="sd">        theta_shape: event shape of the prior, number of parameters.</span>
<span class="sd">        x_o_shape: number of elements in a single data point.</span>
<span class="sd">        embedding: Embedding network</span>
<span class="sd">        hidden_features: For all, number of hidden features</span>
<span class="sd">        mdn_num_components: For MDNs only, number of components</span>
<span class="sd">        made_num_mixture_components: For MADEs only, number of mixture components</span>
<span class="sd">        made_num_blocks: For MADEs only, number of blocks</span>
<span class="sd">        flow_num_transforms: For flows only, number of transforms</span>

<span class="sd">    Returns:</span>
<span class="sd">        Neural network</span>
<span class="sd">    """</span>

    <span class="n">theta_numel</span> <span class="o">=</span> <span class="n">theta_shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">x_o_numel</span> <span class="o">=</span> <span class="n">x_o_shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">x_o_numel</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_check_1d_flow_limitations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"data"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"mdn"</span><span class="p">:</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">MultivariateGaussianMDN</span><span class="p">(</span>
            <span class="n">features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">,</span>
            <span class="n">context_features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">,</span>
            <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
            <span class="n">hidden_net</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">theta_numel</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="p">),</span>
            <span class="n">num_components</span><span class="o">=</span><span class="n">mdn_num_components</span><span class="p">,</span>
            <span class="n">custom_initialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"made"</span><span class="p">:</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">MixtureOfGaussiansMADE</span><span class="p">(</span>
            <span class="n">features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">,</span>
            <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
            <span class="n">context_features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">,</span>
            <span class="n">num_blocks</span><span class="o">=</span><span class="n">made_num_blocks</span><span class="p">,</span>
            <span class="n">num_mixture_components</span><span class="o">=</span><span class="n">made_num_mixture_components</span><span class="p">,</span>
            <span class="n">use_residual_blocks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">random_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">,</span>
            <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">dropout_probability</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">custom_initialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"maf"</span><span class="p">:</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">transforms</span><span class="o">.</span><span class="n">MaskedAffineAutoregressiveTransform</span><span class="p">(</span>
                            <span class="n">features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">,</span>
                            <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
                            <span class="n">context_features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">,</span>
                            <span class="n">num_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">use_residual_blocks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">random_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">activation</span><span class="o">=</span><span class="n">tanh</span><span class="p">,</span>
                            <span class="n">dropout_probability</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                            <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomPermutation</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_num_transforms</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">distributions_</span><span class="o">.</span><span class="n">StandardNormal</span><span class="p">((</span><span class="n">x_o_numel</span><span class="p">,))</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">distribution</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"nsf"</span><span class="p">:</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">transforms</span><span class="o">.</span><span class="n">CompositeTransform</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">transforms</span><span class="o">.</span><span class="n">PiecewiseRationalQuadraticCouplingTransform</span><span class="p">(</span>
                            <span class="n">mask</span><span class="o">=</span><span class="n">create_alternating_binary_mask</span><span class="p">(</span>
                                <span class="n">features</span><span class="o">=</span><span class="n">x_o_numel</span><span class="p">,</span> <span class="n">even</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
                            <span class="p">),</span>
                            <span class="n">transform_net_create_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="n">nets</span><span class="o">.</span><span class="n">ResidualNet</span><span class="p">(</span>
                                <span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span>
                                <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">,</span>
                                <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
                                <span class="n">context_features</span><span class="o">=</span><span class="n">theta_numel</span><span class="p">,</span>
                                <span class="n">num_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">,</span>
                                <span class="n">dropout_probability</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="p">),</span>
                            <span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                            <span class="n">tails</span><span class="o">=</span><span class="s2">"linear"</span><span class="p">,</span>
                            <span class="n">tail_bound</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span>
                            <span class="n">apply_unconditional_transform</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="n">transforms</span><span class="o">.</span><span class="n">LULinear</span><span class="p">(</span><span class="n">x_o_numel</span><span class="p">,</span> <span class="n">identity_init</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_num_transforms</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">distributions_</span><span class="o">.</span><span class="n">StandardNormal</span><span class="p">((</span><span class="n">x_o_numel</span><span class="p">,))</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">distribution</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span>

    <span class="k">return</span> <span class="n">neural_net</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.classifier_nn">
<code class="highlight language-python">
sbi.utils.get_nn_models.classifier_nn<span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">theta_shape</span><span class="p">,</span> <span class="n">x_o_shape</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> </code>
<a class="headerlink" href="#sbi.utils.get_nn_models.classifier_nn" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Neural classifier</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model</code></td>
<td><code>str</code></td>
<td>
<p>Model, one of linear / mlp / resnet</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>theta_shape</code></td>
<td><code>torch.Size</code></td>
<td>
<p>event shape of the prior, number of parameters.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>x_o_shape</code></td>
<td><code>torch.Size</code></td>
<td>
<p>number of elements in a single data point.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>hidden_features</code></td>
<td><code>int</code></td>
<td>
<p>For all, number of hidden features</p>
</td>
<td><code>50</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>nn.Module</code></td>
<td>
<p>Neural network</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">classifier_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">theta_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="n">x_o_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">"""Neural classifier</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Model, one of linear / mlp / resnet</span>
<span class="sd">        theta_shape: event shape of the prior, number of parameters.</span>
<span class="sd">        x_o_shape: number of elements in a single data point.</span>
<span class="sd">        hidden_features: For all, number of hidden features</span>

<span class="sd">    Returns:</span>
<span class="sd">        Neural network</span>
<span class="sd">    """</span>

    <span class="n">theta_numel</span> <span class="o">=</span> <span class="n">theta_shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">x_o_numel</span> <span class="o">=</span> <span class="n">x_o_shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"linear"</span><span class="p">:</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">theta_numel</span> <span class="o">+</span> <span class="n">x_o_numel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"mlp"</span><span class="p">:</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">theta_numel</span> <span class="o">+</span> <span class="n">x_o_numel</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">"resnet"</span><span class="p">:</span>
        <span class="n">neural_net</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">ResidualNet</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">theta_numel</span> <span class="o">+</span> <span class="n">x_o_numel</span><span class="p">,</span>
            <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">hidden_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span>
            <span class="n">context_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">num_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">,</span>
            <span class="n">dropout_probability</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"'model' must be one of ['linear', 'mlp', 'resnet']."</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">neural_net</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
<h2 id="utils">Utils<a class="headerlink" href="#utils" title="Permanent link">¶</a></h2>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="sbi.utils.plot.pairplot">
<code class="highlight language-python">
sbi.utils.plot.pairplot<span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="s1">'hist'</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="s1">'hist'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels_points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samples_colors</span><span class="o">=</span><span class="p">[</span><span class="s1">'#1f77b4'</span><span class="p">,</span> <span class="s1">'#ff7f0e'</span><span class="p">,</span> <span class="s1">'#2ca02c'</span><span class="p">,</span> <span class="s1">'#d62728'</span><span class="p">,</span> <span class="s1">'#9467bd'</span><span class="p">,</span> <span class="s1">'#8c564b'</span><span class="p">,</span> <span class="s1">'#e377c2'</span><span class="p">,</span> <span class="s1">'#7f7f7f'</span><span class="p">,</span> <span class="s1">'#bcbd22'</span><span class="p">,</span> <span class="s1">'#17becf'</span><span class="p">],</span> <span class="n">points_colors</span><span class="o">=</span><span class="p">[</span><span class="s1">'#1f77b4'</span><span class="p">,</span> <span class="s1">'#ff7f0e'</span><span class="p">,</span> <span class="s1">'#2ca02c'</span><span class="p">,</span> <span class="s1">'#d62728'</span><span class="p">,</span> <span class="s1">'#9467bd'</span><span class="p">,</span> <span class="s1">'#8c564b'</span><span class="p">,</span> <span class="s1">'#e377c2'</span><span class="p">,</span> <span class="s1">'#7f7f7f'</span><span class="p">,</span> <span class="s1">'#bcbd22'</span><span class="p">,</span> <span class="s1">'#17becf'</span><span class="p">],</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tickformatter</span><span class="o">=&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">ticker</span><span class="o">.</span><span class="n">FormatStrFormatter</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f74db86cf50</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">tick_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hist_diag</span><span class="o">=</span><span class="p">{</span><span class="s1">'alpha'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">'bins'</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">'density'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'histtype'</span><span class="p">:</span> <span class="s1">'step'</span><span class="p">},</span> <span class="n">hist_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'bins'</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span> <span class="n">kde_diag</span><span class="o">=</span><span class="p">{</span><span class="s1">'bw_method'</span><span class="p">:</span> <span class="s1">'scott'</span><span class="p">,</span> <span class="s1">'bins'</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">'color'</span><span class="p">:</span> <span class="s1">'black'</span><span class="p">},</span> <span class="n">kde_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'bw_method'</span><span class="p">:</span> <span class="s1">'scott'</span><span class="p">,</span> <span class="s1">'bins'</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span> <span class="n">contour_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'levels'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.68</span><span class="p">],</span> <span class="s1">'percentile'</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span> <span class="n">scatter_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'alpha'</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">'edgecolor'</span><span class="p">:</span> <span class="s1">'none'</span><span class="p">,</span> <span class="s1">'rasterized'</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span> <span class="n">plot_offdiag</span><span class="o">=</span><span class="p">{},</span> <span class="n">points_diag</span><span class="o">=</span><span class="p">{},</span> <span class="n">points_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s1">'marker'</span><span class="p">:</span> <span class="s1">'.'</span><span class="p">,</span> <span class="s1">'markersize'</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span> <span class="n">fig_size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">fig_bg_colors</span><span class="o">=</span><span class="p">{</span><span class="s1">'upper'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'diag'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'lower'</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span> <span class="n">fig_subplots_adjust</span><span class="o">=</span><span class="p">{</span><span class="s1">'top'</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="n">subplots</span><span class="o">=</span><span class="p">{},</span> <span class="n">despine</span><span class="o">=</span><span class="p">{</span><span class="s1">'offset'</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span> <span class="n">title_format</span><span class="o">=</span><span class="p">{</span><span class="s1">'fontsize'</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span> </code>
<a class="headerlink" href="#sbi.utils.plot.pairplot" title="Permanent link">¶</a></h3>
<div class="doc doc-contents first">
<p>Plot samples and points.</p>
<p>For developers: if you add arguments that expect dictionaries, make sure to access
them via the opts dictionary instantiated below. E.g. if you want to access the dict
stored in the input variable hist_diag, use opts[<code class="codehilite"><span class="err">hist_diag</span></code>].</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>samples</code></td>
<td><code>Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]</code></td>
<td>
<p>posterior samples used to build the histogram</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>points</code></td>
<td><code>Optional[Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]]</code></td>
<td>
<p>list of additional points to scatter</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>upper</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>plotting style for upper diagonal, {hist, scatter, contour, None}</p>
</td>
<td><code>'hist'</code></td>
</tr>
<tr>
<td><code>diag</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>plotting style for diagonal, {hist, None}</p>
</td>
<td><code>'hist'</code></td>
</tr>
<tr>
<td><code>title</code></td>
<td><code>Optional[str]</code></td>
<td>
<p>title string</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>legend</code></td>
<td><code>Optional[bool]</code></td>
<td>
<p>whether to plot a legend for the points</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>labels</code></td>
<td><code></code></td>
<td>
<p>np.ndarray of strings specifying the names of the parameters</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>labels_points</code></td>
<td><code></code></td>
<td>
<p>np.ndarray of strings specifying the names of the passed points</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>labels_samples</code></td>
<td><code></code></td>
<td>
<p>np.ndarray of strings specifying the names of the passed samples</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>samples_colors</code></td>
<td><code></code></td>
<td>
<p>colors of the samples</p>
</td>
<td><code>['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']</code></td>
</tr>
<tr>
<td><code>points_colors</code></td>
<td><code></code></td>
<td>
<p>colors of the points</p>
</td>
<td><code>['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']</code></td>
</tr>
<tr>
<td><code>subset</code></td>
<td><code></code></td>
<td>
<p>List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and, if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on)</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>limits</code></td>
<td><code></code></td>
<td>
<p>array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>ticks</code></td>
<td><code></code></td>
<td>
<p>location of the ticks for each parameter. If None, just use the min and max along each parameter dimension</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>tickformatter</code></td>
<td><code></code></td>
<td>
<p>passed to _format_axis()</p>
</td>
<td><code>&lt;matplotlib.ticker.FormatStrFormatter object at 0x7f74db86cf50&gt;</code></td>
</tr>
<tr>
<td><code>tick_labels</code></td>
<td><code></code></td>
<td>
<p>np.ndarray containing the ticklabels.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>hist_diag</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to plt.hist() for diagonal plots</p>
</td>
<td><code>{'alpha': 1.0, 'bins': 50, 'density': False, 'histtype': 'step'}</code></td>
</tr>
<tr>
<td><code>hist_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to np.histogram2d() for off diagonal plots</p>
</td>
<td><code>{'bins': 50}</code></td>
</tr>
<tr>
<td><code>kde_diag</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to gaussian_kde() for diagonal plots</p>
</td>
<td><code>{'bw_method': 'scott', 'bins': 50, 'color': 'black'}</code></td>
</tr>
<tr>
<td><code>kde_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to gaussian_kde() for off diagonal plots</p>
</td>
<td><code>{'bw_method': 'scott', 'bins': 50}</code></td>
</tr>
<tr>
<td><code>contour_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary that should contain <code class="codehilite"><span class="err">percentile</span></code> and <code class="codehilite"><span class="err">levels</span></code> keys. <code class="codehilite"><span class="err">percentile</span></code>: bool. If  <code class="codehilite"><span class="err">percentile</span></code>==True, the levels are made with respect to the max probability of the posterior If <code class="codehilite"><span class="err">percentile</span></code>==False, the levels are drawn at absolute positions <code class="codehilite"><span class="err">levels</span></code>: list or np.ndarray: specifies the location where the contours are drawn.</p>
</td>
<td><code>{'levels': [0.68], 'percentile': True}</code></td>
</tr>
<tr>
<td><code>scatter_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary for plt.scatter() on off diagonal</p>
</td>
<td><code>{'alpha': 0.5, 'edgecolor': 'none', 'rasterized': False}</code></td>
</tr>
<tr>
<td><code>plot_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary for plt.plot() on off diagonal</p>
</td>
<td><code>{}</code></td>
</tr>
<tr>
<td><code>points_diag</code></td>
<td><code></code></td>
<td>
<p>dictionary for plt.plot() used for plotting points on diagonal</p>
</td>
<td><code>{}</code></td>
</tr>
<tr>
<td><code>points_offdiag</code></td>
<td><code></code></td>
<td>
<p>dictionary for plt.plot() used for plotting points on off diagonal</p>
</td>
<td><code>{'marker': '.', 'markersize': 20}</code></td>
</tr>
<tr>
<td><code>fig_size</code></td>
<td><code>Tuple</code></td>
<td>
<p>size of the entire figure</p>
</td>
<td><code>(10, 10)</code></td>
</tr>
<tr>
<td><code>fig_bg_colors</code></td>
<td><code></code></td>
<td>
<p>Dictionary that contains <code class="codehilite"><span class="err">upper</span></code>, <code class="codehilite"><span class="err">diag</span></code>, <code class="codehilite"><span class="err">lower</span></code>, and specifies the respective background colors. Passed to ax.set_facecolor()</p>
</td>
<td><code>{'upper': None, 'diag': None, 'lower': None}</code></td>
</tr>
<tr>
<td><code>fig_subplots_adjust</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to fig.subplots_adjust()</p>
</td>
<td><code>{'top': 0.9}</code></td>
</tr>
<tr>
<td><code>subplots</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to plt.subplots()</p>
</td>
<td><code>{}</code></td>
</tr>
<tr>
<td><code>despine</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to set_position() for axis position</p>
</td>
<td><code>{'offset': 5}</code></td>
</tr>
<tr>
<td><code>title_format</code></td>
<td><code></code></td>
<td>
<p>dictionary passed to plt.title()</p>
</td>
<td><code>{'fontsize': 16}</code></td>
</tr>
</tbody>
</table>
<p>Returns: figure and axis of posterior distribution plot</p>
<details class="quote">
<summary>Source code in <code>sbi/utils/plot.py</code></summary>
<table class="highlighttable">
<tr>
<td class="linenos">
<div class="linenodiv">
<pre><span></span>129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574</pre>
</div>
</td>
<td class="code">
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">pairplot</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">upper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"hist"</span><span class="p">,</span>
    <span class="n">diag</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"hist"</span><span class="p">,</span>
    <span class="n">title</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">legend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">labels_points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">labels_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">samples_colors</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"axes.prop_cycle"</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">"color"</span><span class="p">],</span>
    <span class="n">points_colors</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"axes.prop_cycle"</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">"color"</span><span class="p">],</span>
    <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">limits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ticks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tickformatter</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">ticker</span><span class="o">.</span><span class="n">FormatStrFormatter</span><span class="p">(</span><span class="s2">"</span><span class="si">%g</span><span class="s2">"</span><span class="p">),</span>
    <span class="n">tick_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hist_diag</span><span class="o">=</span><span class="p">{</span><span class="s2">"alpha"</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">"bins"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">"density"</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">"histtype"</span><span class="p">:</span> <span class="s2">"step"</span><span class="p">},</span>
    <span class="n">hist_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"bins"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,},</span>
    <span class="n">kde_diag</span><span class="o">=</span><span class="p">{</span><span class="s2">"bw_method"</span><span class="p">:</span> <span class="s2">"scott"</span><span class="p">,</span> <span class="s2">"bins"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">"color"</span><span class="p">:</span> <span class="s2">"black"</span><span class="p">},</span>
    <span class="n">kde_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"bw_method"</span><span class="p">:</span> <span class="s2">"scott"</span><span class="p">,</span> <span class="s2">"bins"</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span>
    <span class="n">contour_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"levels"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.68</span><span class="p">],</span> <span class="s2">"percentile"</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
    <span class="n">scatter_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"alpha"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">"edgecolor"</span><span class="p">:</span> <span class="s2">"none"</span><span class="p">,</span> <span class="s2">"rasterized"</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="n">plot_offdiag</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">points_diag</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">points_offdiag</span><span class="o">=</span><span class="p">{</span><span class="s2">"marker"</span><span class="p">:</span> <span class="s2">"."</span><span class="p">,</span> <span class="s2">"markersize"</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="n">fig_size</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">fig_bg_colors</span><span class="o">=</span><span class="p">{</span><span class="s2">"upper"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"diag"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"lower"</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
    <span class="n">fig_subplots_adjust</span><span class="o">=</span><span class="p">{</span><span class="s2">"top"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span>
    <span class="n">subplots</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">despine</span><span class="o">=</span><span class="p">{</span><span class="s2">"offset"</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">title_format</span><span class="o">=</span><span class="p">{</span><span class="s2">"fontsize"</span><span class="p">:</span> <span class="mi">16</span><span class="p">},</span>
<span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Plot samples and points.</span>

<span class="sd">    For developers: if you add arguments that expect dictionaries, make sure to access</span>
<span class="sd">    them via the opts dictionary instantiated below. E.g. if you want to access the dict</span>
<span class="sd">    stored in the input variable hist_diag, use opts[`hist_diag`].</span>

<span class="sd">    Args:</span>
<span class="sd">        samples: posterior samples used to build the histogram</span>
<span class="sd">        points: list of additional points to scatter</span>
<span class="sd">        upper: plotting style for upper diagonal, {hist, scatter, contour, None}</span>
<span class="sd">        diag: plotting style for diagonal, {hist, None}</span>
<span class="sd">        title: title string</span>
<span class="sd">        legend: whether to plot a legend for the points</span>
<span class="sd">        labels: np.ndarray of strings specifying the names of the parameters</span>
<span class="sd">        labels_points: np.ndarray of strings specifying the names of the passed points</span>
<span class="sd">        labels_samples: np.ndarray of strings specifying the names of the passed samples</span>
<span class="sd">        samples_colors: colors of the samples</span>
<span class="sd">        points_colors: colors of the points</span>
<span class="sd">        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot</span>
<span class="sd">            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,</span>
<span class="sd">            if they exist, the 4th, 5th and so on)</span>
<span class="sd">        limits: array containing the plot xlim for each parameter dimension. If None,</span>
<span class="sd">            just use the min and max of the passed samples</span>
<span class="sd">        ticks: location of the ticks for each parameter. If None, just use the min and</span>
<span class="sd">            max along each parameter dimension</span>
<span class="sd">        tickformatter: passed to _format_axis()</span>
<span class="sd">        tick_labels: np.ndarray containing the ticklabels.</span>
<span class="sd">        hist_diag: dictionary passed to plt.hist() for diagonal plots</span>
<span class="sd">        hist_offdiag: dictionary passed to np.histogram2d() for off diagonal plots</span>
<span class="sd">        kde_diag: dictionary passed to gaussian_kde() for diagonal plots</span>
<span class="sd">        kde_offdiag: dictionary passed to gaussian_kde() for off diagonal plots</span>
<span class="sd">        contour_offdiag: dictionary that should contain `percentile` and `levels` keys.</span>
<span class="sd">            `percentile`: bool.</span>
<span class="sd">                If  `percentile`==True,</span>
<span class="sd">                the levels are made with respect to the max probability of the posterior</span>
<span class="sd">                If `percentile`==False,</span>
<span class="sd">                the levels are drawn at absolute positions</span>
<span class="sd">            `levels`: list or np.ndarray: specifies the location where the contours are</span>
<span class="sd">                drawn.</span>
<span class="sd">        scatter_offdiag: dictionary for plt.scatter() on off diagonal</span>
<span class="sd">        plot_offdiag: dictionary for plt.plot() on off diagonal</span>
<span class="sd">        points_diag: dictionary for plt.plot() used for plotting points on diagonal</span>
<span class="sd">        points_offdiag: dictionary for plt.plot() used for plotting points on off</span>
<span class="sd">            diagonal</span>
<span class="sd">        fig_size: size of the entire figure</span>
<span class="sd">        fig_bg_colors: Dictionary that contains `upper`, `diag`, `lower`, and specifies</span>
<span class="sd">            the respective background colors. Passed to ax.set_facecolor()</span>
<span class="sd">        fig_subplots_adjust: dictionary passed to fig.subplots_adjust()</span>
<span class="sd">        subplots: dictionary passed to plt.subplots()</span>
<span class="sd">        despine: dictionary passed to set_position() for axis position</span>
<span class="sd">        title_format: dictionary passed to plt.title()</span>

<span class="sd">    Returns: figure and axis of posterior distribution plot</span>

<span class="sd">    """</span>

    <span class="c1"># TODO: add color map support</span>
    <span class="c1"># TODO: automatically determine good bin sizes for histograms</span>
    <span class="c1"># TODO: add legend (if legend is True)</span>

    <span class="c1"># get default values of function arguments</span>
    <span class="c1"># https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="n">pairplot</span><span class="p">)</span>

    <span class="c1"># build a dict for the defaults</span>
    <span class="c1"># https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value</span>
    <span class="c1"># answer by gnr</span>
    <span class="n">default_val_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">defaults</span> <span class="ow">or</span> <span class="p">())[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># update the defaults dictionary by the current values of the variables (passed by</span>
    <span class="c1"># the user)</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">default_val_dict</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>

    <span class="c1"># Prepare samples</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">ensure_numpy</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">samples</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample_pack</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
            <span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ensure_numpy</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="c1"># Prepare points</span>
    <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">points</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">points</span> <span class="o">=</span> <span class="n">ensure_numpy</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
        <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">points</span><span class="p">]</span>
    <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">points</span><span class="p">]</span>
    <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">ensure_numpy</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">points</span><span class="p">]</span>

    <span class="c1"># Dimensions</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># TODO: add asserts checking compatibility of dimensions</span>

    <span class="c1"># Prepare labels</span>
    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="o">==</span> <span class="p">[]</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels_dim</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"dim </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">labels_dim</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span>

    <span class="c1"># Prepare limits</span>
    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">]</span> <span class="o">==</span> <span class="p">[]</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">limits</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
            <span class="nb">min</span> <span class="o">=</span> <span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="nb">max</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
                <span class="n">min_</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">d</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
                <span class="nb">min</span> <span class="o">=</span> <span class="n">min_</span> <span class="k">if</span> <span class="n">min_</span> <span class="o">&lt;</span> <span class="nb">min</span> <span class="k">else</span> <span class="nb">min</span>
                <span class="n">max_</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">d</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
                <span class="nb">max</span> <span class="o">=</span> <span class="n">max_</span> <span class="k">if</span> <span class="n">max_</span> <span class="o">&gt;</span> <span class="nb">max</span> <span class="k">else</span> <span class="nb">max</span>
            <span class="n">limits</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">limits</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">limits</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"limits"</span><span class="p">]</span>

    <span class="c1"># Prepare ticks</span>
    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">]</span> <span class="o">==</span> <span class="p">[]</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ticks</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">ticks</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ticks</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"ticks"</span><span class="p">]</span>

    <span class="c1"># Prepare diag/upper/lower</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>
    <span class="c1"># if type(opts['lower']) is not list:</span>
    <span class="c1">#    opts['lower'] = [opts['lower'] for _ in range(len(samples))]</span>
    <span class="n">opts</span><span class="p">[</span><span class="s2">"lower"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Figure out if we subset the plot</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"subset"</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">subset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">subset</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
            <span class="n">subset</span> <span class="o">=</span> <span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"fig_size"</span><span class="p">],</span> <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"subplots"</span><span class="p">])</span>
    <span class="c1"># Cast to ndarray in case of 1D subplots.</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>

    <span class="c1"># Style figure</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"fig_subplots_adjust"</span><span class="p">])</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"title"</span><span class="p">],</span> <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"title_format"</span><span class="p">])</span>

    <span class="c1"># Style axes</span>
    <span class="n">row_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">row</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">row_idx</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">col_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">col</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">col_idx</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="n">col</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="s2">"diag"</span>
            <span class="k">elif</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">col</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="s2">"upper"</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="s2">"lower"</span>

            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">]</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">sca</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>

            <span class="c1"># Background color</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">current</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"fig_bg_colors"</span><span class="p">]</span>
                <span class="ow">and</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"fig_bg_colors"</span><span class="p">][</span><span class="n">current</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">):</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"fig_bg_colors"</span><span class="p">][</span><span class="n">current</span><span class="p">])</span>

            <span class="c1"># Axes</span>
            <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="n">current</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">"off"</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># Limits</span>
            <span class="k">if</span> <span class="n">limits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
                <span class="k">if</span> <span class="n">current</span> <span class="o">!=</span> <span class="s2">"diag"</span><span class="p">:</span>
                    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
            <span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
            <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>

            <span class="c1"># Ticks</span>
            <span class="k">if</span> <span class="n">ticks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">((</span><span class="n">ticks</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">ticks</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
                <span class="k">if</span> <span class="n">current</span> <span class="o">!=</span> <span class="s2">"diag"</span><span class="p">:</span>
                    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">((</span><span class="n">ticks</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">ticks</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>

            <span class="c1"># Despine</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">"right"</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">"top"</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">"bottom"</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="s2">"outward"</span><span class="p">,</span> <span class="n">despine</span><span class="p">[</span><span class="s2">"offset"</span><span class="p">]))</span>

            <span class="c1"># Formatting axes</span>
            <span class="k">if</span> <span class="n">current</span> <span class="o">==</span> <span class="s2">"diag"</span><span class="p">:</span>  <span class="c1"># off-diagnoals</span>
                <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"lower"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">col</span> <span class="o">==</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">_format_axis</span><span class="p">(</span>
                        <span class="n">ax</span><span class="p">,</span>
                        <span class="n">xhide</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xlabel</span><span class="o">=</span><span class="n">labels_dim</span><span class="p">[</span><span class="n">col</span><span class="p">],</span>
                        <span class="n">yhide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">tickformatter</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"tickformatter"</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">_format_axis</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xhide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">yhide</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># off-diagnoals</span>
                <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">_format_axis</span><span class="p">(</span>
                        <span class="n">ax</span><span class="p">,</span>
                        <span class="n">xhide</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xlabel</span><span class="o">=</span><span class="n">labels_dim</span><span class="p">[</span><span class="n">col</span><span class="p">],</span>
                        <span class="n">yhide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">tickformatter</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"tickformatter"</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">_format_axis</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xhide</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">yhide</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"tick_labels"</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="nb">str</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"tick_labels"</span><span class="p">][</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span>
                        <span class="nb">str</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">"tick_labels"</span><span class="p">][</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># Diagonals</span>
            <span class="k">if</span> <span class="n">current</span> <span class="o">==</span> <span class="s2">"diag"</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"hist"</span><span class="p">:</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                                <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                                <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"hist_diag"</span><span class="p">]</span>
                            <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"diag"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"kde"</span><span class="p">:</span>
                            <span class="n">density</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span> <span class="n">bw_method</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"kde_diag"</span><span class="p">][</span><span class="s2">"bw_method"</span><span class="p">]</span>
                            <span class="p">)</span>
                            <span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"kde_diag"</span><span class="p">][</span><span class="s2">"bins"</span><span class="p">])</span>
                            <span class="n">ys</span> <span class="o">=</span> <span class="n">density</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">pass</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">extent</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
                        <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                            <span class="p">[</span><span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span> <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">]],</span>
                            <span class="n">extent</span><span class="p">,</span>
                            <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"points_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                            <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"points_diag"</span><span class="p">]</span>
                        <span class="p">)</span>

            <span class="c1"># Off-diagonals</span>
            <span class="k">else</span><span class="p">:</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"hist"</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"hist2d"</span><span class="p">:</span>
                            <span class="c1"># h = plt.hist2d(</span>
                            <span class="c1">#     v[:, col], v[:, row],</span>
                            <span class="c1">#     range=(</span>
                            <span class="c1">#         [limits[col][0], limits[col][1]],</span>
                            <span class="c1">#         [limits[row][0], limits[row][1]]),</span>
                            <span class="c1">#     **opts['hist_offdiag']</span>
                            <span class="c1">#     )</span>
                            <span class="n">hist</span><span class="p">,</span> <span class="n">xedges</span><span class="p">,</span> <span class="n">yedges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram2d</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                                <span class="nb">range</span><span class="o">=</span><span class="p">[</span>
                                    <span class="p">[</span><span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span>
                                    <span class="p">[</span><span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span>
                                <span class="p">],</span>
                                <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"hist_offdiag"</span><span class="p">]</span>
                            <span class="p">)</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
                                <span class="n">hist</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                <span class="n">origin</span><span class="o">=</span><span class="s2">"lower"</span><span class="p">,</span>
                                <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="n">xedges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xedges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yedges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">yedges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],],</span>
                                <span class="n">aspect</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
                            <span class="p">)</span>

                        <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span>
                            <span class="s2">"kde"</span><span class="p">,</span>
                            <span class="s2">"kde2d"</span><span class="p">,</span>
                            <span class="s2">"contour"</span><span class="p">,</span>
                            <span class="s2">"contourf"</span><span class="p">,</span>
                        <span class="p">]:</span>
                            <span class="n">density</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="n">row</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                <span class="n">bw_method</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"kde_offdiag"</span><span class="p">][</span><span class="s2">"bw_method"</span><span class="p">],</span>
                            <span class="p">)</span>
                            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                                    <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                    <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">opts</span><span class="p">[</span><span class="s2">"kde_offdiag"</span><span class="p">][</span><span class="s2">"bins"</span><span class="p">],</span>
                                <span class="p">),</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                                    <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                    <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">opts</span><span class="p">[</span><span class="s2">"kde_offdiag"</span><span class="p">][</span><span class="s2">"bins"</span><span class="p">],</span>
                                <span class="p">),</span>
                            <span class="p">)</span>
                            <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
                            <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">density</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

                            <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"kde"</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"kde2d"</span><span class="p">:</span>
                                <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
                                    <span class="n">Z</span><span class="p">,</span>
                                    <span class="n">extent</span><span class="o">=</span><span class="p">[</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="p">],</span>
                                    <span class="n">origin</span><span class="o">=</span><span class="s2">"lower"</span><span class="p">,</span>
                                    <span class="n">aspect</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
                                <span class="p">)</span>
                            <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"contour"</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"contour_offdiag"</span><span class="p">][</span><span class="s2">"percentile"</span><span class="p">]:</span>
                                    <span class="n">Z</span> <span class="o">=</span> <span class="n">probs2contours</span><span class="p">(</span>
                                        <span class="n">Z</span><span class="p">,</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"contour_offdiag"</span><span class="p">][</span><span class="s2">"levels"</span><span class="p">]</span>
                                    <span class="p">)</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
                                <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span>
                                    <span class="n">X</span><span class="p">,</span>
                                    <span class="n">Y</span><span class="p">,</span>
                                    <span class="n">Z</span><span class="p">,</span>
                                    <span class="n">origin</span><span class="o">=</span><span class="s2">"lower"</span><span class="p">,</span>
                                    <span class="n">extent</span><span class="o">=</span><span class="p">[</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="p">],</span>
                                    <span class="n">colors</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                                    <span class="n">levels</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"contour_offdiag"</span><span class="p">][</span><span class="s2">"levels"</span><span class="p">],</span>
                                <span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">pass</span>
                        <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"scatter"</span><span class="p">:</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                                <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                                <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"scatter_offdiag"</span><span class="p">]</span>
                            <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"upper"</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"plot"</span><span class="p">:</span>
                            <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                                <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                                <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"samples_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                                <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"plot_offdiag"</span><span class="p">]</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">pass</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

                    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
                        <span class="n">h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                            <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                            <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                            <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">"points_colors"</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                            <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">"points_offdiag"</span><span class="p">]</span>
                        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
            <span class="n">y0</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
            <span class="n">text_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"fontsize"</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"font.size"</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span><span class="p">}</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span> <span class="o">/</span> <span class="mf">8.0</span><span class="p">,</span> <span class="p">(</span><span class="n">y0</span> <span class="o">+</span> <span class="n">y1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span> <span class="s2">"..."</span><span class="p">,</span> <span class="o">**</span><span class="n">text_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">row</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
                    <span class="n">x1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span> <span class="o">/</span> <span class="mf">12.0</span><span class="p">,</span>
                    <span class="n">y0</span> <span class="o">-</span> <span class="p">(</span><span class="n">y1</span> <span class="o">-</span> <span class="n">y0</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1.5</span><span class="p">,</span>
                    <span class="s2">"..."</span><span class="p">,</span>
                    <span class="n">rotation</span><span class="o">=-</span><span class="mi">45</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">text_kwargs</span>
                <span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span>
</code></pre>
</div>
</td>
</tr>
</table>
</details>
</div>
</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-nav">
<nav class="md-footer-nav__inner md-grid">
<a class="md-flex md-footer-nav__link md-footer-nav__link--prev" href="../contribute/" rel="prev" title="Contribute">
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
</div>
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
                  Previous
                </span>
                Contribute
              </span>
</div>
</a>
<a class="md-flex md-footer-nav__link md-footer-nav__link--next" href="../credits/" rel="next" title="Credits">
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
                  Next
                </span>
                Credits
              </span>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
</div>
</a>
</nav>
</div>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org" rel="noopener" target="_blank">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
          Material for MkDocs</a>
</div>
<div class="md-footer-social">
<link href="../assets/fonts/font-awesome.css" rel="stylesheet"/>
<a class="md-footer-social__link fa fa-" href="https://github.com/mackelab/sbi" rel="noopener" target="_blank" title=""></a>
</div>
</div>
</div>
</footer>
</div>
<script src="../assets/javascripts/application.c33a9706.js"></script>
<script>app.initialize({version:"1.1",url:{base:".."}})</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>