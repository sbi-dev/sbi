
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mackelab.org/sbi/reference/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.7">
    
    
      
        <title>API Reference - sbi</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.cd566b2a.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../static/global.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#api-reference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="sbi" class="md-header__button md-logo" aria-label="sbi" data-md-component="logo">
      
  <img src="../static/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            sbi
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API Reference
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="http://github.com/mackelab/sbi/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    mackelab/sbi
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="sbi" class="md-nav__button md-logo" aria-label="sbi" data-md-component="logo">
      
  <img src="../static/logo.svg" alt="logo">

    </a>
    sbi
  </label>
  
    <div class="md-nav__source">
      <a href="http://github.com/mackelab/sbi/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    mackelab/sbi
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../install/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Tutorials and Examples
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tutorials and Examples" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Tutorials and Examples
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_1" type="checkbox" id="__nav_3_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_1">
          Introduction
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Introduction" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          Introduction
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/00_getting_started/" class="md-nav__link">
        Getting started
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/01_gaussian_amortized/" class="md-nav__link">
        Amortized inference
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_2">
          Advanced
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Advanced" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          Advanced
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/02_flexible_interface/" class="md-nav__link">
        Flexible interface
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/03_multiround_inference/" class="md-nav__link">
        Multi-round inference
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/04_density_estimators/" class="md-nav__link">
        Custom density estimators
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/05_embedding_net/" class="md-nav__link">
        Learning summary statistics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/07_conditional_distributions/" class="md-nav__link">
        Conditional distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/08_restriction_estimator/" class="md-nav__link">
        Handling invalid simulations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/09_sensitivity_analysis/" class="md-nav__link">
        Posterior sensitivity analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/10_crafting_summary_statistics/" class="md-nav__link">
        Crafting summary statistics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/11_sampler_interface/" class="md-nav__link">
        Sampler interface
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/14_SBI_with_trial-based_mixed_data/" class="md-nav__link">
        SBI with trial-based (mixed) data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_3" type="checkbox" id="__nav_3_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_3">
          Diagnostics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Diagnostics" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          Diagnostics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/13_diagnosis_posterior_predictive_check.md" class="md-nav__link">
        Posterior predictive checks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/14_diagnosis_sbc.md" class="md-nav__link">
        Simulation-based calibration
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_4" type="checkbox" id="__nav_3_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_4">
          Examples
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Examples" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          Examples
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/00_HH_simulator/" class="md-nav__link">
        Hodgkin-Huxley example
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../contribute/" class="md-nav__link">
        Contribute
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          API Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        API Reference
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    Inference
  </a>
  
    <nav class="md-nav" aria-label="Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.base.infer" class="md-nav__link">
    infer()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.user_input_checks.prepare_for_sbi" class="md-nav__link">
    prepare_for_sbi()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.base.simulate_for_sbi" class="md-nav__link">
    simulate_for_sbi()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snpe.snpe_a.SNPE_A" class="md-nav__link">
    SNPE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snpe.snpe_c.SNPE_C" class="md-nav__link">
    SNPE_C
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snle.snle_a.SNLE_A" class="md-nav__link">
    SNLE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_a.SNRE_A" class="md-nav__link">
    SNRE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_b.SNRE_B" class="md-nav__link">
    SNRE_B
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.mcabc.MCABC" class="md-nav__link">
    MCABC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC" class="md-nav__link">
    SMCABC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#posteriors" class="md-nav__link">
    Posteriors
  </a>
  
    <nav class="md-nav" aria-label="Posteriors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" class="md-nav__link">
    DirectPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="md-nav__link">
    MCMCPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="md-nav__link">
    RejectionPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior" class="md-nav__link">
    VIPosterior
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models" class="md-nav__link">
    Models
  </a>
  
    <nav class="md-nav" aria-label="Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.posterior_nn" class="md-nav__link">
    posterior_nn()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.likelihood_nn" class="md-nav__link">
    likelihood_nn()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.classifier_nn" class="md-nav__link">
    classifier_nn()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#potentials" class="md-nav__link">
    Potentials
  </a>
  
    <nav class="md-nav" aria-label="Potentials">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential" class="md-nav__link">
    posterior_estimator_based_potential()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential" class="md-nav__link">
    likelihood_estimator_based_potential()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential" class="md-nav__link">
    ratio_estimator_based_potential()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#analysis" class="md-nav__link">
    Analysis
  </a>
  
    <nav class="md-nav" aria-label="Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.pairplot" class="md-nav__link">
    pairplot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.marginal_plot" class="md-nav__link">
    marginal_plot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.conditional_pairplot" class="md-nav__link">
    conditional_pairplot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.conditional_density.conditional_corrcoeff" class="md-nav__link">
    conditional_corrcoeff()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../faq/" class="md-nav__link">
        FAQ
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../credits/" class="md-nav__link">
        Credits
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    Inference
  </a>
  
    <nav class="md-nav" aria-label="Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.base.infer" class="md-nav__link">
    infer()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.user_input_checks.prepare_for_sbi" class="md-nav__link">
    prepare_for_sbi()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.base.simulate_for_sbi" class="md-nav__link">
    simulate_for_sbi()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snpe.snpe_a.SNPE_A" class="md-nav__link">
    SNPE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snpe.snpe_c.SNPE_C" class="md-nav__link">
    SNPE_C
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snle.snle_a.SNLE_A" class="md-nav__link">
    SNLE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_a.SNRE_A" class="md-nav__link">
    SNRE_A
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.snre.snre_b.SNRE_B" class="md-nav__link">
    SNRE_B
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.mcabc.MCABC" class="md-nav__link">
    MCABC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC" class="md-nav__link">
    SMCABC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#posteriors" class="md-nav__link">
    Posteriors
  </a>
  
    <nav class="md-nav" aria-label="Posteriors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" class="md-nav__link">
    DirectPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="md-nav__link">
    MCMCPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="md-nav__link">
    RejectionPosterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior" class="md-nav__link">
    VIPosterior
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models" class="md-nav__link">
    Models
  </a>
  
    <nav class="md-nav" aria-label="Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.posterior_nn" class="md-nav__link">
    posterior_nn()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.likelihood_nn" class="md-nav__link">
    likelihood_nn()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.utils.get_nn_models.classifier_nn" class="md-nav__link">
    classifier_nn()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#potentials" class="md-nav__link">
    Potentials
  </a>
  
    <nav class="md-nav" aria-label="Potentials">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential" class="md-nav__link">
    posterior_estimator_based_potential()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential" class="md-nav__link">
    likelihood_estimator_based_potential()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential" class="md-nav__link">
    ratio_estimator_based_potential()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#analysis" class="md-nav__link">
    Analysis
  </a>
  
    <nav class="md-nav" aria-label="Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.pairplot" class="md-nav__link">
    pairplot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.marginal_plot" class="md-nav__link">
    marginal_plot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.plot.conditional_pairplot" class="md-nav__link">
    conditional_pairplot()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.analysis.conditional_density.conditional_corrcoeff" class="md-nav__link">
    conditional_corrcoeff()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="http://github.com/mackelab/sbi/edit/master/docs/reference.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


<h1 id="api-reference">API Reference<a class="headerlink" href="#api-reference" title="Permanent link">&para;</a></h1>
<h2 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">&para;</a></h2>


  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.inference.base.infer">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.base.infer" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Runs simulation-based inference and returns the posterior.</p>
<p>This function provides a simple interface to run sbi. Inference is run for a single
round and hence the returned posterior <span class="arithmatex">\(p(\theta|x)\)</span> can be sampled and evaluated
for any <span class="arithmatex">\(x\)</span> (i.e. it is amortized).</p>
<p>The scope of this function is limited to the most essential features of sbi. For
more flexibility (e.g. multi-round inference, different density estimators) please
use the flexible interface described here:
<a href="https://www.mackelab.org/sbi/tutorial/02_flexible_interface/">https://www.mackelab.org/sbi/tutorial/02_flexible_interface/</a></p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>simulator</code></td>
        <td><code>Callable</code></td>
        <td><p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\mathrm{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Distribution</code></td>
        <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>method</code></td>
        <td><code>str</code></td>
        <td><p>What inference method to use. Either of SNPE, SNLE or SNRE.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_simulations</code></td>
        <td><code>int</code></td>
        <td><p>Number of simulation calls. More simulations means a longer
runtime, but a better posterior estimate.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_workers</code></td>
        <td><code>int</code></td>
        <td><p>Number of parallel workers to use for simulations.</p></td>
        <td><code>1</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: Posterior over parameters conditional on observations (amortized).</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/base.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">infer</span><span class="p">(</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralPosterior</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Runs simulation-based inference and returns the posterior.</span>

<span class="sd">    This function provides a simple interface to run sbi. Inference is run for a single</span>
<span class="sd">    round and hence the returned posterior $p(\theta|x)$ can be sampled and evaluated</span>
<span class="sd">    for any $x$ (i.e. it is amortized).</span>

<span class="sd">    The scope of this function is limited to the most essential features of sbi. For</span>
<span class="sd">    more flexibility (e.g. multi-round inference, different density estimators) please</span>
<span class="sd">    use the flexible interface described here:</span>
<span class="sd">    https://www.mackelab.org/sbi/tutorial/02_flexible_interface/</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        method: What inference method to use. Either of SNPE, SNLE or SNRE.</span>
<span class="sd">        num_simulations: Number of simulation calls. More simulations means a longer</span>
<span class="sd">            runtime, but a better posterior estimate.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>

<span class="sd">    Returns: Posterior over parameters conditional on observations (amortized).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">method_fun</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="p">,</span> <span class="n">method</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span>
            <span class="s2">&quot;Method not available. `method` must be one of &#39;SNPE&#39;, &#39;SNLE&#39;, &#39;SNRE&#39;.&quot;</span>
        <span class="p">)</span>

    <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span> <span class="o">=</span> <span class="n">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>

    <span class="n">inference</span> <span class="o">=</span> <span class="n">method_fun</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">)</span>
    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">simulate_for_sbi</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">num_simulations</span><span class="o">=</span><span class="n">num_simulations</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">inference</span><span class="o">.</span><span class="n">append_simulations</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">inference</span><span class="o">.</span><span class="n">build_posterior</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">posterior</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.utils.user_input_checks.prepare_for_sbi">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">user_input_checks</span><span class="o">.</span><span class="n">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.utils.user_input_checks.prepare_for_sbi" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Prepare simulator, prior and for usage in sbi.</p>
<p>One of the goals is to allow you to use sbi with inputs computed in numpy.</p>
<p>Attempts to meet the following requirements by reshaping and type-casting:</p>
<ul>
<li>the simulator function receives as input and returns a Tensor.<br/></li>
<li>the simulator can simulate batches of parameters and return batches of data.<br/></li>
<li>the prior does not produce batches and samples and evaluates to Tensor.<br/></li>
<li>the output shape is a <code>torch.Size((1,N))</code> (i.e, has a leading batch dimension 1).</li>
</ul>
<p>If this is not possible, a suitable exception will be raised.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>simulator</code></td>
        <td><code>Callable</code></td>
        <td><p>Simulator as provided by the user.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td></td>
        <td><p>Prior as provided by the user.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Callable, torch.distributions.distribution.Distribution]</code></td>
      <td><p>Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/utils/user_input_checks.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Prepare simulator, prior and for usage in sbi.</span>

<span class="sd">    One of the goals is to allow you to use sbi with inputs computed in numpy.</span>

<span class="sd">    Attempts to meet the following requirements by reshaping and type-casting:</span>

<span class="sd">    - the simulator function receives as input and returns a Tensor.&lt;br/&gt;</span>
<span class="sd">    - the simulator can simulate batches of parameters and return batches of data.&lt;br/&gt;</span>
<span class="sd">    - the prior does not produce batches and samples and evaluates to Tensor.&lt;br/&gt;</span>
<span class="sd">    - the output shape is a `torch.Size((1,N))` (i.e, has a leading batch dimension 1).</span>

<span class="sd">    If this is not possible, a suitable exception will be raised.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: Simulator as provided by the user.</span>
<span class="sd">        prior: Prior as provided by the user.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple (simulator, prior, x_shape) checked and matching the requirements of sbi.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Check prior, return PyTorch prior.</span>
    <span class="n">prior</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">prior_returns_numpy</span> <span class="o">=</span> <span class="n">process_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="c1"># Check simulator, returns PyTorch simulator able to simulate batches.</span>
    <span class="n">simulator</span> <span class="o">=</span> <span class="n">process_simulator</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">prior_returns_numpy</span><span class="p">)</span>

    <span class="c1"># Consistency check after making ready for sbi.</span>
    <span class="n">check_sbi_inputs</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.inference.base.simulate_for_sbi">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">simulate_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.base.simulate_for_sbi" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Returns (<span class="arithmatex">\(\theta, x\)</span>) pairs obtained from sampling the proposal and simulating.</p>
<p>This function performs two steps:</p>
<ul>
<li>Sample parameters <span class="arithmatex">\(\theta\)</span> from the <code>proposal</code>.</li>
<li>Simulate these parameters to obtain <span class="arithmatex">\(x\)</span>.</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>simulator</code></td>
        <td><code>Callable</code></td>
        <td><p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\text{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>proposal</code></td>
        <td><code>Any</code></td>
        <td><p>Probability distribution that the parameters <span class="arithmatex">\(\theta\)</span> are sampled
from.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_simulations</code></td>
        <td><code>int</code></td>
        <td><p>Number of simulations that are run.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_workers</code></td>
        <td><code>int</code></td>
        <td><p>Number of parallel workers to use for simulations.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>simulation_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bar</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progress bar for simulating. This will not
affect whether there will be a progressbar while drawing samples from the
proposal.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: Sampled parameters <span class="arithmatex">\(\theta\)</span> and simulation-outputs <span class="arithmatex">\(x\)</span>.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/base.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">simulate_for_sbi</span><span class="p">(</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns ($\theta, x$) pairs obtained from sampling the proposal and simulating.</span>

<span class="sd">    This function performs two steps:</span>

<span class="sd">    - Sample parameters $\theta$ from the `proposal`.</span>
<span class="sd">    - Simulate these parameters to obtain $x$.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\text{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        proposal: Probability distribution that the parameters $\theta$ are sampled</span>
<span class="sd">            from.</span>
<span class="sd">        num_simulations: Number of simulations that are run.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        show_progress_bar: Whether to show a progress bar for simulating. This will not</span>
<span class="sd">            affect whether there will be a progressbar while drawing samples from the</span>
<span class="sd">            proposal.</span>

<span class="sd">    Returns: Sampled parameters $\theta$ and simulation-outputs $x$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">proposal</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">simulate_in_batches</span><span class="p">(</span>
        <span class="n">simulator</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">,</span> <span class="n">show_progress_bar</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.snpe.snpe_a.SNPE_A">
        <code>
sbi.inference.snpe.snpe_a.SNPE_A            (<span title="sbi.inference.snpe.snpe_base.PosteriorEstimator">PosteriorEstimator</span>)
        </code>



<a class="headerlink" href="#sbi.inference.snpe.snpe_a.SNPE_A" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">


        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">SNPE_A</span><span class="p">(</span><span class="n">PosteriorEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">,</span>
        <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SNPE-A [1].</span>

<span class="sd">        [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">            Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">            https://arxiv.org/abs/1605.06376.</span>

<span class="sd">        This class implements SNPE-A. SNPE-A trains across multiple rounds with a</span>
<span class="sd">        maximum-likelihood-loss. This will make training converge to the proposal</span>
<span class="sd">        posterior instead of the true posterior. To correct for this, SNPE-A applies a</span>
<span class="sd">        post-hoc correction after training. This correction has to be performed</span>
<span class="sd">        analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the</span>
<span class="sd">        last round. In the last round, SNPE-A can use a Mixture of Gaussians.</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            density_estimator: If it is a string (only &quot;mdn_snpe_a&quot; is valid), use a</span>
<span class="sd">                pre-configured mixture of densities network. Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`. Note that until the last round only a</span>
<span class="sd">                single (multivariate) Gaussian component is used for training (see</span>
<span class="sd">                Algorithm 1 in [1]). In the last round, this component is replicated</span>
<span class="sd">                `num_components` times, its parameters are perturbed with a very small</span>
<span class="sd">                noise, and then the last training round is done with the expanded</span>
<span class="sd">                Gaussian mixture as estimator for the proposal posterior.</span>
<span class="sd">            num_components: Number of components of the mixture of Gaussians in the</span>
<span class="sd">                last round. This overrides the `num_components` value passed to</span>
<span class="sd">                `posterior_nn()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Catch invalid inputs.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">((</span><span class="n">density_estimator</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">callable</span><span class="p">(</span><span class="n">density_estimator</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;The `density_estimator` passed to SNPE_A needs to be a &quot;</span>
                <span class="s2">&quot;callable or the string &#39;mdn_snpe_a&#39;!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># `num_components` will be used to replicate the Gaussian in the last round.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="n">num_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
        <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
        <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
        <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span>
            <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_components&quot;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">final_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">component_perturbation</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-3</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the proposal posterior.</span>

<span class="sd">        [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">            Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">            https://arxiv.org/abs/1605.06376.</span>

<span class="sd">        Training is performed with maximum likelihood on samples from the latest round,</span>
<span class="sd">        which leads the algorithm to converge to the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            final_round: Whether we are in the last round of training or not. For all</span>
<span class="sd">                but the last round, Algorithm 1 from [1] is executed. In last the</span>
<span class="sd">                round, Algorithm 2 from [1] is executed once.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">                simulations `x`. See Lueckmann, Gonalves et al., NeurIPS 2017.</span>
<span class="sd">            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">                during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round. Not supported for</span>
<span class="sd">                SNPE-A.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">            component_perturbation: The standard deviation applied to all weights and</span>
<span class="sd">                biases when, in the last round, the Mixture of Gaussians is build from</span>
<span class="sd">                a single Gaussian. This value can be problem-specific and also depends</span>
<span class="sd">                on the number of mixture components.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="n">retrain_from_scratch</span><span class="p">,</span> <span class="s2">&quot;&quot;&quot;Retraining from scratch is not supported in SNPE-A yet. The reason for</span>
<span class="s2">        this is that, if we reininitialized the density estimator, the z-scoring would</span>
<span class="s2">        change, which would break the posthoc correction. This is a pure implementation</span>
<span class="s2">        issue.&quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span>
            <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;final_round&quot;</span><span class="p">,</span> <span class="s2">&quot;component_perturbation&quot;</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># SNPE-A always discards the prior samples.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;discard_prior_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
            <span class="c1"># If there is (will be) only one round, train with Algorithm 2 from [1].</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
                <span class="p">)</span>
            <span class="c1"># Run Algorithm 2 from [1].</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span><span class="p">:</span>
                <span class="c1"># Now switch to the specified number of components. This method will</span>
                <span class="c1"># only be used if `retrain_from_scratch=True`. Otherwise,</span>
                <span class="c1"># the MDN will be built from replicating the single-component net for</span>
                <span class="c1"># `num_component` times (via `_expand_mog()`).</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
                <span class="p">)</span>

                <span class="c1"># Extend the MDN to the originally desired number of components.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_expand_mog</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">component_perturbation</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;You have already run SNPE-A with `final_round=True`. Running it&quot;</span>
                    <span class="s2">&quot;again with this setting will not allow computing the posthoc&quot;</span>
                    <span class="s2">&quot;correction applied in SNPE-A. Thus, you will get an error when &quot;</span>
                    <span class="s2">&quot;calling `.build_posterior()` after training.&quot;</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Run Algorithm 1 from [1].</span>
            <span class="c1"># Wrap the function that builds the MDN such that we can make</span>
            <span class="c1"># sure that there is only one component when running.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">correct_for_proposal</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SNPE_A_MDN&quot;</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build mixture of Gaussians that approximates the posterior.</span>

<span class="sd">        Returns a `SNPE_A_MDN` object, which applies the posthoc-correction required in</span>
<span class="sd">        SNPE-A.</span>

<span class="sd">        Args:</span>
<span class="sd">            density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">                If `None`, use the latest neural density estimator that was trained.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">density_estimator</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
            <span class="p">)</span>  <span class="c1"># PosteriorEstimator.train() also returns a deepcopy, mimic this here</span>
            <span class="c1"># If internal net is used device is defined.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Set proposal of the density estimator.</span>
        <span class="c1"># This also evokes the z-scoring correction if necessary.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">proposal</span><span class="p">,</span> <span class="p">(</span><span class="n">MultivariateNormal</span><span class="p">,</span> <span class="n">utils</span><span class="o">.</span><span class="n">BoxUniform</span><span class="p">)</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Prior must be `torch.distributions.MultivariateNormal` or `sbi.utils.</span>
<span class="s2">                BoxUniform`&quot;&quot;&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">DirectPosterior</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;The proposal you passed to `append_simulations` is neither the prior</span>
<span class="s2">                nor a `DirectPosterior`. SNPE-A currently only supports these scenarios.</span>
<span class="s2">                &quot;&quot;&quot;</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Create the SNPE_A_MDN</span>
        <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="n">SNPE_A_MDN</span><span class="p">(</span>
            <span class="n">flow</span><span class="o">=</span><span class="n">density_estimator</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="n">proposal</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapped_density_estimator</span>

    <span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DirectPosterior&quot;</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">        This method first corrects the estimated density with `correct_for_proposal`</span>
<span class="sd">        and then returns a `DirectPosterior`.</span>

<span class="sd">        Args:</span>
<span class="sd">            density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">                If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">            prior: Prior distribution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">                initialization `inference = SNPE_A(prior)` or to `.build_posterior</span>
<span class="s2">                (prior=prior)`.&quot;&quot;&quot;</span>
            <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

        <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">correct_for_proposal</span><span class="p">(</span>
            <span class="n">density_estimator</span><span class="o">=</span><span class="n">density_estimator</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">DirectPosterior</span><span class="p">(</span>
            <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">wrapped_density_estimator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return the log-probability of the proposal posterior.</span>

<span class="sd">        For SNPE-A this is the same as `self._neural_net.log_prob(theta, x)` in</span>
<span class="sd">        `_loss()` to be found in `snpe_base.py`.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters .</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns: Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_expand_mog</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replicate a singe Gaussian trained with Algorithm 1 before continuing</span>
<span class="sd">        with Algorithm 2. The weights and biases of the associated MDN layers</span>
<span class="sd">        are repeated `num_components` times, slightly perturbed to break the</span>
<span class="sd">        symmetry such that the gradients in the subsequent training are not</span>
<span class="sd">        all identical.</span>

<span class="sd">        Args:</span>
<span class="sd">            eps: Standard deviation for the random perturbation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">MultivariateGaussianMDN</span><span class="p">)</span>

        <span class="c1"># Increase the number of components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>

        <span class="c1"># Expand the 1-dim Gaussian.</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                <span class="n">key</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="s2">&quot;means&quot;</span><span class="p">,</span> <span class="s2">&quot;unconstrained&quot;</span><span class="p">,</span> <span class="s2">&quot;upper&quot;</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># let autograd construct a new gradient</span>
                <span class="k">elif</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># let autograd construct a new gradient</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">












  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_a.SNPE_A.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;mdn_snpe_a&#39;</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_a.SNPE_A.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>SNPE-A [1].</p>
<p>[1] <em>Fast epsilon-free Inference of Simulation Models with Bayesian Conditional
    Density Estimation</em>, Papamakarios et al., NeurIPS 2016,
    <a href="https://arxiv.org/abs/1605.06376">https://arxiv.org/abs/1605.06376</a>.</p>
<p>This class implements SNPE-A. SNPE-A trains across multiple rounds with a
maximum-likelihood-loss. This will make training converge to the proposal
posterior instead of the true posterior. To correct for this, SNPE-A applies a
post-hoc correction after training. This correction has to be performed
analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the
last round. In the last round, SNPE-A can use a Mixture of Gaussians.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>If it is a string (only &ldquo;mdn_snpe_a&rdquo; is valid), use a
pre-configured mixture of densities network. Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>. Note that until the last round only a
single (multivariate) Gaussian component is used for training (see
Algorithm 1 in [1]). In the last round, this component is replicated
<code>num_components</code> times, its parameters are perturbed with a very small
noise, and then the last training round is done with the expanded
Gaussian mixture as estimator for the proposal posterior.</p></td>
        <td><code>&#39;mdn_snpe_a&#39;</code></td>
      </tr>
      <tr>
        <td><code>num_components</code></td>
        <td><code>int</code></td>
        <td><p>Number of components of the mixture of Gaussians in the
last round. This overrides the <code>num_components</code> value passed to
<code>posterior_nn()</code>.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>logging_level</code></td>
        <td><code>Union[int, str]</code></td>
        <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
        <td><code>&#39;WARNING&#39;</code></td>
      </tr>
      <tr>
        <td><code>summary_writer</code></td>
        <td><code>Optional[Writer]</code></td>
        <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progressbar during training.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">,</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SNPE-A [1].</span>

<span class="sd">    [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">        Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">        https://arxiv.org/abs/1605.06376.</span>

<span class="sd">    This class implements SNPE-A. SNPE-A trains across multiple rounds with a</span>
<span class="sd">    maximum-likelihood-loss. This will make training converge to the proposal</span>
<span class="sd">    posterior instead of the true posterior. To correct for this, SNPE-A applies a</span>
<span class="sd">    post-hoc correction after training. This correction has to be performed</span>
<span class="sd">    analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the</span>
<span class="sd">    last round. In the last round, SNPE-A can use a Mixture of Gaussians.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        density_estimator: If it is a string (only &quot;mdn_snpe_a&quot; is valid), use a</span>
<span class="sd">            pre-configured mixture of densities network. Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`. Note that until the last round only a</span>
<span class="sd">            single (multivariate) Gaussian component is used for training (see</span>
<span class="sd">            Algorithm 1 in [1]). In the last round, this component is replicated</span>
<span class="sd">            `num_components` times, its parameters are perturbed with a very small</span>
<span class="sd">            noise, and then the last training round is done with the expanded</span>
<span class="sd">            Gaussian mixture as estimator for the proposal posterior.</span>
<span class="sd">        num_components: Number of components of the mixture of Gaussians in the</span>
<span class="sd">            last round. This overrides the `num_components` value passed to</span>
<span class="sd">            `posterior_nn()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Catch invalid inputs.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">((</span><span class="n">density_estimator</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">callable</span><span class="p">(</span><span class="n">density_estimator</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;The `density_estimator` passed to SNPE_A needs to be a &quot;</span>
            <span class="s2">&quot;callable or the string &#39;mdn_snpe_a&#39;!&quot;</span>
        <span class="p">)</span>

    <span class="c1"># `num_components` will be used to replicate the Gaussian in the last round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="n">num_components</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
    <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
    <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
    <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span>
        <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_components&quot;</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_a.SNPE_A.append_simulations">
<code class="codehilite language-python"><span class="n">append_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_a.SNPE_A.append_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Store parameters and simulation outputs to use them for later training.</p>
<p>Data are stored as entries in lists for each type of variable (parameter/data).</p>
<p>Stores <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, prior_masks (indicating if simulations are coming from the
prior or not) and an index indicating which round the batch of simulations came
from.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameter sets.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>Simulation outputs.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>proposal</code></td>
        <td><code>Optional[sbi.inference.posteriors.direct_posterior.DirectPosterior]</code></td>
        <td><p>The distribution that the parameters <span class="arithmatex">\(\theta\)</span> were sampled from.
Pass <code>None</code> if the parameters were sampled from the prior. If not
<code>None</code>, it will trigger a different loss-function.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>PosteriorEstimator</code></td>
      <td><p>NeuralInference object (returned so that this function is chainable).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">append_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DirectPosterior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PosteriorEstimator&quot;</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store parameters and simulation outputs to use them for later training.</span>

<span class="sd">    Data are stored as entries in lists for each type of variable (parameter/data).</span>

<span class="sd">    Stores $\theta$, $x$, prior_masks (indicating if simulations are coming from the</span>
<span class="sd">    prior or not) and an index indicating which round the batch of simulations came</span>
<span class="sd">    from.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets.</span>
<span class="sd">        x: Simulation outputs.</span>
<span class="sd">        proposal: The distribution that the parameters $\theta$ were sampled from.</span>
<span class="sd">            Pass `None` if the parameters were sampled from the prior. If not</span>
<span class="sd">            `None`, it will trigger a different loss-function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        NeuralInference object (returned so that this function is chainable).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">validate_theta_and_x</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_proposal</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">proposal</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="n">proposal</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">RestrictedPrior</span><span class="p">)</span> <span class="ow">and</span> <span class="n">proposal</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="p">)</span>
    <span class="p">):</span>
        <span class="c1"># The `_data_round_index` will later be used to infer if one should train</span>
        <span class="c1"># with MLE loss or with atomic loss (see, in `train()`:</span>
        <span class="c1"># self._round = max(self._data_round_index))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">:</span>
            <span class="c1"># This catches a pretty specific case: if, in the first round, one</span>
            <span class="c1"># passes data that does not come from the prior.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">ImproperEmpirical</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">proposal</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You had not passed a prior at initialization, but now you &quot;</span>
                <span class="s2">&quot;passed a proposal. If you want to run multi-round SNPE, you have &quot;</span>
                <span class="s2">&quot;to specify a prior (set the `.prior` argument or re-initialize &quot;</span>
                <span class="s2">&quot;the object with a prior distribution). If the samples you passed &quot;</span>
                <span class="s2">&quot;to `append_simulations()` were sampled from the prior, you can &quot;</span>
                <span class="s2">&quot;run single-round inference with &quot;</span>
                <span class="s2">&quot;`append_simulations(..., proposal=None)`.&quot;</span>
            <span class="p">)</span>
        <span class="n">theta_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">ImproperEmpirical</span><span class="p">(</span><span class="n">theta_prior</span><span class="p">,</span> <span class="n">ones</span><span class="p">(</span><span class="n">theta_prior</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_a.SNPE_A.build_posterior">
<code class="codehilite language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.snpe.snpe_a.SNPE_A.build_posterior" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build posterior from the neural density estimator.</p>
<p>This method first corrects the estimated density with <code>correct_for_proposal</code>
and then returns a <code>DirectPosterior</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Optional[Module]</code></td>
        <td><p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>Prior distribution.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>DirectPosterior</code></td>
      <td><p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DirectPosterior&quot;</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">    This method first corrects the estimated density with `correct_for_proposal`</span>
<span class="sd">    and then returns a `DirectPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">        prior: Prior distribution.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">            initialization `inference = SNPE_A(prior)` or to `.build_posterior</span>
<span class="s2">            (prior=prior)`.&quot;&quot;&quot;</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

    <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">correct_for_proposal</span><span class="p">(</span>
        <span class="n">density_estimator</span><span class="o">=</span><span class="n">density_estimator</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">DirectPosterior</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">wrapped_density_estimator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_a.SNPE_A.correct_for_proposal">
<code class="codehilite language-python"><span class="n">correct_for_proposal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.snpe.snpe_a.SNPE_A.correct_for_proposal" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build mixture of Gaussians that approximates the posterior.</p>
<p>Returns a <code>SNPE_A_MDN</code> object, which applies the posthoc-correction required in
SNPE-A.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Optional[Module]</code></td>
        <td><p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>SNPE_A_MDN</code></td>
      <td><p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">correct_for_proposal</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SNPE_A_MDN&quot;</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build mixture of Gaussians that approximates the posterior.</span>

<span class="sd">    Returns a `SNPE_A_MDN` object, which applies the posthoc-correction required in</span>
<span class="sd">    SNPE-A.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">density_estimator</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="p">)</span>  <span class="c1"># PosteriorEstimator.train() also returns a deepcopy, mimic this here</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Set proposal of the density estimator.</span>
    <span class="c1"># This also evokes the z-scoring correction if necessary.</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">proposal</span><span class="p">,</span> <span class="p">(</span><span class="n">MultivariateNormal</span><span class="p">,</span> <span class="n">utils</span><span class="o">.</span><span class="n">BoxUniform</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Prior must be `torch.distributions.MultivariateNormal` or `sbi.utils.</span>
<span class="s2">            BoxUniform`&quot;&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">DirectPosterior</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;The proposal you passed to `append_simulations` is neither the prior</span>
<span class="s2">            nor a `DirectPosterior`. SNPE-A currently only supports these scenarios.</span>
<span class="s2">            &quot;&quot;&quot;</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Create the SNPE_A_MDN</span>
    <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="n">SNPE_A_MDN</span><span class="p">(</span>
        <span class="n">flow</span><span class="o">=</span><span class="n">density_estimator</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="n">proposal</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">wrapped_density_estimator</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_a.SNPE_A.get_dataloaders">
<code class="codehilite language-python"><span class="n">get_dataloaders</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_a.SNPE_A.get_dataloaders" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return dataloaders for training and validation.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>dataset</code></td>
        <td><code>TensorDataset</code></td>
        <td><p>holding all theta and x, optionally masks.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>training arg of inference methods.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the current call is resuming training so that no
new training and validation indices into the dataset have to be created.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn).</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader]</code></td>
      <td><p>Tuple of dataloaders for training and validation.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_dataloaders</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Return dataloaders for training and validation.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset: holding all theta and x, optionally masks.</span>
<span class="sd">        training_batch_size: training arg of inference methods.</span>
<span class="sd">        resume_training: Whether the current call is resuming training so that no</span>
<span class="sd">            new training and validation indices into the dataset have to be created.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of dataloaders for training and validation.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Get total number of training examples.</span>
    <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="c1"># Select random train and validation splits from (theta, x) pairs.</span>
    <span class="n">num_training_examples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">validation_fraction</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_examples</span><span class="p">)</span>
    <span class="n">num_validation_examples</span> <span class="o">=</span> <span class="n">num_examples</span> <span class="o">-</span> <span class="n">num_training_examples</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
        <span class="n">permuted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">num_examples</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">permuted_indices</span><span class="p">[:</span><span class="n">num_training_examples</span><span class="p">],</span>
            <span class="n">permuted_indices</span><span class="p">[</span><span class="n">num_training_examples</span><span class="p">:],</span>
        <span class="p">)</span>

    <span class="c1"># Create training and validation loaders using a subset sampler.</span>
    <span class="c1"># Intentionally use dicts to define the default dataloader args</span>
    <span class="c1"># Then, use dataloader_kwargs to override (or add to) any of these defaults</span>
    <span class="c1"># https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict</span>
    <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_training_examples</span><span class="p">),</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_validation_examples</span><span class="p">),</span>
        <span class="s2">&quot;shuffle&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">dataloader_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">train_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>
        <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">val_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_loader_kwargs</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">val_loader_kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_a.SNPE_A.get_simulations">
<code class="codehilite language-python"><span class="n">get_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">starting_round</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_on_invalid</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_a.SNPE_A.get_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns all <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, and prior_masks from rounds &gt;= <code>starting_round</code>.</p>
<p>If requested, do not return invalid data.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>starting_round</code></td>
        <td><code>int</code></td>
        <td><p>The earliest round to return samples from (we start counting
from zero).</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>warn_on_invalid</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to give out a warning if invalid simulations were
found.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: Parameters, simulation outputs, prior masks.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">starting_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">warn_on_invalid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns all $\theta$, $x$, and prior_masks from rounds &gt;= `starting_round`.</span>

<span class="sd">    If requested, do not return invalid data.</span>

<span class="sd">    Args:</span>
<span class="sd">        starting_round: The earliest round to return samples from (we start counting</span>
<span class="sd">            from zero).</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training.</span>
<span class="sd">        warn_on_invalid: Whether to give out a warning if invalid simulations were</span>
<span class="sd">            found.</span>

<span class="sd">    Returns: Parameters, simulation outputs, prior masks.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">prior_masks</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>

    <span class="c1"># Check for NaNs in simulations.</span>
    <span class="n">is_valid_x</span><span class="p">,</span> <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span> <span class="o">=</span> <span class="n">handle_invalid_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
    <span class="c1"># Check for problematic z-scoring</span>
    <span class="n">warn_if_zscoring_changes_data</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">warn_on_invalid</span><span class="p">:</span>
        <span class="n">warn_on_invalid_x</span><span class="p">(</span><span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
        <span class="n">warn_on_invalid_x_for_snpec_leakage</span><span class="p">(</span>
            <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">prior_masks</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_a.SNPE_A.train">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">final_round</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2147483647</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">component_perturbation</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.snpe.snpe_a.SNPE_A.train" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return density estimator that approximates the proposal posterior.</p>
<p>[1] <em>Fast epsilon-free Inference of Simulation Models with Bayesian Conditional
    Density Estimation</em>, Papamakarios et al., NeurIPS 2016,
    <a href="https://arxiv.org/abs/1605.06376">https://arxiv.org/abs/1605.06376</a>.</p>
<p>Training is performed with maximum likelihood on samples from the latest round,
which leads the algorithm to converge to the proposal posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>final_round</code></td>
        <td><code>bool</code></td>
        <td><p>Whether we are in the last round of training or not. For all
but the last round, Algorithm 1 from [1] is executed. In last the
round, Algorithm 2 from [1] is executed once.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Training batch size.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate for Adam optimizer.</p></td>
        <td><code>0.0005</code></td>
      </tr>
      <tr>
        <td><code>validation_fraction</code></td>
        <td><code>float</code></td>
        <td><p>The fraction of data to use for validation.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>stop_after_epochs</code></td>
        <td><code>int</code></td>
        <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
        <td><code>20</code></td>
      </tr>
      <tr>
        <td><code>max_num_epochs</code></td>
        <td><code>int</code></td>
        <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
        <td><code>2147483647</code></td>
      </tr>
      <tr>
        <td><code>clip_max_norm</code></td>
        <td><code>Optional[float]</code></td>
        <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
        <td><code>5.0</code></td>
      </tr>
      <tr>
        <td><code>calibration_kernel</code></td>
        <td><code>Optional[Callable]</code></td>
        <td><p>A function to calibrate the loss with respect to the
simulations <code>x</code>. See Lueckmann, Gonalves et al., NeurIPS 2017.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>retrain_from_scratch</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round. Not supported for
SNPE-A.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>show_train_summary</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[Dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>component_perturbation</code></td>
        <td><code>float</code></td>
        <td><p>The standard deviation applied to all weights and
biases when, in the last round, the Mixture of Gaussians is build from
a single Gaussian. This value can be problem-specific and also depends
on the number of mixture components.</p></td>
        <td><code>0.005</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>Density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">final_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">component_perturbation</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-3</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the proposal posterior.</span>

<span class="sd">    [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">        Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">        https://arxiv.org/abs/1605.06376.</span>

<span class="sd">    Training is performed with maximum likelihood on samples from the latest round,</span>
<span class="sd">    which leads the algorithm to converge to the proposal posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        final_round: Whether we are in the last round of training or not. For all</span>
<span class="sd">            but the last round, Algorithm 1 from [1] is executed. In last the</span>
<span class="sd">            round, Algorithm 2 from [1] is executed once.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">            simulations `x`. See Lueckmann, Gonalves et al., NeurIPS 2017.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round. Not supported for</span>
<span class="sd">            SNPE-A.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">        component_perturbation: The standard deviation applied to all weights and</span>
<span class="sd">            biases when, in the last round, the Mixture of Gaussians is build from</span>
<span class="sd">            a single Gaussian. This value can be problem-specific and also depends</span>
<span class="sd">            on the number of mixture components.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">retrain_from_scratch</span><span class="p">,</span> <span class="s2">&quot;&quot;&quot;Retraining from scratch is not supported in SNPE-A yet. The reason for</span>
<span class="s2">    this is that, if we reininitialized the density estimator, the z-scoring would</span>
<span class="s2">    change, which would break the posthoc correction. This is a pure implementation</span>
<span class="s2">    issue.&quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span>
        <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;final_round&quot;</span><span class="p">,</span> <span class="s2">&quot;component_perturbation&quot;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># SNPE-A always discards the prior samples.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;discard_prior_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
        <span class="c1"># If there is (will be) only one round, train with Algorithm 2 from [1].</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
            <span class="p">)</span>
        <span class="c1"># Run Algorithm 2 from [1].</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span><span class="p">:</span>
            <span class="c1"># Now switch to the specified number of components. This method will</span>
            <span class="c1"># only be used if `retrain_from_scratch=True`. Otherwise,</span>
            <span class="c1"># the MDN will be built from replicating the single-component net for</span>
            <span class="c1"># `num_component` times (via `_expand_mog()`).</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
            <span class="p">)</span>

            <span class="c1"># Extend the MDN to the originally desired number of components.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_expand_mog</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">component_perturbation</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You have already run SNPE-A with `final_round=True`. Running it&quot;</span>
                <span class="s2">&quot;again with this setting will not allow computing the posthoc&quot;</span>
                <span class="s2">&quot;correction applied in SNPE-A. Thus, you will get an error when &quot;</span>
                <span class="s2">&quot;calling `.build_posterior()` after training.&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Run Algorithm 1 from [1].</span>
        <span class="c1"># Wrap the function that builds the MDN such that we can make</span>
        <span class="c1"># sure that there is only one component when running.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C">
        <code>
sbi.inference.snpe.snpe_c.SNPE_C            (<span title="sbi.inference.snpe.snpe_base.PosteriorEstimator">PosteriorEstimator</span>)
        </code>



<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">


        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">SNPE_C</span><span class="p">(</span><span class="n">PosteriorEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SNPE-C / APT [1].</span>

<span class="sd">        [1] _Automatic Posterior Transformation for Likelihood-free Inference_,</span>
<span class="sd">            Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</span>

<span class="sd">        This class implements two loss variants of SNPE-C: the non-atomic and the atomic</span>
<span class="sd">        version. The atomic loss of SNPE-C can be used for any density estimator,</span>
<span class="sd">        i.e. also for normalizing flows. However, it suffers from leakage issues. On</span>
<span class="sd">        the other hand, the non-atomic loss can only be used only if the proposal</span>
<span class="sd">        distribution is a mixture of Gaussians, the density estimator is a mixture of</span>
<span class="sd">        Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from</span>
<span class="sd">        leakage issues. At the beginning of each round, we print whether the non-atomic</span>
<span class="sd">        or the atomic version is used.</span>

<span class="sd">        In this codebase, we will automatically switch to the non-atomic loss if the</span>
<span class="sd">        following criteria are fulfilled:&lt;br/&gt;</span>
<span class="sd">        - proposal is a `DirectPosterior` with density_estimator `mdn`, as built</span>
<span class="sd">            with `utils.sbi.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">        - the density estimator is a `mdn`, as built with</span>
<span class="sd">            `utils.sbi.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">        - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or</span>
<span class="sd">            `isinstance(prior, sbi.utils.BoxUniform)`</span>

<span class="sd">        Note that custom implementations of any of these densities (or estimators) will</span>
<span class="sd">        not trigger the non-atomic loss, and the algorithm will fall back onto using</span>
<span class="sd">        the atomic loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them.</span>
<span class="sd">            density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">                provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_combined_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the distribution $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_atoms: Number of atoms to use for classification.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">                simulations `x`. See Lueckmann, Gonalves et al., NeurIPS 2017.</span>
<span class="sd">            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">                during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            use_combined_loss: Whether to train the neural net also on prior samples</span>
<span class="sd">                using maximum likelihood in addition to training it on all samples using</span>
<span class="sd">                atomic loss. The extra MLE loss helps prevent density leaking with</span>
<span class="sd">                bounded priors.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
        <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
        <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
        <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span> <span class="o">=</span> <span class="n">use_combined_loss</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="s2">&quot;use_combined_loss&quot;</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Set the proposal to the last proposal that was passed by the user. For</span>
            <span class="c1"># atomic SNPE, it does not matter what the proposal is. For non-atomic</span>
            <span class="c1"># SNPE, we only use the latest data that was passed, i.e. the one from the</span>
            <span class="c1"># last proposal.</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">check_dist_class</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">class_to_check</span><span class="o">=</span><span class="p">(</span><span class="n">Uniform</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">)</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;non-atomic&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="k">else</span> <span class="s2">&quot;atomic&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using SNPE-C with </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> loss&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
                <span class="c1"># Take care of z-scoring, pre-compute and store prior terms.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_state_for_mog_proposal</span><span class="p">()</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_state_for_mog_proposal</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Set state variables that are used at each training step of non-atomic SNPE-C.</span>

<span class="sd">        Three things are computed:</span>
<span class="sd">        1) Check if z-scoring was requested. To do so, we check if the `_transform`</span>
<span class="sd">            argument of the net had been a `CompositeTransform`. See pyknos mdn.py.</span>
<span class="sd">        2) Define a (potentially standardized) prior. It&#39;s standardized if z-scoring</span>
<span class="sd">            had been requested.</span>
<span class="sd">        3) Compute (Precision * mean) for the prior. This quantity is used at every</span>
<span class="sd">            training step if the prior is Gaussian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_transform</span><span class="p">,</span> <span class="n">CompositeTransform</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_maybe_z_scored_prior</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prec_m_prod_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">precision_matrix</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_maybe_z_scored_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute and store potentially standardized prior (if z-scoring was done).</span>

<span class="sd">        The proposal posterior is:</span>
<span class="sd">        $pp(\theta|x) = 1/Z * q(\theta|x) * prop(\theta) / p(\theta)$</span>

<span class="sd">        Let&#39;s denote z-scored theta by `a`: a = (theta - mean) / std</span>
<span class="sd">        Then pp&#39;(a|x) = 1/Z_2 * q&#39;(a|x) * prop&#39;(a) / p&#39;(a)$</span>

<span class="sd">        The &#39; indicates that the evaluation occurs in standardized space. The constant</span>
<span class="sd">        scaling factor has been absorbed into Z_2.</span>
<span class="sd">        From the above equation, we see that we need to evaluate the prior **in</span>
<span class="sd">        standardized space**. We build the standardized prior in this function.</span>

<span class="sd">        The standardize transform that is applied to the samples theta does not use</span>
<span class="sd">        the exact prior mean and std (due to implementation issues). Hence, the z-scored</span>
<span class="sd">        prior will not be exactly have mean=0 and std=1.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">_transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_scale</span>
            <span class="n">shift</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">_transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_shift</span>

            <span class="c1"># Following the definintion of the linear transform in</span>
            <span class="c1"># `standardizing_transform` in `sbiutils.py`:</span>
            <span class="c1"># shift=-mean / std</span>
            <span class="c1"># scale=1 / std</span>
            <span class="c1"># Solving these equations for mean and std:</span>
            <span class="n">estim_prior_std</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">scale</span>
            <span class="n">estim_prior_mean</span> <span class="o">=</span> <span class="o">-</span><span class="n">shift</span> <span class="o">*</span> <span class="n">estim_prior_std</span>

            <span class="c1"># Compute the discrepancy of the true prior mean and std and the mean and</span>
            <span class="c1"># std that was empirically estimated from samples.</span>
            <span class="c1"># N(theta|m,s) = N((theta-m_e)/s_e|(m-m_e)/s_e, s/s_e)</span>
            <span class="c1"># Above: m,s are true prior mean and std. m_e,s_e are estimated prior mean</span>
            <span class="c1"># and std (estimated from samples and used to build standardize transform).</span>
            <span class="n">almost_zero_mean</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">mean</span> <span class="o">-</span> <span class="n">estim_prior_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">estim_prior_std</span>
            <span class="n">almost_one_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">variance</span><span class="p">)</span> <span class="o">/</span> <span class="n">estim_prior_std</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
                    <span class="n">almost_zero_mean</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">almost_one_std</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">range_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">almost_one_std</span> <span class="o">*</span> <span class="mf">3.0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">BoxUniform</span><span class="p">(</span>
                    <span class="n">almost_zero_mean</span> <span class="o">-</span> <span class="n">range_</span><span class="p">,</span> <span class="n">almost_zero_mean</span> <span class="o">+</span> <span class="n">range_</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">DirectPosterior</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return the log-probability of the proposal posterior.</span>

<span class="sd">        If the proposal is a MoG, the density estimator is a MoG, and the prior is</span>
<span class="sd">        either Gaussian or uniform, we use non-atomic loss. Else, use atomic loss (which</span>
<span class="sd">        suffers from leakage).</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters .</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns: Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prob_proposal_posterior_mog</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">proposal</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prob_proposal_posterior_atomic</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior_atomic</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return log probability of the proposal posterior for atomic proposals.</span>

<span class="sd">        We have two main options when evaluating the proposal posterior.</span>
<span class="sd">            (1) Generate atoms from the proposal prior.</span>
<span class="sd">            (2) Generate atoms from a more targeted distribution, such as the most</span>
<span class="sd">                recent posterior.</span>
<span class="sd">        If we choose the latter, it is likely beneficial not to do this in the first</span>
<span class="sd">        round, since we would be sampling from a randomly-initialized neural density</span>
<span class="sd">        estimator.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters .</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">num_atoms</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">clamp_and_warn</span><span class="p">(</span><span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Each set of parameter atoms is evaluated using the same x,</span>
        <span class="c1"># so we repeat rows of the data x, e.g. [1, 2] -&gt; [1, 1, 2, 2]</span>
        <span class="n">repeated_x</span> <span class="o">=</span> <span class="n">repeat_rows</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># To generate the full set of atoms for a given item in the batch,</span>
        <span class="c1"># we sample without replacement num_atoms - 1 times from the rest</span>
        <span class="c1"># of the theta in the batch.</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eye</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">choices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_atoms</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">contrasting_theta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">choices</span><span class="p">]</span>

        <span class="c1"># We can now create our sets of atoms from the contrasting parameter sets</span>
        <span class="c1"># we have generated.</span>
        <span class="n">atomic_theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">theta</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">contrasting_theta</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_atoms</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="c1"># Evaluate large batch giving (batch_size * num_atoms) log prob posterior evals.</span>
        <span class="n">log_prob_posterior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">atomic_theta</span><span class="p">,</span> <span class="n">repeated_x</span><span class="p">)</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_posterior</span><span class="p">,</span> <span class="s2">&quot;posterior eval&quot;</span><span class="p">)</span>
        <span class="n">log_prob_posterior</span> <span class="o">=</span> <span class="n">log_prob_posterior</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># Get (batch_size * num_atoms) log prob prior evals.</span>
        <span class="n">log_prob_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">atomic_theta</span><span class="p">)</span>
        <span class="n">log_prob_prior</span> <span class="o">=</span> <span class="n">log_prob_prior</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_prior</span><span class="p">,</span> <span class="s2">&quot;prior eval&quot;</span><span class="p">)</span>

        <span class="c1"># Compute unnormalized proposal posterior.</span>
        <span class="n">unnormalized_log_prob</span> <span class="o">=</span> <span class="n">log_prob_posterior</span> <span class="o">-</span> <span class="n">log_prob_prior</span>

        <span class="c1"># Normalize proposal posterior across discrete set of atoms.</span>
        <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="n">unnormalized_log_prob</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
            <span class="n">unnormalized_log_prob</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_proposal_posterior</span><span class="p">,</span> <span class="s2">&quot;proposal posterior eval&quot;</span><span class="p">)</span>

        <span class="c1"># XXX This evaluates the posterior on _all_ prior samples</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span><span class="p">:</span>
            <span class="n">log_prob_posterior_non_atomic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">masks</span> <span class="o">*</span> <span class="n">log_prob_posterior_non_atomic</span> <span class="o">+</span> <span class="n">log_prob_proposal_posterior</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_proposal_posterior</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior_mog</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">proposal</span><span class="p">:</span> <span class="n">DirectPosterior</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return log-probability of the proposal posterior for MoG proposal.</span>

<span class="sd">        For MoG proposals and MoG density estimators, this can be done in closed form</span>
<span class="sd">        and does not require atomic loss (i.e. there will be no leakage issues).</span>

<span class="sd">        Notation:</span>

<span class="sd">        m are mean vectors.</span>
<span class="sd">        prec are precision matrices.</span>
<span class="sd">        cov are covariance matrices.</span>

<span class="sd">        _p at the end indicates that it is the proposal.</span>
<span class="sd">        _d indicates that it is the density estimator.</span>
<span class="sd">        _pp indicates the proposal posterior.</span>

<span class="sd">        All tensors will have shapes (batch_dim, num_components, ...)</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters .</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Evaluate the proposal. MDNs do not have functionality to run the embedding_net</span>
        <span class="c1"># and then get the mixture_components (**without** calling log_prob()). Hence,</span>
        <span class="c1"># we call them separately here.</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">_embedding_net</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">_distribution</span>
        <span class="p">)</span>  <span class="c1"># defined to avoid ugly black formatting.</span>
        <span class="n">logits_p</span><span class="p">,</span> <span class="n">m_p</span><span class="p">,</span> <span class="n">prec_p</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_mixture_components</span><span class="p">(</span><span class="n">encoded_x</span><span class="p">)</span>
        <span class="n">norm_logits_p</span> <span class="o">=</span> <span class="n">logits_p</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits_p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Evaluate the density estimator.</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_embedding_net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span>  <span class="c1"># defined to avoid black formatting.</span>
        <span class="n">logits_d</span><span class="p">,</span> <span class="n">m_d</span><span class="p">,</span> <span class="n">prec_d</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_mixture_components</span><span class="p">(</span><span class="n">encoded_x</span><span class="p">)</span>
        <span class="n">norm_logits_d</span> <span class="o">=</span> <span class="n">logits_d</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># z-score theta if it z-scoring had been requested.</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_score_theta</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Compute the MoG parameters of the proposal posterior.</span>
        <span class="n">logits_pp</span><span class="p">,</span> <span class="n">m_pp</span><span class="p">,</span> <span class="n">prec_pp</span><span class="p">,</span> <span class="n">cov_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_posterior_transformation</span><span class="p">(</span>
            <span class="n">norm_logits_p</span><span class="p">,</span> <span class="n">m_p</span><span class="p">,</span> <span class="n">prec_p</span><span class="p">,</span> <span class="n">norm_logits_d</span><span class="p">,</span> <span class="n">m_d</span><span class="p">,</span> <span class="n">prec_d</span>
        <span class="p">)</span>

        <span class="c1"># Compute the log_prob of theta under the product.</span>
        <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">mog_log_prob</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">logits_pp</span><span class="p">,</span> <span class="n">m_pp</span><span class="p">,</span> <span class="n">prec_pp</span>
        <span class="p">)</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_proposal_posterior</span><span class="p">,</span> <span class="s2">&quot;proposal posterior eval&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_proposal_posterior</span>

    <span class="k">def</span> <span class="nf">_automatic_posterior_transformation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">logits_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the MoG parameters of the proposal posterior.</span>

<span class="sd">        The proposal posterior is:</span>
<span class="sd">        $pp(\theta|x) = 1/Z * q(\theta|x) * prop(\theta) / p(\theta)$</span>
<span class="sd">        In words: proposal posterior = posterior estimate * proposal / prior.</span>

<span class="sd">        If the posterior estimate and the proposal are MoG and the prior is either</span>
<span class="sd">        Gaussian or uniform, we can solve this in closed-form. The is implemented in</span>
<span class="sd">        this function.</span>

<span class="sd">        This function implements Appendix A1 from Greenberg et al. 2019.</span>

<span class="sd">        We have to build L*K components. How do we do this?</span>
<span class="sd">        Example: proposal has two components, density estimator has three components.</span>
<span class="sd">        Let&#39;s call the two components of the proposal i,j and the three components</span>
<span class="sd">        of the density estimator x,y,z. We have to multiply every component of the</span>
<span class="sd">        proposal with every component of the density estimator. So, what we do is:</span>
<span class="sd">        1) for the proposal, build: i,i,i,j,j,j. Done with torch.repeat_interleave()</span>
<span class="sd">        2) for the density estimator, build: x,y,z,x,y,z. Done with torch.repeat()</span>
<span class="sd">        3) Multiply them with simple matrix operations.</span>

<span class="sd">        Args:</span>
<span class="sd">            logits_p: Component weight of each Gaussian of the proposal.</span>
<span class="sd">            means_p: Mean of each Gaussian of the proposal.</span>
<span class="sd">            precisions_p: Precision matrix of each Gaussian of the proposal.</span>
<span class="sd">            logits_d: Component weight for each Gaussian of the density estimator.</span>
<span class="sd">            means_d: Mean of each Gaussian of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrix of each Gaussian of the density estimator.</span>

<span class="sd">        Returns: (Component weight, mean, precision matrix, covariance matrix) of each</span>
<span class="sd">            Gaussian of the proposal posterior. Has L*K terms (proposal has L terms,</span>
<span class="sd">            density estimator has K terms).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_precisions_proposal_posterior</span><span class="p">(</span>
            <span class="n">precisions_p</span><span class="p">,</span> <span class="n">precisions_d</span>
        <span class="p">)</span>

        <span class="n">means_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_means_proposal_posterior</span><span class="p">(</span>
            <span class="n">covariances_pp</span><span class="p">,</span> <span class="n">means_p</span><span class="p">,</span> <span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_d</span><span class="p">,</span> <span class="n">precisions_d</span>
        <span class="p">)</span>

        <span class="n">logits_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logits_proposal_posterior</span><span class="p">(</span>
            <span class="n">means_pp</span><span class="p">,</span>
            <span class="n">precisions_pp</span><span class="p">,</span>
            <span class="n">covariances_pp</span><span class="p">,</span>
            <span class="n">logits_p</span><span class="p">,</span>
            <span class="n">means_p</span><span class="p">,</span>
            <span class="n">precisions_p</span><span class="p">,</span>
            <span class="n">logits_d</span><span class="p">,</span>
            <span class="n">means_d</span><span class="p">,</span>
            <span class="n">precisions_d</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">logits_pp</span><span class="p">,</span> <span class="n">means_pp</span><span class="p">,</span> <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span>

    <span class="k">def</span> <span class="nf">_precisions_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the precisions and covariances of the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: (Precisions, Covariances) of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">precisions_p_rep</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">precisions_d_rep</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">precisions_pp</span> <span class="o">=</span> <span class="n">precisions_p_rep</span> <span class="o">+</span> <span class="n">precisions_d_rep</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="n">precisions_pp</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">precision_matrix</span>

        <span class="n">covariances_pp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">precisions_pp</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span>

    <span class="k">def</span> <span class="nf">_means_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">covariances_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the means of the proposal posterior.</span>

<span class="sd">        means_pp = C_ix * (P_i * m_i + P_x * m_x - P_o * m_o).</span>

<span class="sd">        Args:</span>
<span class="sd">            covariances_pp: Covariance matrices of the proposal posterior.</span>
<span class="sd">            means_p: Means of the proposal distribution.</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            means_d: Means of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: Means of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># First, compute the product P_i * m_i and P_j * m_j</span>
        <span class="n">prec_m_prod_p</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_p</span><span class="p">)</span>
        <span class="n">prec_m_prod_d</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">,</span> <span class="n">means_d</span><span class="p">)</span>

        <span class="c1"># Repeat them to allow for matrix operations: same trick as for the precisions.</span>
        <span class="n">prec_m_prod_p_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">prec_m_prod_d_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Means = C_ij * (P_i * m_i + P_x * m_x - P_o * m_o).</span>
        <span class="n">summed_cov_m_prod_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_p_rep</span> <span class="o">+</span> <span class="n">prec_m_prod_d_rep</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="n">summed_cov_m_prod_rep</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prec_m_prod_prior</span>

        <span class="n">means_pp</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">covariances_pp</span><span class="p">,</span> <span class="n">summed_cov_m_prod_rep</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">means_pp</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_logits_proposal_posterior</span><span class="p">(</span>
        <span class="n">means_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">covariances_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the component weights (i.e. logits) of the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            means_pp: Means of the proposal posterior.</span>
<span class="sd">            precisions_pp: Precision matrices of the proposal posterior.</span>
<span class="sd">            covariances_pp: Covariance matrices of the proposal posterior.</span>
<span class="sd">            logits_p: Component weights (i.e. logits) of the proposal distribution.</span>
<span class="sd">            means_p: Means of the proposal distribution.</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            logits_d: Component weights (i.e. logits) of the density estimator.</span>
<span class="sd">            means_d: Means of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: Component weights of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Compute log(alpha_i * beta_j)</span>
        <span class="n">logits_p_rep</span> <span class="o">=</span> <span class="n">logits_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">logits_d_rep</span> <span class="o">=</span> <span class="n">logits_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>
        <span class="n">logit_factors</span> <span class="o">=</span> <span class="n">logits_p_rep</span> <span class="o">+</span> <span class="n">logits_d_rep</span>

        <span class="c1"># Compute sqrt(det()/(det()*det()))</span>
        <span class="n">logdet_covariances_pp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">covariances_pp</span><span class="p">)</span>
        <span class="n">logdet_covariances_p</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">)</span>
        <span class="n">logdet_covariances_d</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">)</span>

        <span class="c1"># Repeat the proposal and density estimator terms such that there are LK terms.</span>
        <span class="c1"># Same trick as has been used above.</span>
        <span class="n">logdet_covariances_p_rep</span> <span class="o">=</span> <span class="n">logdet_covariances_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
            <span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">logdet_covariances_d_rep</span> <span class="o">=</span> <span class="n">logdet_covariances_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>

        <span class="n">log_sqrt_det_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">logdet_covariances_pp</span>
            <span class="o">-</span> <span class="p">(</span><span class="n">logdet_covariances_p_rep</span> <span class="o">+</span> <span class="n">logdet_covariances_d_rep</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Compute for proposal, density estimator, and proposal posterior:</span>
        <span class="c1"># mu_i.T * P_i * mu_i</span>
        <span class="n">exponent_p</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_p</span><span class="p">)</span>
        <span class="n">exponent_d</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">,</span> <span class="n">means_d</span><span class="p">)</span>
        <span class="n">exponent_pp</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_pp</span><span class="p">,</span> <span class="n">means_pp</span><span class="p">)</span>

        <span class="c1"># Extend proposal and density estimator exponents to get LK terms.</span>
        <span class="n">exponent_p_rep</span> <span class="o">=</span> <span class="n">exponent_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">exponent_d_rep</span> <span class="o">=</span> <span class="n">exponent_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>
        <span class="n">exponent</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">exponent_p_rep</span> <span class="o">+</span> <span class="n">exponent_d_rep</span> <span class="o">-</span> <span class="n">exponent_pp</span><span class="p">)</span>

        <span class="n">logits_pp</span> <span class="o">=</span> <span class="n">logit_factors</span> <span class="o">+</span> <span class="n">log_sqrt_det_ratio</span> <span class="o">+</span> <span class="n">exponent</span>

        <span class="k">return</span> <span class="n">logits_pp</span>

    <span class="k">def</span> <span class="nf">_maybe_z_score_theta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return potentially standardized theta if z-scoring was requested.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span><span class="p">:</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">theta</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">












  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>SNPE-C / APT [1].</p>
<p>[1] <em>Automatic Posterior Transformation for Likelihood-free Inference</em>,
    Greenberg et al., ICML 2019, <a href="https://arxiv.org/abs/1905.07488">https://arxiv.org/abs/1905.07488</a>.</p>
<p>This class implements two loss variants of SNPE-C: the non-atomic and the atomic
version. The atomic loss of SNPE-C can be used for any density estimator,
i.e. also for normalizing flows. However, it suffers from leakage issues. On
the other hand, the non-atomic loss can only be used only if the proposal
distribution is a mixture of Gaussians, the density estimator is a mixture of
Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from
leakage issues. At the beginning of each round, we print whether the non-atomic
or the atomic version is used.</p>
<p>In this codebase, we will automatically switch to the non-atomic loss if the
following criteria are fulfilled:<br/>
- proposal is a <code>DirectPosterior</code> with density_estimator <code>mdn</code>, as built
    with <code>utils.sbi.posterior_nn()</code>.<br/>
- the density estimator is a <code>mdn</code>, as built with
    <code>utils.sbi.posterior_nn()</code>.<br/>
- <code>isinstance(prior, MultivariateNormal)</code> (from <code>torch.distributions</code>) or
    <code>isinstance(prior, sbi.utils.BoxUniform)</code></p>
<p>Note that custom implementations of any of these densities (or estimators) will
not trigger the non-atomic loss, and the algorithm will fall back onto using
the atomic loss.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>If it is a string, use a pre-configured network of the
provided type (one of nsf, maf, mdn, made). Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>.</p></td>
        <td><code>&#39;maf&#39;</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>logging_level</code></td>
        <td><code>Union[int, str]</code></td>
        <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
        <td><code>&#39;WARNING&#39;</code></td>
      </tr>
      <tr>
        <td><code>summary_writer</code></td>
        <td><code>Optional[Writer]</code></td>
        <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progressbar during training.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SNPE-C / APT [1].</span>

<span class="sd">    [1] _Automatic Posterior Transformation for Likelihood-free Inference_,</span>
<span class="sd">        Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</span>

<span class="sd">    This class implements two loss variants of SNPE-C: the non-atomic and the atomic</span>
<span class="sd">    version. The atomic loss of SNPE-C can be used for any density estimator,</span>
<span class="sd">    i.e. also for normalizing flows. However, it suffers from leakage issues. On</span>
<span class="sd">    the other hand, the non-atomic loss can only be used only if the proposal</span>
<span class="sd">    distribution is a mixture of Gaussians, the density estimator is a mixture of</span>
<span class="sd">    Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from</span>
<span class="sd">    leakage issues. At the beginning of each round, we print whether the non-atomic</span>
<span class="sd">    or the atomic version is used.</span>

<span class="sd">    In this codebase, we will automatically switch to the non-atomic loss if the</span>
<span class="sd">    following criteria are fulfilled:&lt;br/&gt;</span>
<span class="sd">    - proposal is a `DirectPosterior` with density_estimator `mdn`, as built</span>
<span class="sd">        with `utils.sbi.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">    - the density estimator is a `mdn`, as built with</span>
<span class="sd">        `utils.sbi.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">    - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or</span>
<span class="sd">        `isinstance(prior, sbi.utils.BoxUniform)`</span>

<span class="sd">    Note that custom implementations of any of these densities (or estimators) will</span>
<span class="sd">    not trigger the non-atomic loss, and the algorithm will fall back onto using</span>
<span class="sd">    the atomic loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them.</span>
<span class="sd">        density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">            provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C.append_simulations">
<code class="codehilite language-python"><span class="n">append_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C.append_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Store parameters and simulation outputs to use them for later training.</p>
<p>Data are stored as entries in lists for each type of variable (parameter/data).</p>
<p>Stores <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, prior_masks (indicating if simulations are coming from the
prior or not) and an index indicating which round the batch of simulations came
from.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameter sets.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>Simulation outputs.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>proposal</code></td>
        <td><code>Optional[sbi.inference.posteriors.direct_posterior.DirectPosterior]</code></td>
        <td><p>The distribution that the parameters <span class="arithmatex">\(\theta\)</span> were sampled from.
Pass <code>None</code> if the parameters were sampled from the prior. If not
<code>None</code>, it will trigger a different loss-function.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>PosteriorEstimator</code></td>
      <td><p>NeuralInference object (returned so that this function is chainable).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">append_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DirectPosterior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PosteriorEstimator&quot;</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store parameters and simulation outputs to use them for later training.</span>

<span class="sd">    Data are stored as entries in lists for each type of variable (parameter/data).</span>

<span class="sd">    Stores $\theta$, $x$, prior_masks (indicating if simulations are coming from the</span>
<span class="sd">    prior or not) and an index indicating which round the batch of simulations came</span>
<span class="sd">    from.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets.</span>
<span class="sd">        x: Simulation outputs.</span>
<span class="sd">        proposal: The distribution that the parameters $\theta$ were sampled from.</span>
<span class="sd">            Pass `None` if the parameters were sampled from the prior. If not</span>
<span class="sd">            `None`, it will trigger a different loss-function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        NeuralInference object (returned so that this function is chainable).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">validate_theta_and_x</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_proposal</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">proposal</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="n">proposal</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">RestrictedPrior</span><span class="p">)</span> <span class="ow">and</span> <span class="n">proposal</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="p">)</span>
    <span class="p">):</span>
        <span class="c1"># The `_data_round_index` will later be used to infer if one should train</span>
        <span class="c1"># with MLE loss or with atomic loss (see, in `train()`:</span>
        <span class="c1"># self._round = max(self._data_round_index))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">:</span>
            <span class="c1"># This catches a pretty specific case: if, in the first round, one</span>
            <span class="c1"># passes data that does not come from the prior.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">ImproperEmpirical</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">proposal</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You had not passed a prior at initialization, but now you &quot;</span>
                <span class="s2">&quot;passed a proposal. If you want to run multi-round SNPE, you have &quot;</span>
                <span class="s2">&quot;to specify a prior (set the `.prior` argument or re-initialize &quot;</span>
                <span class="s2">&quot;the object with a prior distribution). If the samples you passed &quot;</span>
                <span class="s2">&quot;to `append_simulations()` were sampled from the prior, you can &quot;</span>
                <span class="s2">&quot;run single-round inference with &quot;</span>
                <span class="s2">&quot;`append_simulations(..., proposal=None)`.&quot;</span>
            <span class="p">)</span>
        <span class="n">theta_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">ImproperEmpirical</span><span class="p">(</span><span class="n">theta_prior</span><span class="p">,</span> <span class="n">ones</span><span class="p">(</span><span class="n">theta_prior</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C.build_posterior">
<code class="codehilite language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="s1">&#39;rejection&#39;</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">&#39;slice_np&#39;</span><span class="p">,</span> <span class="n">vi_method</span><span class="o">=</span><span class="s1">&#39;rKL&#39;</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">vi_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">rejection_sampling_parameters</span><span class="o">=</span><span class="p">{})</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C.build_posterior" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build posterior from the neural density estimator.</p>
<p>For SNPE, the posterior distribution that is returned here implements the
following functionality over the raw neural density estimator:
- correct the calculation of the log probability such that it compensates for
    the leakage.
- reject samples that lie outside of the prior bounds.
- alternatively, if leakage is very high (which can happen for multi-round
    SNPE), sample from the posterior with MCMC.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Optional[torch.nn.modules.module.Module]</code></td>
        <td><p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>Prior distribution.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sample_with</code></td>
        <td><code>str</code></td>
        <td><p>Method to use for sampling from the posterior. Must be one of
[<code>mcmc</code> | <code>rejection</code> | <code>vi</code>].</p></td>
        <td><code>&#39;rejection&#39;</code></td>
      </tr>
      <tr>
        <td><code>mcmc_method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>,
<code>hmc</code>, <code>nuts</code>. Currently defaults to <code>slice_np</code> for a custom numpy
implementation of slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for
Pyro-based sampling.</p></td>
        <td><code>&#39;slice_np&#39;</code></td>
      </tr>
      <tr>
        <td><code>vi_method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for VI, one of [<code>rKL</code>, <code>fKL</code>, <code>IW</code>, <code>alpha</code>]. Note
some of the methods admit a <code>mode seeking</code> property (e.g. rKL) whereas
some admit a <code>mass covering</code> one (e.g fKL).</p></td>
        <td><code>&#39;rKL&#39;</code></td>
      </tr>
      <tr>
        <td><code>mcmc_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to <code>MCMCPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>vi_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to <code>VIPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>rejection_sampling_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to
<code>RejectionPosterior</code> or <code>DirectPosterior</code>. By default,
<code>DirectPosterior</code> is used. Only if <code>rejection_sampling_parameters</code>
contains <code>proposal</code>, a <code>RejectionPosterior</code> is instantiated.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[sbi.inference.posteriors.mcmc_posterior.MCMCPosterior, sbi.inference.posteriors.rejection_posterior.RejectionPosterior, sbi.inference.posteriors.vi_posterior.VIPosterior, sbi.inference.posteriors.direct_posterior.DirectPosterior]</code></td>
      <td><p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods
(the returned log-probability is unnormalized).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rejection&quot;</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span><span class="p">,</span>
    <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">vi_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">rejection_sampling_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MCMCPosterior</span><span class="p">,</span> <span class="n">RejectionPosterior</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">,</span> <span class="n">DirectPosterior</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">    For SNPE, the posterior distribution that is returned here implements the</span>
<span class="sd">    following functionality over the raw neural density estimator:</span>
<span class="sd">    - correct the calculation of the log probability such that it compensates for</span>
<span class="sd">        the leakage.</span>
<span class="sd">    - reject samples that lie outside of the prior bounds.</span>
<span class="sd">    - alternatively, if leakage is very high (which can happen for multi-round</span>
<span class="sd">        SNPE), sample from the posterior with MCMC.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        sample_with: Method to use for sampling from the posterior. Must be one of</span>
<span class="sd">            [`mcmc` | `rejection` | `vi`].</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`,</span>
<span class="sd">            `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy</span>
<span class="sd">            implementation of slice sampling; select `hmc`, `nuts` or `slice` for</span>
<span class="sd">            Pyro-based sampling.</span>
<span class="sd">        vi_method: Method used for VI, one of [`rKL`, `fKL`, `IW`, `alpha`]. Note</span>
<span class="sd">            some of the methods admit a `mode seeking` property (e.g. rKL) whereas</span>
<span class="sd">            some admit a `mass covering` one (e.g fKL).</span>
<span class="sd">        mcmc_parameters: Additional kwargs passed to `MCMCPosterior`.</span>
<span class="sd">        vi_parameters: Additional kwargs passed to `VIPosterior`.</span>
<span class="sd">        rejection_sampling_parameters: Additional kwargs passed to</span>
<span class="sd">            `RejectionPosterior` or `DirectPosterior`. By default,</span>
<span class="sd">            `DirectPosterior` is used. Only if `rejection_sampling_parameters`</span>
<span class="sd">            contains `proposal`, a `RejectionPosterior` is instantiated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods</span>
<span class="sd">        (the returned log-probability is unnormalized).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;You did not pass a prior. You have to pass the prior either at &quot;</span>
            <span class="s2">&quot;initialization `inference = SNPE(prior)` or to &quot;</span>
            <span class="s2">&quot;`.build_posterior(prior=prior)`.&quot;</span>
        <span class="p">)</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">posterior_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">density_estimator</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>

    <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">posterior_estimator_based_potential</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;rejection&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;proposal&quot;</span> <span class="ow">in</span> <span class="n">rejection_sampling_parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">RejectionPosterior</span><span class="p">(</span>
                <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
                <span class="o">**</span><span class="n">rejection_sampling_parameters</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">DirectPosterior</span><span class="p">(</span>
                <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">posterior_estimator</span><span class="p">,</span>
                <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;mcmc&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">MCMCPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">mcmc_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">mcmc_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;vi&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">VIPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">vi_method</span><span class="o">=</span><span class="n">vi_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">vi_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="c1"># Store models at end of each round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_bank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C.get_dataloaders">
<code class="codehilite language-python"><span class="n">get_dataloaders</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C.get_dataloaders" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return dataloaders for training and validation.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>dataset</code></td>
        <td><code>TensorDataset</code></td>
        <td><p>holding all theta and x, optionally masks.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>training arg of inference methods.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the current call is resuming training so that no
new training and validation indices into the dataset have to be created.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn).</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader]</code></td>
      <td><p>Tuple of dataloaders for training and validation.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_dataloaders</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Return dataloaders for training and validation.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset: holding all theta and x, optionally masks.</span>
<span class="sd">        training_batch_size: training arg of inference methods.</span>
<span class="sd">        resume_training: Whether the current call is resuming training so that no</span>
<span class="sd">            new training and validation indices into the dataset have to be created.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of dataloaders for training and validation.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Get total number of training examples.</span>
    <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="c1"># Select random train and validation splits from (theta, x) pairs.</span>
    <span class="n">num_training_examples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">validation_fraction</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_examples</span><span class="p">)</span>
    <span class="n">num_validation_examples</span> <span class="o">=</span> <span class="n">num_examples</span> <span class="o">-</span> <span class="n">num_training_examples</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
        <span class="n">permuted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">num_examples</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">permuted_indices</span><span class="p">[:</span><span class="n">num_training_examples</span><span class="p">],</span>
            <span class="n">permuted_indices</span><span class="p">[</span><span class="n">num_training_examples</span><span class="p">:],</span>
        <span class="p">)</span>

    <span class="c1"># Create training and validation loaders using a subset sampler.</span>
    <span class="c1"># Intentionally use dicts to define the default dataloader args</span>
    <span class="c1"># Then, use dataloader_kwargs to override (or add to) any of these defaults</span>
    <span class="c1"># https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict</span>
    <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_training_examples</span><span class="p">),</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_validation_examples</span><span class="p">),</span>
        <span class="s2">&quot;shuffle&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">dataloader_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">train_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>
        <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">val_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_loader_kwargs</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">val_loader_kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C.get_simulations">
<code class="codehilite language-python"><span class="n">get_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">starting_round</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_on_invalid</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C.get_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns all <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, and prior_masks from rounds &gt;= <code>starting_round</code>.</p>
<p>If requested, do not return invalid data.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>starting_round</code></td>
        <td><code>int</code></td>
        <td><p>The earliest round to return samples from (we start counting
from zero).</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>warn_on_invalid</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to give out a warning if invalid simulations were
found.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: Parameters, simulation outputs, prior masks.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">starting_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">warn_on_invalid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns all $\theta$, $x$, and prior_masks from rounds &gt;= `starting_round`.</span>

<span class="sd">    If requested, do not return invalid data.</span>

<span class="sd">    Args:</span>
<span class="sd">        starting_round: The earliest round to return samples from (we start counting</span>
<span class="sd">            from zero).</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training.</span>
<span class="sd">        warn_on_invalid: Whether to give out a warning if invalid simulations were</span>
<span class="sd">            found.</span>

<span class="sd">    Returns: Parameters, simulation outputs, prior masks.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">prior_masks</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>

    <span class="c1"># Check for NaNs in simulations.</span>
    <span class="n">is_valid_x</span><span class="p">,</span> <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span> <span class="o">=</span> <span class="n">handle_invalid_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
    <span class="c1"># Check for problematic z-scoring</span>
    <span class="n">warn_if_zscoring_changes_data</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">warn_on_invalid</span><span class="p">:</span>
        <span class="n">warn_on_invalid_x</span><span class="p">(</span><span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
        <span class="n">warn_on_invalid_x_for_snpec_leakage</span><span class="p">(</span>
            <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">prior_masks</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snpe.snpe_c.SNPE_C.train">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2147483647</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_combined_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.snpe.snpe_c.SNPE_C.train" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_atoms</code></td>
        <td><code>int</code></td>
        <td><p>Number of atoms to use for classification.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Training batch size.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate for Adam optimizer.</p></td>
        <td><code>0.0005</code></td>
      </tr>
      <tr>
        <td><code>validation_fraction</code></td>
        <td><code>float</code></td>
        <td><p>The fraction of data to use for validation.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>stop_after_epochs</code></td>
        <td><code>int</code></td>
        <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
        <td><code>20</code></td>
      </tr>
      <tr>
        <td><code>max_num_epochs</code></td>
        <td><code>int</code></td>
        <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
        <td><code>2147483647</code></td>
      </tr>
      <tr>
        <td><code>clip_max_norm</code></td>
        <td><code>Optional[float]</code></td>
        <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
        <td><code>5.0</code></td>
      </tr>
      <tr>
        <td><code>calibration_kernel</code></td>
        <td><code>Optional[Callable]</code></td>
        <td><p>A function to calibrate the loss with respect to the
simulations <code>x</code>. See Lueckmann, Gonalves et al., NeurIPS 2017.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>discard_prior_samples</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>use_combined_loss</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to train the neural net also on prior samples
using maximum likelihood in addition to training it on all samples using
atomic loss. The extra MLE loss helps prevent density leaking with
bounded priors.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>retrain_from_scratch</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>show_train_summary</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[Dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>Density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snpe/snpe_c.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_combined_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the distribution $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_atoms: Number of atoms to use for classification.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">            simulations `x`. See Lueckmann, Gonalves et al., NeurIPS 2017.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        use_combined_loss: Whether to train the neural net also on prior samples</span>
<span class="sd">            using maximum likelihood in addition to training it on all samples using</span>
<span class="sd">            atomic loss. The extra MLE loss helps prevent density leaking with</span>
<span class="sd">            bounded priors.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
    <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
    <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
    <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span> <span class="o">=</span> <span class="n">use_combined_loss</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="s2">&quot;use_combined_loss&quot;</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Set the proposal to the last proposal that was passed by the user. For</span>
        <span class="c1"># atomic SNPE, it does not matter what the proposal is. For non-atomic</span>
        <span class="c1"># SNPE, we only use the latest data that was passed, i.e. the one from the</span>
        <span class="c1"># last proposal.</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">check_dist_class</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">class_to_check</span><span class="o">=</span><span class="p">(</span><span class="n">Uniform</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">)</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;non-atomic&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="k">else</span> <span class="s2">&quot;atomic&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using SNPE-C with </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> loss&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
            <span class="c1"># Take care of z-scoring, pre-compute and store prior terms.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_state_for_mog_proposal</span><span class="p">()</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A">
        <code>
sbi.inference.snle.snle_a.SNLE_A            (<span title="sbi.inference.snle.snle_base.LikelihoodEstimator">LikelihoodEstimator</span>)
        </code>



<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">


        <details class="quote">
          <summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">SNLE_A</span><span class="p">(</span><span class="n">LikelihoodEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Neural Likelihood [1].</span>

<span class="sd">        [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with</span>
<span class="sd">        Autoregressive Flows_, Papamakarios et al., AISTATS 2019,</span>
<span class="sd">        https://arxiv.org/abs/1805.07226</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">                provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">












  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Sequential Neural Likelihood [1].</p>
<p>[1] Sequential Neural Likelihood: Fast Likelihood-free Inference with
Autoregressive Flows_, Papamakarios et al., AISTATS 2019,
<a href="https://arxiv.org/abs/1805.07226">https://arxiv.org/abs/1805.07226</a></p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>If it is a string, use a pre-configured network of the
provided type (one of nsf, maf, mdn, made). Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>.</p></td>
        <td><code>&#39;maf&#39;</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>logging_level</code></td>
        <td><code>Union[int, str]</code></td>
        <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
        <td><code>&#39;WARNING&#39;</code></td>
      </tr>
      <tr>
        <td><code>summary_writer</code></td>
        <td><code>Optional[Writer]</code></td>
        <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Neural Likelihood [1].</span>

<span class="sd">    [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with</span>
<span class="sd">    Autoregressive Flows_, Papamakarios et al., AISTATS 2019,</span>
<span class="sd">    https://arxiv.org/abs/1805.07226</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">            provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A.append_simulations">
<code class="codehilite language-python"><span class="n">append_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A.append_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Store parameters and simulation outputs to use them for later training.</p>
<p>Data are stored as entries in lists for each type of variable (parameter/data).</p>
<p>Stores <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, prior_masks (indicating if simulations are coming from the
prior or not) and an index indicating which round the batch of simulations came
from.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameter sets.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>Simulation outputs.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>from_round</code></td>
        <td><code>int</code></td>
        <td><p>Which round the data stemmed from. Round 0 means from the prior.
With default settings, this is not used at all for <code>SNLE</code>. Only when
the user later on requests <code>.train(discard_prior_samples=True)</code>, we
use these indices to find which training data stemmed from the prior.</p></td>
        <td><code>0</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>LikelihoodEstimator</code></td>
      <td><p>NeuralInference object (returned so that this function is chainable).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">append_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">from_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LikelihoodEstimator&quot;</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store parameters and simulation outputs to use them for later training.</span>

<span class="sd">    Data are stored as entries in lists for each type of variable (parameter/data).</span>

<span class="sd">    Stores $\theta$, $x$, prior_masks (indicating if simulations are coming from the</span>
<span class="sd">    prior or not) and an index indicating which round the batch of simulations came</span>
<span class="sd">    from.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets.</span>
<span class="sd">        x: Simulation outputs.</span>
<span class="sd">        from_round: Which round the data stemmed from. Round 0 means from the prior.</span>
<span class="sd">            With default settings, this is not used at all for `SNLE`. Only when</span>
<span class="sd">            the user later on requests `.train(discard_prior_samples=True)`, we</span>
<span class="sd">            use these indices to find which training data stemmed from the prior.</span>

<span class="sd">    Returns:</span>
<span class="sd">        NeuralInference object (returned so that this function is chainable).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">validate_theta_and_x</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">from_round</span><span class="p">),</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">from_round</span><span class="p">))</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A.build_posterior">
<code class="codehilite language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="s1">&#39;mcmc&#39;</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">&#39;slice_np&#39;</span><span class="p">,</span> <span class="n">vi_method</span><span class="o">=</span><span class="s1">&#39;rKL&#39;</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">vi_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">rejection_sampling_parameters</span><span class="o">=</span><span class="p">{})</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A.build_posterior" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build posterior from the neural density estimator.</p>
<p>SNLE trains a neural network to approximate the likelihood <span class="arithmatex">\(p(x|\theta)\)</span>. The
posterior wraps the trained network such that one can directly evaluate the
unnormalized posterior log probability <span class="arithmatex">\(p(\theta|x) \propto p(x|\theta) \cdot
p(\theta)\)</span> and draw samples from the posterior with MCMC or rejection sampling.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Optional[torch.nn.modules.module.Module]</code></td>
        <td><p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>Prior distribution.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sample_with</code></td>
        <td><code>str</code></td>
        <td><p>Method to use for sampling from the posterior. Must be one of
[<code>mcmc</code> | <code>rejection</code> | <code>vi</code>].</p></td>
        <td><code>&#39;mcmc&#39;</code></td>
      </tr>
      <tr>
        <td><code>mcmc_method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>,
<code>hmc</code>, <code>nuts</code>. Currently defaults to <code>slice_np</code> for a custom numpy
implementation of slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for
Pyro-based sampling.</p></td>
        <td><code>&#39;slice_np&#39;</code></td>
      </tr>
      <tr>
        <td><code>vi_method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for VI, one of [<code>rKL</code>, <code>fKL</code>, <code>IW</code>, <code>alpha</code>]. Note
some of the methods admit a <code>mode seeking</code> property (e.g. rKL) whereas
some admit a <code>mass covering</code> one (e.g fKL).</p></td>
        <td><code>&#39;rKL&#39;</code></td>
      </tr>
      <tr>
        <td><code>mcmc_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to <code>MCMCPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>vi_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to <code>VIPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>rejection_sampling_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to
<code>RejectionPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[sbi.inference.posteriors.mcmc_posterior.MCMCPosterior, sbi.inference.posteriors.rejection_posterior.RejectionPosterior, sbi.inference.posteriors.vi_posterior.VIPosterior]</code></td>
      <td><p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods
(the returned log-probability is unnormalized).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mcmc&quot;</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span><span class="p">,</span>
    <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">vi_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">rejection_sampling_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MCMCPosterior</span><span class="p">,</span> <span class="n">RejectionPosterior</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">    SNLE trains a neural network to approximate the likelihood $p(x|\theta)$. The</span>
<span class="sd">    posterior wraps the trained network such that one can directly evaluate the</span>
<span class="sd">    unnormalized posterior log probability $p(\theta|x) \propto p(x|\theta) \cdot</span>
<span class="sd">    p(\theta)$ and draw samples from the posterior with MCMC or rejection sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        sample_with: Method to use for sampling from the posterior. Must be one of</span>
<span class="sd">            [`mcmc` | `rejection` | `vi`].</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`,</span>
<span class="sd">            `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy</span>
<span class="sd">            implementation of slice sampling; select `hmc`, `nuts` or `slice` for</span>
<span class="sd">            Pyro-based sampling.</span>
<span class="sd">        vi_method: Method used for VI, one of [`rKL`, `fKL`, `IW`, `alpha`]. Note</span>
<span class="sd">            some of the methods admit a `mode seeking` property (e.g. rKL) whereas</span>
<span class="sd">            some admit a `mass covering` one (e.g fKL).</span>
<span class="sd">        mcmc_parameters: Additional kwargs passed to `MCMCPosterior`.</span>
<span class="sd">        vi_parameters: Additional kwargs passed to `VIPosterior`.</span>
<span class="sd">        rejection_sampling_parameters: Additional kwargs passed to</span>
<span class="sd">            `RejectionPosterior`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods</span>
<span class="sd">        (the returned log-probability is unnormalized).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">        initialization `inference = SNLE(prior)` or to `.build_posterior</span>
<span class="s2">        (prior=prior)`.&quot;&quot;&quot;</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">likelihood_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">likelihood_estimator</span> <span class="o">=</span> <span class="n">density_estimator</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>

    <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">likelihood_estimator_based_potential</span><span class="p">(</span>
        <span class="n">likelihood_estimator</span><span class="o">=</span><span class="n">likelihood_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;mcmc&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">MCMCPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">mcmc_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">mcmc_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;rejection&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">RejectionPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">rejection_sampling_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;vi&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">VIPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">vi_method</span><span class="o">=</span><span class="n">vi_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">vi_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="c1"># Store models at end of each round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_bank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A.get_dataloaders">
<code class="codehilite language-python"><span class="n">get_dataloaders</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A.get_dataloaders" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return dataloaders for training and validation.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>dataset</code></td>
        <td><code>TensorDataset</code></td>
        <td><p>holding all theta and x, optionally masks.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>training arg of inference methods.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the current call is resuming training so that no
new training and validation indices into the dataset have to be created.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn).</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader]</code></td>
      <td><p>Tuple of dataloaders for training and validation.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_dataloaders</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Return dataloaders for training and validation.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset: holding all theta and x, optionally masks.</span>
<span class="sd">        training_batch_size: training arg of inference methods.</span>
<span class="sd">        resume_training: Whether the current call is resuming training so that no</span>
<span class="sd">            new training and validation indices into the dataset have to be created.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of dataloaders for training and validation.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Get total number of training examples.</span>
    <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="c1"># Select random train and validation splits from (theta, x) pairs.</span>
    <span class="n">num_training_examples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">validation_fraction</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_examples</span><span class="p">)</span>
    <span class="n">num_validation_examples</span> <span class="o">=</span> <span class="n">num_examples</span> <span class="o">-</span> <span class="n">num_training_examples</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
        <span class="n">permuted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">num_examples</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">permuted_indices</span><span class="p">[:</span><span class="n">num_training_examples</span><span class="p">],</span>
            <span class="n">permuted_indices</span><span class="p">[</span><span class="n">num_training_examples</span><span class="p">:],</span>
        <span class="p">)</span>

    <span class="c1"># Create training and validation loaders using a subset sampler.</span>
    <span class="c1"># Intentionally use dicts to define the default dataloader args</span>
    <span class="c1"># Then, use dataloader_kwargs to override (or add to) any of these defaults</span>
    <span class="c1"># https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict</span>
    <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_training_examples</span><span class="p">),</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_validation_examples</span><span class="p">),</span>
        <span class="s2">&quot;shuffle&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">dataloader_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">train_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>
        <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">val_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_loader_kwargs</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">val_loader_kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A.get_simulations">
<code class="codehilite language-python"><span class="n">get_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">starting_round</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_on_invalid</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A.get_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns all <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, and prior_masks from rounds &gt;= <code>starting_round</code>.</p>
<p>If requested, do not return invalid data.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>starting_round</code></td>
        <td><code>int</code></td>
        <td><p>The earliest round to return samples from (we start counting
from zero).</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>warn_on_invalid</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to give out a warning if invalid simulations were
found.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: Parameters, simulation outputs, prior masks.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">starting_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">warn_on_invalid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns all $\theta$, $x$, and prior_masks from rounds &gt;= `starting_round`.</span>

<span class="sd">    If requested, do not return invalid data.</span>

<span class="sd">    Args:</span>
<span class="sd">        starting_round: The earliest round to return samples from (we start counting</span>
<span class="sd">            from zero).</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training.</span>
<span class="sd">        warn_on_invalid: Whether to give out a warning if invalid simulations were</span>
<span class="sd">            found.</span>

<span class="sd">    Returns: Parameters, simulation outputs, prior masks.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">prior_masks</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>

    <span class="c1"># Check for NaNs in simulations.</span>
    <span class="n">is_valid_x</span><span class="p">,</span> <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span> <span class="o">=</span> <span class="n">handle_invalid_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
    <span class="c1"># Check for problematic z-scoring</span>
    <span class="n">warn_if_zscoring_changes_data</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">warn_on_invalid</span><span class="p">:</span>
        <span class="n">warn_on_invalid_x</span><span class="p">(</span><span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
        <span class="n">warn_on_invalid_x_for_snpec_leakage</span><span class="p">(</span>
            <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">prior_masks</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snle.snle_a.SNLE_A.train">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2147483647</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snle.snle_a.SNLE_A.train" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Train the density estimator to learn the distribution <span class="arithmatex">\(p(x|\theta)\)</span>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>discard_prior_samples</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>retrain_from_scratch</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>show_train_summary</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to print the number of epochs and validation
loss after the training.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[Dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Flow</code></td>
      <td><p>Density estimator that has learned the distribution <span class="arithmatex">\(p(x|\theta)\)</span>.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snle/snle_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Train the density estimator to learn the distribution $p(x|\theta)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Density estimator that has learned the distribution $p(x|\theta)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Starting index for the training set (1 = discard round-0 samples).</span>
    <span class="n">start_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">discard_prior_samples</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Load data from most recent round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>
    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">(</span>
        <span class="n">start_idx</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">,</span> <span class="n">warn_on_invalid</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># Dataset is shared for training and validation loaders.</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dataloaders</span><span class="p">(</span>
        <span class="n">dataset</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># First round or if retraining from scratch:</span>
    <span class="c1"># Call the `self._build_neural_net` with the rounds&#39; thetas and xs as</span>
    <span class="c1"># arguments, which will build the neural network</span>
    <span class="c1"># This is passed into NeuralPosterior, to create a neural posterior which</span>
    <span class="c1"># can `sample()` and `log_prob()`. The network is accessible via `.net`.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span> <span class="o">=</span> <span class="n">x_shape_from_simulation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span>
        <span class="p">),</span> <span class="s2">&quot;SNLE cannot handle multi-dimensional simulator output.&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_log_prob</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-Inf&quot;</span><span class="p">)</span>

    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">&lt;=</span> <span class="n">max_num_epochs</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converged</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span>
    <span class="p">):</span>

        <span class="c1"># Train for a single epoch.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">train_log_probs_sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="c1"># Evaluate on x with theta as context.</span>
            <span class="n">train_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">theta_batch</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_batch</span><span class="p">)</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span>
            <span class="n">train_log_probs_sum</span> <span class="o">-=</span> <span class="n">train_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">train_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">clip_max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">clip_grad_norm_</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                    <span class="n">max_norm</span><span class="o">=</span><span class="n">clip_max_norm</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">train_log_prob_average</span> <span class="o">=</span> <span class="n">train_log_probs_sum</span> <span class="o">/</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">batch_size</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;train_log_probs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_log_prob_average</span><span class="p">)</span>

        <span class="c1"># Calculate validation performance.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">val_log_prob_sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
                <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="c1"># Evaluate on x with theta as context.</span>
                <span class="n">val_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">theta_batch</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_batch</span><span class="p">)</span>
                <span class="n">val_log_prob_sum</span> <span class="o">-=</span> <span class="n">val_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Take mean over all validation samples.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_val_log_prob</span> <span class="o">=</span> <span class="n">val_log_prob_sum</span> <span class="o">/</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">val_loader</span><span class="o">.</span><span class="n">batch_size</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>
        <span class="c1"># Log validation log prob for every epoch.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_log_probs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_val_log_prob</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_show_progress</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_show_progress_bars</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_report_convergence_at_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="p">)</span>

    <span class="c1"># Update summary.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epochs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;best_validation_log_probs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_best_val_log_prob</span><span class="p">)</span>

    <span class="c1"># Update TensorBoard and summary dict.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summarize</span><span class="p">(</span>
        <span class="n">round_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">,</span>
        <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">theta_bank</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span>
        <span class="n">x_bank</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Update description for progress bar.</span>
    <span class="k">if</span> <span class="n">show_train_summary</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_describe_round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">))</span>

    <span class="c1"># Avoid keeping the gradients in the resulting network, which can</span>
    <span class="c1"># cause memory leakage when benchmarking.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A">
        <code>
sbi.inference.snre.snre_a.SNRE_A            (<span title="sbi.inference.snre.snre_base.RatioEstimator">RatioEstimator</span>)
        </code>



<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">


        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">SNRE_A</span><span class="p">(</span><span class="n">RatioEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;AALR[1], here known as SNRE_A.</span>

<span class="sd">        [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans</span>
<span class="sd">            et al., ICML 2020, https://arxiv.org/abs/1903.04057</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">                inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">                `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">                during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># AALR is defined for `num_atoms=2`.</span>
        <span class="c1"># Proxy to `super().__call__` to ensure right parameter.</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the binary cross-entropy loss for the trained classifier.</span>

<span class="sd">        The classifier takes as input a $(\theta,x)$ pair. It is trained to predict 1</span>
<span class="sd">        if the pair was sampled from the joint $p(\theta,x)$, and to predict 0 if the</span>
<span class="sd">        pair was sampled from the marginals $p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># Alternating pairs where there is one sampled from the joint and one</span>
        <span class="c1"># sampled from the marginals. The first element is sampled from the</span>
        <span class="c1"># joint p(theta, x) and is labelled 1. The second element is sampled</span>
        <span class="c1"># from the marginals p(theta)p(x) and is labelled 0. And so on.</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># two atoms</span>
        <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Binary cross entropy to learn the likelihood (AALR-specific)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">












  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>AALR[1], here known as SNRE_A.</p>
<p>[1] <em>Likelihood-free MCMC with Amortized Approximate Likelihood Ratios</em>, Hermans
    et al., ICML 2020, <a href="https://arxiv.org/abs/1903.04057">https://arxiv.org/abs/1903.04057</a></p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>classifier</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p></td>
        <td><code>&#39;resnet&#39;</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>logging_level</code></td>
        <td><code>Union[int, str]</code></td>
        <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
        <td><code>&#39;warning&#39;</code></td>
      </tr>
      <tr>
        <td><code>summary_writer</code></td>
        <td><code>Optional[Writer]</code></td>
        <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;AALR[1], here known as SNRE_A.</span>

<span class="sd">    [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans</span>
<span class="sd">        et al., ICML 2020, https://arxiv.org/abs/1903.04057</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A.append_simulations">
<code class="codehilite language-python"><span class="n">append_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A.append_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Store parameters and simulation outputs to use them for later training.</p>
<p>Data are stored as entries in lists for each type of variable (parameter/data).</p>
<p>Stores <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, prior_masks (indicating if simulations are coming from the
prior or not) and an index indicating which round the batch of simulations came
from.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameter sets.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>Simulation outputs.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>from_round</code></td>
        <td><code>int</code></td>
        <td><p>Which round the data stemmed from. Round 0 means from the prior.
With default settings, this is not used at all for <code>SNRE</code>. Only when
the user later on requests <code>.train(discard_prior_samples=True)</code>, we
use these indices to find which training data stemmed from the prior.</p></td>
        <td><code>0</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>RatioEstimator</code></td>
      <td><p>NeuralInference object (returned so that this function is chainable).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">append_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">from_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;RatioEstimator&quot;</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store parameters and simulation outputs to use them for later training.</span>

<span class="sd">    Data are stored as entries in lists for each type of variable (parameter/data).</span>

<span class="sd">    Stores $\theta$, $x$, prior_masks (indicating if simulations are coming from the</span>
<span class="sd">    prior or not) and an index indicating which round the batch of simulations came</span>
<span class="sd">    from.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets.</span>
<span class="sd">        x: Simulation outputs.</span>
<span class="sd">        from_round: Which round the data stemmed from. Round 0 means from the prior.</span>
<span class="sd">            With default settings, this is not used at all for `SNRE`. Only when</span>
<span class="sd">            the user later on requests `.train(discard_prior_samples=True)`, we</span>
<span class="sd">            use these indices to find which training data stemmed from the prior.</span>

<span class="sd">    Returns:</span>
<span class="sd">        NeuralInference object (returned so that this function is chainable).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">validate_theta_and_x</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">from_round</span><span class="p">),</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">from_round</span><span class="p">))</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A.build_posterior">
<code class="codehilite language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="s1">&#39;mcmc&#39;</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">&#39;slice_np&#39;</span><span class="p">,</span> <span class="n">vi_method</span><span class="o">=</span><span class="s1">&#39;rKL&#39;</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">vi_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">rejection_sampling_parameters</span><span class="o">=</span><span class="p">{})</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A.build_posterior" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build posterior from the neural density estimator.</p>
<p>SNRE trains a neural network to approximate likelihood ratios. The
posterior wraps the trained network such that one can directly evaluate the
unnormalized posterior log probability <span class="arithmatex">\(p(\theta|x) \propto p(x|\theta) \cdot
p(\theta)\)</span> and draw samples from the posterior with MCMC or rejection sampling.
Note that, in the case of single-round SNRE_A / AALR, it is possible to
evaluate the log-probability of the <strong>normalized</strong> posterior, but sampling
still requires MCMC (or rejection sampling).</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Optional[torch.nn.modules.module.Module]</code></td>
        <td><p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>Prior distribution.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sample_with</code></td>
        <td><code>str</code></td>
        <td><p>Method to use for sampling from the posterior. Must be one of
[<code>mcmc</code> | <code>rejection</code> | <code>vi</code>].</p></td>
        <td><code>&#39;mcmc&#39;</code></td>
      </tr>
      <tr>
        <td><code>mcmc_method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>,
<code>hmc</code>, <code>nuts</code>. Currently defaults to <code>slice_np</code> for a custom numpy
implementation of slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for
Pyro-based sampling.</p></td>
        <td><code>&#39;slice_np&#39;</code></td>
      </tr>
      <tr>
        <td><code>vi_method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for VI, one of [<code>rKL</code>, <code>fKL</code>, <code>IW</code>, <code>alpha</code>]. Note
that some of the methods admit a <code>mode seeking</code> property (e.g. rKL)
whereas some admit a <code>mass covering</code> one (e.g fKL).</p></td>
        <td><code>&#39;rKL&#39;</code></td>
      </tr>
      <tr>
        <td><code>mcmc_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to <code>MCMCPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>vi_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to <code>VIPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>rejection_sampling_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to
<code>RejectionPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[sbi.inference.posteriors.mcmc_posterior.MCMCPosterior, sbi.inference.posteriors.rejection_posterior.RejectionPosterior, sbi.inference.posteriors.vi_posterior.VIPosterior]</code></td>
      <td><p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods
(the returned log-probability is unnormalized).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mcmc&quot;</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span><span class="p">,</span>
    <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">vi_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">rejection_sampling_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MCMCPosterior</span><span class="p">,</span> <span class="n">RejectionPosterior</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">    SNRE trains a neural network to approximate likelihood ratios. The</span>
<span class="sd">    posterior wraps the trained network such that one can directly evaluate the</span>
<span class="sd">    unnormalized posterior log probability $p(\theta|x) \propto p(x|\theta) \cdot</span>
<span class="sd">    p(\theta)$ and draw samples from the posterior with MCMC or rejection sampling.</span>
<span class="sd">    Note that, in the case of single-round SNRE_A / AALR, it is possible to</span>
<span class="sd">    evaluate the log-probability of the **normalized** posterior, but sampling</span>
<span class="sd">    still requires MCMC (or rejection sampling).</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        sample_with: Method to use for sampling from the posterior. Must be one of</span>
<span class="sd">            [`mcmc` | `rejection` | `vi`].</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`,</span>
<span class="sd">            `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy</span>
<span class="sd">            implementation of slice sampling; select `hmc`, `nuts` or `slice` for</span>
<span class="sd">            Pyro-based sampling.</span>
<span class="sd">        vi_method: Method used for VI, one of [`rKL`, `fKL`, `IW`, `alpha`]. Note</span>
<span class="sd">            that some of the methods admit a `mode seeking` property (e.g. rKL)</span>
<span class="sd">            whereas some admit a `mass covering` one (e.g fKL).</span>
<span class="sd">        mcmc_parameters: Additional kwargs passed to `MCMCPosterior`.</span>
<span class="sd">        vi_parameters: Additional kwargs passed to `VIPosterior`.</span>
<span class="sd">        rejection_sampling_parameters: Additional kwargs passed to</span>
<span class="sd">            `RejectionPosterior`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods</span>
<span class="sd">        (the returned log-probability is unnormalized).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">            initialization `inference = SNRE(prior)` or to `.build_posterior</span>
<span class="s2">            (prior=prior)`.&quot;&quot;&quot;</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ratio_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ratio_estimator</span> <span class="o">=</span> <span class="n">density_estimator</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>

    <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">ratio_estimator_based_potential</span><span class="p">(</span>
        <span class="n">ratio_estimator</span><span class="o">=</span><span class="n">ratio_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;mcmc&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">MCMCPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">mcmc_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">mcmc_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;rejection&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">RejectionPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">rejection_sampling_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;vi&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">VIPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">vi_method</span><span class="o">=</span><span class="n">vi_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">vi_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="c1"># Store models at end of each round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_bank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A.get_dataloaders">
<code class="codehilite language-python"><span class="n">get_dataloaders</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A.get_dataloaders" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return dataloaders for training and validation.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>dataset</code></td>
        <td><code>TensorDataset</code></td>
        <td><p>holding all theta and x, optionally masks.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>training arg of inference methods.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the current call is resuming training so that no
new training and validation indices into the dataset have to be created.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn).</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader]</code></td>
      <td><p>Tuple of dataloaders for training and validation.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_dataloaders</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Return dataloaders for training and validation.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset: holding all theta and x, optionally masks.</span>
<span class="sd">        training_batch_size: training arg of inference methods.</span>
<span class="sd">        resume_training: Whether the current call is resuming training so that no</span>
<span class="sd">            new training and validation indices into the dataset have to be created.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of dataloaders for training and validation.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Get total number of training examples.</span>
    <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="c1"># Select random train and validation splits from (theta, x) pairs.</span>
    <span class="n">num_training_examples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">validation_fraction</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_examples</span><span class="p">)</span>
    <span class="n">num_validation_examples</span> <span class="o">=</span> <span class="n">num_examples</span> <span class="o">-</span> <span class="n">num_training_examples</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
        <span class="n">permuted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">num_examples</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">permuted_indices</span><span class="p">[:</span><span class="n">num_training_examples</span><span class="p">],</span>
            <span class="n">permuted_indices</span><span class="p">[</span><span class="n">num_training_examples</span><span class="p">:],</span>
        <span class="p">)</span>

    <span class="c1"># Create training and validation loaders using a subset sampler.</span>
    <span class="c1"># Intentionally use dicts to define the default dataloader args</span>
    <span class="c1"># Then, use dataloader_kwargs to override (or add to) any of these defaults</span>
    <span class="c1"># https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict</span>
    <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_training_examples</span><span class="p">),</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_validation_examples</span><span class="p">),</span>
        <span class="s2">&quot;shuffle&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">dataloader_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">train_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>
        <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">val_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_loader_kwargs</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">val_loader_kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A.get_simulations">
<code class="codehilite language-python"><span class="n">get_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">starting_round</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_on_invalid</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A.get_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns all <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, and prior_masks from rounds &gt;= <code>starting_round</code>.</p>
<p>If requested, do not return invalid data.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>starting_round</code></td>
        <td><code>int</code></td>
        <td><p>The earliest round to return samples from (we start counting
from zero).</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>warn_on_invalid</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to give out a warning if invalid simulations were
found.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: Parameters, simulation outputs, prior masks.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">starting_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">warn_on_invalid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns all $\theta$, $x$, and prior_masks from rounds &gt;= `starting_round`.</span>

<span class="sd">    If requested, do not return invalid data.</span>

<span class="sd">    Args:</span>
<span class="sd">        starting_round: The earliest round to return samples from (we start counting</span>
<span class="sd">            from zero).</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training.</span>
<span class="sd">        warn_on_invalid: Whether to give out a warning if invalid simulations were</span>
<span class="sd">            found.</span>

<span class="sd">    Returns: Parameters, simulation outputs, prior masks.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">prior_masks</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>

    <span class="c1"># Check for NaNs in simulations.</span>
    <span class="n">is_valid_x</span><span class="p">,</span> <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span> <span class="o">=</span> <span class="n">handle_invalid_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
    <span class="c1"># Check for problematic z-scoring</span>
    <span class="n">warn_if_zscoring_changes_data</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">warn_on_invalid</span><span class="p">:</span>
        <span class="n">warn_on_invalid_x</span><span class="p">(</span><span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
        <span class="n">warn_on_invalid_x_for_snpec_leakage</span><span class="p">(</span>
            <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">prior_masks</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_a.SNRE_A.train">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2147483647</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.snre.snre_a.SNRE_A.train" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Training batch size.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate for Adam optimizer.</p></td>
        <td><code>0.0005</code></td>
      </tr>
      <tr>
        <td><code>validation_fraction</code></td>
        <td><code>float</code></td>
        <td><p>The fraction of data to use for validation.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>stop_after_epochs</code></td>
        <td><code>int</code></td>
        <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
        <td><code>20</code></td>
      </tr>
      <tr>
        <td><code>max_num_epochs</code></td>
        <td><code>int</code></td>
        <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
        <td><code>2147483647</code></td>
      </tr>
      <tr>
        <td><code>clip_max_norm</code></td>
        <td><code>Optional[float]</code></td>
        <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
        <td><code>5.0</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>discard_prior_samples</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>retrain_from_scratch</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>show_train_summary</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[Dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_a.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># AALR is defined for `num_atoms=2`.</span>
    <span class="c1"># Proxy to `super().__call__` to ensure right parameter.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B">
        <code>
sbi.inference.snre.snre_b.SNRE_B            (<span title="sbi.inference.snre.snre_base.RatioEstimator">RatioEstimator</span>)
        </code>



<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">


        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">SNRE_B</span><span class="p">(</span><span class="n">RatioEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SRE[1], here known as SNRE_B.</span>

<span class="sd">        [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,</span>
<span class="sd">            ICML 2020, https://arxiv.org/pdf/2002.03712</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">                inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">                `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_atoms: Number of atoms to use for classification.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">                during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return cross-entropy loss for 1-out-of-`num_atoms` classification.</span>

<span class="sd">        The classifier takes as input `num_atoms` $(\theta,x)$ pairs. Out of these</span>
<span class="sd">        pairs, one pair was sampled from the joint $p(\theta,x)$ and all others from the</span>
<span class="sd">        marginals $p(\theta)p(x)$. The classifier is trained to predict which of the</span>
<span class="sd">        pairs was sampled from the joint $p(\theta,x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># For 1-out-of-`num_atoms` classification each datapoint consists</span>
        <span class="c1"># of `num_atoms` points, with one of them being the correct one.</span>
        <span class="c1"># We have a batch of `batch_size` such datapoints.</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># Index 0 is the theta-x-pair sampled from the joint p(theta,x) and hence the</span>
        <span class="c1"># &quot;correct&quot; one for the 1-out-of-N classification.</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">












  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>SRE[1], here known as SNRE_B.</p>
<p>[1] <em>On Contrastive Learning for Likelihood-free Inference</em>, Durkan et al.,
    ICML 2020, <a href="https://arxiv.org/pdf/2002.03712">https://arxiv.org/pdf/2002.03712</a></p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>classifier</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p></td>
        <td><code>&#39;resnet&#39;</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>logging_level</code></td>
        <td><code>Union[int, str]</code></td>
        <td><p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p></td>
        <td><code>&#39;warning&#39;</code></td>
      </tr>
      <tr>
        <td><code>summary_writer</code></td>
        <td><code>Optional[Writer]</code></td>
        <td><p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SRE[1], here known as SNRE_B.</span>

<span class="sd">    [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,</span>
<span class="sd">        ICML 2020, https://arxiv.org/pdf/2002.03712</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B.append_simulations">
<code class="codehilite language-python"><span class="n">append_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">from_round</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B.append_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Store parameters and simulation outputs to use them for later training.</p>
<p>Data are stored as entries in lists for each type of variable (parameter/data).</p>
<p>Stores <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, prior_masks (indicating if simulations are coming from the
prior or not) and an index indicating which round the batch of simulations came
from.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameter sets.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>Simulation outputs.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>from_round</code></td>
        <td><code>int</code></td>
        <td><p>Which round the data stemmed from. Round 0 means from the prior.
With default settings, this is not used at all for <code>SNRE</code>. Only when
the user later on requests <code>.train(discard_prior_samples=True)</code>, we
use these indices to find which training data stemmed from the prior.</p></td>
        <td><code>0</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>RatioEstimator</code></td>
      <td><p>NeuralInference object (returned so that this function is chainable).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">append_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">from_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;RatioEstimator&quot;</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store parameters and simulation outputs to use them for later training.</span>

<span class="sd">    Data are stored as entries in lists for each type of variable (parameter/data).</span>

<span class="sd">    Stores $\theta$, $x$, prior_masks (indicating if simulations are coming from the</span>
<span class="sd">    prior or not) and an index indicating which round the batch of simulations came</span>
<span class="sd">    from.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets.</span>
<span class="sd">        x: Simulation outputs.</span>
<span class="sd">        from_round: Which round the data stemmed from. Round 0 means from the prior.</span>
<span class="sd">            With default settings, this is not used at all for `SNRE`. Only when</span>
<span class="sd">            the user later on requests `.train(discard_prior_samples=True)`, we</span>
<span class="sd">            use these indices to find which training data stemmed from the prior.</span>

<span class="sd">    Returns:</span>
<span class="sd">        NeuralInference object (returned so that this function is chainable).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">validate_theta_and_x</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">from_round</span><span class="p">),</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">from_round</span><span class="p">))</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B.build_posterior">
<code class="codehilite language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="s1">&#39;mcmc&#39;</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="s1">&#39;slice_np&#39;</span><span class="p">,</span> <span class="n">vi_method</span><span class="o">=</span><span class="s1">&#39;rKL&#39;</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">vi_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">rejection_sampling_parameters</span><span class="o">=</span><span class="p">{})</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B.build_posterior" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build posterior from the neural density estimator.</p>
<p>SNRE trains a neural network to approximate likelihood ratios. The
posterior wraps the trained network such that one can directly evaluate the
unnormalized posterior log probability <span class="arithmatex">\(p(\theta|x) \propto p(x|\theta) \cdot
p(\theta)\)</span> and draw samples from the posterior with MCMC or rejection sampling.
Note that, in the case of single-round SNRE_A / AALR, it is possible to
evaluate the log-probability of the <strong>normalized</strong> posterior, but sampling
still requires MCMC (or rejection sampling).</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>density_estimator</code></td>
        <td><code>Optional[torch.nn.modules.module.Module]</code></td>
        <td><p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch.distributions.distribution.Distribution]</code></td>
        <td><p>Prior distribution.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sample_with</code></td>
        <td><code>str</code></td>
        <td><p>Method to use for sampling from the posterior. Must be one of
[<code>mcmc</code> | <code>rejection</code> | <code>vi</code>].</p></td>
        <td><code>&#39;mcmc&#39;</code></td>
      </tr>
      <tr>
        <td><code>mcmc_method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice</code>,
<code>hmc</code>, <code>nuts</code>. Currently defaults to <code>slice_np</code> for a custom numpy
implementation of slice sampling; select <code>hmc</code>, <code>nuts</code> or <code>slice</code> for
Pyro-based sampling.</p></td>
        <td><code>&#39;slice_np&#39;</code></td>
      </tr>
      <tr>
        <td><code>vi_method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for VI, one of [<code>rKL</code>, <code>fKL</code>, <code>IW</code>, <code>alpha</code>]. Note
that some of the methods admit a <code>mode seeking</code> property (e.g. rKL)
whereas some admit a <code>mass covering</code> one (e.g fKL).</p></td>
        <td><code>&#39;rKL&#39;</code></td>
      </tr>
      <tr>
        <td><code>mcmc_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to <code>MCMCPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>vi_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to <code>VIPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>rejection_sampling_parameters</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Additional kwargs passed to
<code>RejectionPosterior</code>.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[sbi.inference.posteriors.mcmc_posterior.MCMCPosterior, sbi.inference.posteriors.rejection_posterior.RejectionPosterior, sbi.inference.posteriors.vi_posterior.VIPosterior]</code></td>
      <td><p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods
(the returned log-probability is unnormalized).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mcmc&quot;</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span><span class="p">,</span>
    <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">vi_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">rejection_sampling_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MCMCPosterior</span><span class="p">,</span> <span class="n">RejectionPosterior</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">    SNRE trains a neural network to approximate likelihood ratios. The</span>
<span class="sd">    posterior wraps the trained network such that one can directly evaluate the</span>
<span class="sd">    unnormalized posterior log probability $p(\theta|x) \propto p(x|\theta) \cdot</span>
<span class="sd">    p(\theta)$ and draw samples from the posterior with MCMC or rejection sampling.</span>
<span class="sd">    Note that, in the case of single-round SNRE_A / AALR, it is possible to</span>
<span class="sd">    evaluate the log-probability of the **normalized** posterior, but sampling</span>
<span class="sd">    still requires MCMC (or rejection sampling).</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        sample_with: Method to use for sampling from the posterior. Must be one of</span>
<span class="sd">            [`mcmc` | `rejection` | `vi`].</span>
<span class="sd">        mcmc_method: Method used for MCMC sampling, one of `slice_np`, `slice`,</span>
<span class="sd">            `hmc`, `nuts`. Currently defaults to `slice_np` for a custom numpy</span>
<span class="sd">            implementation of slice sampling; select `hmc`, `nuts` or `slice` for</span>
<span class="sd">            Pyro-based sampling.</span>
<span class="sd">        vi_method: Method used for VI, one of [`rKL`, `fKL`, `IW`, `alpha`]. Note</span>
<span class="sd">            that some of the methods admit a `mode seeking` property (e.g. rKL)</span>
<span class="sd">            whereas some admit a `mass covering` one (e.g fKL).</span>
<span class="sd">        mcmc_parameters: Additional kwargs passed to `MCMCPosterior`.</span>
<span class="sd">        vi_parameters: Additional kwargs passed to `VIPosterior`.</span>
<span class="sd">        rejection_sampling_parameters: Additional kwargs passed to</span>
<span class="sd">            `RejectionPosterior`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods</span>
<span class="sd">        (the returned log-probability is unnormalized).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">            initialization `inference = SNRE(prior)` or to `.build_posterior</span>
<span class="s2">            (prior=prior)`.&quot;&quot;&quot;</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ratio_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ratio_estimator</span> <span class="o">=</span> <span class="n">density_estimator</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>

    <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">ratio_estimator_based_potential</span><span class="p">(</span>
        <span class="n">ratio_estimator</span><span class="o">=</span><span class="n">ratio_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;mcmc&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">MCMCPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">mcmc_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">mcmc_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;rejection&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">RejectionPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">rejection_sampling_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;vi&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">VIPosterior</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">vi_method</span><span class="o">=</span><span class="n">vi_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span>
            <span class="o">**</span><span class="n">vi_parameters</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="c1"># Store models at end of each round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_bank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B.get_dataloaders">
<code class="codehilite language-python"><span class="n">get_dataloaders</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B.get_dataloaders" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return dataloaders for training and validation.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>dataset</code></td>
        <td><code>TensorDataset</code></td>
        <td><p>holding all theta and x, optionally masks.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>training arg of inference methods.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the current call is resuming training so that no
new training and validation indices into the dataset have to be created.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn).</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader]</code></td>
      <td><p>Tuple of dataloaders for training and validation.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_dataloaders</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Return dataloaders for training and validation.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset: holding all theta and x, optionally masks.</span>
<span class="sd">        training_batch_size: training arg of inference methods.</span>
<span class="sd">        resume_training: Whether the current call is resuming training so that no</span>
<span class="sd">            new training and validation indices into the dataset have to be created.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of dataloaders for training and validation.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Get total number of training examples.</span>
    <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="c1"># Select random train and validation splits from (theta, x) pairs.</span>
    <span class="n">num_training_examples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">validation_fraction</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_examples</span><span class="p">)</span>
    <span class="n">num_validation_examples</span> <span class="o">=</span> <span class="n">num_examples</span> <span class="o">-</span> <span class="n">num_training_examples</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
        <span class="n">permuted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">num_examples</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">permuted_indices</span><span class="p">[:</span><span class="n">num_training_examples</span><span class="p">],</span>
            <span class="n">permuted_indices</span><span class="p">[</span><span class="n">num_training_examples</span><span class="p">:],</span>
        <span class="p">)</span>

    <span class="c1"># Create training and validation loaders using a subset sampler.</span>
    <span class="c1"># Intentionally use dicts to define the default dataloader args</span>
    <span class="c1"># Then, use dataloader_kwargs to override (or add to) any of these defaults</span>
    <span class="c1"># https://stackoverflow.com/questions/44784577/in-method-call-args-how-to-override-keyword-argument-of-unpacked-dict</span>
    <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_training_examples</span><span class="p">),</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_batch_size</span><span class="p">,</span> <span class="n">num_validation_examples</span><span class="p">),</span>
        <span class="s2">&quot;shuffle&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;sampler&quot;</span><span class="p">:</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">dataloader_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">train_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">train_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>
        <span class="n">val_loader_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">val_loader_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_loader_kwargs</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">val_loader_kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B.get_simulations">
<code class="codehilite language-python"><span class="n">get_simulations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">starting_round</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_on_invalid</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B.get_simulations" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns all <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, and prior_masks from rounds &gt;= <code>starting_round</code>.</p>
<p>If requested, do not return invalid data.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>starting_round</code></td>
        <td><code>int</code></td>
        <td><p>The earliest round to return samples from (we start counting
from zero).</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>warn_on_invalid</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to give out a warning if invalid simulations were
found.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: Parameters, simulation outputs, prior masks.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">starting_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">warn_on_invalid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns all $\theta$, $x$, and prior_masks from rounds &gt;= `starting_round`.</span>

<span class="sd">    If requested, do not return invalid data.</span>

<span class="sd">    Args:</span>
<span class="sd">        starting_round: The earliest round to return samples from (we start counting</span>
<span class="sd">            from zero).</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training.</span>
<span class="sd">        warn_on_invalid: Whether to give out a warning if invalid simulations were</span>
<span class="sd">            found.</span>

<span class="sd">    Returns: Parameters, simulation outputs, prior masks.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>
    <span class="n">prior_masks</span> <span class="o">=</span> <span class="n">get_simulations_since_round</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">,</span> <span class="n">starting_round</span>
    <span class="p">)</span>

    <span class="c1"># Check for NaNs in simulations.</span>
    <span class="n">is_valid_x</span><span class="p">,</span> <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span> <span class="o">=</span> <span class="n">handle_invalid_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
    <span class="c1"># Check for problematic z-scoring</span>
    <span class="n">warn_if_zscoring_changes_data</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">warn_on_invalid</span><span class="p">:</span>
        <span class="n">warn_on_invalid_x</span><span class="p">(</span><span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">)</span>
        <span class="n">warn_on_invalid_x_for_snpec_leakage</span><span class="p">(</span>
            <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">],</span> <span class="n">prior_masks</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.snre.snre_b.SNRE_B.train">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2147483647</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.snre.snre_b.SNRE_B.train" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_atoms</code></td>
        <td><code>int</code></td>
        <td><p>Number of atoms to use for classification.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>training_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Training batch size.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate for Adam optimizer.</p></td>
        <td><code>0.0005</code></td>
      </tr>
      <tr>
        <td><code>validation_fraction</code></td>
        <td><code>float</code></td>
        <td><p>The fraction of data to use for validation.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>stop_after_epochs</code></td>
        <td><code>int</code></td>
        <td><p>The number of epochs to wait for improvement on the
validation set before terminating training.</p></td>
        <td><code>20</code></td>
      </tr>
      <tr>
        <td><code>max_num_epochs</code></td>
        <td><code>int</code></td>
        <td><p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p></td>
        <td><code>2147483647</code></td>
      </tr>
      <tr>
        <td><code>clip_max_norm</code></td>
        <td><code>Optional[float]</code></td>
        <td><p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p></td>
        <td><code>5.0</code></td>
      </tr>
      <tr>
        <td><code>exclude_invalid_x</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>resume_training</code></td>
        <td><code>bool</code></td>
        <td><p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>discard_prior_samples</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>retrain_from_scratch</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>show_train_summary</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to print the number of epochs and validation
loss and leakage after the training.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>dataloader_kwargs</code></td>
        <td><code>Optional[Dict]</code></td>
        <td><p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/snre/snre_b.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_atoms: Number of atoms to use for classification.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.abc.mcabc.MCABC">
        <code>
sbi.inference.abc.mcabc.MCABC            (<span title="sbi.inference.abc.abc_base.ABCBASE">ABCBASE</span>)
        </code>



<a class="headerlink" href="#sbi.inference.abc.mcabc.MCABC" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">


        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MCABC</span><span class="p">(</span><span class="n">ABCBASE</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</span>

<span class="sd">        [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.</span>
<span class="sd">        (1999). Population growth of human Y chromosomes: a study of Y chromosome</span>
<span class="sd">        microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</span>

<span class="sd">        Args:</span>
<span class="sd">            simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">                simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">                regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">                can be used.</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">                a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">            num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">            simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">                maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">                same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">                (simulation_batch_size, parameter_dimension).</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
        <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantile</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run MCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">        Args:</span>
<span class="sd">            x_o: Observed data.</span>
<span class="sd">            num_simulations: Number of simulations to run.</span>
<span class="sd">            eps: Acceptance threshold $\epsilon$ for distance between observed and</span>
<span class="sd">                simulated data.</span>
<span class="sd">            quantile: Upper quantile of smallest distances for which the corresponding</span>
<span class="sd">                parameters are returned, e.g, q=0.01 will return the top 1%. Exactly</span>
<span class="sd">                one of quantile or `eps` have to be passed.</span>
<span class="sd">            lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">            sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">                Fearnhead &amp; Prangle 2012.</span>
<span class="sd">            sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">            sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">                sass regression, default 1 - no expansion.</span>
<span class="sd">            kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">                object from which one can sample.</span>
<span class="sd">            kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">                &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">                heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">                default &#39;cv&#39;.</span>
<span class="sd">                &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">                &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">                more details</span>
<span class="sd">            return_summary: Whether to return the distances and data corresponding to</span>
<span class="sd">                the accepted parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            theta (if kde False): accepted parameters</span>
<span class="sd">            kde (if kde True): KDE object based on accepted parameters from which one</span>
<span class="sd">                can .sample() and .log_prob().</span>
<span class="sd">            summary (if summary True): dictionary containing the accepted paramters (if</span>
<span class="sd">                kde True), distances and simulated data x.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Exactly one of eps or quantile need to be passed.</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span>
            <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Eps or quantile must be passed, but not both.&quot;</span>

        <span class="c1"># Run SASS and change the simulator and x_o accordingly.</span>
        <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
            <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Running SASS with </span><span class="si">{</span><span class="n">num_pilot_simulations</span><span class="si">}</span><span class="s2"> pilot samples.&quot;</span>
            <span class="p">)</span>
            <span class="n">num_simulations</span> <span class="o">-=</span> <span class="n">num_pilot_simulations</span>

            <span class="n">pilot_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_pilot_simulations</span><span class="p">,))</span>
            <span class="n">pilot_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">pilot_theta</span><span class="p">)</span>

            <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
                <span class="n">pilot_theta</span><span class="p">,</span> <span class="n">pilot_x</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
            <span class="p">)</span>

            <span class="n">simulator</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
            <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="n">x_o</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">simulator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span>

        <span class="c1"># Simulate and calculate distances.</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Infer shape of x to test and set x_o.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="c1"># Select based on acceptance threshold epsilon.</span>
        <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">&lt;</span> <span class="n">eps</span>
            <span class="n">num_accepted</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">assert</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;No parameters accepted, eps=</span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2"> too small&quot;</span>

            <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
            <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
            <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>

        <span class="c1"># Select based on quantile on sorted distances.</span>
        <span class="k">elif</span> <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_top_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_simulations</span> <span class="o">*</span> <span class="n">quantile</span><span class="p">)</span>
            <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
            <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
            <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;One of epsilon or quantile has to be passed.&quot;</span><span class="p">)</span>

        <span class="c1"># Maybe adjust theta with LRA.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
            <span class="n">final_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">theta_accepted</span><span class="p">,</span> <span class="n">x_accepted</span><span class="p">,</span> <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">final_theta</span> <span class="o">=</span> <span class="n">theta_accepted</span>

        <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&quot;&quot;KDE on </span><span class="si">{</span><span class="n">final_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples with bandwidth option</span>
<span class="s2">                </span><span class="si">{</span><span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;bandwidth&quot;</span> <span class="ow">in</span> <span class="n">kde_kwargs</span> <span class="k">else</span> <span class="s2">&quot;cv&quot;</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">                Beware that KDE can give unreliable results when used with too few</span>
<span class="s2">                samples and in high dimensions.&quot;&quot;&quot;</span>
            <span class="p">)</span>

            <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">kde_dist</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span>
                    <span class="n">theta</span><span class="o">=</span><span class="n">final_theta</span><span class="p">,</span> <span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">kde_dist</span>
        <span class="k">elif</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_theta</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_theta</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.mcabc.MCABC.__call__">
<code class="codehilite language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quantile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_fraction</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.mcabc.MCABC.__call__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Run MCABC and return accepted parameters or KDE object fitted on them.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x_o</code></td>
        <td><code>Union[torch.Tensor, numpy.ndarray]</code></td>
        <td><p>Observed data.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_simulations</code></td>
        <td><code>int</code></td>
        <td><p>Number of simulations to run.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>eps</code></td>
        <td><code>Optional[float]</code></td>
        <td><p>Acceptance threshold <span class="arithmatex">\(\epsilon\)</span> for distance between observed and
simulated data.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>quantile</code></td>
        <td><code>Optional[float]</code></td>
        <td><p>Upper quantile of smallest distances for which the corresponding
parameters are returned, e.g, q=0.01 will return the top 1%. Exactly
one of quantile or <code>eps</code> have to be passed.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>lra</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to run linear regression adjustment as in Beaumont et al. 2002</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sass</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to determine semi-automatic summary statistics as in
Fearnhead &amp; Prangle 2012.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sass_fraction</code></td>
        <td><code>float</code></td>
        <td><p>Fraction of simulation budget used for the initial sass run.</p></td>
        <td><code>0.25</code></td>
      </tr>
      <tr>
        <td><code>sass_expansion_degree</code></td>
        <td><code>int</code></td>
        <td><p>Degree of the polynomial feature expansion for the
sass regression, default 1 - no expansion.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>kde</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to run KDE on the accepted parameters to return a KDE
object from which one can sample.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>kde_kwargs</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>kwargs for performing KDE:
&lsquo;bandwidth=&rsquo;; either a float, or a string naming a bandwidth
heuristics, e.g., &lsquo;cv&rsquo; (cross validation), &lsquo;silvermann&rsquo; or &lsquo;scott&rsquo;,
default &lsquo;cv&rsquo;.
&lsquo;transform&rsquo;: transform applied to the parameters before doing KDE.
&lsquo;sample_weights&rsquo;: weights associated with samples. See &lsquo;get_kde&rsquo; for
more details</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>return_summary</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to return the distances and data corresponding to
the accepted parameters.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>theta (if kde False)</code></td>
      <td><p>accepted parameters
kde (if kde True): KDE object based on accepted parameters from which one
    can .sample() and .log_prob().
summary (if summary True): dictionary containing the accepted paramters (if
    kde True), distances and simulated data x.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantile</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run MCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_o: Observed data.</span>
<span class="sd">        num_simulations: Number of simulations to run.</span>
<span class="sd">        eps: Acceptance threshold $\epsilon$ for distance between observed and</span>
<span class="sd">            simulated data.</span>
<span class="sd">        quantile: Upper quantile of smallest distances for which the corresponding</span>
<span class="sd">            parameters are returned, e.g, q=0.01 will return the top 1%. Exactly</span>
<span class="sd">            one of quantile or `eps` have to be passed.</span>
<span class="sd">        lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">        sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">            Fearnhead &amp; Prangle 2012.</span>
<span class="sd">        sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">        sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">            sass regression, default 1 - no expansion.</span>
<span class="sd">        kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">            object from which one can sample.</span>
<span class="sd">        kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">            &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">            heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">            default &#39;cv&#39;.</span>
<span class="sd">            &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">            &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">            more details</span>
<span class="sd">        return_summary: Whether to return the distances and data corresponding to</span>
<span class="sd">            the accepted parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        theta (if kde False): accepted parameters</span>
<span class="sd">        kde (if kde True): KDE object based on accepted parameters from which one</span>
<span class="sd">            can .sample() and .log_prob().</span>
<span class="sd">        summary (if summary True): dictionary containing the accepted paramters (if</span>
<span class="sd">            kde True), distances and simulated data x.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Exactly one of eps or quantile need to be passed.</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span>
        <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">&quot;Eps or quantile must be passed, but not both.&quot;</span>

    <span class="c1"># Run SASS and change the simulator and x_o accordingly.</span>
    <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
        <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Running SASS with </span><span class="si">{</span><span class="n">num_pilot_simulations</span><span class="si">}</span><span class="s2"> pilot samples.&quot;</span>
        <span class="p">)</span>
        <span class="n">num_simulations</span> <span class="o">-=</span> <span class="n">num_pilot_simulations</span>

        <span class="n">pilot_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_pilot_simulations</span><span class="p">,))</span>
        <span class="n">pilot_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">pilot_theta</span><span class="p">)</span>

        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
            <span class="n">pilot_theta</span><span class="p">,</span> <span class="n">pilot_x</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
        <span class="p">)</span>

        <span class="n">simulator</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="n">x_o</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">simulator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span>

    <span class="c1"># Simulate and calculate distances.</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="c1"># Infer shape of x to test and set x_o.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># Select based on acceptance threshold epsilon.</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">&lt;</span> <span class="n">eps</span>
        <span class="n">num_accepted</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;No parameters accepted, eps=</span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2"> too small&quot;</span>

        <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
        <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
        <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>

    <span class="c1"># Select based on quantile on sorted distances.</span>
    <span class="k">elif</span> <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_top_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_simulations</span> <span class="o">*</span> <span class="n">quantile</span><span class="p">)</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
        <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
        <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;One of epsilon or quantile has to be passed.&quot;</span><span class="p">)</span>

    <span class="c1"># Maybe adjust theta with LRA.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
        <span class="n">final_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">theta_accepted</span><span class="p">,</span> <span class="n">x_accepted</span><span class="p">,</span> <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">final_theta</span> <span class="o">=</span> <span class="n">theta_accepted</span>

    <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&quot;&quot;KDE on </span><span class="si">{</span><span class="n">final_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples with bandwidth option</span>
<span class="s2">            </span><span class="si">{</span><span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;bandwidth&quot;</span> <span class="ow">in</span> <span class="n">kde_kwargs</span> <span class="k">else</span> <span class="s2">&quot;cv&quot;</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">            Beware that KDE can give unreliable results when used with too few</span>
<span class="s2">            samples and in high dimensions.&quot;&quot;&quot;</span>
        <span class="p">)</span>

        <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kde_dist</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">theta</span><span class="o">=</span><span class="n">final_theta</span><span class="p">,</span> <span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kde_dist</span>
    <span class="k">elif</span> <span class="n">return_summary</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_theta</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_theta</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.mcabc.MCABC.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.mcabc.MCABC.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</p>
<p>[1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.
(1999). Population growth of human Y chromosomes: a study of Y chromosome
microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>simulator</code></td>
        <td><code>Callable</code></td>
        <td><p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\mathrm{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td></td>
        <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>distance</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>Distance function to compare observed and simulated data. Can be
a custom function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>.</p></td>
        <td><code>&#39;l2&#39;</code></td>
      </tr>
      <tr>
        <td><code>num_workers</code></td>
        <td><code>int</code></td>
        <td><p>Number of parallel workers to use for simulations.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>simulation_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</span>

<span class="sd">    [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.</span>
<span class="sd">    (1999). Population growth of human Y chromosomes: a study of Y chromosome</span>
<span class="sd">    microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">            a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.mcabc.MCABC.choose_distance_function">
<code class="codehilite language-python"><span class="n">choose_distance_function</span><span class="p">(</span><span class="n">distance_type</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.mcabc.MCABC.choose_distance_function" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return distance function for given distance type.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">choose_distance_function</span><span class="p">(</span><span class="n">distance_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return distance function for given distance type.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;l2&quot;</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">((</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;l1&quot;</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Distance </span><span class="si">{distance_type}</span><span class="s2"> not supported.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">distance_fun</span><span class="p">(</span><span class="n">observed_data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">simulated_data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return distance over batch dimension.</span>

<span class="sd">        Args:</span>
<span class="sd">            observed_data: Observed data, could be 1D.</span>
<span class="sd">            simulated_data: Batch of simulated data, has batch dimension.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Torch tensor with batch of distances.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">simulated_data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;simulated data needs batch dimension&quot;</span>

        <span class="k">return</span> <span class="n">distance</span><span class="p">(</span><span class="n">observed_data</span><span class="p">,</span> <span class="n">simulated_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">distance_fun</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.mcabc.MCABC.get_sass_transform">
<code class="codehilite language-python"><span class="n">get_sass_transform</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.mcabc.MCABC.get_sass_transform" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return semi-automatic summary statitics function.</p>
<p>Running weighted linear regressin as in
Fearnhead &amp; Prandle 2012: <a href="https://arxiv.org/abs/1004.1112">https://arxiv.org/abs/1004.1112</a></p>
<p>Following implementation in
<a href="https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity">https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity</a>
and
<a href="https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic">https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic</a></p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_sass_transform</span><span class="p">(</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return semi-automatic summary statitics function.</span>

<span class="sd">    Running weighted linear regressin as in</span>
<span class="sd">    Fearnhead &amp; Prandle 2012: https://arxiv.org/abs/1004.1112</span>

<span class="sd">    Following implementation in</span>
<span class="sd">    https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity</span>
<span class="sd">    and</span>
<span class="sd">    https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">expansion_degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Transform x, remove intercept.</span>
    <span class="n">x_expanded</span> <span class="o">=</span> <span class="n">expansion</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sumstats_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_expanded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">parameter_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">regression_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">x_expanded</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">theta</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
        <span class="p">)</span>
        <span class="n">sumstats_map</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">coef_</span>

    <span class="n">sumstats_map</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sumstats_map</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sumstats_transform</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">x_expanded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">expansion</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_expanded</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">sumstats_map</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sumstats_transform</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.mcabc.MCABC.run_lra">
<code class="codehilite language-python"><span class="n">run_lra</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">observation</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.mcabc.MCABC.run_lra" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return parameters adjusted with linear regression adjustment.</p>
<p>Implementation as in Beaumont et al. 2002: <a href="https://arxiv.org/abs/1707.01254">https://arxiv.org/abs/1707.01254</a></p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">run_lra</span><span class="p">(</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">observation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return parameters adjusted with linear regression adjustment.</span>

<span class="sd">    Implementation as in Beaumont et al. 2002: https://arxiv.org/abs/1707.01254</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta_adjusted</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="k">for</span> <span class="n">parameter_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">regression_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">theta</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">],</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">theta_adjusted</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">observation</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">theta_adjusted</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">]</span> <span class="o">-=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">theta_adjusted</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC">
        <code>
sbi.inference.abc.smcabc.SMCABC            (<span title="sbi.inference.abc.abc_base.ABCBASE">ABCBASE</span>)
        </code>



<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">


        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">SMCABC</span><span class="p">(</span><span class="n">ABCBASE</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
        <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
        <span class="n">algorithm_variant</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Monte Carlo Approximate Bayesian Computation.</span>

<span class="sd">        We distinguish between three different SMC methods here:</span>
<span class="sd">            - A: Toni et al. 2010 (Phd Thesis)</span>
<span class="sd">            - B: Sisson et al. 2007 (with correction from 2009)</span>
<span class="sd">            - C: Beaumont et al. 2009</span>

<span class="sd">        In Toni et al. 2010 we find an overview of the differences on page 34:</span>
<span class="sd">            - B: same as A except for resampling of weights if the effective sampling</span>
<span class="sd">                size is too small.</span>
<span class="sd">            - C: same as A except for calculation of the covariance of the perturbation</span>
<span class="sd">                kernel: the kernel covariance is a scaled version of the covariance of</span>
<span class="sd">                the previous population.</span>

<span class="sd">        Args:</span>
<span class="sd">            simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">                simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">                regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">                can be used.</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">                a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">            num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">            simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">                maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">                same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">                (simulation_batch_size, parameter_dimension).</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">            kernel: Perturbation kernel.</span>
<span class="sd">            algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">kernels</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Kernel &#39;</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported. Choose one from </span><span class="si">{</span><span class="n">kernels</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

        <span class="n">algorithm_variants</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="n">algorithm_variants</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;SMCABC variant &#39;</span><span class="si">{</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported, choose one from&quot;</span>
            <span class="s2">&quot; </span><span class="si">{algorithm_variants}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">=</span> <span class="n">algorithm_variant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance_to_x0</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Define simulator that keeps track of budget.</span>
        <span class="k">def</span> <span class="nf">simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">simulate_with_budget</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">epsilon_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">distance_based_decay</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">ess_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">kde_sample_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run SMCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">        Args:</span>
<span class="sd">            x_o: Observed data.</span>
<span class="sd">            num_particles: Number of particles in each population.</span>
<span class="sd">            num_initial_pop: Number of simulations used for initial population.</span>
<span class="sd">            num_simulations: Total number of possible simulations.</span>
<span class="sd">            epsilon_decay: Factor with which the acceptance threshold $\epsilon$ decays.</span>
<span class="sd">            distance_based_decay: Whether the $\epsilon$ decay is constant over</span>
<span class="sd">                populations or calculated from the previous populations distribution of</span>
<span class="sd">                distances.</span>
<span class="sd">            ess_min: Threshold of effective sampling size for resampling weights. Not</span>
<span class="sd">                used when None (default).</span>
<span class="sd">            kernel_variance_scale: Factor for scaling the perturbation kernel variance.</span>
<span class="sd">            use_last_pop_samples: Whether to fill up the current population with</span>
<span class="sd">                samples from the previous population when the budget is used up. If</span>
<span class="sd">                False, the current population is discarded and the previous population</span>
<span class="sd">                is returned.</span>
<span class="sd">            lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">            lra_with_weights: Whether to run lra as weighted linear regression with SMC</span>
<span class="sd">                weights</span>
<span class="sd">            sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">                Fearnhead &amp; Prangle 2012.</span>
<span class="sd">            sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">            sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">                sass regression, default 1 - no expansion.</span>
<span class="sd">            kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">                object from which one can sample.</span>
<span class="sd">            kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">                &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">                heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">                default &#39;cv&#39;.</span>
<span class="sd">                &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">                &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">                more details</span>
<span class="sd">            kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw</span>
<span class="sd">                particles.</span>
<span class="sd">            return_summary: Whether to return a dictionary with all accepted particles,</span>
<span class="sd">                weights, etc. at the end.</span>

<span class="sd">        Returns:</span>
<span class="sd">            theta (if kde False): accepted parameters of the last population.</span>
<span class="sd">            kde (if kde True): KDE object fitted on accepted parameters, from which one</span>
<span class="sd">                can .sample() and .log_prob().</span>
<span class="sd">            summary (if return_summary True): dictionary containing the accepted</span>
<span class="sd">                paramters (if kde True), distances and simulated data x of all</span>
<span class="sd">                populations.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">pop_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="n">num_simulations</span>

        <span class="c1"># Pilot run for SASS.</span>
        <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
            <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Running SASS with </span><span class="si">{</span><span class="n">num_pilot_simulations</span><span class="si">}</span><span class="s2"> pilot samples.&quot;</span>
            <span class="p">)</span>
            <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_sass_set_xo</span><span class="p">(</span>
                <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">lra</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
            <span class="p">)</span>
            <span class="c1"># Udpate simulator and xo</span>
            <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">sass_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">sass_simulator</span>

        <span class="c1"># run initial population</span>
        <span class="n">particles</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
            <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span>
        <span class="p">)</span>
        <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">, ess=</span><span class="si">{</span><span class="mf">1.0</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;num_sims=</span><span class="si">{</span><span class="n">num_initial_pop</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="n">all_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
        <span class="n">all_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
        <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
        <span class="n">all_epsilons</span> <span class="o">=</span> <span class="p">[</span><span class="n">epsilon</span><span class="p">]</span>
        <span class="n">all_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span><span class="p">:</span>

            <span class="n">pop_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># Decay based on quantile of distances from previous pop.</span>
            <span class="k">if</span> <span class="n">distance_based_decay</span><span class="p">:</span>
                <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_epsilon</span><span class="p">(</span>
                    <span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">epsilon_decay</span>
                <span class="p">)</span>
            <span class="c1"># Constant decay.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epsilon</span> <span class="o">*=</span> <span class="n">epsilon_decay</span>

            <span class="c1"># Get kernel variance from previous pop.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_kernel_variance</span><span class="p">(</span>
                <span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span>
                <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="n">kernel_variance_scale</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_next_population</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="n">use_last_pop_samples</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Resample population if effective sampling size is too small.</span>
            <span class="k">if</span> <span class="n">ess_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_if_ess_too_small</span><span class="p">(</span>
                    <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2"> done: eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; num_sims=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># collect results</span>
            <span class="n">all_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">)</span>
            <span class="n">all_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
            <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">all_epsilons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">all_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Maybe run LRA and adjust weights.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
            <span class="n">adjusted_particles</span><span class="p">,</span> <span class="n">adjusted_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra_update_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">observation</span><span class="o">=</span><span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">),</span>
                <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">lra_with_weights</span><span class="o">=</span><span class="n">lra_with_weights</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">final_particles</span> <span class="o">=</span> <span class="n">adjusted_particles</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">final_particles</span> <span class="o">=</span> <span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&quot;&quot;KDE on </span><span class="si">{</span><span class="n">final_particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples with bandwidth option</span>
<span class="s2">                </span><span class="si">{</span><span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;bandwidth&quot;</span> <span class="ow">in</span> <span class="n">kde_kwargs</span> <span class="k">else</span> <span class="s2">&quot;cv&quot;</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">                Beware that KDE can give unreliable results when used with too few</span>
<span class="s2">                samples and in high dimensions.&quot;&quot;&quot;</span>
            <span class="p">)</span>
            <span class="c1"># Maybe get particles weights from last population for weighted KDE.</span>
            <span class="k">if</span> <span class="n">kde_sample_weights</span><span class="p">:</span>
                <span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;sample_weights&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>

            <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_particles</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">(</span>
                    <span class="n">kde_dist</span><span class="p">,</span>
                    <span class="nb">dict</span><span class="p">(</span>
                        <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                        <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                        <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                        <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                        <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">kde_dist</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">final_particles</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">(</span>
                    <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                    <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                    <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                    <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_particles</span>

    <span class="k">def</span> <span class="nf">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Return particles, epsilon and distances of initial population.&quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">num_particles</span> <span class="o">&lt;=</span> <span class="n">num_initial_pop</span>
        <span class="p">),</span> <span class="s2">&quot;number of initial round simulations must be greater than population size&quot;</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_initial_pop</span><span class="p">,))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Infer x shape to test and set x_o.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">sortidx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">particles</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">]</span>
        <span class="c1"># Take last accepted distance as epsilon.</span>
        <span class="n">initial_epsilon</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][</span><span class="n">num_particles</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">initial_epsilon</span><span class="p">):</span>
            <span class="n">initial_epsilon</span> <span class="o">=</span> <span class="mf">1e8</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span>
            <span class="n">initial_epsilon</span><span class="p">,</span>
            <span class="n">distances</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">],</span>
            <span class="n">x</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sample_next_population</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">distances</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Return particles, weights and distances of new population.&quot;&quot;&quot;</span>

        <span class="n">new_particles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_distances</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_x</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">num_accepted_particles</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">while</span> <span class="n">num_accepted_particles</span> <span class="o">&lt;</span> <span class="n">num_particles</span><span class="p">:</span>

            <span class="c1"># Upperbound for batch size to not exceed simulation budget.</span>
            <span class="n">num_batch</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="n">num_particles</span> <span class="o">-</span> <span class="n">num_accepted_particles</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Sample from previous population and perturb.</span>
            <span class="n">particle_candidates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_and_perturb</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_batch</span>
            <span class="p">)</span>
            <span class="c1"># Simulate and select based on distance.</span>
            <span class="n">x_candidates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span><span class="p">(</span><span class="n">particle_candidates</span><span class="p">)</span>
            <span class="n">dists</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x_candidates</span><span class="p">)</span>
            <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">dists</span> <span class="o">&lt;=</span> <span class="n">epsilon</span>
            <span class="n">num_accepted_batch</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">num_accepted_batch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">new_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particle_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">new_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
                        <span class="n">particle_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">],</span>
                        <span class="n">particles</span><span class="p">,</span>
                        <span class="n">log_weights</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">new_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dists</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">new_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">num_accepted_particles</span> <span class="o">+=</span> <span class="n">num_accepted_batch</span>

            <span class="c1"># If simulation budget was exceeded and we still need particles, take</span>
            <span class="c1"># previous population or fill up with previous population.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span>
                <span class="ow">and</span> <span class="n">num_accepted_particles</span> <span class="o">&lt;</span> <span class="n">num_particles</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">use_last_pop_samples</span><span class="p">:</span>
                    <span class="n">num_remaining</span> <span class="o">=</span> <span class="n">num_particles</span> <span class="o">-</span> <span class="n">num_accepted_particles</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Simulation Budget exceeded, filling up with </span><span class="si">{</span><span class="n">num_remaining</span><span class="si">}</span><span class="s2"></span>
<span class="s2">                        samples from last population.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                    <span class="c1"># Some new particles have been accepted already, therefore</span>
                    <span class="c1"># fill up the remaining once with old particles and weights.</span>
                    <span class="n">new_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">,</span> <span class="p">:])</span>
                    <span class="c1"># Recalculate weights with new particles.</span>
                    <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_particles</span><span class="p">),</span>
                            <span class="n">particles</span><span class="p">,</span>
                            <span class="n">log_weights</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                    <span class="n">new_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">])</span>
                    <span class="n">new_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;Simulation Budget exceeded, returning previous population.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">new_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
                    <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
                    <span class="n">new_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
                    <span class="n">new_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

                <span class="k">break</span>

        <span class="c1"># collect lists of tensors into tensors</span>
        <span class="n">new_particles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_particles</span><span class="p">)</span>
        <span class="n">new_log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_log_weights</span><span class="p">)</span>
        <span class="n">new_distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_distances</span><span class="p">)</span>
        <span class="n">new_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>

        <span class="c1"># normalize the new weights</span>
        <span class="n">new_log_weights</span> <span class="o">-=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">new_log_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Return sorted wrt distances.</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">new_distances</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">new_particles</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_log_weights</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_next_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distances</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">quantile</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return epsilon for next round based on quantile of this round&#39;s distances.</span>

<span class="sd">        Note: distances are made unique to avoid repeated distances from simulations</span>
<span class="sd">        that result in the same observation.</span>

<span class="sd">        Args:</span>
<span class="sd">            distances: The distances accepted in this round.</span>
<span class="sd">            quantile: Quantile in the distance distribution to determine new epsilon.</span>

<span class="sd">        Returns:</span>
<span class="sd">            epsilon: Epsilon for the next population.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Take unique distances to skip same distances simulations (return is sorted).</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="c1"># Cumsum as cdf proxy.</span>
        <span class="n">distances_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">distances</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="c1"># Take the q quantile of distances.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">qidx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">distances_cdf</span> <span class="o">&gt;=</span> <span class="n">quantile</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Accepted unique distances=</span><span class="si">{</span><span class="n">distances</span><span class="si">}</span><span class="s2"> don&#39;t match &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;quantile=</span><span class="si">{</span><span class="n">quantile</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">. Selecting last distance.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">qidx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># The new epsilon is given by that distance.</span>
        <span class="k">return</span> <span class="n">distances</span><span class="p">[</span><span class="n">qidx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_calculate_new_log_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">new_particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">old_particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">old_log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return new log weights following formulas in publications A,B anc C.&quot;&quot;&quot;</span>

        <span class="c1"># Prior can be batched across new particles.</span>
        <span class="n">prior_log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">new_particles</span><span class="p">)</span>

        <span class="c1"># Contstruct function to get kernel log prob for given old particle.</span>
        <span class="c1"># The kernel is centered on each old particle as in all three variants (A,B,C).</span>
        <span class="k">def</span> <span class="nf">kernel_log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_new_kernel</span><span class="p">(</span><span class="n">old_particles</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">)</span>

        <span class="c1"># We still have to loop over particles here because</span>
        <span class="c1"># the kernel log probs are already batched across old particles.</span>
        <span class="n">log_weighted_sum</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">old_log_weights</span> <span class="o">+</span> <span class="n">kernel_log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">new_particle</span> <span class="ow">in</span> <span class="n">new_particles</span>
            <span class="p">],</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># new weights are prior probs over weighted sum:</span>
        <span class="k">return</span> <span class="n">prior_log_probs</span> <span class="o">-</span> <span class="n">log_weighted_sum</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">sample_from_population_with_weights</span><span class="p">(</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return samples from particles sampled with weights.&quot;&quot;&quot;</span>

        <span class="c1"># define multinomial with weights as probs</span>
        <span class="n">multi</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
        <span class="c1"># sample num samples, with replacement</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">multi</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,)))</span>
        <span class="c1"># get indices of success trials</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># return those indices from trace</span>
        <span class="k">return</span> <span class="n">particles</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_sample_and_perturb</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sample and perturb batch of new parameters from trace.</span>

<span class="sd">        Reject sampled and perturbed parameters outside of prior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_accepted</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">num_accepted</span> <span class="o">&lt;</span> <span class="n">num_samples</span><span class="p">:</span>
            <span class="n">parms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span> <span class="o">-</span> <span class="n">num_accepted</span>
            <span class="p">)</span>

            <span class="c1"># Create kernel on params and perturb.</span>
            <span class="n">parms_perturbed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_new_kernel</span><span class="p">(</span><span class="n">parms</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

            <span class="n">is_within_prior</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">parms_perturbed</span><span class="p">)</span>
            <span class="n">num_accepted</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">is_within_prior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="k">if</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parms_perturbed</span><span class="p">[</span><span class="n">is_within_prior</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_kernel_variance</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="c1"># For variant C, Beaumont et al. 2009, the kernel variance comes from the</span>
            <span class="c1"># previous population.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">==</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span>
                <span class="c1"># Calculate weighted covariance of particles.</span>
                <span class="n">population_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">aweights</span><span class="o">=</span><span class="n">weights</span><span class="p">)),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># Make sure variance is nonsingular.</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">population_cov</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sd">&quot;&quot;&quot;&quot;Singular particle covariance, using unit covariance.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                    <span class="n">population_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">population_cov</span>
            <span class="c1"># While for Toni et al. and Sisson et al. it comes from the parameter</span>
            <span class="c1"># ranges.</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">):</span>
                <span class="n">particle_ranges</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_particle_ranges</span><span class="p">(</span>
                    <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="n">samples_per_dim</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">particle_ranges</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variant, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="c1"># Variance spans the range of parameters for every dimension.</span>
            <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_particle_ranges</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="n">samples_per_dim</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return new kernel distribution for a given set of paramters.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="k">return</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
                <span class="n">loc</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="n">low</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="n">high</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="c1"># Move batch shape to event shape to get Uniform that is multivariate in</span>
            <span class="c1"># parameter dimension.</span>
            <span class="k">return</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resample_if_ess_too_small</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">ess_min</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">pop_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Return resampled particles and uniform weights if effectice sampling size is</span>
<span class="sd">        too small.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">log_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_particles</span>
        <span class="c1"># Resampling of weights for low ESS only for Sisson et al. 2007.</span>
        <span class="k">if</span> <span class="n">ess</span> <span class="o">&lt;</span> <span class="n">ess_min</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ESS=</span><span class="si">{</span><span class="n">ess</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> too low, resampling pop </span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
            <span class="c1"># First resample, then set to uniform weights as in Sisson et al. 2007.</span>
            <span class="n">particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_particles</span>
            <span class="p">)</span>
            <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span>

    <span class="k">def</span> <span class="nf">run_lra_update_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">observation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Return particles and weights adjusted with LRA.</span>

<span class="sd">        Runs (weighted) linear regression from xs onto particles to adjust the</span>
<span class="sd">        particles.</span>

<span class="sd">        Updates the SMC weights according to the new particles.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">adjusted_particels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">xs</span><span class="p">,</span>
            <span class="n">observation</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">log_weights</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">if</span> <span class="n">lra_with_weights</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Update SMC weights with LRA adjusted weights</span>
        <span class="n">adjusted_log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
            <span class="n">new_particles</span><span class="o">=</span><span class="n">adjusted_particels</span><span class="p">,</span>
            <span class="n">old_particles</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
            <span class="n">old_log_weights</span><span class="o">=</span><span class="n">log_weights</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">adjusted_particels</span><span class="p">,</span> <span class="n">adjusted_log_weights</span>

    <span class="k">def</span> <span class="nf">run_sass_set_xo</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_pilot_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return transform for semi-automatic summary statistics.</span>

<span class="sd">        Runs an single round of rejection abc with fixed budget and accepts</span>
<span class="sd">        num_particles simulations to run the regression for sass.</span>

<span class="sd">        Sets self.x_o once the x_shape can be derived from simulations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="p">(</span><span class="n">pilot_particles</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">pilot_xs</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
            <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span>
        <span class="p">)</span>
        <span class="c1"># Adjust with LRA.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="n">pilot_particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">pilot_particles</span><span class="p">,</span> <span class="n">pilot_xs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
            <span class="n">pilot_particles</span><span class="p">,</span>
            <span class="n">pilot_xs</span><span class="p">,</span>
            <span class="n">expansion_degree</span><span class="o">=</span><span class="n">sass_expansion_degree</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sass_transform</span>

    <span class="k">def</span> <span class="nf">get_particle_ranges</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return range of particles in each parameter dimension.&quot;&quot;&quot;</span>

        <span class="c1"># get weighted samples</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span>
            <span class="n">weights</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">samples_per_dim</span> <span class="o">*</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1"># Variance spans the range of particles for every dimension.</span>
        <span class="n">particle_ranges</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
        <span class="k">assert</span> <span class="n">particle_ranges</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">particle_ranges</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.__call__">
<code class="codehilite language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">epsilon_decay</span><span class="p">,</span> <span class="n">distance_based_decay</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ess_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">kde_sample_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lra_with_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_fraction</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.__call__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Run SMCABC and return accepted parameters or KDE object fitted on them.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x_o</code></td>
        <td><code>Union[torch.Tensor, numpy.ndarray]</code></td>
        <td><p>Observed data.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_particles</code></td>
        <td><code>int</code></td>
        <td><p>Number of particles in each population.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_initial_pop</code></td>
        <td><code>int</code></td>
        <td><p>Number of simulations used for initial population.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_simulations</code></td>
        <td><code>int</code></td>
        <td><p>Total number of possible simulations.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>epsilon_decay</code></td>
        <td><code>float</code></td>
        <td><p>Factor with which the acceptance threshold <span class="arithmatex">\(\epsilon\)</span> decays.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>distance_based_decay</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the <span class="arithmatex">\(\epsilon\)</span> decay is constant over
populations or calculated from the previous populations distribution of
distances.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>ess_min</code></td>
        <td><code>Optional[float]</code></td>
        <td><p>Threshold of effective sampling size for resampling weights. Not
used when None (default).</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>kernel_variance_scale</code></td>
        <td><code>float</code></td>
        <td><p>Factor for scaling the perturbation kernel variance.</p></td>
        <td><code>1.0</code></td>
      </tr>
      <tr>
        <td><code>use_last_pop_samples</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to fill up the current population with
samples from the previous population when the budget is used up. If
False, the current population is discarded and the previous population
is returned.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>lra</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to run linear regression adjustment as in Beaumont et al. 2002</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>lra_with_weights</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to run lra as weighted linear regression with SMC
weights</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sass</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to determine semi-automatic summary statistics as in
Fearnhead &amp; Prangle 2012.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sass_fraction</code></td>
        <td><code>float</code></td>
        <td><p>Fraction of simulation budget used for the initial sass run.</p></td>
        <td><code>0.25</code></td>
      </tr>
      <tr>
        <td><code>sass_expansion_degree</code></td>
        <td><code>int</code></td>
        <td><p>Degree of the polynomial feature expansion for the
sass regression, default 1 - no expansion.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>kde</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to run KDE on the accepted parameters to return a KDE
object from which one can sample.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>kde_kwargs</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>kwargs for performing KDE:
&lsquo;bandwidth=&rsquo;; either a float, or a string naming a bandwidth
heuristics, e.g., &lsquo;cv&rsquo; (cross validation), &lsquo;silvermann&rsquo; or &lsquo;scott&rsquo;,
default &lsquo;cv&rsquo;.
&lsquo;transform&rsquo;: transform applied to the parameters before doing KDE.
&lsquo;sample_weights&rsquo;: weights associated with samples. See &lsquo;get_kde&rsquo; for
more details</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>kde_sample_weights</code></td>
        <td><code>bool</code></td>
        <td><p>Whether perform weighted KDE with SMC weights or on raw
particles.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>return_summary</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to return a dictionary with all accepted particles,
weights, etc. at the end.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>theta (if kde False)</code></td>
      <td><p>accepted parameters of the last population.
kde (if kde True): KDE object fitted on accepted parameters, from which one
    can .sample() and .log_prob().
summary (if return_summary True): dictionary containing the accepted
    paramters (if kde True), distances and simulated data x of all
    populations.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
    <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">epsilon_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">distance_based_decay</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">ess_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">kde_sample_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run SMCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_o: Observed data.</span>
<span class="sd">        num_particles: Number of particles in each population.</span>
<span class="sd">        num_initial_pop: Number of simulations used for initial population.</span>
<span class="sd">        num_simulations: Total number of possible simulations.</span>
<span class="sd">        epsilon_decay: Factor with which the acceptance threshold $\epsilon$ decays.</span>
<span class="sd">        distance_based_decay: Whether the $\epsilon$ decay is constant over</span>
<span class="sd">            populations or calculated from the previous populations distribution of</span>
<span class="sd">            distances.</span>
<span class="sd">        ess_min: Threshold of effective sampling size for resampling weights. Not</span>
<span class="sd">            used when None (default).</span>
<span class="sd">        kernel_variance_scale: Factor for scaling the perturbation kernel variance.</span>
<span class="sd">        use_last_pop_samples: Whether to fill up the current population with</span>
<span class="sd">            samples from the previous population when the budget is used up. If</span>
<span class="sd">            False, the current population is discarded and the previous population</span>
<span class="sd">            is returned.</span>
<span class="sd">        lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">        lra_with_weights: Whether to run lra as weighted linear regression with SMC</span>
<span class="sd">            weights</span>
<span class="sd">        sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">            Fearnhead &amp; Prangle 2012.</span>
<span class="sd">        sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">        sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">            sass regression, default 1 - no expansion.</span>
<span class="sd">        kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">            object from which one can sample.</span>
<span class="sd">        kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">            &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">            heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">            default &#39;cv&#39;.</span>
<span class="sd">            &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">            &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">            more details</span>
<span class="sd">        kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw</span>
<span class="sd">            particles.</span>
<span class="sd">        return_summary: Whether to return a dictionary with all accepted particles,</span>
<span class="sd">            weights, etc. at the end.</span>

<span class="sd">    Returns:</span>
<span class="sd">        theta (if kde False): accepted parameters of the last population.</span>
<span class="sd">        kde (if kde True): KDE object fitted on accepted parameters, from which one</span>
<span class="sd">            can .sample() and .log_prob().</span>
<span class="sd">        summary (if return_summary True): dictionary containing the accepted</span>
<span class="sd">            paramters (if kde True), distances and simulated data x of all</span>
<span class="sd">            populations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">pop_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="n">num_simulations</span>

    <span class="c1"># Pilot run for SASS.</span>
    <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
        <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Running SASS with </span><span class="si">{</span><span class="n">num_pilot_simulations</span><span class="si">}</span><span class="s2"> pilot samples.&quot;</span>
        <span class="p">)</span>
        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_sass_set_xo</span><span class="p">(</span>
            <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">lra</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
        <span class="p">)</span>
        <span class="c1"># Udpate simulator and xo</span>
        <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">sass_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">sass_simulator</span>

    <span class="c1"># run initial population</span>
    <span class="n">particles</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span>
    <span class="p">)</span>
    <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">, ess=</span><span class="si">{</span><span class="mf">1.0</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;num_sims=</span><span class="si">{</span><span class="n">num_initial_pop</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="n">all_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
    <span class="n">all_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
    <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
    <span class="n">all_epsilons</span> <span class="o">=</span> <span class="p">[</span><span class="n">epsilon</span><span class="p">]</span>
    <span class="n">all_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span><span class="p">:</span>

        <span class="n">pop_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Decay based on quantile of distances from previous pop.</span>
        <span class="k">if</span> <span class="n">distance_based_decay</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_epsilon</span><span class="p">(</span>
                <span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">epsilon_decay</span>
            <span class="p">)</span>
        <span class="c1"># Constant decay.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">*=</span> <span class="n">epsilon_decay</span>

        <span class="c1"># Get kernel variance from previous pop.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_kernel_variance</span><span class="p">(</span>
            <span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="n">kernel_variance_scale</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_next_population</span><span class="p">(</span>
            <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="n">use_last_pop_samples</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Resample population if effective sampling size is too small.</span>
        <span class="k">if</span> <span class="n">ess_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_if_ess_too_small</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;population=</span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2"> done: eps=</span><span class="si">{</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">,&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; num_sims=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># collect results</span>
        <span class="n">all_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">)</span>
        <span class="n">all_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
        <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">all_epsilons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="n">all_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Maybe run LRA and adjust weights.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
        <span class="n">adjusted_particles</span><span class="p">,</span> <span class="n">adjusted_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra_update_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">observation</span><span class="o">=</span><span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">),</span>
            <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">lra_with_weights</span><span class="o">=</span><span class="n">lra_with_weights</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">final_particles</span> <span class="o">=</span> <span class="n">adjusted_particles</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">final_particles</span> <span class="o">=</span> <span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&quot;&quot;KDE on </span><span class="si">{</span><span class="n">final_particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples with bandwidth option</span>
<span class="s2">            </span><span class="si">{</span><span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;bandwidth&quot;</span> <span class="ow">in</span> <span class="n">kde_kwargs</span> <span class="k">else</span> <span class="s2">&quot;cv&quot;</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">            Beware that KDE can give unreliable results when used with too few</span>
<span class="s2">            samples and in high dimensions.&quot;&quot;&quot;</span>
        <span class="p">)</span>
        <span class="c1"># Maybe get particles weights from last population for weighted KDE.</span>
        <span class="k">if</span> <span class="n">kde_sample_weights</span><span class="p">:</span>
            <span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;sample_weights&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>

        <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_particles</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">kde_dist</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">(</span>
                    <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                    <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                    <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                    <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kde_dist</span>

    <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">final_particles</span><span class="p">,</span>
            <span class="nb">dict</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_particles</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">algorithm_variant</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Sequential Monte Carlo Approximate Bayesian Computation.</p>
<p>We distinguish between three different SMC methods here:
    - A: Toni et al. 2010 (Phd Thesis)
    - B: Sisson et al. 2007 (with correction from 2009)
    - C: Beaumont et al. 2009</p>
<p>In Toni et al. 2010 we find an overview of the differences on page 34:
    - B: same as A except for resampling of weights if the effective sampling
        size is too small.
    - C: same as A except for calculation of the covariance of the perturbation
        kernel: the kernel covariance is a scaled version of the covariance of
        the previous population.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>simulator</code></td>
        <td><code>Callable</code></td>
        <td><p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\mathrm{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Distribution</code></td>
        <td><p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>distance</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>Distance function to compare observed and simulated data. Can be
a custom function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>.</p></td>
        <td><code>&#39;l2&#39;</code></td>
      </tr>
      <tr>
        <td><code>num_workers</code></td>
        <td><code>int</code></td>
        <td><p>Number of parallel workers to use for simulations.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>simulation_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progressbar during simulation and
sampling.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>kernel</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Perturbation kernel.</p></td>
        <td><code>&#39;gaussian&#39;</code></td>
      </tr>
      <tr>
        <td><code>algorithm_variant</code></td>
        <td><code>str</code></td>
        <td><p>Indicating the choice of algorithm variant, A, B, or C.</p></td>
        <td><code>&#39;C&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="n">algorithm_variant</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Monte Carlo Approximate Bayesian Computation.</span>

<span class="sd">    We distinguish between three different SMC methods here:</span>
<span class="sd">        - A: Toni et al. 2010 (Phd Thesis)</span>
<span class="sd">        - B: Sisson et al. 2007 (with correction from 2009)</span>
<span class="sd">        - C: Beaumont et al. 2009</span>

<span class="sd">    In Toni et al. 2010 we find an overview of the differences on page 34:</span>
<span class="sd">        - B: same as A except for resampling of weights if the effective sampling</span>
<span class="sd">            size is too small.</span>
<span class="sd">        - C: same as A except for calculation of the covariance of the perturbation</span>
<span class="sd">            kernel: the kernel covariance is a scaled version of the covariance of</span>
<span class="sd">            the previous population.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">            a custom function or one of `l1`, `l2`, `mse`.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        kernel: Perturbation kernel.</span>
<span class="sd">        algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">kernels</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Kernel &#39;</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported. Choose one from </span><span class="si">{</span><span class="n">kernels</span><span class="si">}</span><span class="s2">.&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

    <span class="n">algorithm_variants</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="n">algorithm_variants</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;SMCABC variant &#39;</span><span class="si">{</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported, choose one from&quot;</span>
        <span class="s2">&quot; </span><span class="si">{algorithm_variants}</span><span class="s2">.&quot;</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">=</span> <span class="n">algorithm_variant</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">distance_to_x0</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Define simulator that keeps track of budget.</span>
    <span class="k">def</span> <span class="nf">simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">simulate_with_budget</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.choose_distance_function">
<code class="codehilite language-python"><span class="n">choose_distance_function</span><span class="p">(</span><span class="n">distance_type</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.choose_distance_function" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return distance function for given distance type.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">choose_distance_function</span><span class="p">(</span><span class="n">distance_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return distance function for given distance type.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;l2&quot;</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">((</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;l1&quot;</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xo</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">xo</span> <span class="o">-</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Distance </span><span class="si">{distance_type}</span><span class="s2"> not supported.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">distance_fun</span><span class="p">(</span><span class="n">observed_data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">simulated_data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return distance over batch dimension.</span>

<span class="sd">        Args:</span>
<span class="sd">            observed_data: Observed data, could be 1D.</span>
<span class="sd">            simulated_data: Batch of simulated data, has batch dimension.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Torch tensor with batch of distances.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">simulated_data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;simulated data needs batch dimension&quot;</span>

        <span class="k">return</span> <span class="n">distance</span><span class="p">(</span><span class="n">observed_data</span><span class="p">,</span> <span class="n">simulated_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">distance_fun</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.get_new_kernel">
<code class="codehilite language-python"><span class="n">get_new_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.get_new_kernel" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return new kernel distribution for a given set of paramters.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return new kernel distribution for a given set of paramters.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
        <span class="n">low</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="c1"># Move batch shape to event shape to get Uniform that is multivariate in</span>
        <span class="c1"># parameter dimension.</span>
        <span class="k">return</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.get_particle_ranges">
<code class="codehilite language-python"><span class="n">get_particle_ranges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.get_particle_ranges" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return range of particles in each parameter dimension.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_particle_ranges</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return range of particles in each parameter dimension.&quot;&quot;&quot;</span>

    <span class="c1"># get weighted samples</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
        <span class="n">particles</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">samples_per_dim</span> <span class="o">*</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="c1"># Variance spans the range of particles for every dimension.</span>
    <span class="n">particle_ranges</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="k">assert</span> <span class="n">particle_ranges</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">particle_ranges</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.get_sass_transform">
<code class="codehilite language-python"><span class="n">get_sass_transform</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.get_sass_transform" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return semi-automatic summary statitics function.</p>
<p>Running weighted linear regressin as in
Fearnhead &amp; Prandle 2012: <a href="https://arxiv.org/abs/1004.1112">https://arxiv.org/abs/1004.1112</a></p>
<p>Following implementation in
<a href="https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity">https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity</a>
and
<a href="https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic">https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic</a></p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_sass_transform</span><span class="p">(</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return semi-automatic summary statitics function.</span>

<span class="sd">    Running weighted linear regressin as in</span>
<span class="sd">    Fearnhead &amp; Prandle 2012: https://arxiv.org/abs/1004.1112</span>

<span class="sd">    Following implementation in</span>
<span class="sd">    https://abcpy.readthedocs.io/en/latest/_modules/abcpy/statistics.html#Identity</span>
<span class="sd">    and</span>
<span class="sd">    https://pythonhosted.org/abcpy/_modules/abcpy/summaryselections.html#Semiautomatic</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">expansion_degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Transform x, remove intercept.</span>
    <span class="n">x_expanded</span> <span class="o">=</span> <span class="n">expansion</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sumstats_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_expanded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">parameter_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">regression_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">x_expanded</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">theta</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
        <span class="p">)</span>
        <span class="n">sumstats_map</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">coef_</span>

    <span class="n">sumstats_map</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sumstats_map</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sumstats_transform</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">x_expanded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">expansion</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_expanded</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">sumstats_map</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sumstats_transform</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small">
<code class="codehilite language-python"><span class="n">resample_if_ess_too_small</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return resampled particles and uniform weights if effectice sampling size is
too small.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">resample_if_ess_too_small</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">ess_min</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">pop_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Return resampled particles and uniform weights if effectice sampling size is</span>
<span class="sd">    too small.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">log_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_particles</span>
    <span class="c1"># Resampling of weights for low ESS only for Sisson et al. 2007.</span>
    <span class="k">if</span> <span class="n">ess</span> <span class="o">&lt;</span> <span class="n">ess_min</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ESS=</span><span class="si">{</span><span class="n">ess</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> too low, resampling pop </span><span class="si">{</span><span class="n">pop_idx</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="c1"># First resample, then set to uniform weights as in Sisson et al. 2007.</span>
        <span class="n">particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_particles</span>
        <span class="p">)</span>
        <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.run_lra">
<code class="codehilite language-python"><span class="n">run_lra</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">observation</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.run_lra" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return parameters adjusted with linear regression adjustment.</p>
<p>Implementation as in Beaumont et al. 2002: <a href="https://arxiv.org/abs/1707.01254">https://arxiv.org/abs/1707.01254</a></p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">run_lra</span><span class="p">(</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">observation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return parameters adjusted with linear regression adjustment.</span>

<span class="sd">    Implementation as in Beaumont et al. 2002: https://arxiv.org/abs/1707.01254</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta_adjusted</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="k">for</span> <span class="n">parameter_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">regression_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">theta</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">],</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">theta_adjusted</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">observation</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">theta_adjusted</span><span class="p">[:,</span> <span class="n">parameter_idx</span><span class="p">]</span> <span class="o">-=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">theta_adjusted</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.run_lra_update_weights">
<code class="codehilite language-python"><span class="n">run_lra_update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">observation</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">lra_with_weights</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.run_lra_update_weights" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return particles and weights adjusted with LRA.</p>
<p>Runs (weighted) linear regression from xs onto particles to adjust the
particles.</p>
<p>Updates the SMC weights according to the new particles.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">run_lra_update_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">observation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Return particles and weights adjusted with LRA.</span>

<span class="sd">    Runs (weighted) linear regression from xs onto particles to adjust the</span>
<span class="sd">    particles.</span>

<span class="sd">    Updates the SMC weights according to the new particles.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">adjusted_particels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="n">xs</span><span class="p">,</span>
        <span class="n">observation</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">log_weights</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">if</span> <span class="n">lra_with_weights</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Update SMC weights with LRA adjusted weights</span>
    <span class="n">adjusted_log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
        <span class="n">new_particles</span><span class="o">=</span><span class="n">adjusted_particels</span><span class="p">,</span>
        <span class="n">old_particles</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
        <span class="n">old_log_weights</span><span class="o">=</span><span class="n">log_weights</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">adjusted_particels</span><span class="p">,</span> <span class="n">adjusted_log_weights</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.run_sass_set_xo">
<code class="codehilite language-python"><span class="n">run_sass_set_xo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.run_sass_set_xo" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return transform for semi-automatic summary statistics.</p>
<p>Runs an single round of rejection abc with fixed budget and accepts
num_particles simulations to run the regression for sass.</p>
<p>Sets self.x_o once the x_shape can be derived from simulations.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">run_sass_set_xo</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_pilot_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return transform for semi-automatic summary statistics.</span>

<span class="sd">    Runs an single round of rejection abc with fixed budget and accepts</span>
<span class="sd">    num_particles simulations to run the regression for sass.</span>

<span class="sd">    Sets self.x_o once the x_shape can be derived from simulations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="p">(</span><span class="n">pilot_particles</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">pilot_xs</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span>
    <span class="p">)</span>
    <span class="c1"># Adjust with LRA.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="n">pilot_particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">pilot_particles</span><span class="p">,</span> <span class="n">pilot_xs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
    <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
        <span class="n">pilot_particles</span><span class="p">,</span>
        <span class="n">pilot_xs</span><span class="p">,</span>
        <span class="n">expansion_degree</span><span class="o">=</span><span class="n">sass_expansion_degree</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">sass_transform</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights">
<code class="codehilite language-python"><span class="n">sample_from_population_with_weights</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return samples from particles sampled with weights.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">sample_from_population_with_weights</span><span class="p">(</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Return samples from particles sampled with weights.&quot;&quot;&quot;</span>

    <span class="c1"># define multinomial with weights as probs</span>
    <span class="n">multi</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="c1"># sample num samples, with replacement</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">multi</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,)))</span>
    <span class="c1"># get indices of success trials</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># return those indices from trace</span>
    <span class="k">return</span> <span class="n">particles</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>

<h2 id="posteriors">Posteriors<a class="headerlink" href="#posteriors" title="Permanent link">&para;</a></h2>


  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior">
        <code>
sbi.inference.posteriors.direct_posterior.DirectPosterior            (<span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span>)
        </code>



<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Posterior <span class="arithmatex">\(p(\theta|x_o)\)</span> with <code>log_prob()</code> and <code>sample()</code> methods, only
applicable to SNPE.<br/><br/>
SNPE trains a neural network to directly approximate the posterior distribution.
However, for bounded priors, the neural network can have leakage: it puts non-zero
mass in regions where the prior is zero. The <code>DirectPosterior</code> class wraps the
trained network to deal with these cases.<br/><br/>
Specifically, this class offers the following functionality:<br/>
- correct the calculation of the log probability such that it compensates for the
  leakage.<br/>
- reject samples that lie outside of the prior bounds.<br/><br/>
This class can not be used in combination with SNLE or SNRE.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">DirectPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Posterior $p(\theta|x_o)$ with `log_prob()` and `sample()` methods, only</span>
<span class="sd">    applicable to SNPE.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNPE trains a neural network to directly approximate the posterior distribution.</span>
<span class="sd">    However, for bounded priors, the neural network can have leakage: it puts non-zero</span>
<span class="sd">    mass in regions where the prior is zero. The `DirectPosterior` class wraps the</span>
<span class="sd">    trained network to deal with these cases.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    Specifically, this class offers the following functionality:&lt;br/&gt;</span>
<span class="sd">    - correct the calculation of the log probability such that it compensates for the</span>
<span class="sd">      leakage.&lt;br/&gt;</span>
<span class="sd">    - reject samples that lie outside of the prior bounds.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    This class can not be used in combination with SNLE or SNRE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">posterior_estimator</span><span class="p">:</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">            posterior_estimator: The trained neural posterior.</span>
<span class="sd">            max_sampling_batch_size: Batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Because `DirectPosterior` does not take the `potential_fn` as input, it</span>
        <span class="c1"># builds it itself. The `potential_fn` and `theta_transform` are used only for</span>
        <span class="c1"># obtaining the MAP.</span>
        <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
        <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">posterior_estimator_based_potential</span><span class="p">(</span>
            <span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">posterior_estimator</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;It samples the posterior network and rejects samples that</span>
<span class="s2">            lie outside of the prior bounds.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">rejection_sample_posterior_within_prior</span><span class="p">(</span>
            <span class="n">posterior_nn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">samples</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">leakage_correction_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of the posterior $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">                Renormalization of the posterior is useful when some</span>
<span class="sd">                probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">                The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">                need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">                `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">                - outside of the prior support regardless of this setting.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>
<span class="sd">            leakage_correction_params: A `dict` of keyword arguments to override the</span>
<span class="sd">                default values of `leakage_correction()`. Possible options are:</span>
<span class="sd">                `num_rejection_samples`, `force_update`, `show_progress_bars`, and</span>
<span class="sd">                `rejection_sampling_batch_size`.</span>
<span class="sd">                These parameters only have an effect if `norm_posterior=True`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `(len(),)`-shaped log posterior probability $\log p(\theta|x)$ for  in the</span>
<span class="sd">            support of the prior, - (corresponding to 0 probability) outside.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># TODO Train exited here, entered after sampling?</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="n">theta_repeated</span><span class="p">,</span> <span class="n">x_repeated</span> <span class="o">=</span> <span class="n">match_theta_and_x_batch_shapes</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>

            <span class="c1"># Evaluate on device, move back to cpu for comparison with prior.</span>
            <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
                <span class="n">theta_repeated</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">x_repeated</span>
            <span class="p">)</span>

            <span class="c1"># Force probability to be zero outside prior support.</span>
            <span class="n">in_prior_support</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta_repeated</span><span class="p">)</span>

            <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">in_prior_support</span><span class="p">,</span>
                <span class="n">unnorm_log_prob</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">leakage_correction_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">leakage_correction_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
            <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">leakage_correction_params</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">norm_posterior</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">leakage_correction</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">num_rejection_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">rejection_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return leakage correction factor for a leaky posterior density estimate.</span>

<span class="sd">        The factor is estimated from the acceptance probability during rejection</span>
<span class="sd">        sampling from the posterior.</span>

<span class="sd">        This is to avoid re-estimating the acceptance probability from scratch</span>
<span class="sd">        whenever `log_prob` is called and `norm_posterior=True`. Here, it</span>
<span class="sd">        is estimated only once for `self.default_x` and saved for later. We</span>
<span class="sd">        re-evaluate only whenever a new `x` is passed.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            num_rejection_samples: Number of samples used to estimate correction factor.</span>
<span class="sd">            show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">            rejection_sampling_batch_size: Batch size for rejection sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Saved or newly-estimated correction factor (as a scalar `Tensor`).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>

            <span class="k">return</span> <span class="n">rejection_sample_posterior_within_prior</span><span class="p">(</span>
                <span class="n">posterior_nn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
                <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_rejection_samples</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="n">sample_for_correction_factor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">rejection_sampling_batch_size</span><span class="p">,</span>
            <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
        <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
            <span class="k">return</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;posterior&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether or not to show a progressbar for sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">







  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.default_x">
<code class="codehilite language-python"><span class="n">default_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.default_x" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return default x used by <code>.sample(), .log_prob</code> as conditioning context.</p>
    </div>

  </div>







  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">


<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prior</code></td>
        <td><code>Distribution</code></td>
        <td><p>Prior distribution with <code>.log_prob()</code> and <code>.sample()</code>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>posterior_estimator</code></td>
        <td><code>Flow</code></td>
        <td><p>The trained neural posterior.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>max_sampling_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Batchsize of samples being drawn from
the proposal at every iteration.</p></td>
        <td><code>10000</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>x_shape</code></td>
        <td><code>Optional[torch.Size]</code></td>
        <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">posterior_estimator</span><span class="p">:</span> <span class="n">flows</span><span class="o">.</span><span class="n">Flow</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">        posterior_estimator: The trained neural posterior.</span>
<span class="sd">        max_sampling_batch_size: Batchsize of samples being drawn from</span>
<span class="sd">            the proposal at every iteration.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Because `DirectPosterior` does not take the `potential_fn` as input, it</span>
    <span class="c1"># builds it itself. The `potential_fn` and `theta_transform` are used only for</span>
    <span class="c1"># obtaining the MAP.</span>
    <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
    <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">posterior_estimator_based_potential</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="kc">None</span>
    <span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">posterior_estimator</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;It samples the posterior network and rejects samples that</span>
<span class="s2">        lie outside of the prior bounds.&quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>






  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction">
<code class="codehilite language-python"><span class="n">leakage_correction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_rejection_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rejection_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return leakage correction factor for a leaky posterior density estimate.</p>
<p>The factor is estimated from the acceptance probability during rejection
sampling from the posterior.</p>
<p>This is to avoid re-estimating the acceptance probability from scratch
whenever <code>log_prob</code> is called and <code>norm_posterior=True</code>. Here, it
is estimated only once for <code>self.default_x</code> and saved for later. We
re-evaluate only whenever a new <code>x</code> is passed.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_rejection_samples</code></td>
        <td><code>int</code></td>
        <td><p>Number of samples used to estimate correction factor.</p></td>
        <td><code>10000</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show a progress bar during sampling.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>rejection_sampling_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Batch size for rejection sampling.</p></td>
        <td><code>10000</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>Saved or newly-estimated correction factor (as a scalar <code>Tensor</code>).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">leakage_correction</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_rejection_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">rejection_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return leakage correction factor for a leaky posterior density estimate.</span>

<span class="sd">    The factor is estimated from the acceptance probability during rejection</span>
<span class="sd">    sampling from the posterior.</span>

<span class="sd">    This is to avoid re-estimating the acceptance probability from scratch</span>
<span class="sd">    whenever `log_prob` is called and `norm_posterior=True`. Here, it</span>
<span class="sd">    is estimated only once for `self.default_x` and saved for later. We</span>
<span class="sd">    re-evaluate only whenever a new `x` is passed.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        num_rejection_samples: Number of samples used to estimate correction factor.</span>
<span class="sd">        show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">        rejection_sampling_batch_size: Batch size for rejection sampling.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Saved or newly-estimated correction factor (as a scalar `Tensor`).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>

        <span class="k">return</span> <span class="n">rejection_sample_posterior_within_prior</span><span class="p">(</span>
            <span class="n">posterior_nn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_rejection_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">sample_for_correction_factor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">rejection_sampling_batch_size</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
    <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
        <span class="k">return</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">leakage_correction_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the log-probability of the posterior <span class="arithmatex">\(p(\theta|x)\)</span>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>norm_posterior</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to enforce a normalized posterior density.
Renormalization of the posterior is useful when some
probability falls out or leaks out of the prescribed prior support.
The normalizing factor is calculated via rejection sampling, so if you
need speedier but unnormalized log posterior estimates set here
<code>norm_posterior=False</code>. The returned log posterior is set to
- outside of the prior support regardless of this setting.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>track_gradients</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>leakage_correction_params</code></td>
        <td><code>Optional[dict]</code></td>
        <td><p>A <code>dict</code> of keyword arguments to override the
default values of <code>leakage_correction()</code>. Possible options are:
<code>num_rejection_samples</code>, <code>force_update</code>, <code>show_progress_bars</code>, and
<code>rejection_sampling_batch_size</code>.
These parameters only have an effect if <code>norm_posterior=True</code>.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p><code>(len(),)</code>-shaped log posterior probability <span class="arithmatex">\(\log p(\theta|x)\)</span> for  in the
support of the prior, - (corresponding to 0 probability) outside.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">leakage_correction_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of the posterior $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">            Renormalization of the posterior is useful when some</span>
<span class="sd">            probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">            The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">            need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">            `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">            - outside of the prior support regardless of this setting.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">        leakage_correction_params: A `dict` of keyword arguments to override the</span>
<span class="sd">            default values of `leakage_correction()`. Possible options are:</span>
<span class="sd">            `num_rejection_samples`, `force_update`, `show_progress_bars`, and</span>
<span class="sd">            `rejection_sampling_batch_size`.</span>
<span class="sd">            These parameters only have an effect if `norm_posterior=True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(),)`-shaped log posterior probability $\log p(\theta|x)$ for  in the</span>
<span class="sd">        support of the prior, - (corresponding to 0 probability) outside.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># TODO Train exited here, entered after sampling?</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="n">theta_repeated</span><span class="p">,</span> <span class="n">x_repeated</span> <span class="o">=</span> <span class="n">match_theta_and_x_batch_shapes</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>

        <span class="c1"># Evaluate on device, move back to cpu for comparison with prior.</span>
        <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="n">theta_repeated</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">x_repeated</span>
        <span class="p">)</span>

        <span class="c1"># Force probability to be zero outside prior support.</span>
        <span class="n">in_prior_support</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta_repeated</span><span class="p">)</span>

        <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">in_prior_support</span><span class="p">,</span>
            <span class="n">unnorm_log_prob</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">leakage_correction_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">leakage_correction_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
        <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">leakage_correction_params</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">norm_posterior</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.map">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.map" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_iter</code></td>
        <td><code>int</code></td>
        <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate of the optimizer.</p></td>
        <td><code>0.01</code></td>
      </tr>
      <tr>
        <td><code>init_method</code></td>
        <td><code>Union[str, torch.Tensor]</code></td>
        <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
        <td><code>&#39;posterior&#39;</code></td>
      </tr>
      <tr>
        <td><code>num_init_samples</code></td>
        <td><code>int</code></td>
        <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>num_to_optimize</code></td>
        <td><code>int</code></td>
        <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>save_best_every</code></td>
        <td><code>int</code></td>
        <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether or not to show a progressbar for sampling from
the posterior.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>force_update</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>log_prob_kwargs</code></td>
        <td></td>
        <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>The MAP estimate.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;posterior&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether or not to show a progressbar for sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.potential">
<code class="codehilite language-python"><span class="n">potential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.potential" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Evaluates <span class="arithmatex">\(\theta\)</span> under the potential that is used to sample the posterior.</p>
<p>The potential is the unnormalized log-probability of <span class="arithmatex">\(\theta\)</span> under the
posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>track_gradients</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">potential</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Evaluates $\theta$ under the potential that is used to sample the posterior.</span>

<span class="sd">    The potential is the unnormalized log-probability of $\theta$ under the</span>
<span class="sd">    posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.sample">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.sample" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return samples from posterior distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>sample_shape</code></td>
        <td><code>Union[torch.Size, Tuple[int, ...]]</code></td>
        <td><p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p></td>
        <td><code>torch.Size([])</code></td>
      </tr>
      <tr>
        <td><code>sample_with</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show sampling progress monitor.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
        <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="n">rejection_sample_posterior_within_prior</span><span class="p">(</span>
        <span class="n">posterior_nn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.direct_posterior.DirectPosterior.set_default_x">
<code class="codehilite language-python"><span class="n">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.set_default_x" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Set new default x for <code>.sample(), .log_prob</code> to use as conditioning context.</p>
<p>Reset the MAP stored for the old default x if applicable.</p>
<p>This is a pure convenience to avoid having to repeatedly specify <code>x</code> in calls to
<code>.sample()</code> and <code>.log_prob()</code> - only $  heta$ needs to be passed.</p>
<p>This convenience is particularly useful when the posterior is focused, i.e.
has been trained over multiple rounds to be accurate in the vicinity of a
particular <code>x=x_o</code> (you can check if your posterior object is focused by
printing it).</p>
<p>NOTE: this method is chainable, i.e. will return the NeuralPosterior object so
that calls like <code>posterior.set_default_x(my_x).sample(mytheta)</code> are possible.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>The default observation to set for the posterior <span class="arithmatex">\(p(     heta|x)\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>NeuralPosterior</code></td>
      <td><p><code>NeuralPosterior</code> that will use a default <code>x</code> when not explicitly passed.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Set new default x for `.sample(), .log_prob` to use as conditioning context.</span>

<span class="sd">    Reset the MAP stored for the old default x if applicable.</span>

<span class="sd">    This is a pure convenience to avoid having to repeatedly specify `x` in calls to</span>
<span class="sd">    `.sample()` and `.log_prob()` - only $\theta$ needs to be passed.</span>

<span class="sd">    This convenience is particularly useful when the posterior is focused, i.e.</span>
<span class="sd">    has been trained over multiple rounds to be accurate in the vicinity of a</span>
<span class="sd">    particular `x=x_o` (you can check if your posterior object is focused by</span>
<span class="sd">    printing it).</span>

<span class="sd">    NOTE: this method is chainable, i.e. will return the NeuralPosterior object so</span>
<span class="sd">    that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The default observation to set for the posterior $p(\theta|x)$.</span>
<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` that will use a default `x` when not explicitly passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span> <span class="n">allow_iid_x</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">allow_iid_x</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_map</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior">
        <code>
sbi.inference.posteriors.mcmc_posterior.MCMCPosterior            (<span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span>)
        </code>



<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Provides MCMC to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>MCMCPosterior</code> allows to sample from the posterior with MCMC.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MCMCPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides MCMC to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `MCMCPosterior` allows to sample from the posterior with MCMC.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sir&quot;</span><span class="p">,</span>
        <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples.</span>
<span class="sd">            proposal: Proposal distribution that is used to initialize the MCMC chain.</span>
<span class="sd">            theta_transform: Transformation that will be applied during sampling.</span>
<span class="sd">                Allows to perform MCMC in unconstrained space.</span>
<span class="sd">            method: Method used for MCMC sampling, one of `slice_np`,</span>
<span class="sd">                `slice_np_vectorized`, `slice`, `hmc`, `nuts`. `slice_np` is a custom</span>
<span class="sd">                numpy implementation of slice sampling. `slice_np_vectorized` is</span>
<span class="sd">                identical to `slice_np`, but if `num_chains&gt;1`, the chains are</span>
<span class="sd">                vectorized for `slice_np_vectorized` whereas they are run sequentially</span>
<span class="sd">                for `slice_np`. The samplers `hmc`, `nuts` or `slice` sample with Pyro.</span>
<span class="sd">            thin: The thinning factor for the chain.</span>
<span class="sd">            warmup_steps: The initial number of samples to discard.</span>
<span class="sd">            num_chains: The number of chains.</span>
<span class="sd">            init_strategy: The initialisation strategy for chains; `proposal` will draw</span>
<span class="sd">                init locations from `proposal`, whereas `sir` will use Sequential-</span>
<span class="sd">                Importance-Resampling (SIR). SIR initially samples</span>
<span class="sd">                `init_strategy_num_candidates` from the `proposal`, evaluates all of</span>
<span class="sd">                them under the `potential_fn`, and then resamples the initial locations</span>
<span class="sd">                with weights proportional to the `potential_fn`-value.</span>
<span class="sd">            init_strategy_num_candidates: Number of candidates to to find init</span>
<span class="sd">                locations in `init_strategy=sir`.</span>
<span class="sd">            num_workers: number of cpu cores used to parallelize mcmc</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="n">thin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="o">=</span> <span class="n">num_chains</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="o">=</span> <span class="n">init_strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_num_candidates</span> <span class="o">=</span> <span class="n">init_strategy_num_candidates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides MCMC to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns MCMC method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span>

    <span class="nd">@mcmc_method</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;See `set_mcmc_method`.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_mcmc_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: Method to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `NeuralPosterior` for chainable calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="sd">&quot;&quot;&quot;`.log_prob()` is deprecated for methods that can only evaluate the</span>
<span class="sd">            log-probability up to a normalizing constant. Use `.potential()` instead.&quot;&quot;&quot;</span>
        <span class="p">)</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$ with MCMC.</span>

<span class="sd">        Check the `__init__()` method for a description of all arguments as well as</span>
<span class="sd">        their default values.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            mcmc_parameters: Dictionary that is passed only to support the API of</span>
<span class="sd">                `sbi` v0.17.2 or older.</span>
<span class="sd">            mcmc_method: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. Please use `method` instead.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">thin</span>
        <span class="n">warmup_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="k">if</span> <span class="n">warmup_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">warmup_steps</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
        <span class="n">init_strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="k">if</span> <span class="n">init_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">init_strategy</span>
        <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="k">if</span> <span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_workers</span>
        <span class="n">init_strategy_num_candidates</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_num_candidates</span>
            <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">init_strategy_num_candidates</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">mcmc_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
                <span class="s2">&quot;is deprecated and will be removed in a future release. Use `method` &quot;</span>
                <span class="s2">&quot;instead of `mcmc_method`.&quot;</span>
            <span class="p">)</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">mcmc_method</span>
        <span class="k">if</span> <span class="n">mcmc_parameters</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
                <span class="s2">&quot;is deprecated and will be removed in a future release. Instead, pass &quot;</span>
                <span class="s2">&quot;the variable to `.sample()` directly, e.g. &quot;</span>
                <span class="s2">&quot;`posterior.sample((1,), num_chains=5)`.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># The following lines are only for backwards compatibility with sbi v0.17.2 or</span>
        <span class="c1"># older.</span>
        <span class="n">m_p</span> <span class="o">=</span> <span class="n">mcmc_parameters</span>  <span class="c1"># define to shorten the variable name</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="s2">&quot;mcmc_method&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">thin</span><span class="p">,</span> <span class="s2">&quot;thin&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="s2">&quot;warmup_steps&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="s2">&quot;num_chains&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">init_strategy</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">init_strategy</span><span class="p">,</span> <span class="s2">&quot;init_strategy&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">init_strategy_num_candidates</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span>
            <span class="n">init_strategy_num_candidates</span><span class="p">,</span> <span class="s2">&quot;init_strategy_num_candidates&quot;</span><span class="p">,</span> <span class="n">m_p</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="n">initial_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_params</span><span class="p">(</span>
            <span class="n">init_strategy</span><span class="p">,</span> <span class="n">num_chains</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">,</span> <span class="n">show_progress_bars</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="n">track_gradients</span> <span class="o">=</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;slice_np&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_np_mcmc</span><span class="p">(</span>
                    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                    <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                    <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                    <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">vectorized</span><span class="o">=</span><span class="p">(</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">),</span>
                    <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                    <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">,</span> <span class="s2">&quot;slice&quot;</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pyro_mcmc</span><span class="p">(</span>
                    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                    <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                    <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                    <span class="n">mcmc_method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                    <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">_build_mcmc_init_fn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">Transform</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return function that, when called, creates an initial parameter set for MCMC.</span>

<span class="sd">        Args:</span>
<span class="sd">            proposal: Proposal distribution.</span>
<span class="sd">            potential_fn: Potential function that the candidate samples are weighted</span>
<span class="sd">                with.</span>
<span class="sd">            init_strategy: Specifies the initialization method. Either of</span>
<span class="sd">                [`proposal`|`sir`|`latest_sample`].</span>
<span class="sd">            kwargs: Passed on to init function. This way, init specific keywords can</span>
<span class="sd">                be set through `mcmc_parameters`. Unused arguments should be absorbed.</span>

<span class="sd">        Returns: Initialization function.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;proposal&quot;</span> <span class="ow">or</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;prior&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;prior&quot;</span><span class="p">:</span>
                <span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;You set `init_strategy=prior`. As of sbi v0.18.0, this is &quot;</span>
                    <span class="s2">&quot;deprecated and it will be removed in a future release. Use &quot;</span>
                    <span class="s2">&quot;`init_strategy=proposal` instead.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">proposal_init</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">sir</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;latest_sample&quot;</span><span class="p">:</span>
            <span class="n">latest_sample</span> <span class="o">=</span> <span class="n">IterateParameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_init_params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">latest_sample</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_get_initial_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return initial parameters for MCMC obtained with given init strategy.</span>

<span class="sd">        Parallelizes across CPU cores only for SIR.</span>

<span class="sd">        Args:</span>
<span class="sd">            init_strategy: Specifies the initialization method. Either of</span>
<span class="sd">                [`proposal`|`sir`|`latest_sample`].</span>
<span class="sd">            num_chains: number of MCMC chains, generates initial params for each</span>
<span class="sd">            num_workers: number of CPU cores for parallization</span>
<span class="sd">            show_progress_bars: whether to show progress bars for SIR init</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: initial parameters, one for each chain</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Build init function</span>
        <span class="n">init_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_mcmc_init_fn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">init_strategy</span><span class="o">=</span><span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>

        <span class="c1"># Parallelize inits for SIR only.</span>
        <span class="k">if</span> <span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">seeded_init_fn</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">init_fn</span><span class="p">()</span>

            <span class="n">seeds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_workers</span><span class="p">,))</span>

            <span class="c1"># Generate initial params parallelized over num_workers.</span>
            <span class="k">with</span> <span class="n">tqdm_joblib</span><span class="p">(</span>
                <span class="n">tqdm</span><span class="p">(</span>
                    <span class="nb">range</span><span class="p">(</span><span class="n">num_chains</span><span class="p">),</span>  <span class="c1"># type: ignore</span>
                    <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bars</span><span class="p">,</span>
                    <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Generating </span><span class="si">{</span><span class="n">num_chains</span><span class="si">}</span><span class="s2"> MCMC inits with </span><span class="si">{</span><span class="n">num_workers</span><span class="si">}</span><span class="s2"></span>
<span class="s2">                         workers.&quot;&quot;&quot;</span><span class="p">,</span>
                    <span class="n">total</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">):</span>
                <span class="n">initial_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)(</span>
                        <span class="n">delayed</span><span class="p">(</span><span class="n">seeded_init_fn</span><span class="p">)(</span><span class="n">seed</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span>
                    <span class="p">)</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">initial_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">init_fn</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chains</span><span class="p">)]</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">initial_params</span>

    <span class="k">def</span> <span class="nf">_slice_np_mcmc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">potential_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">initial_params</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">vectorized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Custom implementation of slice sampling using Numpy.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Desired number of samples.</span>
<span class="sd">            potential_function: A callable **class**.</span>
<span class="sd">            initial_params: Initial parameters for MCMC chain.</span>
<span class="sd">            thin: Thinning (subsampling) factor.</span>
<span class="sd">            warmup_steps: Initial number of samples to discard.</span>
<span class="sd">            vectorized: Whether to use a vectorized implementation of</span>
<span class="sd">                the Slice sampler (still experimental).</span>
<span class="sd">            num_workers: number of CPU cores to use</span>
<span class="sd">            seed: seed that will be used to generate sub-seeds for each worker</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling;</span>
<span class="sd">                can only be turned off for vectorized sampler.</span>

<span class="sd">        Returns: Tensor of shape (num_samples, shape_of_single_theta).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_chains</span><span class="p">,</span> <span class="n">dim_samples</span> <span class="o">=</span> <span class="n">initial_params</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">slice_np_parallized</span><span class="p">(</span>
            <span class="n">potential_function</span><span class="p">,</span>
            <span class="n">initial_params</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="p">,</span>
            <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>
            <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
            <span class="n">vectorized</span><span class="o">=</span><span class="n">vectorized</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Save sample as potential next init (if init_strategy == &#39;latest_sample&#39;).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_init_params</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="n">dim_samples</span><span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim_samples</span><span class="p">)[:</span><span class="n">num_samples</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">assert</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">num_samples</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_pyro_mcmc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">potential_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">initial_params</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice&quot;</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples obtained using Pyro HMC, NUTS for slice kernels.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Desired number of samples.</span>
<span class="sd">            potential_function: A callable **class**. A class, but not a function,</span>
<span class="sd">                is picklable for Pyro MCMC to use it across chains in parallel,</span>
<span class="sd">                even when the potential function requires evaluating a neural network.</span>
<span class="sd">            mcmc_method: One of `hmc`, `nuts` or `slice`.</span>
<span class="sd">            thin: Thinning (subsampling) factor.</span>
<span class="sd">            warmup_steps: Initial number of samples to discard.</span>
<span class="sd">            num_chains: Whether to sample in parallel. If None, use all but one CPU.</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling.</span>

<span class="sd">        Returns: Tensor of shape (num_samples, shape_of_single_theta).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>

        <span class="n">kernels</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">slice</span><span class="o">=</span><span class="n">Slice</span><span class="p">,</span> <span class="n">hmc</span><span class="o">=</span><span class="n">HMC</span><span class="p">,</span> <span class="n">nuts</span><span class="o">=</span><span class="n">NUTS</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">kernels</span><span class="p">[</span><span class="n">mcmc_method</span><span class="p">](</span><span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_function</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="p">(</span><span class="n">thin</span> <span class="o">*</span> <span class="n">num_samples</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_chains</span> <span class="o">+</span> <span class="n">num_chains</span><span class="p">,</span>
            <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
            <span class="n">initial_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">initial_params</span><span class="p">},</span>
            <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
            <span class="n">mp_context</span><span class="o">=</span><span class="s2">&quot;spawn&quot;</span><span class="p">,</span>
            <span class="n">disable_progbar</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">transforms</span><span class="o">=</span><span class="p">{},</span>
        <span class="p">)</span>
        <span class="n">sampler</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">initial_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># .shape[1] = dim of theta</span>
        <span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[::</span><span class="n">thin</span><span class="p">][:</span><span class="n">num_samples</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">num_samples</span>

        <span class="k">return</span> <span class="n">samples</span>

    <span class="k">def</span> <span class="nf">_prepare_potential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Combines potential and transform and takes care of gradients and pyro.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: Which MCMC method to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A potential function that is ready to be used in MCMC.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice&quot;</span><span class="p">:</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">):</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="s2">&quot;slice_np&quot;</span> <span class="ow">in</span> <span class="n">method</span><span class="p">:</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="n">prepared_potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">transformed_potential</span><span class="p">,</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">pyro</span><span class="p">:</span>
            <span class="n">prepared_potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">pyro_potential_wrapper</span><span class="p">,</span> <span class="n">potential</span><span class="o">=</span><span class="n">prepared_potential</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">prepared_potential</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether or not to show a progressbar for sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">







  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.default_x">
<code class="codehilite language-python"><span class="n">default_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.default_x" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return default x used by <code>.sample(), .log_prob</code> as conditioning context.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.mcmc_method">
<code class="codehilite language-python"><span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.mcmc_method" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns MCMC method.</p>
    </div>

  </div>







  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;slice_np&#39;</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">init_strategy</span><span class="o">=</span><span class="s1">&#39;sir&#39;</span><span class="p">,</span> <span class="n">init_strategy_num_candidates</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">


<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>potential_fn</code></td>
        <td><code>Callable</code></td>
        <td><p>The potential function from which to draw samples.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>proposal</code></td>
        <td><code>Any</code></td>
        <td><p>Proposal distribution that is used to initialize the MCMC chain.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>theta_transform</code></td>
        <td><code>Optional[torch Transform]</code></td>
        <td><p>Transformation that will be applied during sampling.
Allows to perform MCMC in unconstrained space.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>method</code></td>
        <td><code>str</code></td>
        <td><p>Method used for MCMC sampling, one of <code>slice_np</code>,
<code>slice_np_vectorized</code>, <code>slice</code>, <code>hmc</code>, <code>nuts</code>. <code>slice_np</code> is a custom
numpy implementation of slice sampling. <code>slice_np_vectorized</code> is
identical to <code>slice_np</code>, but if <code>num_chains&gt;1</code>, the chains are
vectorized for <code>slice_np_vectorized</code> whereas they are run sequentially
for <code>slice_np</code>. The samplers <code>hmc</code>, <code>nuts</code> or <code>slice</code> sample with Pyro.</p></td>
        <td><code>&#39;slice_np&#39;</code></td>
      </tr>
      <tr>
        <td><code>thin</code></td>
        <td><code>int</code></td>
        <td><p>The thinning factor for the chain.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>warmup_steps</code></td>
        <td><code>int</code></td>
        <td><p>The initial number of samples to discard.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>num_chains</code></td>
        <td><code>int</code></td>
        <td><p>The number of chains.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>init_strategy</code></td>
        <td><code>str</code></td>
        <td><p>The initialisation strategy for chains; <code>proposal</code> will draw
init locations from <code>proposal</code>, whereas <code>sir</code> will use Sequential-
Importance-Resampling (SIR). SIR initially samples
<code>init_strategy_num_candidates</code> from the <code>proposal</code>, evaluates all of
them under the <code>potential_fn</code>, and then resamples the initial locations
with weights proportional to the <code>potential_fn</code>-value.</p></td>
        <td><code>&#39;sir&#39;</code></td>
      </tr>
      <tr>
        <td><code>init_strategy_num_candidates</code></td>
        <td><code>int</code></td>
        <td><p>Number of candidates to to find init
locations in <code>init_strategy=sir</code>.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>num_workers</code></td>
        <td><code>int</code></td>
        <td><p>number of cpu cores used to parallelize mcmc</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>x_shape</code></td>
        <td><code>Optional[torch.Size]</code></td>
        <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span><span class="p">,</span>
    <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sir&quot;</span><span class="p">,</span>
    <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples.</span>
<span class="sd">        proposal: Proposal distribution that is used to initialize the MCMC chain.</span>
<span class="sd">        theta_transform: Transformation that will be applied during sampling.</span>
<span class="sd">            Allows to perform MCMC in unconstrained space.</span>
<span class="sd">        method: Method used for MCMC sampling, one of `slice_np`,</span>
<span class="sd">            `slice_np_vectorized`, `slice`, `hmc`, `nuts`. `slice_np` is a custom</span>
<span class="sd">            numpy implementation of slice sampling. `slice_np_vectorized` is</span>
<span class="sd">            identical to `slice_np`, but if `num_chains&gt;1`, the chains are</span>
<span class="sd">            vectorized for `slice_np_vectorized` whereas they are run sequentially</span>
<span class="sd">            for `slice_np`. The samplers `hmc`, `nuts` or `slice` sample with Pyro.</span>
<span class="sd">        thin: The thinning factor for the chain.</span>
<span class="sd">        warmup_steps: The initial number of samples to discard.</span>
<span class="sd">        num_chains: The number of chains.</span>
<span class="sd">        init_strategy: The initialisation strategy for chains; `proposal` will draw</span>
<span class="sd">            init locations from `proposal`, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling (SIR). SIR initially samples</span>
<span class="sd">            `init_strategy_num_candidates` from the `proposal`, evaluates all of</span>
<span class="sd">            them under the `potential_fn`, and then resamples the initial locations</span>
<span class="sd">            with weights proportional to the `potential_fn`-value.</span>
<span class="sd">        init_strategy_num_candidates: Number of candidates to to find init</span>
<span class="sd">            locations in `init_strategy=sir`.</span>
<span class="sd">        num_workers: number of cpu cores used to parallelize mcmc</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="n">thin</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="o">=</span> <span class="n">num_chains</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="o">=</span> <span class="n">init_strategy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_num_candidates</span> <span class="o">=</span> <span class="n">init_strategy_num_candidates</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides MCMC to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.log_prob">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.log_prob" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the log-probability of theta under the posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>track_gradients</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p><code>len($\theta$)</code>-shaped log-probability.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warn</span><span class="p">(</span>
        <span class="sd">&quot;&quot;&quot;`.log_prob()` is deprecated for methods that can only evaluate the</span>
<span class="sd">        log-probability up to a normalizing constant. Use `.potential()` instead.&quot;&quot;&quot;</span>
    <span class="p">)</span>
    <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.map">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.map" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_iter</code></td>
        <td><code>int</code></td>
        <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate of the optimizer.</p></td>
        <td><code>0.01</code></td>
      </tr>
      <tr>
        <td><code>init_method</code></td>
        <td><code>Union[str, torch.Tensor]</code></td>
        <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
        <td><code>&#39;proposal&#39;</code></td>
      </tr>
      <tr>
        <td><code>num_init_samples</code></td>
        <td><code>int</code></td>
        <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>num_to_optimize</code></td>
        <td><code>int</code></td>
        <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>save_best_every</code></td>
        <td><code>int</code></td>
        <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether or not to show a progressbar for sampling from
the posterior.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>force_update</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>log_prob_kwargs</code></td>
        <td></td>
        <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>The MAP estimate.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether or not to show a progressbar for sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.potential">
<code class="codehilite language-python"><span class="n">potential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.potential" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Evaluates <span class="arithmatex">\(\theta\)</span> under the potential that is used to sample the posterior.</p>
<p>The potential is the unnormalized log-probability of <span class="arithmatex">\(\theta\)</span> under the
posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>track_gradients</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">potential</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Evaluates $\theta$ under the potential that is used to sample the posterior.</span>

<span class="sd">    The potential is the unnormalized log-probability of $\theta$ under the</span>
<span class="sd">    posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy_num_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="p">{},</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return samples from posterior distribution <span class="arithmatex">\(p(\theta|x)\)</span> with MCMC.</p>
<p>Check the <code>__init__()</code> method for a description of all arguments as well as
their default values.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>sample_shape</code></td>
        <td><code>Union[torch.Size, Tuple[int, ...]]</code></td>
        <td><p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p></td>
        <td><code>torch.Size([])</code></td>
      </tr>
      <tr>
        <td><code>mcmc_parameters</code></td>
        <td><code>Dict</code></td>
        <td><p>Dictionary that is passed only to support the API of
<code>sbi</code> v0.17.2 or older.</p></td>
        <td><code>{}</code></td>
      </tr>
      <tr>
        <td><code>mcmc_method</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. Please use <code>method</code> instead.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sample_with</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show sampling progress monitor.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>Samples from posterior.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">thin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$ with MCMC.</span>

<span class="sd">    Check the `__init__()` method for a description of all arguments as well as</span>
<span class="sd">    their default values.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        mcmc_parameters: Dictionary that is passed only to support the API of</span>
<span class="sd">            `sbi` v0.17.2 or older.</span>
<span class="sd">        mcmc_method: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. Please use `method` instead.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># Replace arguments that were not passed with their default.</span>
    <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>
    <span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">thin</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="k">if</span> <span class="n">warmup_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">warmup_steps</span>
    <span class="n">num_chains</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
    <span class="n">init_strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="k">if</span> <span class="n">init_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">init_strategy</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="k">if</span> <span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_workers</span>
    <span class="n">init_strategy_num_candidates</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_num_candidates</span>
        <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">init_strategy_num_candidates</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">mcmc_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
            <span class="s2">&quot;is deprecated and will be removed in a future release. Use `method` &quot;</span>
            <span class="s2">&quot;instead of `mcmc_method`.&quot;</span>
        <span class="p">)</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">mcmc_method</span>
    <span class="k">if</span> <span class="n">mcmc_parameters</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
            <span class="s2">&quot;is deprecated and will be removed in a future release. Instead, pass &quot;</span>
            <span class="s2">&quot;the variable to `.sample()` directly, e.g. &quot;</span>
            <span class="s2">&quot;`posterior.sample((1,), num_chains=5)`.&quot;</span>
        <span class="p">)</span>
    <span class="c1"># The following lines are only for backwards compatibility with sbi v0.17.2 or</span>
    <span class="c1"># older.</span>
    <span class="n">m_p</span> <span class="o">=</span> <span class="n">mcmc_parameters</span>  <span class="c1"># define to shorten the variable name</span>
    <span class="n">method</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="s2">&quot;mcmc_method&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">thin</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">thin</span><span class="p">,</span> <span class="s2">&quot;thin&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="s2">&quot;warmup_steps&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">num_chains</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="s2">&quot;num_chains&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">init_strategy</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">init_strategy</span><span class="p">,</span> <span class="s2">&quot;init_strategy&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">init_strategy_num_candidates</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span>
        <span class="n">init_strategy_num_candidates</span><span class="p">,</span> <span class="s2">&quot;init_strategy_num_candidates&quot;</span><span class="p">,</span> <span class="n">m_p</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="n">initial_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_params</span><span class="p">(</span>
        <span class="n">init_strategy</span><span class="p">,</span> <span class="n">num_chains</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">,</span> <span class="n">show_progress_bars</span>  <span class="c1"># type: ignore</span>
    <span class="p">)</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="n">track_gradients</span> <span class="o">=</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;slice_np&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_np_mcmc</span><span class="p">(</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">vectorized</span><span class="o">=</span><span class="p">(</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">),</span>
                <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts&quot;</span><span class="p">,</span> <span class="s2">&quot;slice&quot;</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pyro_mcmc</span><span class="p">(</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                <span class="n">mcmc_method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NameError</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_default_x">
<code class="codehilite language-python"><span class="n">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_default_x" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Set new default x for <code>.sample(), .log_prob</code> to use as conditioning context.</p>
<p>Reset the MAP stored for the old default x if applicable.</p>
<p>This is a pure convenience to avoid having to repeatedly specify <code>x</code> in calls to
<code>.sample()</code> and <code>.log_prob()</code> - only $  heta$ needs to be passed.</p>
<p>This convenience is particularly useful when the posterior is focused, i.e.
has been trained over multiple rounds to be accurate in the vicinity of a
particular <code>x=x_o</code> (you can check if your posterior object is focused by
printing it).</p>
<p>NOTE: this method is chainable, i.e. will return the NeuralPosterior object so
that calls like <code>posterior.set_default_x(my_x).sample(mytheta)</code> are possible.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>The default observation to set for the posterior <span class="arithmatex">\(p(     heta|x)\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>NeuralPosterior</code></td>
      <td><p><code>NeuralPosterior</code> that will use a default <code>x</code> when not explicitly passed.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Set new default x for `.sample(), .log_prob` to use as conditioning context.</span>

<span class="sd">    Reset the MAP stored for the old default x if applicable.</span>

<span class="sd">    This is a pure convenience to avoid having to repeatedly specify `x` in calls to</span>
<span class="sd">    `.sample()` and `.log_prob()` - only $\theta$ needs to be passed.</span>

<span class="sd">    This convenience is particularly useful when the posterior is focused, i.e.</span>
<span class="sd">    has been trained over multiple rounds to be accurate in the vicinity of a</span>
<span class="sd">    particular `x=x_o` (you can check if your posterior object is focused by</span>
<span class="sd">    printing it).</span>

<span class="sd">    NOTE: this method is chainable, i.e. will return the NeuralPosterior object so</span>
<span class="sd">    that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The default observation to set for the posterior $p(\theta|x)$.</span>
<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` that will use a default `x` when not explicitly passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span> <span class="n">allow_iid_x</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">allow_iid_x</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_map</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_mcmc_method">
<code class="codehilite language-python"><span class="n">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_mcmc_method" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Sets sampling method to for MCMC and returns <code>NeuralPosterior</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>method</code></td>
        <td><code>str</code></td>
        <td><p>Method to use.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>NeuralPosterior</code></td>
      <td><p><code>NeuralPosterior</code> for chainable calls.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: Method to use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior">
        <code>
sbi.inference.posteriors.rejection_posterior.RejectionPosterior            (<span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span>)
        </code>



<a class="headerlink" href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Provides rerjeciton sampling to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>RejectionPosterior</code> allows to sample from the posterior with rejection sampling.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">RejectionPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides rerjeciton sampling to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `RejectionPosterior` allows to sample from the posterior with rejection sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples.</span>
<span class="sd">            proposal: The proposal distribution.</span>
<span class="sd">            theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">                during but only when calling `.map()`.</span>
<span class="sd">            max_sampling_batch_size: The batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration.</span>
<span class="sd">            num_samples_to_find_max: The number of samples that are used to find the</span>
<span class="sd">                maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">            num_iter_to_find_max: The number of gradient ascent iterations to find the</span>
<span class="sd">                maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">            m: Multiplier to the `potential_fn / proposal` ratio.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="n">num_samples_to_find_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="n">num_samples_to_find_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="n">num_iter_to_find_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides rejection sampling to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`.log_prob()` is deprecated for methods that can only evaluate the log-probability up to a normalizing constant. Use `.potential()` instead.&quot;</span>
        <span class="p">)</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior $p(\theta|x)$ via rejection sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>
        <span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span>
            <span class="k">if</span> <span class="n">num_samples_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">num_samples_to_find_max</span>
        <span class="p">)</span>
        <span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span>
            <span class="k">if</span> <span class="n">num_iter_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">num_iter_to_find_max</span>
        <span class="p">)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">m</span>

        <span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rejection_sample</span><span class="p">(</span>
            <span class="n">potential</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">warn_acceptance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="n">num_samples_to_find_max</span><span class="p">,</span>
            <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="n">num_iter_to_find_max</span><span class="p">,</span>
            <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether or not to show a progressbar for sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">







  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.default_x">
<code class="codehilite language-python"><span class="n">default_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.default_x" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return default x used by <code>.sample(), .log_prob</code> as conditioning context.</p>
    </div>

  </div>







  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">


<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>potential_fn</code></td>
        <td><code>Callable</code></td>
        <td><p>The potential function from which to draw samples.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>proposal</code></td>
        <td><code>Any</code></td>
        <td><p>The proposal distribution.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>theta_transform</code></td>
        <td><code>Optional[torch Transform]</code></td>
        <td><p>Transformation that is applied to parameters. Is not used
during but only when calling <code>.map()</code>.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>max_sampling_batch_size</code></td>
        <td><code>int</code></td>
        <td><p>The batchsize of samples being drawn from
the proposal at every iteration.</p></td>
        <td><code>10000</code></td>
      </tr>
      <tr>
        <td><code>num_samples_to_find_max</code></td>
        <td><code>int</code></td>
        <td><p>The number of samples that are used to find the
maximum of the <code>potential_fn / proposal</code> ratio.</p></td>
        <td><code>10000</code></td>
      </tr>
      <tr>
        <td><code>num_iter_to_find_max</code></td>
        <td><code>int</code></td>
        <td><p>The number of gradient ascent iterations to find the
maximum of the <code>potential_fn / proposal</code> ratio.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>m</code></td>
        <td><code>float</code></td>
        <td><p>Multiplier to the <code>potential_fn / proposal</code> ratio.</p></td>
        <td><code>1.2</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>x_shape</code></td>
        <td><code>Optional[torch.Size]</code></td>
        <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">m</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples.</span>
<span class="sd">        proposal: The proposal distribution.</span>
<span class="sd">        theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">            during but only when calling `.map()`.</span>
<span class="sd">        max_sampling_batch_size: The batchsize of samples being drawn from</span>
<span class="sd">            the proposal at every iteration.</span>
<span class="sd">        num_samples_to_find_max: The number of samples that are used to find the</span>
<span class="sd">            maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">        num_iter_to_find_max: The number of gradient ascent iterations to find the</span>
<span class="sd">            maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">        m: Multiplier to the `potential_fn / proposal` ratio.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="n">num_samples_to_find_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="n">num_samples_to_find_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="n">num_iter_to_find_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides rejection sampling to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.log_prob">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.log_prob" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the log-probability of theta under the posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>track_gradients</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p><code>len($\theta$)</code>-shaped log-probability.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;`.log_prob()` is deprecated for methods that can only evaluate the log-probability up to a normalizing constant. Use `.potential()` instead.&quot;</span>
    <span class="p">)</span>
    <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.map">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.map" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_iter</code></td>
        <td><code>int</code></td>
        <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate of the optimizer.</p></td>
        <td><code>0.01</code></td>
      </tr>
      <tr>
        <td><code>init_method</code></td>
        <td><code>Union[str, torch.Tensor]</code></td>
        <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
        <td><code>&#39;proposal&#39;</code></td>
      </tr>
      <tr>
        <td><code>num_init_samples</code></td>
        <td><code>int</code></td>
        <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>num_to_optimize</code></td>
        <td><code>int</code></td>
        <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>save_best_every</code></td>
        <td><code>int</code></td>
        <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether or not to show a progressbar for sampling from
the posterior.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>force_update</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>log_prob_kwargs</code></td>
        <td></td>
        <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>The MAP estimate.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether or not to show a progressbar for sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.potential">
<code class="codehilite language-python"><span class="n">potential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.potential" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Evaluates <span class="arithmatex">\(\theta\)</span> under the potential that is used to sample the posterior.</p>
<p>The potential is the unnormalized log-probability of <span class="arithmatex">\(\theta\)</span> under the
posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>track_gradients</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">potential</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Evaluates $\theta$ under the potential that is used to sample the posterior.</span>

<span class="sd">    The potential is the unnormalized log-probability of $\theta$ under the</span>
<span class="sd">    posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.sample">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.sample" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return samples from posterior <span class="arithmatex">\(p(\theta|x)\)</span> via rejection sampling.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>sample_shape</code></td>
        <td><code>Union[torch.Size, Tuple[int, ...]]</code></td>
        <td><p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p></td>
        <td><code>torch.Size([])</code></td>
      </tr>
      <tr>
        <td><code>sample_with</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to show sampling progress monitor.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>Samples from posterior.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">m</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior $p(\theta|x)$ via rejection sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>
    <span class="c1"># Replace arguments that were not passed with their default.</span>
    <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
        <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
    <span class="p">)</span>
    <span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span>
        <span class="k">if</span> <span class="n">num_samples_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">num_samples_to_find_max</span>
    <span class="p">)</span>
    <span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span>
        <span class="k">if</span> <span class="n">num_iter_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">num_iter_to_find_max</span>
    <span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">m</span>

    <span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rejection_sample</span><span class="p">(</span>
        <span class="n">potential</span><span class="p">,</span>
        <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">warn_acceptance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="n">num_samples_to_find_max</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="n">num_iter_to_find_max</span><span class="p">,</span>
        <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.set_default_x">
<code class="codehilite language-python"><span class="n">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.set_default_x" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Set new default x for <code>.sample(), .log_prob</code> to use as conditioning context.</p>
<p>Reset the MAP stored for the old default x if applicable.</p>
<p>This is a pure convenience to avoid having to repeatedly specify <code>x</code> in calls to
<code>.sample()</code> and <code>.log_prob()</code> - only $  heta$ needs to be passed.</p>
<p>This convenience is particularly useful when the posterior is focused, i.e.
has been trained over multiple rounds to be accurate in the vicinity of a
particular <code>x=x_o</code> (you can check if your posterior object is focused by
printing it).</p>
<p>NOTE: this method is chainable, i.e. will return the NeuralPosterior object so
that calls like <code>posterior.set_default_x(my_x).sample(mytheta)</code> are possible.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>The default observation to set for the posterior <span class="arithmatex">\(p(     heta|x)\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>NeuralPosterior</code></td>
      <td><p><code>NeuralPosterior</code> that will use a default <code>x</code> when not explicitly passed.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Set new default x for `.sample(), .log_prob` to use as conditioning context.</span>

<span class="sd">    Reset the MAP stored for the old default x if applicable.</span>

<span class="sd">    This is a pure convenience to avoid having to repeatedly specify `x` in calls to</span>
<span class="sd">    `.sample()` and `.log_prob()` - only $\theta$ needs to be passed.</span>

<span class="sd">    This convenience is particularly useful when the posterior is focused, i.e.</span>
<span class="sd">    has been trained over multiple rounds to be accurate in the vicinity of a</span>
<span class="sd">    particular `x=x_o` (you can check if your posterior object is focused by</span>
<span class="sd">    printing it).</span>

<span class="sd">    NOTE: this method is chainable, i.e. will return the NeuralPosterior object so</span>
<span class="sd">    that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The default observation to set for the posterior $p(\theta|x)$.</span>
<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` that will use a default `x` when not explicitly passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span> <span class="n">allow_iid_x</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">allow_iid_x</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_map</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior">
        <code>
sbi.inference.posteriors.vi_posterior.VIPosterior            (<span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span>)
        </code>



<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Provides VI (Variational Inference) to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>VIPosterior</code> allows to learn a tractable variational posterior <span class="arithmatex">\(q(\theta)\)</span> which
approximates the true posterior <span class="arithmatex">\(p(\theta|x_o)\)</span>. After this second training stage,
we can produce approximate posterior samples, by just sampling from q with no
additional cost. For additional information see [1] and [2].<br/><br/>
References:<br/>
[1] Variational methods for simulation-based inference, Manuel Glckler, Michael
Deistler, Jakob Macke, 2022, <a href="https://openreview.net/forum?id=kZ0UYdhqkNY">https://openreview.net/forum?id=kZ0UYdhqkNY</a><br/>
[2] Sequential Neural Posterior and Likelihood Approximation, Samuel Wiqvist, Jes
Frellsen, Umberto Picchini, 2021, <a href="https://arxiv.org/abs/2102.06522">https://arxiv.org/abs/2102.06522</a></p>

        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">VIPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides VI (Variational Inference) to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `VIPosterior` allows to learn a tractable variational posterior $q(\theta)$ which</span>
<span class="sd">    approximates the true posterior $p(\theta|x_o)$. After this second training stage,</span>
<span class="sd">    we can produce approximate posterior samples, by just sampling from q with no</span>
<span class="sd">    additional cost. For additional information see [1] and [2].&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    References:&lt;br/&gt;</span>
<span class="sd">    [1] Variational methods for simulation-based inference, Manuel Glckler, Michael</span>
<span class="sd">    Deistler, Jakob Macke, 2022, https://openreview.net/forum?id=kZ0UYdhqkNY&lt;br/&gt;</span>
<span class="sd">    [2] Sequential Neural Posterior and Likelihood Approximation, Samuel Wiqvist, Jes</span>
<span class="sd">    Frellsen, Umberto Picchini, 2021, https://arxiv.org/abs/2102.06522</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples.</span>
<span class="sd">            prior: This is the prior distribution. Note that this is only</span>
<span class="sd">                used to check/construct the variational distribution or within some</span>
<span class="sd">                quality metrics. Please make sure that this matches with the prior</span>
<span class="sd">                within the potential_fn. If `None` is given, we will try to infer it</span>
<span class="sd">                from potential_fn or q, if this fails we raise an Error.</span>
<span class="sd">            q: Variational distribution, either string, `TransformedDistribution`, or a</span>
<span class="sd">                `VIPosterior` object. This specifies a parametric class of distribution</span>
<span class="sd">                over which the best possible posterior approximation is searched. For</span>
<span class="sd">                string input, we currently support [nsf, scf, maf, mcf, gaussian,</span>
<span class="sd">                gaussian_diag]. You can also specify your own variational family by</span>
<span class="sd">                passing a pyro `TransformedDistribution`.</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns a distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms` within the</span>
<span class="sd">                `get_flow_builder` method specifying the number of transformations</span>
<span class="sd">                within a normalizing flow. If q is already a `VIPosterior`, then the</span>
<span class="sd">                arguments will be copied from it (relevant for multi-round training).</span>
<span class="sd">            theta_transform: Maps form prior support to unconstrained space. The</span>
<span class="sd">                inverse is used here to ensure that the posterior support is equal to</span>
<span class="sd">                that of the prior.</span>
<span class="sd">            vi_method: This specifies the variational methods which are used to fit q to</span>
<span class="sd">                the posterior. We currently support [rKL, fKL, IW, alpha]. Note that</span>
<span class="sd">                some of the divergences are `mode seeking` i.e. they underestimate</span>
<span class="sd">                variance and collapse on multimodal targets (`rKL`, `alpha` for alpha &gt;</span>
<span class="sd">                1) and some are `mass covering` i.e. they overestimate variance but</span>
<span class="sd">                typically cover all modes (`fKL`, `IW`, `alpha` for alpha &lt; 1).</span>
<span class="sd">            device: Training device, e.g., `cpu`, `cuda` or `cuda:0`. We will ensure</span>
<span class="sd">                that all other objects are also on this device.</span>
<span class="sd">            x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">                the shape of the observed data and give a descriptive error.</span>
<span class="sd">            parameters: List of parameters of the variational posterior. This is only</span>
<span class="sd">                required for user-defined q i.e. if q does not have a `parameters`</span>
<span class="sd">                attribute.</span>
<span class="sd">            modules: List of modules of the variational posterior. This is only</span>
<span class="sd">                required for user-defined q i.e. if q does not have a `modules`</span>
<span class="sd">                attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="c1"># Especially the prior may be on another device -&gt; move it...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="c1"># Get prior and previous builds</span>
        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">prior</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="s2">&quot;prior&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Distribution</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;We could not find a suitable prior distribution within `potential_fn`&quot;</span>
                <span class="s2">&quot;or `q` (if a VIPosterior is given). Please explicitly specify a prior.&quot;</span>
            <span class="p">)</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># In contrast to MCMC we want to project into constrained space.</span>
        <span class="k">if</span> <span class="n">theta_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span><span class="o">.</span><span class="n">inv</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span>

        <span class="c1"># This will set the variational distribution and VI method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">vi_method</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides Variational inference to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _normalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">q</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the variational posterior.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q</span>

    <span class="nd">@q</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">q</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the variational distribution. If the distribution does not admit access</span>
<span class="sd">        through `parameters` and `modules` function, please use `set_q` if you want to</span>
<span class="sd">        explicitly specify the parameters and modules.</span>


<span class="sd">        Args:</span>
<span class="sd">            q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">                object. This specifies a parametric class of distribution over which</span>
<span class="sd">                the best possible posterior approximation is searched. For string input,</span>
<span class="sd">                we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">                course, you can also specify your own variational family by passing a</span>
<span class="sd">                `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">                Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">                parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">                using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">                is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">                (relevant for multi-round training).</span>


<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_q</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Defines the variational family.</span>

<span class="sd">        You can specify over which parameters/modules we optimize. This is required for</span>
<span class="sd">        custom distributions which e.g. do not inherit nn.Modules or has the function</span>
<span class="sd">        `parameters` or `modules` to give direct access to trainable parameters.</span>
<span class="sd">        Further, you can pass a function, which constructs a variational distribution</span>
<span class="sd">        if called.</span>

<span class="sd">        Args:</span>
<span class="sd">            q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">                object. This specifies a parametric class of distribution over which</span>
<span class="sd">                the best possible posterior approximation is searched. For string input,</span>
<span class="sd">                we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">                course, you can also specify your own variational family by passing a</span>
<span class="sd">                `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">                Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">                parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">                using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">                is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">                (relevant for multi-round training).</span>
<span class="sd">            parameters: List of parameters associated with the distribution object.</span>
<span class="sd">            modules: List of modules associated with the distribution object.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">adapt_variational_distribution</span><span class="p">(</span>
                <span class="n">q</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
                <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="n">self_custom_q_init_cache</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">self_custom_q_init_cache</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Callable</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">get_flow_builder</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span>

            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">event_shape</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_build_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_trained_on</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vi_method</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">vi_method</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_device</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_x</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_arg</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span>
        <span class="p">),</span> <span class="s2">&quot;Something went wrong when initializing the variational distribution. Please create an issue on github https://github.com/mackelab/sbi/issues&quot;</span>
        <span class="n">check_variational_distribution</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">q</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Variational inference method e.g. one of [rKL, fKL, IW, alpha].&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span>

    <span class="nd">@vi_method</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;See `set_vi_method`.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets variational inference method.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: One of [rKL, fKL, IW, alpha].</span>

<span class="sd">        Returns:</span>
<span class="sd">            `VIPosterior` for chainable calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span> <span class="o">=</span> <span class="n">get_VI_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;naive&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Samples from the variational posterior distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Shape of samples</span>
<span class="sd">            method: Sampling method, alternatively we can debias the approximation by</span>
<span class="sd">                using simple and efficient sampling schemes. We support one of [naive,</span>
<span class="sd">                sir].</span>
<span class="sd">            kwargs: Hyperparameters for the sampling methods.</span>
<span class="sd">                naive: Just samples from q, no parameters.</span>
<span class="sd">                sir: Performs sampling importance resampling.</span>
<span class="sd">                    `K`: Number of importance samples</span>
<span class="sd">                    `num_samples_batch`: How many samples are drawn in parallel (For</span>
<span class="sd">                        large K you may have to decrease this due to memory limitation).</span>


<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit using observation </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="s2">&quot;Please train.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">sampling_function</span> <span class="o">=</span> <span class="n">get_sampling_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">sampling_function</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the variational posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit using observation </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.</span><span class="se">\</span>
<span class="s2">                     Please train.&quot;</span>
            <span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_particles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
        <span class="n">max_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
        <span class="n">min_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
        <span class="n">warm_up_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">reset_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">check_for_convergence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">quality_control</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;This method trains the variational posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: The observation.</span>
<span class="sd">            n_particles: Number of samples to approximate expectations within the</span>
<span class="sd">                variational bounds. The larger the more accurate are gradient</span>
<span class="sd">                estimates, but the computational cost per iteration increases.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            gamma: Learning rate decay per iteration. We use an exponential decay</span>
<span class="sd">                scheduler.</span>
<span class="sd">            max_num_iters: Maximum number of iterations.</span>
<span class="sd">            min_num_iters: Minimum number of iterations.</span>
<span class="sd">            clip_value: Gradient clipping value, decreasing may help if you see invalid</span>
<span class="sd">                values.</span>
<span class="sd">            warm_up_rounds: Initialize the posterior as the prior.</span>
<span class="sd">            retrain_from_scratch: Retrain the variational distributions from scratch.</span>
<span class="sd">            reset_optimizer: Reset the divergence optimizer</span>
<span class="sd">            show_progress_bar: If any progress report should be displayed.</span>
<span class="sd">            quality_control: If False quality control is skipped.</span>
<span class="sd">            quality_control_metric: Which metric to use for evaluating the quality.</span>
<span class="sd">            kwargs: Hyperparameters check corresponding `DivergenceOptimizer` for detail</span>
<span class="sd">                eps: Determines sensitivity of convergence check.</span>
<span class="sd">                retain_graph: Boolean which decides whether to retain the computation</span>
<span class="sd">                    graph. This may be required for some `exotic` user-specified q&#39;s.</span>
<span class="sd">                optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See</span>
<span class="sd">                    `DivergenceOptimizer` for details.</span>
<span class="sd">                scheduler: A PyTorch learning rate scheduler. See</span>
<span class="sd">                    `DivergenceOptimizer` for details.</span>
<span class="sd">                alpha: Only used if vi_method=`alpha`. Determines the alpha divergence.</span>
<span class="sd">                K: Only used if vi_method=`IW`. Determines the number of importance</span>
<span class="sd">                    weighted particles.</span>
<span class="sd">                stick_the_landing: If one should use the STL estimator (only for rKL,</span>
<span class="sd">                    IW, alpha).</span>
<span class="sd">                dreg: If one should use the DREG estimator (only for rKL, IW, alpha).</span>
<span class="sd">                weight_transform: Callable applied to importance weights (only for fKL)</span>
<span class="sd">        Returns:</span>
<span class="sd">            VIPosterior: `VIPosterior` (can be used to chain calls).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update optimizer with current arguments.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="o">**</span><span class="nb">locals</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">})</span>

        <span class="c1"># Init q and the optimizer if necessary</span>
        <span class="k">if</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
                <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">reset_optimizer</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
                <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Check context</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_2d_float32_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="p">)</span>

        <span class="n">already_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

        <span class="c1"># Optimize</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">reset_loss_stats</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="n">iters</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">iters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">)</span>

        <span class="c1"># Warmup before training</span>
        <span class="k">if</span> <span class="n">reset_optimizer</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up_was_done</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">already_trained</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                    <span class="s2">&quot;Warmup phase, this may take a few seconds...&quot;</span>
                <span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up</span><span class="p">(</span><span class="n">warm_up_rounds</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iters</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">mean_loss</span><span class="p">,</span> <span class="n">std_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_loss_stats</span><span class="p">()</span>
            <span class="c1"># Update progress bar</span>
            <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">tqdm</span><span class="p">)</span>
                <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                    <span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">std_loss</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Check for convergence</span>
            <span class="k">if</span> <span class="n">check_for_convergence</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">min_num_iters</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">converged</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged with loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">break</span>
        <span class="c1"># Training finished:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">x</span>

        <span class="c1"># Evaluate quality</span>
        <span class="k">if</span> <span class="n">quality_control</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">quality_control_metric</span><span class="o">=</span><span class="n">quality_control_metric</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Quality control did not work, we reset the variational </span><span class="se">\</span>
<span class="s2">                        posterior,please check your setting. </span><span class="se">\</span>
<span class="s2">                        </span><span class="se">\n</span><span class="s2"> Following error occured </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
                    <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;This function will evaluate the quality of the variational posterior</span>
<span class="sd">        distribution. We currently support two different metrics of type `psis`, which</span>
<span class="sd">        checks the quality based on the tails of importance weights (there should not be</span>
<span class="sd">        much with a large one), or `prop` which checks the proportionality between q</span>
<span class="sd">        and potential_fn.</span>

<span class="sd">        NOTE: In our experience `prop` is sensitive to distinguish ``good`` from ``ok``</span>
<span class="sd">        whereas `psis` is more sensitive in distinguishing `very bad` from `ok`.</span>

<span class="sd">        Args:</span>
<span class="sd">            quality_control_metric: The metric of choice, we currently support [psis,</span>
<span class="sd">                prop, prop_prior].</span>
<span class="sd">            N: Number of samples which is used to evaluate the metric.</span>


<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">quality_control_fn</span><span class="p">,</span> <span class="n">quality_control_msg</span> <span class="o">=</span> <span class="n">get_quality_metric</span><span class="p">(</span>
            <span class="n">quality_control_metric</span>
        <span class="p">)</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quality_control_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)),</span> <span class="mi">3</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quality Score: </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="o">+</span> <span class="n">quality_control_msg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether or not to show a progressbar for sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">







  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.default_x">
<code class="codehilite language-python"><span class="n">default_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.default_x" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return default x used by <code>.sample(), .log_prob</code> as conditioning context.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.q">
<code class="codehilite language-python"><span class="n">q</span><span class="p">:</span> <span class="n">Distribution</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.q" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the variational posterior.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.vi_method">
<code class="codehilite language-python"><span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.vi_method" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Variational inference method e.g. one of [rKL, fKL, IW, alpha].</p>
    </div>

  </div>







  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.__init__">
<code class="codehilite language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vi_method</span><span class="o">=</span><span class="s1">&#39;rKL&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[],</span> <span class="n">modules</span><span class="o">=</span><span class="p">[])</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__init__" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">


<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>potential_fn</code></td>
        <td><code>Callable</code></td>
        <td><p>The potential function from which to draw samples.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Optional[torch Distribution]</code></td>
        <td><p>This is the prior distribution. Note that this is only
used to check/construct the variational distribution or within some
quality metrics. Please make sure that this matches with the prior
within the potential_fn. If <code>None</code> is given, we will try to infer it
from potential_fn or q, if this fails we raise an Error.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>q</code></td>
        <td><code>Union[str, pyro TransformedDistribution, VIPosterior, Callable]</code></td>
        <td><p>Variational distribution, either string, <code>TransformedDistribution</code>, or a
<code>VIPosterior</code> object. This specifies a parametric class of distribution
over which the best possible posterior approximation is searched. For
string input, we currently support [nsf, scf, maf, mcf, gaussian,
gaussian_diag]. You can also specify your own variational family by
passing a pyro <code>TransformedDistribution</code>.
Additionally, we allow a <code>Callable</code>, which allows you the pass a
<code>builder</code> function, which if called returns a distribution. This may be
useful for setting the hyperparameters e.g. <code>num_transfroms</code> within the
<code>get_flow_builder</code> method specifying the number of transformations
within a normalizing flow. If q is already a <code>VIPosterior</code>, then the
arguments will be copied from it (relevant for multi-round training).</p></td>
        <td><code>&#39;maf&#39;</code></td>
      </tr>
      <tr>
        <td><code>theta_transform</code></td>
        <td><code>Optional[torch Transform]</code></td>
        <td><p>Maps form prior support to unconstrained space. The
inverse is used here to ensure that the posterior support is equal to
that of the prior.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>vi_method</code></td>
        <td><code>str</code></td>
        <td><p>This specifies the variational methods which are used to fit q to
the posterior. We currently support [rKL, fKL, IW, alpha]. Note that
some of the divergences are <code>mode seeking</code> i.e. they underestimate
variance and collapse on multimodal targets (<code>rKL</code>, <code>alpha</code> for alpha &gt;
1) and some are <code>mass covering</code> i.e. they overestimate variance but
typically cover all modes (<code>fKL</code>, <code>IW</code>, <code>alpha</code> for alpha &lt; 1).</p></td>
        <td><code>&#39;rKL&#39;</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>Training device, e.g., <code>cpu</code>, <code>cuda</code> or <code>cuda:0</code>. We will ensure
that all other objects are also on this device.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>x_shape</code></td>
        <td><code>Optional[torch.Size]</code></td>
        <td><p>Shape of a single simulator output. If passed, it is used to check
the shape of the observed data and give a descriptive error.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>parameters</code></td>
        <td><code>Iterable</code></td>
        <td><p>List of parameters of the variational posterior. This is only
required for user-defined q i.e. if q does not have a <code>parameters</code>
attribute.</p></td>
        <td><code>[]</code></td>
      </tr>
      <tr>
        <td><code>modules</code></td>
        <td><code>Iterable</code></td>
        <td><p>List of modules of the variational posterior. This is only
required for user-defined q i.e. if q does not have a <code>modules</code>
attribute.</p></td>
        <td><code>[]</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples.</span>
<span class="sd">        prior: This is the prior distribution. Note that this is only</span>
<span class="sd">            used to check/construct the variational distribution or within some</span>
<span class="sd">            quality metrics. Please make sure that this matches with the prior</span>
<span class="sd">            within the potential_fn. If `None` is given, we will try to infer it</span>
<span class="sd">            from potential_fn or q, if this fails we raise an Error.</span>
<span class="sd">        q: Variational distribution, either string, `TransformedDistribution`, or a</span>
<span class="sd">            `VIPosterior` object. This specifies a parametric class of distribution</span>
<span class="sd">            over which the best possible posterior approximation is searched. For</span>
<span class="sd">            string input, we currently support [nsf, scf, maf, mcf, gaussian,</span>
<span class="sd">            gaussian_diag]. You can also specify your own variational family by</span>
<span class="sd">            passing a pyro `TransformedDistribution`.</span>
<span class="sd">            Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">            `builder` function, which if called returns a distribution. This may be</span>
<span class="sd">            useful for setting the hyperparameters e.g. `num_transfroms` within the</span>
<span class="sd">            `get_flow_builder` method specifying the number of transformations</span>
<span class="sd">            within a normalizing flow. If q is already a `VIPosterior`, then the</span>
<span class="sd">            arguments will be copied from it (relevant for multi-round training).</span>
<span class="sd">        theta_transform: Maps form prior support to unconstrained space. The</span>
<span class="sd">            inverse is used here to ensure that the posterior support is equal to</span>
<span class="sd">            that of the prior.</span>
<span class="sd">        vi_method: This specifies the variational methods which are used to fit q to</span>
<span class="sd">            the posterior. We currently support [rKL, fKL, IW, alpha]. Note that</span>
<span class="sd">            some of the divergences are `mode seeking` i.e. they underestimate</span>
<span class="sd">            variance and collapse on multimodal targets (`rKL`, `alpha` for alpha &gt;</span>
<span class="sd">            1) and some are `mass covering` i.e. they overestimate variance but</span>
<span class="sd">            typically cover all modes (`fKL`, `IW`, `alpha` for alpha &lt; 1).</span>
<span class="sd">        device: Training device, e.g., `cpu`, `cuda` or `cuda:0`. We will ensure</span>
<span class="sd">            that all other objects are also on this device.</span>
<span class="sd">        x_shape: Shape of a single simulator output. If passed, it is used to check</span>
<span class="sd">            the shape of the observed data and give a descriptive error.</span>
<span class="sd">        parameters: List of parameters of the variational posterior. This is only</span>
<span class="sd">            required for user-defined q i.e. if q does not have a `parameters`</span>
<span class="sd">            attribute.</span>
<span class="sd">        modules: List of modules of the variational posterior. This is only</span>
<span class="sd">            required for user-defined q i.e. if q does not have a `modules`</span>
<span class="sd">            attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="c1"># Especially the prior may be on another device -&gt; move it...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

    <span class="c1"># Get prior and previous builds</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="s2">&quot;prior&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Distribution</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;We could not find a suitable prior distribution within `potential_fn`&quot;</span>
            <span class="s2">&quot;or `q` (if a VIPosterior is given). Please explicitly specify a prior.&quot;</span>
        <span class="p">)</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># In contrast to MCMC we want to project into constrained space.</span>
    <span class="k">if</span> <span class="n">theta_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span><span class="o">.</span><span class="n">inv</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span>

    <span class="c1"># This will set the variational distribution and VI method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">vi_method</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides Variational inference to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _normalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.evaluate">
<code class="codehilite language-python"><span class="n">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="o">=</span><span class="s1">&#39;psis&#39;</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">50000</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.evaluate" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>This function will evaluate the quality of the variational posterior
distribution. We currently support two different metrics of type <code>psis</code>, which
checks the quality based on the tails of importance weights (there should not be
much with a large one), or <code>prop</code> which checks the proportionality between q
and potential_fn.</p>
<p>NOTE: In our experience <code>prop</code> is sensitive to distinguish <code>good</code> from <code>ok</code>
whereas <code>psis</code> is more sensitive in distinguishing <code>very bad</code> from <code>ok</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>quality_control_metric</code></td>
        <td><code>str</code></td>
        <td><p>The metric of choice, we currently support [psis,
prop, prop_prior].</p></td>
        <td><code>&#39;psis&#39;</code></td>
      </tr>
      <tr>
        <td><code>N</code></td>
        <td><code>int</code></td>
        <td><p>Number of samples which is used to evaluate the metric.</p></td>
        <td><code>50000</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;This function will evaluate the quality of the variational posterior</span>
<span class="sd">    distribution. We currently support two different metrics of type `psis`, which</span>
<span class="sd">    checks the quality based on the tails of importance weights (there should not be</span>
<span class="sd">    much with a large one), or `prop` which checks the proportionality between q</span>
<span class="sd">    and potential_fn.</span>

<span class="sd">    NOTE: In our experience `prop` is sensitive to distinguish ``good`` from ``ok``</span>
<span class="sd">    whereas `psis` is more sensitive in distinguishing `very bad` from `ok`.</span>

<span class="sd">    Args:</span>
<span class="sd">        quality_control_metric: The metric of choice, we currently support [psis,</span>
<span class="sd">            prop, prop_prior].</span>
<span class="sd">        N: Number of samples which is used to evaluate the metric.</span>


<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quality_control_fn</span><span class="p">,</span> <span class="n">quality_control_msg</span> <span class="o">=</span> <span class="n">get_quality_metric</span><span class="p">(</span>
        <span class="n">quality_control_metric</span>
    <span class="p">)</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quality_control_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)),</span> <span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quality Score: </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="o">+</span> <span class="n">quality_control_msg</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.log_prob">
<code class="codehilite language-python"><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.log_prob" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the log-probability of theta under the variational posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameters</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>track_gradients</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis but increases memory
consumption.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p><code>len($\theta$)</code>-shaped log-probability.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the variational posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit using observation </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.</span><span class="se">\</span>
<span class="s2">                 Please train.&quot;</span>
        <span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.map">
<code class="codehilite language-python"><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.map" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Optional[Tensor]</code></td>
        <td><p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_iter</code></td>
        <td><code>int</code></td>
        <td><p>Number of optimization steps that the algorithm takes
to find the MAP.</p></td>
        <td><code>1000</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate of the optimizer.</p></td>
        <td><code>0.01</code></td>
      </tr>
      <tr>
        <td><code>init_method</code></td>
        <td><code>Union[str, Tensor]</code></td>
        <td><p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p></td>
        <td><code>&#39;proposal&#39;</code></td>
      </tr>
      <tr>
        <td><code>num_init_samples</code></td>
        <td><code>int</code></td>
        <td><p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p></td>
        <td><code>10000</code></td>
      </tr>
      <tr>
        <td><code>num_to_optimize</code></td>
        <td><code>int</code></td>
        <td><p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>save_best_every</code></td>
        <td><code>int</code></td>
        <td><p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bars</code></td>
        <td><code>bool</code></td>
        <td><p>Whether or not to show a progressbar for sampling from
the posterior.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>force_update</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>log_prob_kwargs</code></td>
        <td></td>
        <td><p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>The MAP estimate.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether or not to show a progressbar for sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.potential">
<code class="codehilite language-python"><span class="n">potential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.potential" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Evaluates <span class="arithmatex">\(\theta\)</span> under the potential that is used to sample the posterior.</p>
<p>The potential is the unnormalized log-probability of <span class="arithmatex">\(\theta\)</span> under the
posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>theta</code></td>
        <td><code>Tensor</code></td>
        <td><p>Parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>track_gradients</code></td>
        <td><code>bool</code></td>
        <td><p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">potential</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Evaluates $\theta$ under the potential that is used to sample the posterior.</span>

<span class="sd">    The potential is the unnormalized log-probability of $\theta$ under the</span>
<span class="sd">    posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.sample">
<code class="codehilite language-python"><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;naive&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.sample" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Samples from the variational posterior distribution.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>sample_shape</code></td>
        <td><code>Union[torch.Size, Tuple[int, ...]]</code></td>
        <td><p>Shape of samples</p></td>
        <td><code>torch.Size([])</code></td>
      </tr>
      <tr>
        <td><code>method</code></td>
        <td><code>str</code></td>
        <td><p>Sampling method, alternatively we can debias the approximation by
using simple and efficient sampling schemes. We support one of [naive,
sir].</p></td>
        <td><code>&#39;naive&#39;</code></td>
      </tr>
      <tr>
        <td><code>kwargs</code></td>
        <td></td>
        <td><p>Hyperparameters for the sampling methods.
naive: Just samples from q, no parameters.
sir: Performs sampling importance resampling.
    <code>K</code>: Number of importance samples
    <code>num_samples_batch</code>: How many samples are drawn in parallel (For
        large K you may have to decrease this due to memory limitation).</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>Samples from posterior.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;naive&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Samples from the variational posterior distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Shape of samples</span>
<span class="sd">        method: Sampling method, alternatively we can debias the approximation by</span>
<span class="sd">            using simple and efficient sampling schemes. We support one of [naive,</span>
<span class="sd">            sir].</span>
<span class="sd">        kwargs: Hyperparameters for the sampling methods.</span>
<span class="sd">            naive: Just samples from q, no parameters.</span>
<span class="sd">            sir: Performs sampling importance resampling.</span>
<span class="sd">                `K`: Number of importance samples</span>
<span class="sd">                `num_samples_batch`: How many samples are drawn in parallel (For</span>
<span class="sd">                    large K you may have to decrease this due to memory limitation).</span>


<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit using observation </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="s2">&quot;Please train.&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sampling_function</span> <span class="o">=</span> <span class="n">get_sampling_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">sampling_function</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.set_default_x">
<code class="codehilite language-python"><span class="n">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_default_x" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Set new default x for <code>.sample(), .log_prob</code> to use as conditioning context.</p>
<p>Reset the MAP stored for the old default x if applicable.</p>
<p>This is a pure convenience to avoid having to repeatedly specify <code>x</code> in calls to
<code>.sample()</code> and <code>.log_prob()</code> - only $  heta$ needs to be passed.</p>
<p>This convenience is particularly useful when the posterior is focused, i.e.
has been trained over multiple rounds to be accurate in the vicinity of a
particular <code>x=x_o</code> (you can check if your posterior object is focused by
printing it).</p>
<p>NOTE: this method is chainable, i.e. will return the NeuralPosterior object so
that calls like <code>posterior.set_default_x(my_x).sample(mytheta)</code> are possible.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>The default observation to set for the posterior <span class="arithmatex">\(p(     heta|x)\)</span>.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>NeuralPosterior</code></td>
      <td><p><code>NeuralPosterior</code> that will use a default <code>x</code> when not explicitly passed.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_default_x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Set new default x for `.sample(), .log_prob` to use as conditioning context.</span>

<span class="sd">    Reset the MAP stored for the old default x if applicable.</span>

<span class="sd">    This is a pure convenience to avoid having to repeatedly specify `x` in calls to</span>
<span class="sd">    `.sample()` and `.log_prob()` - only $\theta$ needs to be passed.</span>

<span class="sd">    This convenience is particularly useful when the posterior is focused, i.e.</span>
<span class="sd">    has been trained over multiple rounds to be accurate in the vicinity of a</span>
<span class="sd">    particular `x=x_o` (you can check if your posterior object is focused by</span>
<span class="sd">    printing it).</span>

<span class="sd">    NOTE: this method is chainable, i.e. will return the NeuralPosterior object so</span>
<span class="sd">    that calls like `posterior.set_default_x(my_x).sample(mytheta)` are possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The default observation to set for the posterior $p(\theta|x)$.</span>
<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` that will use a default `x` when not explicitly passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_shape</span><span class="p">,</span> <span class="n">allow_iid_x</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">allow_iid_x</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_map</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.set_q">
<code class="codehilite language-python"><span class="n">set_q</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[],</span> <span class="n">modules</span><span class="o">=</span><span class="p">[])</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_q" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Defines the variational family.</p>
<p>You can specify over which parameters/modules we optimize. This is required for
custom distributions which e.g. do not inherit nn.Modules or has the function
<code>parameters</code> or <code>modules</code> to give direct access to trainable parameters.
Further, you can pass a function, which constructs a variational distribution
if called.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>q</code></td>
        <td><code>Union[str, pyro TransformedDistribution, VIPosterior, Callable]</code></td>
        <td><p>Variational distribution, either string, distribution, or a VIPosterior
object. This specifies a parametric class of distribution over which
the best possible posterior approximation is searched. For string input,
we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of
course, you can also specify your own variational family by passing a
<code>parameterized</code> distribution object i.e. a torch.distributions
Distribution with methods <code>parameters</code> returning an iterable of all
parameters (you can pass them within the paramters/modules attribute).
Additionally, we allow a <code>Callable</code>, which allows you the pass a
<code>builder</code> function, which if called returns an distribution. This may be
useful for setting the hyperparameters e.g. <code>num_transfroms:int</code> by
using the <code>get_flow_builder</code> method specifying the hyperparameters. If q
is already a <code>VIPosterior</code>, then the arguments will be copied from it
(relevant for multi-round training).</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>parameters</code></td>
        <td><code>Iterable</code></td>
        <td><p>List of parameters associated with the distribution object.</p></td>
        <td><code>[]</code></td>
      </tr>
      <tr>
        <td><code>modules</code></td>
        <td><code>Iterable</code></td>
        <td><p>List of modules associated with the distribution object.</p></td>
        <td><code>[]</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_q</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Defines the variational family.</span>

<span class="sd">    You can specify over which parameters/modules we optimize. This is required for</span>
<span class="sd">    custom distributions which e.g. do not inherit nn.Modules or has the function</span>
<span class="sd">    `parameters` or `modules` to give direct access to trainable parameters.</span>
<span class="sd">    Further, you can pass a function, which constructs a variational distribution</span>
<span class="sd">    if called.</span>

<span class="sd">    Args:</span>
<span class="sd">        q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">            object. This specifies a parametric class of distribution over which</span>
<span class="sd">            the best possible posterior approximation is searched. For string input,</span>
<span class="sd">            we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">            course, you can also specify your own variational family by passing a</span>
<span class="sd">            `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">            Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">            parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">            Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">            `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">            useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">            using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">            is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">            (relevant for multi-round training).</span>
<span class="sd">        parameters: List of parameters associated with the distribution object.</span>
<span class="sd">        modules: List of modules associated with the distribution object.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">adapt_variational_distribution</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
            <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="n">self_custom_q_init_cache</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">self_custom_q_init_cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Callable</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">get_flow_builder</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">event_shape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_build_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_trained_on</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vi_method</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">vi_method</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_arg</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span>
    <span class="p">),</span> <span class="s2">&quot;Something went wrong when initializing the variational distribution. Please create an issue on github https://github.com/mackelab/sbi/issues&quot;</span>
    <span class="n">check_variational_distribution</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">q</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.set_vi_method">
<code class="codehilite language-python"><span class="n">set_vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_vi_method" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Sets variational inference method.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>method</code></td>
        <td><code>str</code></td>
        <td><p>One of [rKL, fKL, IW, alpha].</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>VIPosterior</code></td>
      <td><p><code>VIPosterior</code> for chainable calls.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Sets variational inference method.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: One of [rKL, fKL, IW, alpha].</span>

<span class="sd">    Returns:</span>
<span class="sd">        `VIPosterior` for chainable calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span> <span class="o">=</span> <span class="n">get_VI_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="sbi.inference.posteriors.vi_posterior.VIPosterior.train">
<code class="codehilite language-python"><span class="n">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_particles</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">max_num_iters</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">min_num_iters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">clip_value</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">warm_up_rounds</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check_for_convergence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">quality_control</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="o">=</span><span class="s1">&#39;psis&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.posteriors.vi_posterior.VIPosterior.train" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>This method trains the variational posterior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Optional[Tensor]</code></td>
        <td><p>The observation.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>n_particles</code></td>
        <td><code>int</code></td>
        <td><p>Number of samples to approximate expectations within the
variational bounds. The larger the more accurate are gradient
estimates, but the computational cost per iteration increases.</p></td>
        <td><code>256</code></td>
      </tr>
      <tr>
        <td><code>learning_rate</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate of the optimizer.</p></td>
        <td><code>0.001</code></td>
      </tr>
      <tr>
        <td><code>gamma</code></td>
        <td><code>float</code></td>
        <td><p>Learning rate decay per iteration. We use an exponential decay
scheduler.</p></td>
        <td><code>0.999</code></td>
      </tr>
      <tr>
        <td><code>max_num_iters</code></td>
        <td><code>int</code></td>
        <td><p>Maximum number of iterations.</p></td>
        <td><code>2000</code></td>
      </tr>
      <tr>
        <td><code>min_num_iters</code></td>
        <td><code>int</code></td>
        <td><p>Minimum number of iterations.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>clip_value</code></td>
        <td><code>float</code></td>
        <td><p>Gradient clipping value, decreasing may help if you see invalid
values.</p></td>
        <td><code>10.0</code></td>
      </tr>
      <tr>
        <td><code>warm_up_rounds</code></td>
        <td><code>int</code></td>
        <td><p>Initialize the posterior as the prior.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>retrain_from_scratch</code></td>
        <td><code>bool</code></td>
        <td><p>Retrain the variational distributions from scratch.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>reset_optimizer</code></td>
        <td><code>bool</code></td>
        <td><p>Reset the divergence optimizer</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>show_progress_bar</code></td>
        <td><code>bool</code></td>
        <td><p>If any progress report should be displayed.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>quality_control</code></td>
        <td><code>bool</code></td>
        <td><p>If False quality control is skipped.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>quality_control_metric</code></td>
        <td><code>str</code></td>
        <td><p>Which metric to use for evaluating the quality.</p></td>
        <td><code>&#39;psis&#39;</code></td>
      </tr>
      <tr>
        <td><code>kwargs</code></td>
        <td></td>
        <td><p>Hyperparameters check corresponding <code>DivergenceOptimizer</code> for detail
eps: Determines sensitivity of convergence check.
retain_graph: Boolean which decides whether to retain the computation
    graph. This may be required for some <code>exotic</code> user-specified q&rsquo;s.
optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See
    <code>DivergenceOptimizer</code> for details.
scheduler: A PyTorch learning rate scheduler. See
    <code>DivergenceOptimizer</code> for details.
alpha: Only used if vi_method=<code>alpha</code>. Determines the alpha divergence.
K: Only used if vi_method=<code>IW</code>. Determines the number of importance
    weighted particles.
stick_the_landing: If one should use the STL estimator (only for rKL,
    IW, alpha).
dreg: If one should use the DREG estimator (only for rKL, IW, alpha).
weight_transform: Callable applied to importance weights (only for fKL)</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>VIPosterior</code></td>
      <td><p><code>VIPosterior</code> (can be used to chain calls).</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_particles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
    <span class="n">max_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
    <span class="n">min_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
    <span class="n">warm_up_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">reset_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_for_convergence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">quality_control</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;This method trains the variational posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The observation.</span>
<span class="sd">        n_particles: Number of samples to approximate expectations within the</span>
<span class="sd">            variational bounds. The larger the more accurate are gradient</span>
<span class="sd">            estimates, but the computational cost per iteration increases.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        gamma: Learning rate decay per iteration. We use an exponential decay</span>
<span class="sd">            scheduler.</span>
<span class="sd">        max_num_iters: Maximum number of iterations.</span>
<span class="sd">        min_num_iters: Minimum number of iterations.</span>
<span class="sd">        clip_value: Gradient clipping value, decreasing may help if you see invalid</span>
<span class="sd">            values.</span>
<span class="sd">        warm_up_rounds: Initialize the posterior as the prior.</span>
<span class="sd">        retrain_from_scratch: Retrain the variational distributions from scratch.</span>
<span class="sd">        reset_optimizer: Reset the divergence optimizer</span>
<span class="sd">        show_progress_bar: If any progress report should be displayed.</span>
<span class="sd">        quality_control: If False quality control is skipped.</span>
<span class="sd">        quality_control_metric: Which metric to use for evaluating the quality.</span>
<span class="sd">        kwargs: Hyperparameters check corresponding `DivergenceOptimizer` for detail</span>
<span class="sd">            eps: Determines sensitivity of convergence check.</span>
<span class="sd">            retain_graph: Boolean which decides whether to retain the computation</span>
<span class="sd">                graph. This may be required for some `exotic` user-specified q&#39;s.</span>
<span class="sd">            optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See</span>
<span class="sd">                `DivergenceOptimizer` for details.</span>
<span class="sd">            scheduler: A PyTorch learning rate scheduler. See</span>
<span class="sd">                `DivergenceOptimizer` for details.</span>
<span class="sd">            alpha: Only used if vi_method=`alpha`. Determines the alpha divergence.</span>
<span class="sd">            K: Only used if vi_method=`IW`. Determines the number of importance</span>
<span class="sd">                weighted particles.</span>
<span class="sd">            stick_the_landing: If one should use the STL estimator (only for rKL,</span>
<span class="sd">                IW, alpha).</span>
<span class="sd">            dreg: If one should use the DREG estimator (only for rKL, IW, alpha).</span>
<span class="sd">            weight_transform: Callable applied to importance weights (only for fKL)</span>
<span class="sd">    Returns:</span>
<span class="sd">        VIPosterior: `VIPosterior` (can be used to chain calls).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Update optimizer with current arguments.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="o">**</span><span class="nb">locals</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">})</span>

    <span class="c1"># Init q and the optimizer if necessary</span>
    <span class="k">if</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">reset_optimizer</span>
        <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Check context</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_2d_float32_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="p">)</span>

    <span class="n">already_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

    <span class="c1"># Optimize</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">reset_loss_stats</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
        <span class="n">iters</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">iters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">)</span>

    <span class="c1"># Warmup before training</span>
    <span class="k">if</span> <span class="n">reset_optimizer</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up_was_done</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">already_trained</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="s2">&quot;Warmup phase, this may take a few seconds...&quot;</span>
            <span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up</span><span class="p">(</span><span class="n">warm_up_rounds</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iters</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">mean_loss</span><span class="p">,</span> <span class="n">std_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_loss_stats</span><span class="p">()</span>
        <span class="c1"># Update progress bar</span>
        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">tqdm</span><span class="p">)</span>
            <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">std_loss</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Check for convergence</span>
        <span class="k">if</span> <span class="n">check_for_convergence</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">min_num_iters</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">converged</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged with loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
    <span class="c1"># Training finished:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Evaluate quality</span>
    <span class="k">if</span> <span class="n">quality_control</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">quality_control_metric</span><span class="o">=</span><span class="n">quality_control_metric</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Quality control did not work, we reset the variational </span><span class="se">\</span>
<span class="s2">                    posterior,please check your setting. </span><span class="se">\</span>
<span class="s2">                    </span><span class="se">\n</span><span class="s2"> Following error occured </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
                <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>

<h2 id="models">Models<a class="headerlink" href="#models" title="Permanent link">&para;</a></h2>


  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.posterior_nn">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_nn_models</span><span class="o">.</span><span class="n">posterior_nn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_transforms</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embedding_net</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.utils.get_nn_models.posterior_nn" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Returns a function that builds a density estimator for learning the posterior.</p>
<p>This function will usually be used for SNPE. The returned function is to be passed
to the inference class when using the flexible interface.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>model</code></td>
        <td><code>str</code></td>
        <td><p>The type of density estimator that will be created. One of [<code>mdn</code>,
<code>made</code>, <code>maf</code>, <code>nsf</code>].</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>z_score_theta</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Whether to z-score parameters <span class="arithmatex">\(\theta\)</span> before passing them into
the network, can take one of the following:
- <code>none</code>, or None: do not z-score.
- <code>independent</code>: z-score each dimension independently.
- <code>structured</code>: treat dimensions as related, therefore compute mean and std
over the entire batch, instead of per-dimension. Should be used when each
sample is, for example, a time series or an image.</p></td>
        <td><code>&#39;independent&#39;</code></td>
      </tr>
      <tr>
        <td><code>z_score_x</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Whether to z-score simulation outputs <span class="arithmatex">\(x\)</span> before passing them into
the network, same options as z_score_theta.</p></td>
        <td><code>&#39;independent&#39;</code></td>
      </tr>
      <tr>
        <td><code>hidden_features</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>num_transforms</code></td>
        <td><code>int</code></td>
        <td><p>Number of transforms when a flow is used. Only relevant if
density estimator is a normalizing flow (i.e. currently either a <code>maf</code> or a
<code>nsf</code>). Ignored if density estimator is a <code>mdn</code> or <code>made</code>.</p></td>
        <td><code>5</code></td>
      </tr>
      <tr>
        <td><code>num_bins</code></td>
        <td><code>int</code></td>
        <td><p>Number of bins used for the splines in <code>nsf</code>. Ignored if density
estimator not <code>nsf</code>.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>embedding_net</code></td>
        <td><code>Module</code></td>
        <td><p>Optional embedding network for simulation outputs <span class="arithmatex">\(x\)</span>. This
embedding net allows to learn features from potentially high-dimensional
simulation outputs.</p></td>
        <td><code>Identity()</code></td>
      </tr>
      <tr>
        <td><code>num_components</code></td>
        <td><code>int</code></td>
        <td><p>Number of mixture components for a mixture of Gaussians.
Ignored if density estimator is not an mdn.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>kwargs</code></td>
        <td></td>
        <td><p>additional custom arguments passed to downstream build functions.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">posterior_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">num_transforms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">num_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function that builds a density estimator for learning the posterior.</span>

<span class="sd">    This function will usually be used for SNPE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of density estimator that will be created. One of [`mdn`,</span>
<span class="sd">            `made`, `maf`, `nsf`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network, can take one of the following:</span>
<span class="sd">            - `none`, or None: do not z-score.</span>
<span class="sd">            - `independent`: z-score each dimension independently.</span>
<span class="sd">            - `structured`: treat dimensions as related, therefore compute mean and std</span>
<span class="sd">            over the entire batch, instead of per-dimension. Should be used when each</span>
<span class="sd">            sample is, for example, a time series or an image.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network, same options as z_score_theta.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        num_transforms: Number of transforms when a flow is used. Only relevant if</span>
<span class="sd">            density estimator is a normalizing flow (i.e. currently either a `maf` or a</span>
<span class="sd">            `nsf`). Ignored if density estimator is a `mdn` or `made`.</span>
<span class="sd">        num_bins: Number of bins used for the splines in `nsf`. Ignored if density</span>
<span class="sd">            estimator not `nsf`.</span>
<span class="sd">        embedding_net: Optional embedding network for simulation outputs $x$. This</span>
<span class="sd">            embedding net allows to learn features from potentially high-dimensional</span>
<span class="sd">            simulation outputs.</span>
<span class="sd">        num_components: Number of mixture components for a mixture of Gaussians.</span>
<span class="sd">            Ignored if density estimator is not an mdn.</span>
<span class="sd">        kwargs: additional custom arguments passed to downstream build functions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">&quot;z_score_x&quot;</span><span class="p">,</span>
                <span class="s2">&quot;z_score_y&quot;</span><span class="p">,</span>
                <span class="s2">&quot;hidden_features&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_transforms&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_bins&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embedding_net&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_components&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">num_transforms</span><span class="p">,</span>
                <span class="n">num_bins</span><span class="p">,</span>
                <span class="n">embedding_net</span><span class="p">,</span>
                <span class="n">num_components</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn_snpe_a</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">num_components</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build function for SNPE-A</span>

<span class="sd">        Extract the number of components from the kwargs, such that they are exposed as</span>
<span class="sd">        a kwargs, offering the possibility to later override this kwarg with</span>
<span class="sd">        `functools.partial`. This is necessary in order to make sure that the MDN in</span>
<span class="sd">        SNPE-A only has one component when running the Algorithm 1 part.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">build_mdn</span><span class="p">(</span>
            <span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span>
            <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span>
            <span class="n">num_components</span><span class="o">=</span><span class="n">num_components</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mdn&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mdn</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;made&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_made</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;maf&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_maf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;nsf&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_nsf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_components</span> <span class="o">!=</span> <span class="mi">10</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You set `num_components`. For SNPE-A, this has to be done at &quot;</span>
                <span class="s2">&quot;instantiation of the inference object, i.e. &quot;</span>
                <span class="s2">&quot;`inference = SNPE_A(..., num_components=20)`&quot;</span>
            <span class="p">)</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_components&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">build_fn_snpe_a</span> <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span> <span class="k">else</span> <span class="n">build_fn</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.likelihood_nn">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_nn_models</span><span class="o">.</span><span class="n">likelihood_nn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_transforms</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embedding_net</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.utils.get_nn_models.likelihood_nn" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Returns a function that builds a density estimator for learning the likelihood.</p>
<p>This function will usually be used for SNLE. The returned function is to be passed
to the inference class when using the flexible interface.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>model</code></td>
        <td><code>str</code></td>
        <td><p>The type of density estimator that will be created. One of [<code>mdn</code>,
<code>made</code>, <code>maf</code>, <code>nsf</code>].</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>z_score_theta</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Whether to z-score parameters <span class="arithmatex">\(\theta\)</span> before passing them into
the network, can take one of the following:
- <code>none</code>, or None: do not z-score.
- <code>independent</code>: z-score each dimension independently.
- <code>structured</code>: treat dimensions as related, therefore compute mean and std
over the entire batch, instead of per-dimension. Should be used when each
sample is, for example, a time series or an image.</p></td>
        <td><code>&#39;independent&#39;</code></td>
      </tr>
      <tr>
        <td><code>z_score_x</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Whether to z-score simulation outputs <span class="arithmatex">\(x\)</span> before passing them into
the network, same options as z_score_theta.</p></td>
        <td><code>&#39;independent&#39;</code></td>
      </tr>
      <tr>
        <td><code>hidden_features</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>num_transforms</code></td>
        <td><code>int</code></td>
        <td><p>Number of transforms when a flow is used. Only relevant if
density estimator is a normalizing flow (i.e. currently either a <code>maf</code> or a
<code>nsf</code>). Ignored if density estimator is a <code>mdn</code> or <code>made</code>.</p></td>
        <td><code>5</code></td>
      </tr>
      <tr>
        <td><code>num_bins</code></td>
        <td><code>int</code></td>
        <td><p>Number of bins used for the splines in <code>nsf</code>. Ignored if density
estimator not <code>nsf</code>.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>embedding_net</code></td>
        <td><code>Module</code></td>
        <td><p>Optional embedding network for parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><code>Identity()</code></td>
      </tr>
      <tr>
        <td><code>num_components</code></td>
        <td><code>int</code></td>
        <td><p>Number of mixture components for a mixture of Gaussians.
Ignored if density estimator is not an mdn.</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>kwargs</code></td>
        <td></td>
        <td><p>additional custom arguments passed to downstream build functions.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">likelihood_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">num_transforms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">num_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">embedding_net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function that builds a density estimator for learning the likelihood.</span>

<span class="sd">    This function will usually be used for SNLE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of density estimator that will be created. One of [`mdn`,</span>
<span class="sd">            `made`, `maf`, `nsf`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network, can take one of the following:</span>
<span class="sd">            - `none`, or None: do not z-score.</span>
<span class="sd">            - `independent`: z-score each dimension independently.</span>
<span class="sd">            - `structured`: treat dimensions as related, therefore compute mean and std</span>
<span class="sd">            over the entire batch, instead of per-dimension. Should be used when each</span>
<span class="sd">            sample is, for example, a time series or an image.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network, same options as z_score_theta.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        num_transforms: Number of transforms when a flow is used. Only relevant if</span>
<span class="sd">            density estimator is a normalizing flow (i.e. currently either a `maf` or a</span>
<span class="sd">            `nsf`). Ignored if density estimator is a `mdn` or `made`.</span>
<span class="sd">        num_bins: Number of bins used for the splines in `nsf`. Ignored if density</span>
<span class="sd">            estimator not `nsf`.</span>
<span class="sd">        embedding_net: Optional embedding network for parameters $\theta$.</span>
<span class="sd">        num_components: Number of mixture components for a mixture of Gaussians.</span>
<span class="sd">            Ignored if density estimator is not an mdn.</span>
<span class="sd">        kwargs: additional custom arguments passed to downstream build functions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">&quot;z_score_x&quot;</span><span class="p">,</span>
                <span class="s2">&quot;z_score_y&quot;</span><span class="p">,</span>
                <span class="s2">&quot;hidden_features&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_transforms&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_bins&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embedding_net&quot;</span><span class="p">,</span>
                <span class="s2">&quot;num_components&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">num_transforms</span><span class="p">,</span>
                <span class="n">num_bins</span><span class="p">,</span>
                <span class="n">embedding_net</span><span class="p">,</span>
                <span class="n">num_components</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mdn&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mdn</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;made&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_made</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;maf&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_maf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;nsf&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_nsf</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mnle&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mnle</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">return</span> <span class="n">build_fn</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.utils.get_nn_models.classifier_nn">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_nn_models</span><span class="o">.</span><span class="n">classifier_nn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z_score_theta</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">z_score_x</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">embedding_net_theta</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="n">embedding_net_x</span><span class="o">=</span><span class="n">Identity</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.utils.get_nn_models.classifier_nn" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Returns a function that builds a classifier for learning density ratios.</p>
<p>This function will usually be used for SNRE. The returned function is to be passed
to the inference class when using the flexible interface.</p>
<p>Note that in the view of the SNRE classifier we build below, x=theta and y=x.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>model</code></td>
        <td><code>str</code></td>
        <td><p>The type of classifier that will be created. One of [<code>linear</code>, <code>mlp</code>,
<code>resnet</code>].</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>z_score_theta</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Whether to z-score parameters <span class="arithmatex">\(\theta\)</span> before passing them into
the network, can take one of the following:
- <code>none</code>, or None: do not z-score.
- <code>independent</code>: z-score each dimension independently.
- <code>structured</code>: treat dimensions as related, therefore compute mean and std
over the entire batch, instead of per-dimension. Should be used when each
sample is, for example, a time series or an image.</p></td>
        <td><code>&#39;independent&#39;</code></td>
      </tr>
      <tr>
        <td><code>z_score_x</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Whether to z-score simulation outputs <span class="arithmatex">\(x\)</span> before passing them into
the network, same options as z_score_theta.</p></td>
        <td><code>&#39;independent&#39;</code></td>
      </tr>
      <tr>
        <td><code>hidden_features</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>embedding_net_theta</code></td>
        <td><code>Module</code></td>
        <td><p>Optional embedding network for parameters <span class="arithmatex">\(\theta\)</span>.</p></td>
        <td><code>Identity()</code></td>
      </tr>
      <tr>
        <td><code>embedding_net_x</code></td>
        <td><code>Module</code></td>
        <td><p>Optional embedding network for simulation outputs <span class="arithmatex">\(x\)</span>. This
embedding net allows to learn features from potentially high-dimensional
simulation outputs.</p></td>
        <td><code>Identity()</code></td>
      </tr>
      <tr>
        <td><code>kwargs</code></td>
        <td></td>
        <td><p>additional custom arguments passed to downstream build functions.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/utils/get_nn_models.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">classifier_nn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">z_score_theta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">z_score_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;independent&quot;</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">embedding_net_theta</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">embedding_net_x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function that builds a classifier for learning density ratios.</span>

<span class="sd">    This function will usually be used for SNRE. The returned function is to be passed</span>
<span class="sd">    to the inference class when using the flexible interface.</span>

<span class="sd">    Note that in the view of the SNRE classifier we build below, x=theta and y=x.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The type of classifier that will be created. One of [`linear`, `mlp`,</span>
<span class="sd">            `resnet`].</span>
<span class="sd">        z_score_theta: Whether to z-score parameters $\theta$ before passing them into</span>
<span class="sd">            the network, can take one of the following:</span>
<span class="sd">            - `none`, or None: do not z-score.</span>
<span class="sd">            - `independent`: z-score each dimension independently.</span>
<span class="sd">            - `structured`: treat dimensions as related, therefore compute mean and std</span>
<span class="sd">            over the entire batch, instead of per-dimension. Should be used when each</span>
<span class="sd">            sample is, for example, a time series or an image.</span>
<span class="sd">        z_score_x: Whether to z-score simulation outputs $x$ before passing them into</span>
<span class="sd">            the network, same options as z_score_theta.</span>
<span class="sd">        hidden_features: Number of hidden features.</span>
<span class="sd">        embedding_net_theta:  Optional embedding network for parameters $\theta$.</span>
<span class="sd">        embedding_net_x:  Optional embedding network for simulation outputs $x$. This</span>
<span class="sd">            embedding net allows to learn features from potentially high-dimensional</span>
<span class="sd">            simulation outputs.</span>
<span class="sd">        kwargs: additional custom arguments passed to downstream build functions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s2">&quot;z_score_x&quot;</span><span class="p">,</span>
                <span class="s2">&quot;z_score_y&quot;</span><span class="p">,</span>
                <span class="s2">&quot;hidden_features&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embedding_net_x&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embedding_net_y&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="p">(</span>
                <span class="n">z_score_theta</span><span class="p">,</span>
                <span class="n">z_score_x</span><span class="p">,</span>
                <span class="n">hidden_features</span><span class="p">,</span>
                <span class="n">embedding_net_theta</span><span class="p">,</span>
                <span class="n">embedding_net_x</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_fn</span><span class="p">(</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_linear_classifier</span><span class="p">(</span>
                <span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;mlp&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_mlp_classifier</span><span class="p">(</span><span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;resnet&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">build_resnet_classifier</span><span class="p">(</span>
                <span class="n">batch_x</span><span class="o">=</span><span class="n">batch_theta</span><span class="p">,</span> <span class="n">batch_y</span><span class="o">=</span><span class="n">batch_x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">return</span> <span class="n">build_fn</span>
</code></pre></div>
        </details>
    </div>

  </div>

<h2 id="potentials">Potentials<a class="headerlink" href="#potentials" title="Permanent link">&para;</a></h2>


  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">potentials</span><span class="o">.</span><span class="n">posterior_based_potential</span><span class="o">.</span><span class="n">posterior_estimator_based_potential</span><span class="p">(</span><span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Returns the potential for posterior-based methods.</p>
<p>It also returns a transformation that can be used to transform the potential into
unconstrained space.</p>
<p>The potential is the same as the log-probability of the <code>posterior_estimator</code>, but
it is set to <span class="arithmatex">\(-\inf\)</span> outside of the prior bounds.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>posterior_estimator</code></td>
        <td><code>Module</code></td>
        <td><p>The neural network modelling the posterior.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Distribution</code></td>
        <td><p>The prior distribution.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>x_o</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>The observed data at which to evaluate the posterior.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Callable, torch Transform]</code></td>
      <td><p>The potential function and a transformation that maps
to unconstrained space.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/potentials/posterior_based_potential.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">posterior_estimator_based_potential</span><span class="p">(</span>
    <span class="n">posterior_estimator</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">TorchTransform</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the potential for posterior-based methods.</span>

<span class="sd">    It also returns a transformation that can be used to transform the potential into</span>
<span class="sd">    unconstrained space.</span>

<span class="sd">    The potential is the same as the log-probability of the `posterior_estimator`, but</span>
<span class="sd">    it is set to $-\inf$ outside of the prior bounds.</span>

<span class="sd">    Args:</span>
<span class="sd">        posterior_estimator: The neural network modelling the posterior.</span>
<span class="sd">        prior: The prior distribution.</span>
<span class="sd">        x_o: The observed data at which to evaluate the posterior.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The potential function and a transformation that maps</span>
<span class="sd">        to unconstrained space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">potential_fn</span> <span class="o">=</span> <span class="n">PosteriorBasedPotential</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">potentials</span><span class="o">.</span><span class="n">likelihood_based_potential</span><span class="o">.</span><span class="n">likelihood_estimator_based_potential</span><span class="p">(</span><span class="n">likelihood_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Returns potential <span class="arithmatex">\(\log(p(x_o|\theta)p(\theta))\)</span> for likelihood-based methods.</p>
<p>It also returns a transformation that can be used to transform the potential into
unconstrained space.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>likelihood_estimator</code></td>
        <td><code>Module</code></td>
        <td><p>The neural network modelling the likelihood.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Distribution</code></td>
        <td><p>The prior distribution.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>x_o</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>The observed data at which to evaluate the likelihood.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Callable, torch Transform]</code></td>
      <td><p>The potential function <span class="arithmatex">\(p(x_o|\theta)p(\theta)\)</span> and a transformation that maps
to unconstrained space.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/potentials/likelihood_based_potential.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">likelihood_estimator_based_potential</span><span class="p">(</span>
    <span class="n">likelihood_estimator</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">TorchTransform</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns potential $\log(p(x_o|\theta)p(\theta))$ for likelihood-based methods.</span>

<span class="sd">    It also returns a transformation that can be used to transform the potential into</span>
<span class="sd">    unconstrained space.</span>

<span class="sd">    Args:</span>
<span class="sd">        likelihood_estimator: The neural network modelling the likelihood.</span>
<span class="sd">        prior: The prior distribution.</span>
<span class="sd">        x_o: The observed data at which to evaluate the likelihood.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The potential function $p(x_o|\theta)p(\theta)$ and a transformation that maps</span>
<span class="sd">        to unconstrained space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">likelihood_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">potential_fn</span> <span class="o">=</span> <span class="n">LikelihoodBasedPotential</span><span class="p">(</span>
        <span class="n">likelihood_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">potentials</span><span class="o">.</span><span class="n">ratio_based_potential</span><span class="o">.</span><span class="n">ratio_estimator_based_potential</span><span class="p">(</span><span class="n">ratio_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Returns the potential for ratio-based methods.</p>
<p>It also returns a transformation that can be used to transform the potential into
unconstrained space.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>ratio_estimator</code></td>
        <td><code>Module</code></td>
        <td><p>The neural network modelling likelihood-to-evidence ratio.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prior</code></td>
        <td><code>Distribution</code></td>
        <td><p>The prior distribution.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>x_o</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>The observed data at which to evaluate the likelihood-to-evidence ratio.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Callable, torch Transform]</code></td>
      <td><p>The potential function and a transformation that maps
to unconstrained space.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>sbi/inference/potentials/ratio_based_potential.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">ratio_estimator_based_potential</span><span class="p">(</span>
    <span class="n">ratio_estimator</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">TorchTransform</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the potential for ratio-based methods.</span>

<span class="sd">    It also returns a transformation that can be used to transform the potential into</span>
<span class="sd">    unconstrained space.</span>

<span class="sd">    Args:</span>
<span class="sd">        ratio_estimator: The neural network modelling likelihood-to-evidence ratio.</span>
<span class="sd">        prior: The prior distribution.</span>
<span class="sd">        x_o: The observed data at which to evaluate the likelihood-to-evidence ratio.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The potential function and a transformation that maps</span>
<span class="sd">        to unconstrained space.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">ratio_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">potential_fn</span> <span class="o">=</span> <span class="n">RatioBasedPotential</span><span class="p">(</span><span class="n">ratio_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span>
</code></pre></div>
        </details>
    </div>

  </div>

<h2 id="analysis">Analysis<a class="headerlink" href="#analysis" title="Permanent link">&para;</a></h2>


  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.analysis.plot.pairplot">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">points_colors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#1f77b4&#39;</span><span class="p">,</span> <span class="s1">&#39;#ff7f0e&#39;</span><span class="p">,</span> <span class="s1">&#39;#2ca02c&#39;</span><span class="p">,</span> <span class="s1">&#39;#d62728&#39;</span><span class="p">,</span> <span class="s1">&#39;#9467bd&#39;</span><span class="p">,</span> <span class="s1">&#39;#8c564b&#39;</span><span class="p">,</span> <span class="s1">&#39;#e377c2&#39;</span><span class="p">,</span> <span class="s1">&#39;#7f7f7f&#39;</span><span class="p">,</span> <span class="s1">&#39;#bcbd22&#39;</span><span class="p">,</span> <span class="s1">&#39;#17becf&#39;</span><span class="p">],</span> <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.analysis.plot.pairplot" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Plot samples in a 2D grid showing marginals and pairwise marginals.</p>
<p>Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution
that the samples were drawn from. Each upper-diagonal plot can be interpreted as a
2D-marginal of the distribution.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>samples</code></td>
        <td><code>Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]</code></td>
        <td><p>Samples used to build the histogram.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>points</code></td>
        <td><code>Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]</code></td>
        <td><p>List of additional points to scatter.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>limits</code></td>
        <td><code>Union[List, torch.Tensor]</code></td>
        <td><p>Array containing the plot xlim for each parameter dimension. If None,
just use the min and max of the passed samples</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>subset</code></td>
        <td><code>Optional[List[int]]</code></td>
        <td><p>List containing the dimensions to plot. E.g. subset=[1,3] will plot
plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and,
if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on).</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>upper</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Plotting style for upper diagonal, {hist, scatter, contour, cond, None}.</p></td>
        <td><code>&#39;hist&#39;</code></td>
      </tr>
      <tr>
        <td><code>diag</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Plotting style for diagonal, {hist, cond, None}.</p></td>
        <td><code>&#39;hist&#39;</code></td>
      </tr>
      <tr>
        <td><code>figsize</code></td>
        <td><code>Tuple</code></td>
        <td><p>Size of the entire figure.</p></td>
        <td><code>(10, 10)</code></td>
      </tr>
      <tr>
        <td><code>labels</code></td>
        <td><code>Optional[List[str]]</code></td>
        <td><p>List of strings specifying the names of the parameters.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>ticks</code></td>
        <td><code>Union[List, torch.Tensor]</code></td>
        <td><p>Position of the ticks.</p></td>
        <td><code>[]</code></td>
      </tr>
      <tr>
        <td><code>points_colors</code></td>
        <td><code>List[str]</code></td>
        <td><p>Colors of the <code>points</code>.</p></td>
        <td><code>[&#39;#1f77b4&#39;, &#39;#ff7f0e&#39;, &#39;#2ca02c&#39;, &#39;#d62728&#39;, &#39;#9467bd&#39;, &#39;#8c564b&#39;, &#39;#e377c2&#39;, &#39;#7f7f7f&#39;, &#39;#bcbd22&#39;, &#39;#17becf&#39;]</code></td>
      </tr>
      <tr>
        <td><code>fig</code></td>
        <td></td>
        <td><p>matplotlib figure to plot on.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>axes</code></td>
        <td></td>
        <td><p>matplotlib axes corresponding to fig.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>Additional arguments to adjust the plot, see the source code in
<code>_get_default_opts()</code> in <code>sbi.utils.plot</code> for more details.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: figure and axis of posterior distribution plot</p>

        <details class="quote">
          <summary>Source code in <code>sbi/analysis/plot.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">pairplot</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">limits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">upper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">,</span>
    <span class="n">diag</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">,</span>
    <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ticks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">points_colors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;axes.prop_cycle&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">&quot;color&quot;</span><span class="p">],</span>
    <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot samples in a 2D grid showing marginals and pairwise marginals.</span>

<span class="sd">    Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution</span>
<span class="sd">    that the samples were drawn from. Each upper-diagonal plot can be interpreted as a</span>
<span class="sd">    2D-marginal of the distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        samples: Samples used to build the histogram.</span>
<span class="sd">        points: List of additional points to scatter.</span>
<span class="sd">        limits: Array containing the plot xlim for each parameter dimension. If None,</span>
<span class="sd">            just use the min and max of the passed samples</span>
<span class="sd">        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot</span>
<span class="sd">            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,</span>
<span class="sd">            if they exist, the 4th, 5th and so on).</span>
<span class="sd">        upper: Plotting style for upper diagonal, {hist, scatter, contour, cond, None}.</span>
<span class="sd">        diag: Plotting style for diagonal, {hist, cond, None}.</span>
<span class="sd">        figsize: Size of the entire figure.</span>
<span class="sd">        labels: List of strings specifying the names of the parameters.</span>
<span class="sd">        ticks: Position of the ticks.</span>
<span class="sd">        points_colors: Colors of the `points`.</span>
<span class="sd">        fig: matplotlib figure to plot on.</span>
<span class="sd">        axes: matplotlib axes corresponding to fig.</span>
<span class="sd">        **kwargs: Additional arguments to adjust the plot, see the source code in</span>
<span class="sd">            `_get_default_opts()` in `sbi.utils.plot` for more details.</span>

<span class="sd">    Returns: figure and axis of posterior distribution plot</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># TODO: add color map support</span>
    <span class="c1"># TODO: automatically determine good bin sizes for histograms</span>
    <span class="c1"># TODO: add legend (if legend is True)</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_get_default_opts</span><span class="p">()</span>
    <span class="c1"># update the defaults dictionary by the current values of the variables (passed by</span>
    <span class="c1"># the user)</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="n">prepare_for_plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">limits</span><span class="p">)</span>

    <span class="c1"># Prepare diag/upper/lower</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>
    <span class="c1"># if type(opts[&#39;lower&#39;]) is not list:</span>
    <span class="c1">#    opts[&#39;lower&#39;] = [opts[&#39;lower&#39;] for _ in range(len(samples))]</span>
    <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;lower&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">diag_func</span> <span class="o">=</span> <span class="n">get_diag_func</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">upper_func</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;hist&quot;</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;hist2d&quot;</span><span class="p">:</span>
                    <span class="n">hist</span><span class="p">,</span> <span class="n">xedges</span><span class="p">,</span> <span class="n">yedges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram2d</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                        <span class="nb">range</span><span class="o">=</span><span class="p">[</span>
                            <span class="p">[</span><span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span>
                            <span class="p">[</span><span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span>
                        <span class="p">],</span>
                        <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;hist_offdiag&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
                        <span class="n">hist</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                        <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span>
                        <span class="n">extent</span><span class="o">=</span><span class="p">(</span>
                            <span class="n">xedges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">xedges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">yedges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">yedges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="p">),</span>
                        <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="s2">&quot;kde&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;kde2d&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;contour&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;contourf&quot;</span><span class="p">,</span>
                <span class="p">]:</span>
                    <span class="n">density</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="n">row</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                        <span class="n">bw_method</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;kde_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;bw_method&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                            <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;kde_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;bins&quot;</span><span class="p">],</span>
                        <span class="p">),</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
                            <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;kde_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;bins&quot;</span><span class="p">],</span>
                        <span class="p">),</span>
                    <span class="p">)</span>
                    <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
                    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">density</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;kde&quot;</span> <span class="ow">or</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;kde2d&quot;</span><span class="p">:</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
                            <span class="n">Z</span><span class="p">,</span>
                            <span class="n">extent</span><span class="o">=</span><span class="p">(</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="p">),</span>
                            <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span>
                            <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;contour&quot;</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;contour_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;percentile&quot;</span><span class="p">]:</span>
                            <span class="n">Z</span> <span class="o">=</span> <span class="n">probs2contours</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;contour_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;levels&quot;</span><span class="p">])</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span>
                            <span class="n">X</span><span class="p">,</span>
                            <span class="n">Y</span><span class="p">,</span>
                            <span class="n">Z</span><span class="p">,</span>
                            <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span>
                            <span class="n">extent</span><span class="o">=</span><span class="p">[</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="p">],</span>
                            <span class="n">colors</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;samples_colors&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                            <span class="n">levels</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;contour_offdiag&quot;</span><span class="p">][</span><span class="s2">&quot;levels&quot;</span><span class="p">],</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;scatter&quot;</span><span class="p">:</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;samples_colors&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                        <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;scatter_offdiag&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;upper&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;plot&quot;</span><span class="p">:</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">col</span><span class="p">],</span>
                        <span class="n">v</span><span class="p">[:,</span> <span class="n">row</span><span class="p">],</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;samples_colors&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">],</span>
                        <span class="o">**</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;plot_offdiag&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">pass</span>

    <span class="k">return</span> <span class="n">_arrange_plots</span><span class="p">(</span>
        <span class="n">diag_func</span><span class="p">,</span> <span class="n">upper_func</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.analysis.plot.marginal_plot">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">marginal_plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">limits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">diag</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">points_colors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#1f77b4&#39;</span><span class="p">,</span> <span class="s1">&#39;#ff7f0e&#39;</span><span class="p">,</span> <span class="s1">&#39;#2ca02c&#39;</span><span class="p">,</span> <span class="s1">&#39;#d62728&#39;</span><span class="p">,</span> <span class="s1">&#39;#9467bd&#39;</span><span class="p">,</span> <span class="s1">&#39;#8c564b&#39;</span><span class="p">,</span> <span class="s1">&#39;#e377c2&#39;</span><span class="p">,</span> <span class="s1">&#39;#7f7f7f&#39;</span><span class="p">,</span> <span class="s1">&#39;#bcbd22&#39;</span><span class="p">,</span> <span class="s1">&#39;#17becf&#39;</span><span class="p">],</span> <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.analysis.plot.marginal_plot" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Plot samples in a row showing 1D marginals of selected dimensions.</p>
<p>Each of the plots can be interpreted as a 1D-marginal of the distribution
that the samples were drawn from.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>samples</code></td>
        <td><code>Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]</code></td>
        <td><p>Samples used to build the histogram.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>points</code></td>
        <td><code>Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]</code></td>
        <td><p>List of additional points to scatter.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>limits</code></td>
        <td><code>Union[List, torch.Tensor]</code></td>
        <td><p>Array containing the plot xlim for each parameter dimension. If None,
just use the min and max of the passed samples</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>subset</code></td>
        <td><code>Optional[List[int]]</code></td>
        <td><p>List containing the dimensions to plot. E.g. subset=[1,3] will plot
plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and,
if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on).</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>diag</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Plotting style for 1D marginals, {hist, kde cond, None}.</p></td>
        <td><code>&#39;hist&#39;</code></td>
      </tr>
      <tr>
        <td><code>figsize</code></td>
        <td><code>Tuple</code></td>
        <td><p>Size of the entire figure.</p></td>
        <td><code>(10, 10)</code></td>
      </tr>
      <tr>
        <td><code>labels</code></td>
        <td><code>Optional[List[str]]</code></td>
        <td><p>List of strings specifying the names of the parameters.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>ticks</code></td>
        <td><code>Union[List, torch.Tensor]</code></td>
        <td><p>Position of the ticks.</p></td>
        <td><code>[]</code></td>
      </tr>
      <tr>
        <td><code>points_colors</code></td>
        <td><code>List[str]</code></td>
        <td><p>Colors of the <code>points</code>.</p></td>
        <td><code>[&#39;#1f77b4&#39;, &#39;#ff7f0e&#39;, &#39;#2ca02c&#39;, &#39;#d62728&#39;, &#39;#9467bd&#39;, &#39;#8c564b&#39;, &#39;#e377c2&#39;, &#39;#7f7f7f&#39;, &#39;#bcbd22&#39;, &#39;#17becf&#39;]</code></td>
      </tr>
      <tr>
        <td><code>fig</code></td>
        <td></td>
        <td><p>matplotlib figure to plot on.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>axes</code></td>
        <td></td>
        <td><p>matplotlib axes corresponding to fig.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>Additional arguments to adjust the plot, see the source code in
<code>_get_default_opts()</code> in <code>sbi.utils.plot</code> for more details.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: figure and axis of posterior distribution plot</p>

        <details class="quote">
          <summary>Source code in <code>sbi/analysis/plot.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">marginal_plot</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">limits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">diag</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">,</span>
    <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ticks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">points_colors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;axes.prop_cycle&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">&quot;color&quot;</span><span class="p">],</span>
    <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot samples in a row showing 1D marginals of selected dimensions.</span>

<span class="sd">    Each of the plots can be interpreted as a 1D-marginal of the distribution</span>
<span class="sd">    that the samples were drawn from.</span>

<span class="sd">    Args:</span>
<span class="sd">        samples: Samples used to build the histogram.</span>
<span class="sd">        points: List of additional points to scatter.</span>
<span class="sd">        limits: Array containing the plot xlim for each parameter dimension. If None,</span>
<span class="sd">            just use the min and max of the passed samples</span>
<span class="sd">        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot</span>
<span class="sd">            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,</span>
<span class="sd">            if they exist, the 4th, 5th and so on).</span>
<span class="sd">        diag: Plotting style for 1D marginals, {hist, kde cond, None}.</span>
<span class="sd">        figsize: Size of the entire figure.</span>
<span class="sd">        labels: List of strings specifying the names of the parameters.</span>
<span class="sd">        ticks: Position of the ticks.</span>
<span class="sd">        points_colors: Colors of the `points`.</span>
<span class="sd">        fig: matplotlib figure to plot on.</span>
<span class="sd">        axes: matplotlib axes corresponding to fig.</span>
<span class="sd">        **kwargs: Additional arguments to adjust the plot, see the source code in</span>
<span class="sd">            `_get_default_opts()` in `sbi.utils.plot` for more details.</span>

<span class="sd">    Returns: figure and axis of posterior distribution plot</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_get_default_opts</span><span class="p">()</span>
    <span class="c1"># update the defaults dictionary by the current values of the variables (passed by</span>
    <span class="c1"># the user)</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span> <span class="o">=</span> <span class="n">prepare_for_plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">limits</span><span class="p">)</span>

    <span class="c1"># Prepare diag/upper/lower</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">opts</span><span class="p">[</span><span class="s2">&quot;diag&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))]</span>

    <span class="n">diag_func</span> <span class="o">=</span> <span class="n">get_diag_func</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_arrange_plots</span><span class="p">(</span>
        <span class="n">diag_func</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.analysis.plot.conditional_pairplot">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">conditional_pairplot</span><span class="p">(</span><span class="n">density</span><span class="p">,</span> <span class="n">condition</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">points_colors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#1f77b4&#39;</span><span class="p">,</span> <span class="s1">&#39;#ff7f0e&#39;</span><span class="p">,</span> <span class="s1">&#39;#2ca02c&#39;</span><span class="p">,</span> <span class="s1">&#39;#d62728&#39;</span><span class="p">,</span> <span class="s1">&#39;#9467bd&#39;</span><span class="p">,</span> <span class="s1">&#39;#8c564b&#39;</span><span class="p">,</span> <span class="s1">&#39;#e377c2&#39;</span><span class="p">,</span> <span class="s1">&#39;#7f7f7f&#39;</span><span class="p">,</span> <span class="s1">&#39;#bcbd22&#39;</span><span class="p">,</span> <span class="s1">&#39;#17becf&#39;</span><span class="p">],</span> <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.analysis.plot.conditional_pairplot" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Plot conditional distribution given all other parameters.</p>
<p>The conditionals can be interpreted as slices through the <code>density</code> at a location
given by <code>condition</code>.</p>
<p>For example:
Say we have a 3D density with parameters <span class="arithmatex">\(\theta_0\)</span>, <span class="arithmatex">\(\theta_1\)</span>, <span class="arithmatex">\(\theta_2\)</span> and
a condition <span class="arithmatex">\(c\)</span> passed by the user in the <code>condition</code> argument.
For the plot of <span class="arithmatex">\(\theta_0\)</span> on the diagonal, this will plot the conditional
<span class="arithmatex">\(p(\theta_0 | \theta_1=c[1], \theta_2=c[2])\)</span>. For the upper
diagonal of <span class="arithmatex">\(\theta_1\)</span> and <span class="arithmatex">\(\theta_2\)</span>, it will plot
<span class="arithmatex">\(p(\theta_1, \theta_2 | \theta_0=c[0])\)</span>. All other diagonals and upper-diagonals
are built in the corresponding way.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>density</code></td>
        <td><code>Any</code></td>
        <td><p>Probability density with a <code>log_prob()</code> method.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>condition</code></td>
        <td><code>Tensor</code></td>
        <td><p>Condition that all but the one/two regarded parameters are fixed to.
The condition should be of shape (1, dim_theta), i.e. it could e.g. be
a sample from the posterior distribution.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>limits</code></td>
        <td><code>Union[List, torch.Tensor]</code></td>
        <td><p>Limits in between which each parameter will be evaluated.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>points</code></td>
        <td><code>Union[List[numpy.ndarray], List[torch.Tensor], numpy.ndarray, torch.Tensor]</code></td>
        <td><p>Additional points to scatter.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>subset</code></td>
        <td><code>Optional[List[int]]</code></td>
        <td><p>List containing the dimensions to plot. E.g. subset=[1,3] will plot
plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and,
if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>resolution</code></td>
        <td><code>int</code></td>
        <td><p>Resolution of the grid at which we evaluate the <code>pdf</code>.</p></td>
        <td><code>50</code></td>
      </tr>
      <tr>
        <td><code>figsize</code></td>
        <td><code>Tuple</code></td>
        <td><p>Size of the entire figure.</p></td>
        <td><code>(10, 10)</code></td>
      </tr>
      <tr>
        <td><code>labels</code></td>
        <td><code>Optional[List[str]]</code></td>
        <td><p>List of strings specifying the names of the parameters.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>ticks</code></td>
        <td><code>Union[List, torch.Tensor]</code></td>
        <td><p>Position of the ticks.</p></td>
        <td><code>[]</code></td>
      </tr>
      <tr>
        <td><code>points_colors</code></td>
        <td><code>List[str]</code></td>
        <td><p>Colors of the <code>points</code>.</p></td>
        <td><code>[&#39;#1f77b4&#39;, &#39;#ff7f0e&#39;, &#39;#2ca02c&#39;, &#39;#d62728&#39;, &#39;#9467bd&#39;, &#39;#8c564b&#39;, &#39;#e377c2&#39;, &#39;#7f7f7f&#39;, &#39;#bcbd22&#39;, &#39;#17becf&#39;]</code></td>
      </tr>
      <tr>
        <td><code>fig</code></td>
        <td></td>
        <td><p>matplotlib figure to plot on.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>axes</code></td>
        <td></td>
        <td><p>matplotlib axes corresponding to fig.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>Additional arguments to adjust the plot, see the source code in
<code>_get_default_opts()</code> in <code>sbi.utils.plot</code> for more details.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: figure and axis of posterior distribution plot</p>

        <details class="quote">
          <summary>Source code in <code>sbi/analysis/plot.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">conditional_pairplot</span><span class="p">(</span>
    <span class="n">density</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">condition</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">limits</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">resolution</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ticks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">points_colors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;axes.prop_cycle&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">&quot;color&quot;</span><span class="p">],</span>
    <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot conditional distribution given all other parameters.</span>

<span class="sd">    The conditionals can be interpreted as slices through the `density` at a location</span>
<span class="sd">    given by `condition`.</span>

<span class="sd">    For example:</span>
<span class="sd">    Say we have a 3D density with parameters $\theta_0$, $\theta_1$, $\theta_2$ and</span>
<span class="sd">    a condition $c$ passed by the user in the `condition` argument.</span>
<span class="sd">    For the plot of $\theta_0$ on the diagonal, this will plot the conditional</span>
<span class="sd">    $p(\theta_0 | \theta_1=c[1], \theta_2=c[2])$. For the upper</span>
<span class="sd">    diagonal of $\theta_1$ and $\theta_2$, it will plot</span>
<span class="sd">    $p(\theta_1, \theta_2 | \theta_0=c[0])$. All other diagonals and upper-diagonals</span>
<span class="sd">    are built in the corresponding way.</span>

<span class="sd">    Args:</span>
<span class="sd">        density: Probability density with a `log_prob()` method.</span>
<span class="sd">        condition: Condition that all but the one/two regarded parameters are fixed to.</span>
<span class="sd">            The condition should be of shape (1, dim_theta), i.e. it could e.g. be</span>
<span class="sd">            a sample from the posterior distribution.</span>
<span class="sd">        limits: Limits in between which each parameter will be evaluated.</span>
<span class="sd">        points: Additional points to scatter.</span>
<span class="sd">        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot</span>
<span class="sd">            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,</span>
<span class="sd">            if they exist, the 4th, 5th and so on)</span>
<span class="sd">        resolution: Resolution of the grid at which we evaluate the `pdf`.</span>
<span class="sd">        figsize: Size of the entire figure.</span>
<span class="sd">        labels: List of strings specifying the names of the parameters.</span>
<span class="sd">        ticks: Position of the ticks.</span>
<span class="sd">        points_colors: Colors of the `points`.</span>

<span class="sd">        fig: matplotlib figure to plot on.</span>
<span class="sd">        axes: matplotlib axes corresponding to fig.</span>
<span class="sd">        **kwargs: Additional arguments to adjust the plot, see the source code in</span>
<span class="sd">            `_get_default_opts()` in `sbi.utils.plot` for more details.</span>

<span class="sd">    Returns: figure and axis of posterior distribution plot</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">density</span><span class="o">.</span><span class="n">_device</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">density</span><span class="p">,</span> <span class="s2">&quot;_device&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="c1"># Setting these is required because _pairplot_scaffold will check if opts[&#39;diag&#39;] is</span>
    <span class="c1"># `None`. This would break if opts has no key &#39;diag&#39;. Same for &#39;upper&#39;.</span>
    <span class="n">diag</span> <span class="o">=</span> <span class="s2">&quot;cond&quot;</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="s2">&quot;cond&quot;</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">_get_default_opts</span><span class="p">()</span>
    <span class="c1"># update the defaults dictionary by the current values of the variables (passed by</span>
    <span class="c1"># the user)</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
    <span class="n">opts</span> <span class="o">=</span> <span class="n">_update</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;lower&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">eps_margins</span> <span class="o">=</span> <span class="n">prepare_for_conditional_plot</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="n">diag_func</span> <span class="o">=</span> <span class="n">get_conditional_diag_func</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">eps_margins</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">upper_func</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">p_image</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">eval_conditional_density</span><span class="p">(</span>
                <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;density&quot;</span><span class="p">],</span>
                <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;condition&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                <span class="n">limits</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                <span class="n">row</span><span class="p">,</span>
                <span class="n">col</span><span class="p">,</span>
                <span class="n">resolution</span><span class="o">=</span><span class="n">resolution</span><span class="p">,</span>
                <span class="n">eps_margins1</span><span class="o">=</span><span class="n">eps_margins</span><span class="p">[</span><span class="n">row</span><span class="p">],</span>
                <span class="n">eps_margins2</span><span class="o">=</span><span class="n">eps_margins</span><span class="p">[</span><span class="n">col</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
            <span class="n">p_image</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
            <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span>
            <span class="n">extent</span><span class="o">=</span><span class="p">(</span>
                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                <span class="n">limits</span><span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                <span class="n">limits</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="p">),</span>
            <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">_arrange_plots</span><span class="p">(</span>
        <span class="n">diag_func</span><span class="p">,</span> <span class="n">upper_func</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 class="doc doc-heading" id="sbi.analysis.conditional_density.conditional_corrcoeff">
<code class="codehilite language-python"><span class="n">sbi</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">conditional_density</span><span class="o">.</span><span class="n">conditional_corrcoeff</span><span class="p">(</span><span class="n">density</span><span class="p">,</span> <span class="n">limits</span><span class="p">,</span> <span class="n">condition</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>


<a class="headerlink" href="#sbi.analysis.conditional_density.conditional_corrcoeff" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents first">

      <p>Returns the conditional correlation matrix of a distribution.</p>
<p>To compute the conditional distribution, we condition all but two parameters to
values from <code>condition</code>, and then compute the Pearson correlation
coefficient <span class="arithmatex">\(\rho\)</span> between the remaining two parameters under the distribution
<code>density</code>. We do so for any pair of parameters specified in <code>subset</code>, thus
creating a matrix containing conditional correlations between any pair of
parameters.</p>
<p>If <code>condition</code> is a batch of conditions, this function computes the conditional
correlation matrix for each one of them and returns the mean.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>density</code></td>
        <td><code>Any</code></td>
        <td><p>Probability density function with <code>.log_prob()</code> function.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>limits</code></td>
        <td><code>Tensor</code></td>
        <td><p>Limits within which to evaluate the <code>density</code>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>condition</code></td>
        <td><code>Tensor</code></td>
        <td><p>Values to condition the <code>density</code> on. If a batch of conditions is
passed, we compute the conditional correlation matrix for each of them and
return the average conditional correlation matrix.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>subset</code></td>
        <td><code>Optional[List[int]]</code></td>
        <td><p>Evaluate the conditional distribution only on a subset of dimensions.
If <code>None</code> this function uses all dimensions.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>resolution</code></td>
        <td><code>int</code></td>
        <td><p>Number of grid points on which the conditional distribution is
evaluated. A higher value increases the accuracy of the estimated
correlation but also increases the computational cost.</p></td>
        <td><code>50</code></td>
      </tr>
  </tbody>
</table>      <p>Returns: Average conditional correlation matrix of shape either <code>(num_dim, num_dim)</code>
or <code>(len(subset), len(subset))</code> if <code>subset</code> was specified.</p>

        <details class="quote">
          <summary>Source code in <code>sbi/analysis/conditional_density.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">conditional_corrcoeff</span><span class="p">(</span>
    <span class="n">density</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">limits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">condition</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">resolution</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the conditional correlation matrix of a distribution.</span>

<span class="sd">    To compute the conditional distribution, we condition all but two parameters to</span>
<span class="sd">    values from `condition`, and then compute the Pearson correlation</span>
<span class="sd">    coefficient $\rho$ between the remaining two parameters under the distribution</span>
<span class="sd">    `density`. We do so for any pair of parameters specified in `subset`, thus</span>
<span class="sd">    creating a matrix containing conditional correlations between any pair of</span>
<span class="sd">    parameters.</span>

<span class="sd">    If `condition` is a batch of conditions, this function computes the conditional</span>
<span class="sd">    correlation matrix for each one of them and returns the mean.</span>

<span class="sd">    Args:</span>
<span class="sd">        density: Probability density function with `.log_prob()` function.</span>
<span class="sd">        limits: Limits within which to evaluate the `density`.</span>
<span class="sd">        condition: Values to condition the `density` on. If a batch of conditions is</span>
<span class="sd">            passed, we compute the conditional correlation matrix for each of them and</span>
<span class="sd">            return the average conditional correlation matrix.</span>
<span class="sd">        subset: Evaluate the conditional distribution only on a subset of dimensions.</span>
<span class="sd">            If `None` this function uses all dimensions.</span>
<span class="sd">        resolution: Number of grid points on which the conditional distribution is</span>
<span class="sd">            evaluated. A higher value increases the accuracy of the estimated</span>
<span class="sd">            correlation but also increases the computational cost.</span>

<span class="sd">    Returns: Average conditional correlation matrix of shape either `(num_dim, num_dim)`</span>
<span class="sd">    or `(len(subset), len(subset))` if `subset` was specified.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">density</span><span class="o">.</span><span class="n">_device</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">density</span><span class="p">,</span> <span class="s2">&quot;_device&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="n">subset_</span> <span class="o">=</span> <span class="n">subset</span> <span class="k">if</span> <span class="n">subset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">range</span><span class="p">(</span><span class="n">condition</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">correlation_matrices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cond</span> <span class="ow">in</span> <span class="n">condition</span><span class="p">:</span>
        <span class="n">correlation_matrices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">compute_corrcoeff</span><span class="p">(</span>
                        <span class="n">eval_conditional_density</span><span class="p">(</span>
                            <span class="n">density</span><span class="p">,</span>
                            <span class="n">cond</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                            <span class="n">limits</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                            <span class="n">dim1</span><span class="o">=</span><span class="n">dim1</span><span class="p">,</span>
                            <span class="n">dim2</span><span class="o">=</span><span class="n">dim2</span><span class="p">,</span>
                            <span class="n">resolution</span><span class="o">=</span><span class="n">resolution</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="n">limits</span><span class="p">[[</span><span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">]]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">dim1</span> <span class="ow">in</span> <span class="n">subset_</span>
                    <span class="k">for</span> <span class="n">dim2</span> <span class="ow">in</span> <span class="n">subset_</span>
                    <span class="k">if</span> <span class="n">dim1</span> <span class="o">&lt;</span> <span class="n">dim2</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">average_correlations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">correlation_matrices</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># `average_correlations` is still a vector containing the upper triangular entries.</span>
    <span class="c1"># Below, assemble them into a matrix:</span>
    <span class="n">av_correlation_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset_</span><span class="p">)),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">triu_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span>
        <span class="n">row</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_</span><span class="p">),</span> <span class="n">offset</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="n">av_correlation_matrix</span><span class="p">[</span><span class="n">triu_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">triu_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">average_correlations</span>

    <span class="c1"># Make the matrix symmetric by copying upper diagonal to lower diagonal.</span>
    <span class="n">av_correlation_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">av_correlation_matrix</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span>
        <span class="n">av_correlation_matrix</span><span class="o">.</span><span class="n">T</span>
    <span class="p">)</span>

    <span class="n">av_correlation_matrix</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">av_correlation_matrix</span>
</code></pre></div>
        </details>
    </div>

  </div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../contribute/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Contribute" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Contribute
            </div>
          </div>
        </a>
      
      
        
        <a href="../faq/" class="md-footer__link md-footer__link--next" aria-label="Next: FAQ" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              FAQ
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/mackelab/sbi" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.22074ed6.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.01de222e.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>