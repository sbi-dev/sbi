{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>sbi</code>: simulation-based inference toolkit","text":"<p><code>sbi</code> provides access to simulation-based inference methods via a user-friendly interface:</p> <pre><code>import torch\nfrom sbi.inference import SNPE\n\n# define shifted Gaussian simulator.\ndef simulator(\u03b8): return \u03b8 + torch.randn_like(\u03b8)\n# draw parameters from Gaussian prior.\n\u03b8 = torch.randn(1000, 2)\n# simulate data\nx = simulator(\u03b8)\n\n# choose sbi method and train\ninference = SNPE()\ninference.append_simulations(\u03b8, x).train()\n\n# do inference given observed data\nx_o = torch.ones(2)\nposterior = inference.build_posterior()\nsamples = posterior.sample((1000,), x=x_o)\n</code></pre>"},{"location":"#overview","title":"Overview","text":"<p>To get started, install the <code>sbi</code> package with:</p> <pre><code>pip install sbi\n</code></pre> <p>for more advanced install options, see our Install Guide.</p> <p>Then, check out our material:</p> <ul> <li> <p> Motivation and approach General motivation for the SBI framework and methods included in <code>sbi</code>.</p> </li> <li> <p> Tutorials and Examples Various examples illustrating how to get    started or use the <code>sbi</code> package.</p> </li> <li> <p> Reference API The detailed description of the package classes and functions.</p> </li> <li> <p> Citation How to cite the <code>sbi</code> package.</p> </li> </ul>"},{"location":"#motivation-and-approach","title":"Motivation and approach","text":"<p>Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated.</p> <p>A key challenge in simulation-based science is constraining these simulation models\u2019 parameters, which are interpretable quantities, with observational data. Bayesian inference provides a general and powerful framework to invert the simulators, i.e. describe the parameters that are consistent both with empirical data and prior knowledge.</p> <p>In the case of simulators, a key quantity required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\), is typically intractable, rendering conventional statistical approaches inapplicable.</p> <p><code>sbi</code> implements powerful machine-learning methods that address this problem. Roughly, these algorithms can be categorized as:</p> <ul> <li>Neural Posterior Estimation (amortized <code>NPE</code> and sequential <code>SNPE</code>),</li> <li>Neural Likelihood Estimation (<code>(S)NLE</code>), and</li> <li>Neural Ratio Estimation (<code>(S)NRE</code>).</li> </ul> <p>Depending on the characteristics of the problem, e.g. the dimensionalities of the parameter space and the observation space, one of the methods will be more suitable.</p> <p></p> <p>Goal: Algorithmically identify mechanistic models that are consistent with data.</p> <p>Each of the methods above needs three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and observational data (or summary statistics thereof).</p> <p>The methods then proceed by</p> <ol> <li>sampling parameters from the prior followed by simulating synthetic data from    these parameters,</li> <li>learning the (probabilistic) association between data (or data features) and    underlying parameters, i.e. to learn statistical inference from simulated    data. How this association is learned differs between the above methods, but    all use deep neural networks.</li> <li>This learned neural network is then applied to empirical data to derive the    full space of parameters consistent with the data and the prior, i.e. the    posterior distribution. The posterior assigns high probability to parameters    that are consistent with both the data and the prior, and low probability to    inconsistent parameters. While SNPE directly learns the posterior    distribution, SNLE and SNRE need an extra MCMC sampling step to construct a    posterior.</li> <li>If needed, an initial estimate of the posterior can be used to adaptively    generate additional informative simulations.</li> </ol> <p>See Cranmer, Brehmer, Louppe (2020) for a recent review on simulation-based inference.</p>"},{"location":"#implemented-algorithms","title":"Implemented algorithms","text":"<p><code>sbi</code> implements a variety of amortized and sequential SBI methods.</p> <p>Amortized methods return a posterior that can be applied to many different observations without retraining (e.g., NPE), whereas sequential methods focus the inference on one particular observation to be more simulation-efficient (e.g., SNPE).</p> <p>Below, we list all implemented methods and the corresponding publications. To see how to access these methods in <code>sbi</code>, check out our Inference API\u2019s reference and the tutorial on implemented methods.</p>"},{"location":"#posterior-estimation-snpe","title":"Posterior estimation (<code>(S)NPE</code>)","text":"<ul> <li> <p>Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density   Estimation by Papamakarios &amp; Murray (NeurIPS 2016)   [PDF] [BibTeX]</p> </li> <li> <p>Flexible statistical inference for mechanistic models of neural dynamics  by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher &amp; Macke (NeurIPS   2017)   [PDF] [BibTeX]</p> </li> <li> <p>Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher &amp; Macke (ICML 2019) [PDF] [BibTeX]</p> </li> <li> <p>BayesFlow: Learning complex stochastic models with invertible neural   networks by Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., &amp; K\u00f6the, U. (IEEE transactions on neural networks and learning systems 2020) [Paper]</p> </li> <li> <p>Truncated proposals for scalable and hassle-free simulation-based   inference  by Deistler, Goncalves &amp; Macke (NeurIPS 2022)   [Paper]</p> </li> </ul>"},{"location":"#likelihood-estimation-snle","title":"Likelihood-estimation (<code>(S)NLE</code>)","text":"<ul> <li> <p>Sequential neural likelihood: Fast likelihood-free inference with   autoregressive flows by Papamakarios, Sterratt &amp; Murray (AISTATS 2019)   [PDF] [BibTeX]</p> </li> <li> <p>Variational methods for simulation-based inference  by Gl\u00f6ckler,   Deistler, Macke (ICLR 2022) [Paper]</p> </li> <li> <p>Flexible and efficient simulation-based inference for models of   decision-making  by Boelts, Lueckmann, Gao, Macke (Elife 2022)   [Paper]</p> </li> </ul>"},{"location":"#likelihood-ratio-estimation-snre","title":"Likelihood-ratio-estimation (<code>(S)NRE</code>)","text":"<ul> <li> <p>Likelihood-free MCMC with Amortized Approximate Likelihood Ratios by   Hermans, Begy &amp; Louppe (ICML 2020)   [PDF]</p> </li> <li> <p>On Contrastive Learning for Likelihood-free Inference by Durkan,   Murray &amp; Papamakarios (ICML 2020)   [PDF]</p> </li> <li> <p>Towards Reliable Simulation-Based Inference with Balanced Neural Ratio   Estimation by Delaunoy, Hermans, Rozet, Wehenkel &amp; Louppe (NeurIPS 2022)   [PDF]</p> </li> <li> <p>Contrastive Neural Ratio Estimation by Benjamin Kurt Miller, Christoph   Weniger &amp; Patrick Forr\u00e9 (NeurIPS 2022)   [PDF]</p> </li> </ul>"},{"location":"#diagnostics","title":"Diagnostics","text":"<ul> <li> <p>Simulation-based calibration by Talts, Betancourt, Simpson, Vehtari,   Gelman (arxiv 2018)[Paper]</p> </li> <li> <p>Expected coverage (sample-based) as computed in Deistler, Goncalves, &amp;   Macke (NeurIPS 2022)[Paper] and in   Rozet &amp; Louppe [Paper]</p> </li> <li> <p>Local C2ST by Linhart, Gramfort &amp; Rodrigues (NeurIPS   2023)[Paper]</p> </li> <li> <p>TARP by Lemos, Coogan, Hezaveh &amp; Perreault-Levasseur (ICML   2023)[Paper]</p> </li> </ul>"},{"location":"citation/","title":"Citation","text":"<p>If you use <code>sbi</code> consider citing the sbi software paper, in addition to the original research articles describing the specific sbi-algorithm(s) you are using.</p> <pre><code>@article{tejero-cantero2020sbi,\n  doi = {10.21105/joss.02505},\n  url = {https://doi.org/10.21105/joss.02505},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {52},\n  pages = {2505},\n  author = {Alvaro Tejero-Cantero and Jan Boelts and Michael Deistler and Jan-Matthis Lueckmann and Conor Durkan and Pedro J. Gon\u00e7alves and David S. Greenberg and Jakob H. Macke},\n  title = {sbi: A toolkit for simulation-based inference},\n  journal = {Journal of Open Source Software}\n}\n</code></pre> <p>The above citation refers to the original version of the <code>sbi</code> project and has a persistent DOI. Additionally, new releases of <code>sbi</code> are citable via Zenodo, where we create a new DOI for every release.</p>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others\u2019 confidential information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct that could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Representing our community includes using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting <code>sbi</code> developer Jan Boelts via email (jan.boelts@mailbox.org) or anonymously via https://forms.gle/AEmwEznWAKdvrBQbA. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla\u2019s code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contribute/","title":"How to contribute","text":"<p>Important</p> <p>By participating in the <code>sbi</code> community, all members are expected to comply with our Code of Conduct. This ensures a positive and inclusive environment for everyone involved.</p>"},{"location":"contribute/#user-experiences-bugs-and-feature-requests","title":"User experiences, bugs, and feature requests","text":"<p>If you are using <code>sbi</code> to infer the parameters of a simulator, we would be delighted to know how it worked for you. If it didn\u2019t work according to plan, please open up an issue or discussion and tell us more about your use case: the dimensionality of the input parameters and of the output, as well as the setup you used to run inference (i.e., number of simulations, number of rounds, etc.).</p> <p>To report bugs and suggest features \u2013 including better documentation \u2013 please equally head over to issues on GitHub and tell us everything.</p>"},{"location":"contribute/#contributing-code","title":"Contributing code","text":"<p>Contributions to the <code>sbi</code> package are always welcome! The preferred way to do it is via pull requests onto our main repository. To give credit to contributors, we consider adding contributors who repeatedly and substantially contributed to <code>sbi</code> to the list of authors of the package at the end of every year. Additionally, we mention all contributors in the releases.</p> <p>Note</p> <p>To avoid doing duplicated work, we strongly suggest that you go take a look at our current open issues and pull requests to see if someone else is already doing it. Also, in case you\u2019re planning to work on something that has not yet been proposed by others (e.g. adding a new feature, adding a new example), it is preferable to first open a new issue explaining what you intend to propose and then working on your pull request after getting some feedback from others.</p>"},{"location":"contribute/#contribution-workflow","title":"Contribution workflow","text":"<p>The following steps describe all parts of the workflow for doing a contribution such as installing locally <code>sbi</code> from source, creating a <code>conda</code> environment, setting up your <code>git</code> repository, etc. We\u2019ve taken strong inspiration from the contribution guides of <code>scikit-learn</code> and <code>mne</code>:</p> <p>Step 1: Create an account on GitHub if you do not already have one.</p> <p>Step 2: Fork the project repository: click on the \u2018Fork\u2019 button near the top of the page. This will create a copy of the <code>sbi</code> codebase under your GitHub user account. See more details on how to fork a repository here.</p> <p>Step 3: Clone your fork of the <code>sbi</code> repo from your GitHub account to your local disk: <pre><code>git clone git@github.com:$USERNAME/sbi.git\ncd sbi\n</code></pre></p> <p>Step 4: Install a recent version of Python (we currently recommend 3.10) for instance using <code>miniforge</code>. We strongly recommend you create a specific <code>conda</code> environment for doing development on <code>sbi</code> as per: <pre><code>conda create -n sbi_dev python=3.10\nconda activate sbi_dev\n</code></pre></p> <p>Step 5: Install <code>sbi</code> in editable mode with <pre><code>pip install -e \".[dev]\"\n</code></pre> This installs the <code>sbi</code> package into the current environment by creating a link to the source code directory (instead of copying the code to pip\u2019s <code>site_packages</code> directory, which is what normally happens). This means that any edits you make to the <code>sbi</code> source code will be reflected the next time you open a Python interpreter and <code>import sbi</code> (the <code>-e</code> flag of pip stands for an \u201ceditable\u201d installation, and the <code>dev</code> flag installs development and testing dependencies). This requires at least Python 3.8.</p> <p>Step 6: Add the upstream remote. This saves a reference to the main <code>sbi</code> repository, which you can use to keep your repository synchronized with the latest changes: <pre><code>git remote add upstream git@github.com:sbi-dev/sbi.git\n</code></pre> Check that the upstream and origin remote aliases are configured correctly by running <code>git remote -v</code> which should display: <pre><code>origin  git@github.com:$USERNAME/sbi.git (fetch)\norigin  git@github.com:$USERNAME/sbi.git (push)\nupstream        git@github.com:sbi-dev/sbi.git (fetch)\nupstream        git@github.com:sbi-dev/sbi.git (push)\n</code></pre></p> <p>Step 7: Install <code>pre-commit</code> to run code style checks before each commit: <pre><code>pip install pre-commit\npre-commit install\n</code></pre></p> <p>You should now have a working installation of <code>sbi</code> and a git repository properly configured for making contributions. The following steps describe the process of modifying code and submitting a pull request:</p> <p>Step 8: Synchronize your main branch with the upstream/main branch. See more details on GitHub Docs: <pre><code>git checkout main\ngit fetch upstream\ngit merge upstream/main\n</code></pre></p> <p>Step 9: Create a feature branch to hold your development changes: <pre><code>git checkout -b my_feature\n</code></pre> and start making changes. Always use a feature branch! It\u2019s good practice to never work on the main branch, as this allows you to easily get back to a working state of the code if needed (e.g., if you\u2019re working on multiple changes at once, or need to pull in recent changes from someone else to get your new feature to work properly). In most cases you should make PRs into the upstream\u2019s main branch.</p> <p>Step 10: Develop your code on your feature branch on the computer, using Git to do the version control. When you\u2019re done editing, add changed files using <code>git add</code> and then <code>git commit</code> to record your changes: <pre><code>git add modified_files\ngit commit -m \"description of your commit\"\n</code></pre> Then push the changes to your GitHub account with: <pre><code>git push -u origin my_feature\n</code></pre> The <code>-u</code> flag ensures that your local branch will be automatically linked with the remote branch, so you can later use <code>git push</code> and <code>git pull</code> without any extra arguments.</p> <p>Step 11: Follow these instructions to create a pull request from your fork. This will send a notification to <code>sbi</code> maintainers and trigger reviews and comments regarding your contribution.</p> <p>Note</p> <p>It is often helpful to keep your local feature branch synchronized with the latest changes of the main <code>sbi</code> repository: <pre><code>git fetch upstream\ngit merge upstream/main\n</code></pre></p>"},{"location":"contribute/#style-conventions-and-testing","title":"Style conventions and testing","text":"<p>All our docstrings and comments are written following the Google Style.</p> <p>For code linting and formating, we use <code>ruff</code>, which is installed alongside <code>sbi</code>.</p> <p>You can exclude slow tests and those which require a GPU with <pre><code>pytest -m \"not slow and not gpu\"\n</code></pre> Additionally, we recommend to run tests with <pre><code>pytest -n auto -m \"not slow and not gpu\"\n</code></pre> in parallel. GPU tests should probably not be run this way. If you see unexpected behavior (tests fail if they shouldn\u2019t), try to run them without <code>-n auto</code> and see if it persists. When writing new tests and debugging things, it may make sense to also run them without <code>-n auto</code>.</p> <p>When you create a PR onto <code>main</code>, our Continuous Integration (CI) actions on GitHub will perform the following checks:</p> <ul> <li><code>ruff</code> for linting and formatting   (including <code>black</code>, <code>isort</code>, and <code>flake8</code>)</li> <li><code>pyright</code> for static type checking.</li> <li><code>pytest</code> for running a subset of   fast tests from our test suite.</li> </ul> <p>If any of these fail, try reproducing and solving the error locally:</p> <ul> <li><code>ruff</code>: Make sure you have <code>pre-commit</code> installed locally with the same version as  specified in the  <code>pyproject.toml</code>. Execute it   using <code>pre-commit run --all-files</code>. <code>ruff</code> tends to give informative error messages   that help you fix the problem. Note that pre-commit only detects problems with <code>ruff</code>   linting and formatting, but does not fix them. You can fix them either by running   <code>ruff check . --fix(linting)</code>, followed by <code>ruff format . --fix(formatting)</code>, or by   hand.</li> <li><code>pyright</code>: Run it locally using <code>pyright sbi/</code> and ensure you are using the same   <code>pyright</code> version as used in the CI (which is the case if you have installed   it with <code>pip install -e \".[dev]\"</code> but note that you have to rerun it once   someone updates the version in the <code>pyproject.toml</code>).</li> <li>Known issues and fixes:<ul> <li>If using <code>**kwargs</code>, you either have to specify all possible types of <code>kwargs</code>, e.g. <code>**kwargs: Union[int, boolean]</code> or use <code>**kwargs: Any</code></li> </ul> </li> <li><code>pytest</code>: On GitHub Actions you can see which test failed. Reproduce it locally, e.g., using <code>pytest -n auto tests/linearGaussian_snpe_test.py</code>. Note that this will run for a few minutes and should result in passes and expected fails (xfailed).</li> <li>Commit and push again until CI tests pass. Don\u2019t hesitate to ask for help by   commenting on the PR.</li> </ul>"},{"location":"contribute/#contributing-to-the-documentation","title":"Contributing to the documentation","text":"<p>Most of the documentation for <code>sbi</code> is written in markdown and the website is generated using <code>mkdocs</code> with <code>mkdocstrings</code> and <code>mike</code>. The tutorials and examples are converted from jupyter notebooks into markdown files to be shown on the website. To work on improvements of the documentation, you should first  install the <code>doc</code> dependencies:</p> <pre><code>pip install -e \".[doc]\"\n</code></pre> <p>Then, you can build the website locally by executing in the <code>docs</code> folder</p> <pre><code>mike serve\n</code></pre> <p>This will build the website on a local host address shown in the terminal. Changes to the website files or a browser refresh will immediately rebuild the website.</p> <p>If you want to build the latest version of the tutorial notebooks, you need to convert them to markdown first:</p> <pre><code>cd docs\njupyter nbconvert --to markdown ../examples/*.ipynb --output-dir docs/examples/\njupyter nbconvert --to markdown ../tutorials/*.ipynb --output-dir docs/tutorials/\nmike serve\n</code></pre>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#community-and-contributions","title":"Community and Contributions","text":"<p><code>sbi</code> is a community-driven package. We are grateful to all our contributors who have played a significant role in shaping <code>sbi</code>. Their valuable input, suggestions, and direct contributions to the codebase have been instrumental in the development of <code>sbi</code>.</p>"},{"location":"credits/#license","title":"License","text":"<p><code>sbi</code> is licensed under the Apache License (Apache-2.0) and</p> <p>Copyright (C) 2020 \u00c1lvaro Tejero-Cantero, Jakob H. Macke, Jan-Matthis L\u00fcckmann, Michael Deistler, Jan F. B\u00f6lts.</p> <p>Copyright (C) 2020 Conor M. Durkan.</p> <p>All contributors hold the copyright of their specific contributions.</p>"},{"location":"credits/#support","title":"Support","text":"<p><code>sbi</code> has been supported by the German Federal Ministry of Education and Research (BMBF) through project ADIMEM (FKZ 01IS18052 A-D), project SiMaLeSAM (FKZ 01IS21055A) and the T\u00fcbingen AI Center (FKZ 01IS18039A). Since 2024, <code>sbi</code> has been supported by the appliedAI Institute for Europe gGmbH.</p> <p></p>"},{"location":"credits/#important-dependencies-and-prior-art","title":"Important dependencies and prior art","text":"<ul> <li><code>sbi</code> is the successor to <code>delfi</code>, a Theano-based   toolbox for sequential neural posterior estimation developed at   mackelab.If you were using <code>delfi</code>, we strongly recommend   moving your inference over to <code>sbi</code>. Please open issues if you find unexpected   behavior or missing features. We will consider these bugs and give them priority.</li> <li><code>sbi</code> as a PyTorch-based toolbox started as a fork of   conormdurkan/lfi, by Conor   M.Durkan.</li> <li><code>sbi</code> uses <code>PyTorch</code> and tries to align with the interfaces (e.g. for probability   distributions) adopted by <code>PyTorch</code>.</li> <li>See README.md for a   list of publications describing the methods implemented in <code>sbi</code>.</li> </ul>"},{"location":"faq/","title":"Frequently asked questions","text":"<ol> <li>What should I do when my \u2018posterior samples are outside of the prior support\u2019 in SNPE?</li> <li>Can the algorithms deal with invalid data, e.g., NaN or inf?</li> <li>When using multiple workers, I get a pickling error. Can I still use multiprocessing?</li> <li>Can I use the GPU for training the density estimator?</li> <li>How should I save and load objects in <code>sbi</code>?</li> <li>Can I stop neural network training and resume it later?</li> <li>How can I use a prior that is not defined in PyTorch?</li> </ol> <p>See also discussion page and issue tracker on the <code>sbi</code> GitHub repository for recent questions and problems.</p>"},{"location":"install/","title":"Installation","text":"<p><code>sbi</code> requires Python 3.8 or higher. A GPU is not required, but can lead to speed-up in some cases. We recommend using a <code>conda</code> virtual environment (Miniconda installation instructions). If <code>conda</code> is installed on the system, an environment for installing <code>sbi</code> can be created as follows:</p> <pre><code># Create an environment for sbi (indicate Python 3.8 or higher); activate it\n$ conda create -n sbi_env python=3.10 &amp;&amp; conda activate sbi_env\n</code></pre> <p>Independent of whether you are using <code>conda</code> or not, <code>sbi</code> can be installed using <code>pip</code>:</p> <pre><code>pip install sbi\n</code></pre> <p>To test the installation, drop into a Python prompt and run</p> <pre><code>from sbi.examples.minimal import simple\nposterior = simple()\nprint(posterior)\n</code></pre>"},{"location":"examples/00_HH_simulator/","title":"Inference on Hodgkin-Huxley model: tutorial","text":"<p>In this tutorial, we use <code>sbi</code> to do inference on a Hodgkin-Huxley model from neuroscience (Hodgkin and Huxley, 1952). </p> <p>We want to infer the posterior distribution of two parameters (\\(\\bar g_{Na}\\),\\(\\bar g_K\\)) based on a current-clamp recording, that we generate synthetically (in practice, this would be an experimental observation).</p> <p>Note, you find the original version of this notebook at https://github.com/sbi-dev/sbi/blob/main/examples/00_HH_simulator.ipynb in the <code>sbi</code> repository.</p> <p>First we are going to import basic packages.</p> <pre><code># visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom sbi import analysis as analysis\n\n# sbi\nfrom sbi import utils as utils\nfrom sbi.inference import SNPE, simulate_for_sbi\nfrom sbi.utils.user_input_checks import (\n    check_sbi_inputs,\n    process_prior,\n    process_simulator,\n)\n</code></pre> <pre><code># remove top and right axis from plots\nmpl.rcParams[\"axes.spines.right\"] = False\nmpl.rcParams[\"axes.spines.top\"] = False\n</code></pre>"},{"location":"examples/00_HH_simulator/#different-required-components","title":"Different required components","text":"<p>Before running inference, let us define the different required components: 1. observational data  - the observations in this case a simulated volatge trace (or summary statistics thereof)  1. a candidate (mechanistic) model - the simulator in this case the Hodgkin-Huxley model 1. the - prior over the model parameters in this case over (\\(\\bar g_{Na}\\),\\(\\bar g_K\\))</p> <p>Note: that you do not need to fully understand the details of the HH-model and model specific jargon to get an intuition for how SBI works in this scientific use case. </p>"},{"location":"examples/00_HH_simulator/#1-observed-data","title":"1. Observed data","text":"<p>Let us assume we current-clamped a neuron and recorded the following voltage trace:</p> <p> </p> <p>In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters (\\(\\bar g_{Na}\\),\\(\\bar g_K\\)). We will come back to this point later in the tutorial.</p>"},{"location":"examples/00_HH_simulator/#2-simulator","title":"2. Simulator","text":"<p>We would like to infer the posterior over the two parameters (\\(\\color{orange}{\\bar g_{Na}}\\),\\(\\color{orange}{\\bar g_K}\\)) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008, and is defined by the following set of differential equations (parameters of interest highlighted in orange):</p> \\[ \\scriptsize \\begin{align} \\color{black}{C_m\\frac{dV}{dt}}&amp; \\color{black}{=g_1\\left(E_1-V\\right)}+                     \\color{orange}{\\bar{g}_{Na}} \\color{black}{m^3h\\left(E_{Na}-V\\right)+}                     \\color{orange}{\\bar{g}_{K}} \\color{black}{n^4\\left(E_K-V\\right)+\\bar{g}_Mp\\left(E_K-V\\right)+I_{inj}+\\sigma\\eta\\left(t\\right)}\\\\                     \\color{black}{\\frac{dq}{dt}}&amp;\\color{black}{=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in\\{m,h,n,p\\}} \\end{align} \\] <p>Above, \\(V\\) represents the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) (\\(\\text{Na}^+\\), \\(\\text{K}^+\\), M), \\(E_c\\) is the reversal potential of \\(c\\), (\\(m\\), \\(h\\), \\(n\\), \\(p\\)) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\). Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\), given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008).</p> <p>The input current \\(I_{\\text{inj}}\\) is defined as</p> <pre><code>from HH_helper_functions import syn_current\n\n# current, onset time of stimulation, offset time of stimulation, time step, time, area of some\nI_inj, t_on, t_off, dt, t, A_soma = syn_current()\n</code></pre> <p>The Hodgkin-Huxley simulator takes the parameters as input together with other arguments such as the initial voltage state, the integration timestep, time and injected current:</p> <pre><code>from HH_helper_functions import HHsimulator\n</code></pre> <p>Putting the input current and the simulator together:</p> <pre><code>def run_HH_model(params):\n\n    params = np.asarray(params)\n\n    # input current, time step\n    I_inj, t_on, t_off, dt, t, A_soma = syn_current()\n\n    t = np.arange(0, len(I_inj), 1) * dt\n\n    # initial voltage V0\n    initial_voltage = -70\n\n    voltage_trace = HHsimulator(initial_voltage, params.reshape(1, -1), dt, t, I_inj)\n\n    return dict(data=voltage_trace.reshape(-1), time=t, dt=dt, I_inj=I_inj.reshape(-1))\n</code></pre> <p>To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters (\\(\\bar g_{Na}\\),\\(\\bar g_K\\)), given the input current \\(I_{\\text{inj}}\\):</p> <pre><code># three sets of (g_Na, g_K)\nparams = np.array([[50.0, 1.0], [4.0, 1.5], [20.0, 15.0]])\n\nnum_samples = len(params[:, 0])\nsim_samples = np.zeros((num_samples, len(I_inj)))\nfor i in range(num_samples):\n    sim_samples[i, :] = run_HH_model(params=params[i, :])[\"data\"]\n</code></pre> <pre><code># colors for traces\ncol_min = 2\nnum_colors = num_samples + col_min\ncm1 = mpl.cm.Blues\ncol1 = [cm1(1.0 * i / num_colors) for i in range(col_min, num_colors)]\n\nfig = plt.figure(figsize=(7, 5))\ngs = mpl.gridspec.GridSpec(2, 1, height_ratios=[4, 1])\nax = plt.subplot(gs[0])\n# plot the three voltage traces for different parameter sets\nfor i in range(num_samples):\n    plt.plot(t, sim_samples[i, :], color=col1[i], lw=2)\nplt.ylabel(\"voltage (mV)\")\nax.set_xticks([])\nax.set_yticks([-80, -20, 40])\n\n# plot the injected current\nax = plt.subplot(gs[1])\nplt.plot(t, I_inj * A_soma * 1e3, \"k\", lw=2)\nplt.xlabel(\"time (ms)\")\nplt.ylabel(\"input (nA)\")\n\nax.set_xticks([0, max(t) / 2, max(t)])\nax.set_yticks([0, 1.1 * np.max(I_inj * A_soma * 1e3)])\nax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(\"%.2f\"))\nplt.show()\n</code></pre> <p></p> <p>As can be seen, the voltage traces can be quite diverse for different parameter values.</p> <p>Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin-Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first four voltage moments: mean, standard deviation, skewness and kurtosis. Using the function <code>calculate_summary_statistics()</code> imported below, we obtain these statistics from the output of the Hodgkin Huxley simulator.</p> <pre><code>from HH_helper_functions import calculate_summary_statistics\n</code></pre> <p>Note: the summary features depend on the simulator and observations under investigation. Check out our tutorials on crafting summary statistics. </p> <p>Lastly, we define a function that performs all of the above steps at once, to have one object we pass to the inference method as our simulator. The function <code>simulation_wrapper</code> takes in the parameters, runs the Hodgkin Huxley model and then returns the summary statistics.</p> <pre><code>def simulation_wrapper(params):\n    \"\"\"\n    Returns summary statistics from conductance values in `params`.\n\n    Summarizes the output of the HH simulator and converts it to `torch.Tensor`.\n    \"\"\"\n    obs = run_HH_model(params)\n    summstats = torch.as_tensor(calculate_summary_statistics(obs))\n    return summstats\n</code></pre> <p>Note: <code>sbi</code> takes any function as simulator. Thus, <code>sbi</code> also has the flexibility to use simulators that utilize external packages, e.g., Brian (http://briansimulator.org/), nest (https://www.nest-simulator.org/), or NEURON (https://neuron.yale.edu/neuron/). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a Python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to.</p>"},{"location":"examples/00_HH_simulator/#3-prior-over-model-parameters","title":"3. Prior over model parameters","text":"<p>Now that we have the simulator, we need to define a function with the prior over the model parameters (\\(\\bar g_{Na}\\),\\(\\bar g_K\\)), which in this case is chosen to be a Uniform distribution:</p> <p>Note: This is where you would incorporate prior knowlegde about the parameters you want to infer, e.g., ranges known from literature. </p> <pre><code>prior_min = [0.5, 1e-4]\nprior_max = [80.0, 15.0]\nprior = utils.torchutils.BoxUniform(\n    low=torch.as_tensor(prior_min), high=torch.as_tensor(prior_max)\n)\n\n# Check prior, simulator, consistency\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\nsimulation_wrapper = process_simulator(simulation_wrapper, prior, prior_returns_numpy)\ncheck_sbi_inputs(simulation_wrapper, prior)\n</code></pre>"},{"location":"examples/00_HH_simulator/#running-inference","title":"Running inference","text":"<p>Now that we have all the required components, we can run inference with <code>SNPE</code> to identify parameters whose activity matches this trace.</p> <p>Note, that here we perform Neural Posterior Estimation (NPE). Single round sequential NPE which we call via SNPE corresponds to NPE. </p> <p>Note that this might take a few minutes.</p> <pre><code># Create inference object. Here, NPE is used.\ninference = SNPE(prior=prior)\n\n# generate simulations and pass to the inference object\ntheta, x = simulate_for_sbi(simulation_wrapper, proposal=prior,\n                             num_simulations=300, num_workers=4)\ninference = inference.append_simulations(theta, x)\n\n# train the density estimator and build the posterior\ndensity_estimator = inference.train()\nposterior = inference.build_posterior(density_estimator)\n</code></pre> <pre><code>Running 300 simulations in 300 batches.:   0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n\n Neural network successfully converged after 296 epochs.\n</code></pre> <p>Note: <code>sbi</code> can parallelize your simulator. If you experience problems with parallelization, try setting <code>num_workers=1</code> and please give us an error report as a GitHub issue.</p>"},{"location":"examples/00_HH_simulator/#coming-back-to-the-observed-data","title":"Coming back to the observed data","text":"<p>As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters (\\(\\bar g_{Na}\\),\\(\\bar g_K\\)). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data:</p> <pre><code># true parameters and respective labels\ntrue_params = np.array([50.0, 5.0])\nlabels_params = [r\"$g_{Na}$\", r\"$g_{K}$\"]\n</code></pre> <pre><code>observation_trace = run_HH_model(true_params)\nobservation_summary_statistics = calculate_summary_statistics(observation_trace)\n</code></pre> <p>As we have already shown above, the observed voltage traces look as follows:</p> <pre><code>fig = plt.figure(figsize=(7, 5))\ngs = mpl.gridspec.GridSpec(2, 1, height_ratios=[4, 1])\nax = plt.subplot(gs[0])\nplt.plot(observation_trace[\"time\"], observation_trace[\"data\"])\nplt.ylabel(\"voltage (mV)\")\nplt.title(\"observed data\")\nplt.setp(ax, xticks=[], yticks=[-80, -20, 40])\n\nax = plt.subplot(gs[1])\nplt.plot(observation_trace[\"time\"], I_inj * A_soma * 1e3, \"k\", lw=2)\nplt.xlabel(\"time (ms)\")\nplt.ylabel(\"input (nA)\")\n\nax.set_xticks([0, max(observation_trace[\"time\"]) / 2, max(observation_trace[\"time\"])])\nax.set_yticks([0, 1.1 * np.max(I_inj * A_soma * 1e3)])\nax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(\"%.2f\"))\n</code></pre> <p></p>"},{"location":"examples/00_HH_simulator/#analysis-of-the-posterior-given-the-observed-data","title":"Analysis of the posterior given the observed data","text":"<p>After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters (\\(\\bar g_{Na}\\),\\(\\bar g_K\\)), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets \\(\\bar g_{Na}^{samples}\\),\\(\\bar g_K^{samples}\\)) from the posterior:</p> <pre><code>samples = posterior.sample((10000,), x=observation_summary_statistics)\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>fig, axes = analysis.pairplot(\n    samples,\n    limits=[[0.5, 80], [1e-4, 15.0]],\n    ticks=[[0.5, 80], [1e-4, 15.0]],\n    figsize=(5, 5),\n    points=true_params,\n    points_offdiag={\"markersize\": 6},\n    points_colors=\"r\",\n    labels=labels_params,\n);\n</code></pre> <p></p> <p>As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data:</p> <pre><code># Draw a sample from the posterior and convert to numpy for plotting.\nposterior_sample = posterior.sample((1,), x=observation_summary_statistics).numpy()\n</code></pre> <pre><code>Drawing 1 posterior samples:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>fig = plt.figure(figsize=(7, 5))\n\n# plot observation\nt = observation_trace[\"time\"]\ny_obs = observation_trace[\"data\"]\nplt.plot(t, y_obs, lw=2, label=\"observation\")\n\n# simulate and plot samples\nx = run_HH_model(posterior_sample)\nplt.plot(t, x[\"data\"], \"--\", lw=2, label=\"posterior sample\")\n\nplt.xlabel(\"time (ms)\")\nplt.ylabel(\"voltage (mV)\")\n\nax = plt.gca()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.3, 1), loc=\"upper right\")\n\nax.set_xticks([0, 60, 120])\nax.set_yticks([-80, -20, 40]);\n</code></pre> <p></p> <p>As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that <code>SNPE</code> did a good job at capturing the observed data in this simple case.</p>"},{"location":"examples/00_HH_simulator/#references","title":"References","text":"<p>A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952.</p> <p>M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.</p>"},{"location":"examples/01_decision_making_model/","title":"SBI for decision-making models","text":"<p>In a previous tutorial, we showed how to use SBI with trial-based iid data. Such scenarios can arise, for example, in models of perceptual decision making. In addition to trial-based iid data points, these models often come with mixed data types and varying experimental conditions. Here, we show how <code>sbi</code> can be used to perform inference in such models with the <code>MNLE</code> method.</p>"},{"location":"examples/01_decision_making_model/#trial-based-sbi-with-mixed-data-types","title":"Trial-based SBI with mixed data types","text":"<p>In some cases, models with trial-based data additionally return data with mixed data types, e.g., continous and discrete data. For example, most computational models of decision-making have continuous reaction times and discrete choices as output.</p> <p>This can induce a problem when performing trial-based SBI that relies on learning a neural likelihood: It is challenging for most density estimators to handle both, continuous and discrete data at the same time. However, there is a recent SBI method for solving this problem, it\u2019s called Mixed Neural Likelihood Estimation (MNLE). It works just like NLE, but with mixed data types. The trick is that it learns two separate density estimators, one for the discrete part of the data, and one for the continuous part, and combines the two to obtain the final neural likelihood. Crucially, the continuous density estimator is trained conditioned on the output of the discrete one, such that statistical dependencies between the discrete and continuous data (e.g., between choices and reaction times) are modeled as well. The interested reader is referred to the original paper available here.</p> <p>MNLE was recently added to <code>sbi</code> (see this PR and also issue) and follows the same API as <code>SNLE</code>.</p> <p>In this tutorial we will show how to apply <code>MNLE</code> to mixed data, and how to deal with varying experimental conditions.</p>"},{"location":"examples/01_decision_making_model/#toy-problem-for-mnle","title":"Toy problem for <code>MNLE</code>","text":"<p>To illustrate <code>MNLE</code> we set up a toy simulator that outputs mixed data and for which we know the likelihood such we can obtain reference posterior samples via MCMC.</p> <p>Simulator: To simulate mixed data we do the following</p> <ul> <li>Sample reaction time from <code>inverse Gamma</code></li> <li>Sample choices from <code>Binomial</code></li> <li>Return reaction time \\(rt \\in (0, \\infty)\\) and choice index \\(c \\in \\{0, 1\\}\\)</li> </ul> \\[ c \\sim \\text{Binomial}(\\rho) \\\\ rt \\sim \\text{InverseGamma}(\\alpha=2, \\beta) \\\\ \\] <p>Prior: The priors of the two parameters \\(\\rho\\) and \\(\\beta\\) are independent. We define a <code>Beta</code> prior over the probabilty parameter of the <code>Binomial</code> used in the simulator and a <code>Gamma</code> prior over the shape-parameter of the <code>inverse Gamma</code> used in the simulator:</p> \\[ p(\\beta, \\rho) = p(\\beta) \\; p(\\rho) ; \\\\ p(\\beta) = \\text{Gamma}(1, 0.5) \\\\ p(\\text{probs}) = \\text{Beta}(2, 2) \\] <p>Because the <code>InverseGamma</code> and the <code>Binomial</code> likelihoods are well-defined we can perform MCMC on this problem and obtain reference-posterior samples.</p> <pre><code>import matplotlib.pyplot as plt\nimport torch\nfrom pyro.distributions import InverseGamma\nfrom torch import Tensor\nfrom torch.distributions import Beta, Binomial, Categorical, Gamma\n\nfrom sbi.analysis import pairplot\nfrom sbi.inference import MNLE, MCMCPosterior\nfrom sbi.inference.potentials.base_potential import BasePotential\nfrom sbi.inference.potentials.likelihood_based_potential import (\n    MixedLikelihoodBasedPotential,\n)\nfrom sbi.utils import MultipleIndependent, mcmc_transform\nfrom sbi.utils.conditional_density_utils import ConditionedPotential\nfrom sbi.utils.metrics import c2st\nfrom sbi.utils.torchutils import atleast_2d\n</code></pre> <pre><code># Toy simulator for mixed data\ndef mixed_simulator(theta: Tensor, concentration_scaling: float = 1.0):\n    \"\"\"Returns a sample from a mixed distribution given parameters theta.\n\n    Args:\n        theta: batch of parameters, shape (batch_size, 2) concentration_scaling:\n        scaling factor for the concentration parameter of the InverseGamma\n        distribution, mimics an experimental condition.\n\n    \"\"\"\n    beta, ps = theta[:, :1], theta[:, 1:]\n\n    choices = Binomial(probs=ps).sample()\n    rts = InverseGamma(\n        concentration=concentration_scaling * torch.ones_like(beta), rate=beta\n    ).sample()\n\n    return torch.cat((rts, choices), dim=1)\n\n\n# The potential function defines the ground truth likelihood and allows us to\n# obtain reference posterior samples via MCMC.\nclass PotentialFunctionProvider(BasePotential):\n    allow_iid_x = True  # type: ignore\n\n    def __init__(self, prior, x_o, concentration_scaling=1.0, device=\"cpu\"):\n        super().__init__(prior, x_o, device)\n        self.concentration_scaling = concentration_scaling\n\n    def __call__(self, theta, track_gradients: bool = True):\n        theta = atleast_2d(theta)\n\n        with torch.set_grad_enabled(track_gradients):\n            iid_ll = self.iid_likelihood(theta)\n\n        return iid_ll + self.prior.log_prob(theta)\n\n    def iid_likelihood(self, theta):\n        lp_choices = torch.stack(\n            [\n                Binomial(probs=th.reshape(1, -1)).log_prob(self.x_o[:, 1:])\n                for th in theta[:, 1:]\n            ],\n            dim=1,\n        )\n\n        lp_rts = torch.stack(\n            [\n                InverseGamma(\n                    concentration=self.concentration_scaling * torch.ones_like(beta_i),\n                    rate=beta_i,\n                ).log_prob(self.x_o[:, :1])\n                for beta_i in theta[:, :1]\n            ],\n            dim=1,\n        )\n\n        joint_likelihood = (lp_choices + lp_rts).squeeze()\n\n        assert joint_likelihood.shape == torch.Size([self.x_o.shape[0], theta.shape[0]])\n        return joint_likelihood.sum(0)\n</code></pre> <pre><code># Define independent prior.\nprior = MultipleIndependent(\n    [\n        Gamma(torch.tensor([1.0]), torch.tensor([0.5])),\n        Beta(torch.tensor([2.0]), torch.tensor([2.0])),\n    ],\n    validate_args=False,\n)\n</code></pre>"},{"location":"examples/01_decision_making_model/#obtain-reference-posterior-samples-via-analytical-likelihood-and-mcmc","title":"Obtain reference-posterior samples via analytical likelihood and MCMC","text":"<pre><code>torch.manual_seed(42)\nnum_trials = 10\nnum_samples = 1000\ntheta_o = prior.sample((1,))\nx_o = mixed_simulator(theta_o.repeat(num_trials, 1))\n</code></pre> <pre><code>mcmc_kwargs = dict(\n    num_chains=20,\n    warmup_steps=50,\n    method=\"slice_np_vectorized\",\n    init_strategy=\"proposal\",\n)\n\ntrue_posterior = MCMCPosterior(\n    potential_fn=PotentialFunctionProvider(prior, x_o),\n    proposal=prior,\n    theta_transform=mcmc_transform(prior, enable_transform=True),\n    **mcmc_kwargs,\n)\ntrue_samples = true_posterior.sample((num_samples,))\n</code></pre> <pre><code>/Users/janteusen/qode/sbi/sbi/utils/sbiutils.py:341: UserWarning: An x with a batch size of 10 was passed. It will be interpreted as a batch of independent and identically\n            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n            same underlying (unknown) parameter. The resulting posterior will be with\n            respect to entire batch, i.e,. p(theta | X).\n  warnings.warn(\n\n\n\nRunning vectorized MCMC with 20 chains:   0%|          | 0/20000 [00:00&lt;?, ?it/s]\n</code></pre>"},{"location":"examples/01_decision_making_model/#train-mnle-and-generate-samples-via-mcmc","title":"Train MNLE and generate samples via MCMC","text":"<pre><code># Training data\nnum_simulations = 20000\n# For training the MNLE emulator we need to define a proposal distribution, the prior is\n# a good choice.\nproposal = prior\ntheta = proposal.sample((num_simulations,))\nx = mixed_simulator(theta)\n\n# Train MNLE and obtain MCMC-based posterior.\ntrainer = MNLE()\nestimator = trainer.append_simulations(theta, x).train(training_batch_size=1000)\n</code></pre> <pre><code>/Users/janteusen/qode/sbi/sbi/neural_nets/mnle.py:60: UserWarning: The mixed neural likelihood estimator assumes that x contains\n        continuous data in the first n-1 columns (e.g., reaction times) and\n        categorical data in the last column (e.g., corresponding choices). If\n        this is not the case for the passed `x` do not use this function.\n  warnings.warn(\n\n\n Neural network successfully converged after 73 epochs.\n</code></pre> <pre><code># Build posterior from the trained estimator and prior.\nmnle_posterior = trainer.build_posterior(prior=prior)\n\nmnle_samples = mnle_posterior.sample((num_samples,), x=x_o, **mcmc_kwargs)\n</code></pre> <pre><code>/Users/janteusen/qode/sbi/sbi/utils/sbiutils.py:341: UserWarning: An x with a batch size of 10 was passed. It will be interpreted as a batch of independent and identically\n            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n            same underlying (unknown) parameter. The resulting posterior will be with\n            respect to entire batch, i.e,. p(theta | X).\n  warnings.warn(\n\n\n\nRunning vectorized MCMC with 20 chains:   0%|          | 0/20000 [00:00&lt;?, ?it/s]\n</code></pre>"},{"location":"examples/01_decision_making_model/#compare-mnle-and-reference-posterior","title":"Compare MNLE and reference posterior","text":"<pre><code># Plot them in one pairplot as contours (obtained via KDE on the samples).\nfig, ax = pairplot(\n    [\n        prior.sample((1000,)),\n        true_samples,\n        mnle_samples,\n    ],\n    points=theta_o,\n    diag=\"kde\",\n    upper=\"contour\",\n    kde_offdiag=dict(bins=50),\n    kde_diag=dict(bins=100),\n    contour_offdiag=dict(levels=[0.95]),\n    points_colors=[\"k\"],\n    points_offdiag=dict(marker=\"*\", markersize=10),\n    labels=[r\"$\\beta$\", r\"$\\rho$\"],\n)\n\nplt.sca(ax[1, 1])\nplt.legend(\n    [\"Prior\", \"Reference\", \"MNLE\", r\"$\\theta_o$\"],\n    frameon=False,\n    fontsize=12,\n);\n</code></pre> <pre><code>WARNING:root:upper is deprecated, use offdiag instead.\n</code></pre> <p>We see that the inferred <code>MNLE</code> posterior nicely matches the reference posterior, and how both inferred a posterior that is quite different from the prior.</p> <p>Because MNLE training is amortized we can obtain another posterior given a different observation with potentially a different number of trials, just by running MCMC again (without re-training <code>MNLE</code>):</p>"},{"location":"examples/01_decision_making_model/#repeat-inference-with-different-x_o-that-contains-more-trials","title":"Repeat inference with different <code>x_o</code> that contains more trials","text":"<pre><code>num_trials = 50\nx_o = mixed_simulator(theta_o.repeat(num_trials, 1))\ntrue_samples = true_posterior.sample((num_samples,), x=x_o, **mcmc_kwargs)\nmnle_samples = mnle_posterior.sample((num_samples,), x=x_o, **mcmc_kwargs)\n</code></pre> <pre><code>/Users/janteusen/qode/sbi/sbi/utils/sbiutils.py:341: UserWarning: An x with a batch size of 50 was passed. It will be interpreted as a batch of independent and identically\n            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n            same underlying (unknown) parameter. The resulting posterior will be with\n            respect to entire batch, i.e,. p(theta | X).\n  warnings.warn(\n\n\n\nRunning vectorized MCMC with 20 chains:   0%|          | 0/20000 [00:00&lt;?, ?it/s]\n\n\n\nRunning vectorized MCMC with 20 chains:   0%|          | 0/20000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code># Plot them in one pairplot as contours (obtained via KDE on the samples).\nfig, ax = pairplot(\n    [\n        prior.sample((1000,)),\n        true_samples,\n        mnle_samples,\n    ],\n    points=theta_o,\n    diag=\"kde\",\n    upper=\"contour\",\n    kde_offdiag=dict(bins=50),\n    kde_diag=dict(bins=100),\n    contour_offdiag=dict(levels=[0.95]),\n    points_colors=[\"k\"],\n    points_offdiag=dict(marker=\"*\", markersize=10),\n    labels=[r\"$\\beta$\", r\"$\\rho$\"],\n)\n\nplt.sca(ax[1, 1])\nplt.legend(\n    [\"Prior\", \"Reference\", \"MNLE\", r\"$\\theta_o$\"],\n    frameon=False,\n    fontsize=12,\n);\n</code></pre> <pre><code>WARNING:root:upper is deprecated, use offdiag instead.\n</code></pre> <pre><code>print(c2st(true_samples, mnle_samples)[0])\n</code></pre> <pre><code>tensor(0.5255)\n</code></pre> <p>Again we can see that the posteriors match nicely. In addition, we observe that the posterior\u2019s (epistemic) uncertainty reduces as we increase the number of trials.</p> <p>Note: <code>MNLE</code> is trained on single-trial data. Theoretically, density estimation is perfectly accurate only in the limit of infinite training data. Thus, training with a finite amount of training data naturally induces a small bias in the density estimator. As we observed above, this bias is so small that we don\u2019t really notice it, e.g., the <code>c2st</code> scores were close to 0.5. However, when we increase the number of trials in <code>x_o</code> dramatically (on the order of 1000s) the small bias can accumulate over the trials and inference with <code>MNLE</code> can become less accurate.</p>"},{"location":"examples/01_decision_making_model/#mnle-with-experimental-conditions","title":"MNLE with experimental conditions","text":"<p>In the perceptual decision-making research it is common to design experiments with varying experimental decisions, e.g., to vary the difficulty of the task. During parameter inference, it can be beneficial to incorporate the experimental conditions. In MNLE, we are learning an emulator that should be able to generate synthetic experimental data including reaction times and choices given different experimental conditions. Thus, to make MNLE work with experimental conditions, we need to include them in the training process, i.e., treat them like auxiliary parameters of the simulator:</p> <pre><code># define a simulator wrapper in which the experimental condition are contained\n# in theta and passed to the simulator.\ndef sim_wrapper(theta):\n    # simulate with experiment conditions\n    return mixed_simulator(\n        theta=theta[:, :2],\n        concentration_scaling=theta[:, 2:]\n        + 1,  # add 1 to deal with 0 values from Categorical distribution\n    )\n</code></pre> <pre><code># Define a proposal that contains both, priors for the parameters and a discrte\n# prior over experimental conditions.\nproposal = MultipleIndependent(\n    [\n        Gamma(torch.tensor([1.0]), torch.tensor([0.5])),\n        Beta(torch.tensor([2.0]), torch.tensor([2.0])),\n        Categorical(probs=torch.ones(1, 3)),\n    ],\n    validate_args=False,\n)\n\n# Simulated data\nnum_simulations = 10000\nnum_samples = 1000\ntheta = proposal.sample((num_simulations,))\nx = sim_wrapper(theta)\nassert x.shape == (num_simulations, 2)\n\n# simulate observed data and define ground truth parameters\nnum_trials = 10\ntheta_o = proposal.sample((1,))\ntheta_o[0, 2] = 2.0  # set condition to 2 as in original simulator.\nx_o = sim_wrapper(theta_o.repeat(num_trials, 1))\n</code></pre>"},{"location":"examples/01_decision_making_model/#obtain-ground-truth-posterior-via-mcmc","title":"Obtain ground truth posterior via MCMC","text":"<p>We obtain a ground-truth posterior via MCMC by using the PotentialFunctionProvider.</p> <p>For that, we first the define the actual prior, i.e., the distribution over the parameter we want to infer (not the proposal).</p> <p>Thus, we leave out the discrete prior over experimental conditions.</p> <pre><code>prior = MultipleIndependent(\n    [\n        Gamma(torch.tensor([1.0]), torch.tensor([0.5])),\n        Beta(torch.tensor([2.0]), torch.tensor([2.0])),\n    ],\n    validate_args=False,\n)\nprior_transform = mcmc_transform(prior)\n\n# We can now use the PotentialFunctionProvider to obtain a ground-truth\n# posterior via MCMC.\ntrue_posterior_samples = MCMCPosterior(\n    PotentialFunctionProvider(\n        prior,\n        x_o,\n        concentration_scaling=float(theta_o[0, 2])\n        + 1.0,  # add one because the sim_wrapper adds one (see above)\n    ),\n    theta_transform=prior_transform,\n    proposal=prior,\n    **mcmc_kwargs,\n).sample((num_samples,), show_progress_bars=True)\n</code></pre> <pre><code>/Users/janteusen/qode/sbi/sbi/utils/sbiutils.py:341: UserWarning: An x with a batch size of 10 was passed. It will be interpreted as a batch of independent and identically\n            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n            same underlying (unknown) parameter. The resulting posterior will be with\n            respect to entire batch, i.e,. p(theta | X).\n  warnings.warn(\n\n\n\nRunning vectorized MCMC with 20 chains:   0%|          | 0/20000 [00:00&lt;?, ?it/s]\n</code></pre>"},{"location":"examples/01_decision_making_model/#train-mnle-including-experimental-conditions","title":"Train MNLE including experimental conditions","text":"<pre><code>trainer = MNLE(proposal)\nestimator = trainer.append_simulations(theta, x).train(training_batch_size=100)\n</code></pre> <pre><code>/Users/janteusen/qode/sbi/sbi/neural_nets/mnle.py:60: UserWarning: The mixed neural likelihood estimator assumes that x contains\n        continuous data in the first n-1 columns (e.g., reaction times) and\n        categorical data in the last column (e.g., corresponding choices). If\n        this is not the case for the passed `x` do not use this function.\n  warnings.warn(\n\n\n Neural network successfully converged after 92 epochs.\n</code></pre>"},{"location":"examples/01_decision_making_model/#construct-conditional-potential-function","title":"Construct conditional potential function","text":"<p>To obtain posterior samples conditioned on a particular experimental condition (and on x_o), we need to construct a corresponding potential function.</p> <pre><code># We define the potential function for the complete, unconditional MNLE-likelihood\npotential_fn = MixedLikelihoodBasedPotential(estimator, proposal, x_o)\n\n# Then we use the potential to construct the conditional potential function.\n# Here, we tell the constructor to condition on the last dimension (index 2) by\n# passing dims_to_sample=[0, 1].\nconditioned_potential_fn = ConditionedPotential(\n    potential_fn,\n    condition=theta_o,\n    dims_to_sample=[0, 1],\n    allow_iid_x=True,  # we also need to explicitly tell that MNLE allows iid_x\n)\n\n# Using this potential function, we can now obtain conditional samples.\nmnle_posterior = MCMCPosterior(\n    potential_fn=conditioned_potential_fn,\n    theta_transform=prior_transform,\n    proposal=prior,\n    **mcmc_kwargs\n)\nconditional_samples = mnle_posterior.sample((num_samples,), x=x_o)\n</code></pre> <pre><code>Running vectorized MCMC with 20 chains:   0%|          | 0/20000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code># Finally, we can compare the ground truth conditional posterior with the\n# MNLE-conditional posterior.\nfig, ax = pairplot(\n    [\n        prior.sample((1000,)),\n        true_posterior_samples,\n        conditional_samples,\n    ],\n    points=theta_o,\n    diag=\"kde\",\n    offdiag=\"contour\",\n    kde_offdiag=dict(bins=50),\n    kde_diag=dict(bins=100),\n    contour_offdiag=dict(levels=[0.95]),\n    points_colors=[\"k\"],\n    points_offdiag=dict(marker=\"*\", markersize=10),\n    labels=[r\"$\\beta$\", r\"$\\rho$\"],\n)\n\nplt.sca(ax[1, 1])\nplt.legend(\n    [\"Prior\", \"Reference\", \"MNLE\", r\"$\\theta_o$\"],\n    frameon=False,\n    fontsize=12,\n);\n</code></pre> <p></p> <p>They match accurately, showing that we can indeed post-hoc condition the trained MNLE likelihood on different experimental conditions.</p>"},{"location":"faq/question_01_leakage/","title":"What should I do when my \u2018posterior samples are outside the prior support\u2019 in SNPE?","text":"<p>When working with multi-round SNPE, you might have experienced the following warning:</p> <pre><code>Only x% posterior samples are within the prior support. It may take a long time to\ncollect the remaining 10000 samples. Consider interrupting (Ctrl-C) and switching to\n'sample_with_mcmc=True'.\n</code></pre> <p>The reason for this issue is described in more detail here and here. The following fixes are possible:</p> <ul> <li> <p>sample with MCMC: <code>samples = posterior((num_samples,), x=x_o, sample_with_mcmc=True)</code>. This approach will make sampling slower, but samples will not \u201cleak\u201d.</p> </li> <li> <p>resort to single-round SNPE and (if necessary) increase your simulation budget.</p> </li> <li> <p>if your prior is either Gaussian (torch.distributions.MultivariateNormal) or Uniform (sbi.utils.BoxUniform), you can avoid leakage by using a mixture density network as density estimator. I.e., using the flexible interface, set <code>density_estimator='mdn'</code>. When running inference, there should be a print statement \u201cUsing SNPE-C with non-atomic loss\u201d.</p> </li> <li> <p>use a different algorithm, e.g., SNRE and SNLE. Note, however, that these algorithms can have different issues and potential pitfalls.</p> </li> </ul>"},{"location":"faq/question_02_nans/","title":"Can the algorithms deal with invalid data, e.g., NaN or inf?","text":"<p>Yes. By default, whenever a simulation returns at least one <code>NaN</code> or <code>inf</code>, it is completely excluded from the training data. In other words, the simulation is simply discarded.</p> <p>In cases where a very large fraction of simulations return <code>NaN</code> or <code>inf</code>, discarding many simulations can be wasteful. There are two options to deal with this: Either you use the <code>RestrictionEstimator</code> to learn regions in parameter space that do not produce <code>NaN</code> or <code>inf</code>, see here. Alternatively, you can manually substitute the \u2018invalid\u2019 values with a reasonable replacement. For example, at the end of your simulation code, you search for invalid entries and replace them with a floating point number. Importantly, in order for neural network training work well, the floating point number should still be in a reasonable range, i.e., maybe a few standard deviations outside of \u2018good\u2019 values.</p> <p>If you are running multi-round SNPE, however, things can go fully wrong if invalid data are encountered. In that case, you will get the following warning</p> <pre><code>When invalid simulations are excluded, multi-round SNPE-C can leak into the regions\nwhere parameters led to invalid simulations. This can lead to poor results.\n</code></pre> <p>Hence, if you are running multi-round SNPE and a significant fraction of simulations returns at least one invalid number, we strongly recommend manually replacing the value in your simulation code as described above (or resorting to single-round SNPE, or using a different <code>sbi</code> method entirely).</p>"},{"location":"faq/question_03_pickling_error/","title":"When using multiple workers, I get a pickling error. Can I still use multiprocessing?","text":"<p>Yes, but you will have to make a few adjustments to your code.</p> <p>Some background: When using <code>num_workers &gt; 1</code>, you might experience an error that a certain object from your simulator could not be pickled (an example can be found here).</p> <p>This can be fixed by forcing <code>sbi</code> to pickle with <code>dill</code> instead of the default <code>cloudpickle</code>. To do so, adjust your code as follows:</p> <ul> <li>Install <code>dill</code>:</li> </ul> <pre><code>pip install dill\n</code></pre> <ul> <li>At the very beginning of your python script, set the pickler to <code>dill</code>:</li> </ul> <pre><code>from joblib.externals.loky import set_loky_pickler\nset_loky_pickler(\"dill\")\n</code></pre> <ul> <li>Move all imports required by your simulator into the simulator:</li> </ul> <pre><code># Imports specified outside of the simulator will break dill:\nimport torch\ndef my_simulator(parameters):\n    return torch.ones(1,10)\n\n# Therefore, move the imports into the simulator:\ndef my_simulator(parameters):\n    import torch\n    return torch.ones(1,10)\n</code></pre>"},{"location":"faq/question_03_pickling_error/#alternative-parallelize-yourself","title":"Alternative: parallelize yourself","text":"<p>You can also write your own code to parallelize simulations with whatever multiprocessing framework you prefer. You can then simulate your data outside of <code>sbi</code> and pass the simulated data as shown in the flexible interface:</p>"},{"location":"faq/question_03_pickling_error/#some-more-background","title":"Some more background","text":"<p><code>sbi</code> uses <code>joblib</code> to parallelize simulations, which in turn uses <code>pickle</code> or <code>cloudpickle</code> to serialize the simulator. Almost all simulators will be picklable with <code>cloudpickle</code>, but we have experienced issues, e.g., with <code>neuron</code> simulators, see here.</p>"},{"location":"faq/question_04_gpu/","title":"Can I use the GPU for training the density estimator?","text":"<p>TLDR; Yes, by passing <code>device=\"cuda\"</code> and by passing a prior that lives on the device name you passed. But we expect no speed-ups for default density estimators.</p>"},{"location":"faq/question_04_gpu/#setup","title":"Setup","text":"<p>Yes, we support GPU training. When creating the inference object in the flexible interface, you can pass the <code>device</code> as an argument, e.g.,</p> <pre><code>inference = SNPE(prior, device=\"cuda\", density_estimator=\"maf\")\n</code></pre> <p>The device is set to <code>\"cpu\"</code> by default. But it can be set to anything, as long as it maps to an existing PyTorch GPU device, e.g., <code>device=\"cuda\"</code> or <code>device=\"cuda:2\"</code>. <code>sbi</code> will take care of copying the <code>net</code> and the training data to and from the <code>device</code>. We also support MPS as a GPU device for GPU-accelarated training on an Apple Silicon chip, e.g., it is possible to pass <code>device=\"mps\"</code>.</p> <p>Note that the prior must be on the training device already, e.g., when passing <code>device=\"cuda:0\"</code>, make sure to pass a prior object that was created on that device, e.g.,</p> <pre><code>prior = torch.distributions.MultivariateNormal(loc=torch.zeros(2,\ndevice=\"cuda:0\"), covariance_matrix=torch.eye(2, device=\"cuda:0\"))\n</code></pre>"},{"location":"faq/question_04_gpu/#performance","title":"Performance","text":"<p>Whether or not you reduce your training time when training on a GPU depends on the problem at hand. We provide a couple of default density estimators for <code>SNPE</code>, <code>SNLE</code> and <code>SNRE</code>, e.g., a mixture density network (<code>density_estimator=\"mdn\"</code>) or a Masked Autoregressive Flow (<code>density_estimator=\"maf\"</code>). For these default density estimators, we do not expect a speed-up. This is because the underlying neural networks are relatively shallow and not tall, e.g., they do not have many parameters or matrix operations that benefit from being executed on the GPU.</p> <p>A speed-up through training on the GPU will most likely become visible when using convolutional modules in your neural networks. E.g., when passing an embedding net for image processing like in this example: https://github.com/sbi-dev/sbi/blob/main/tutorials/05_embedding_net.ipynb.</p>"},{"location":"faq/question_05_pickling/","title":"How should I save and load objects in <code>sbi</code>?","text":"<p><code>NeuralPosterior</code> objects are picklable.</p> <pre><code>import pickle\n\n# ... run inference\nposterior = inference.build_posterior()\n\nwith open(\"/path/to/my_posterior.pkl\", \"wb\") as handle:\n    pickle.dump(posterior, handle)\n</code></pre> <p>Note: posterior objects that were saved under <code>sbi v0.17.2</code> or older can not be loaded under <code>sbi v0.18.0</code> or newer.</p> <p>Note: if you try to load a posterior that was saved under <code>sbi v0.14.x</code> or earlier under <code>sbi v0.15.x</code> until <code>sbi v0.17.x</code>, you have to add:</p> <pre><code>import sys\nfrom sbi.utils import user_input_checks_utils\n\nsys.modules[\"sbi.user_input.user_input_checks_utils\"] = user_input_checks_utils\n</code></pre> <p>to your script before loading the posterior.</p> <p>As of <code>sbi v0.18.0</code>, <code>NeuralInference</code> objects are also picklable.</p> <pre><code>import pickle\n\n# ... run inference\nposterior = inference.build_posterior()\n\nwith open(\"/path/to/my_inference.pkl\", \"wb\") as handle:\n    pickle.dump(inference, handle)\n</code></pre> <p>However, saving and loading the <code>inference</code> object will slightly modify the object (in order to make it serializable). These modifications lead to the following two changes in behavior:</p> <p>1) Retraining from scratch is not supported, i.e. <code>.train(...,    retrain_from_scratch=True)</code> does not work. 2) When the loaded object calls the <code>.train()</code> method, it generates a new    tensorboard summary writer (instead of appending to the current one).</p>"},{"location":"faq/question_05_pickling/#i-trained-a-model-on-a-gpu-can-i-load-it-on-a-cpu","title":"I trained a model on a GPU. Can I load it on a CPU?","text":"<p>The code snippet below allows to load inference objects on a CPU if they were saved on a GPU. Note that the neural net also needs to be moved to CPU.</p> <pre><code>import io\nimport pickle\n\n#https://stackoverflow.com/questions/57081727/load-pickle-file-obtained-from-gpu-to-cpu\nclass CPU_Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'torch.storage' and name == '_load_from_bytes':\n            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n        else:\n            return super().find_class(module, name)\n\nwith open(\"/path/to/my_inference.pkl\", \"rb\") as f:\n    inference = CPU_Unpickler(f).load()\n\nposterior = inference.build_posterior(inference._neural_net.to(\"cpu\"))\n</code></pre> <p>Loading inference objects on CPU can be useful for inspection. However, resuming training on CPU for an inference object trained on a GPU is currently not supported. If this is strictly required by your workflow, consider setting <code>inference._device = \"cpu\"</code> before calling <code>inference.train()</code>.</p>"},{"location":"faq/question_06_resume_training/","title":"Can I stop neural network training and resume it later?","text":"<p>Many clusters have a time limit, and <code>sbi</code> might exceed this limit. You can circumvent this problem by using the flexible interface. After simulations are finished, <code>sbi</code> trains a neural network. If this process takes too long, you can stop training and resume it later. The syntax is:</p> <pre><code>inference = SNPE(prior=prior)\ninference = inference.append_simulations(theta, x)\ninference.train(max_num_epochs=300)  # Pick `max_num_epochs` such that it does not exceed the runtime.\n\nwith open(\"path/to/my/inference.pkl\", \"wb\") as handle:\n    pickle.dump(inference, handle)\n\n# To resume training:\nwith open(\"path/to/my/inference.pkl\", \"rb\") as handle:\n    inference_from_disk = pickle.load(handle)\ninference_from_disk.train(resume_training=True, max_num_epochs=600)  # Run epochs 301 until 600 (or stop early).\nposterior = inference_from_disk.build_posterior()\n</code></pre>"},{"location":"faq/question_07_custom_prior/","title":"Can I use a custom prior with sbi?","text":"<p>As <code>sbi</code> works with torch distributions only, we recommend using those whenever possible. For example, when you are used to using <code>scipy.stats</code> distributions as priors, then we recommend using the corresponding <code>torch.distributions</code> instead. Most <code>scipy</code> distributions are implemented in <code>PyTorch</code> as well.</p> <p>In case you want to use a custom prior that is not in the set of common distributions that\u2019s possible as well: You need to write a prior class that mimicks the behaviour of a <code>torch.distributions.Distribution</code> class. <code>sbi</code> will wrap this class to make it a fully functional torch <code>Distribution</code>.</p> <p>Essentially, the class needs two methods:</p> <ul> <li><code>.sample(sample_shape)</code>, where sample_shape is a shape tuple, e.g., <code>(n,)</code>,   and returns a batch of n samples, e.g., of shape (n, 2)` for a two dimenional   prior.</li> <li><code>.log_prob(value)</code> method that returns the \u201clog probs\u201d of parameters under the   prior, e.g., for a batches of n parameters with shape <code>(n, ndims)</code> it should   return a log probs array of shape <code>(n,)</code>.</li> </ul> <p>For <code>sbi</code> &gt; 0.17.2 this could look like the following:</p> <pre><code>class CustomUniformPrior:\n    \"\"\"User defined numpy uniform prior.\n\n    Custom prior with user-defined valid .sample and .log_prob methods.\n    \"\"\"\n\n    def __init__(self, lower: Tensor, upper: Tensor, return_numpy: bool = False):\n        self.lower = lower\n        self.upper = upper\n        self.dist = BoxUniform(lower, upper)\n        self.return_numpy = return_numpy\n\n    def sample(self, sample_shape=torch.Size([])):\n        samples = self.dist.sample(sample_shape)\n        return samples.numpy() if self.return_numpy else samples\n\n    def log_prob(self, values):\n        if self.return_numpy:\n            values = torch.as_tensor(values)\n        log_probs = self.dist.log_prob(values)\n        return log_probs.numpy() if self.return_numpy else log_probs\n</code></pre> <p>Once you have such a class, you can wrap it into a <code>Distribution</code> using the <code>process_prior</code> function <code>sbi</code> provides:</p> <pre><code>from sbi.utils import process_prior\n\ncustom_prior = CustomUniformPrior(torch.zeros(2), torch.ones(2))\nprior, *_ = process_prior(custom_prior)  # Keeping only the first return.\n# use this wrapped prior in sbi...\n</code></pre> <p>In <code>sbi</code> it is sometimes required to check the support of the prior, e.g., when the prior support is bounded and one wants to reject samples from the posterior density estimator that lie outside the prior support. In torch <code>Distributions</code> this is handled automatically. However, when using a custom prior, it is not. Thus, if your prior has bounded support (like the one above), it makes sense to pass the bounds to the wrapper function such that <code>sbi</code> can pass them to torch <code>Distributions</code>:</p> <pre><code>from sbi.utils import process_prior\n\ncustom_prior = CustomUniformPrior(torch.zeros(2), torch.ones(2))\nprior = process_prior(custom_prior,\n                      custom_prior_wrapper_kwargs=dict(lower_bound=torch.zeros(2),\n                                                       upper_bound=torch.ones(2)))\n# use this wrapped prior in sbi...\n</code></pre> <p>Note that in <code>custom_prior_wrapper_kwargs</code> you can pass additinal arguments for the wrapper, e.g., <code>validate_args</code> or <code>arg_constraints</code> see the <code>Distribution</code> documentation for more details.</p> <p>If you are using <code>sbi</code> &lt; 0.17.2 and use <code>SNLE</code> the code above will produce a <code>NotImplementedError</code> (see #581). In this case, you need to update to a newer version of <code>sbi</code> or use <code>SNPE</code> instead.</p>"},{"location":"reference/","title":"API Reference:","text":"<ul> <li>Inference SBI algorithms and helper functions.</li> <li>Neural Networks Utilities to build neural network-based density estimators and feature extractors.</li> <li>Posteriors Posterior classes</li> <li>Potentials Potential function classes for posterior sampling.</li> <li>Analysis Utilities for SBI visualizations and analyses.</li> </ul>"},{"location":"reference/analysis/","title":"Analysis","text":""},{"location":"reference/analysis/#sbi.analysis.plot.pairplot","title":"<code>pairplot(samples, points=None, limits=None, subset=None, upper='hist', lower=None, diag='hist', figsize=(10, 10), labels=None, ticks=None, offdiag=None, diag_kwargs=None, upper_kwargs=None, lower_kwargs=None, fig_kwargs=None, fig=None, axes=None, **kwargs)</code>","text":"<p>Plot samples in a 2D grid showing marginals and pairwise marginals.</p> <p>Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution that the samples were drawn from. Each upper-diagonal plot can be interpreted as a 2D-marginal of the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Union[List[ndarray], List[Tensor], ndarray, Tensor]</code> <p>Samples used to build the histogram.</p> required <code>points</code> <code>Optional[Union[List[ndarray], List[Tensor], ndarray, Tensor]]</code> <p>List of additional points to scatter.</p> <code>None</code> <code>limits</code> <code>Optional[Union[List, Tensor]]</code> <p>Array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples</p> <code>None</code> <code>subset</code> <code>Optional[List[int]]</code> <p>List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and, if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on).</p> <code>None</code> <code>upper</code> <code>Optional[Union[List[Optional[str]], str]]</code> <p>Plotting style for upper diagonal, {hist, scatter, contour, kde, None}.</p> <code>'hist'</code> <code>lower</code> <code>Optional[Union[List[Optional[str]], str]]</code> <p>Plotting style for upper diagonal, {hist, scatter, contour, kde, None}.</p> <code>None</code> <code>diag</code> <code>Optional[Union[List[Optional[str]], str]]</code> <p>Plotting style for diagonal, {hist, scatter, kde}.</p> <code>'hist'</code> <code>figsize</code> <code>Tuple</code> <p>Size of the entire figure.</p> <code>(10, 10)</code> <code>labels</code> <code>Optional[List[str]]</code> <p>List of strings specifying the names of the parameters.</p> <code>None</code> <code>ticks</code> <code>Optional[Union[List, Tensor]]</code> <p>Position of the ticks.</p> <code>None</code> <code>offdiag</code> <code>Optional[Union[List[Optional[str]], str]]</code> <p>deprecated, use upper instead.</p> <code>None</code> <code>diag_kwargs</code> <code>Optional[Union[List[Optional[Dict]], Dict]]</code> <p>Additional arguments to adjust the diagonal plot, see the source code in <code>_get_default_diag_kwarg()</code></p> <code>None</code> <code>upper_kwargs</code> <code>Optional[Union[List[Optional[Dict]], Dict]]</code> <p>Additional arguments to adjust the upper diagonal plot, see the source code in <code>_get_default_offdiag_kwarg()</code></p> <code>None</code> <code>lower_kwargs</code> <code>Optional[Union[List[Optional[Dict]], Dict]]</code> <p>Additional arguments to adjust the lower diagonal plot, see the source code in <code>_get_default_offdiag_kwarg()</code></p> <code>None</code> <code>fig_kwargs</code> <code>Optional[Dict]</code> <p>Additional arguments to adjust the overall figure, see the source code in <code>_get_default_fig_kwargs()</code></p> <code>None</code> <code>fig</code> <code>Optional[FigureBase]</code> <p>matplotlib figure to plot on.</p> <code>None</code> <code>axes</code> <code>Optional[Axes]</code> <p>matplotlib axes corresponding to fig.</p> <code>None</code> <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional arguments to adjust the plot (deprecated).</p> <code>{}</code> <p>Returns: figure and axis of posterior distribution plot</p> Source code in <code>sbi/analysis/plot.py</code> <pre><code>def pairplot(\n    samples: Union[List[np.ndarray], List[torch.Tensor], np.ndarray, torch.Tensor],\n    points: Optional[\n        Union[List[np.ndarray], List[torch.Tensor], np.ndarray, torch.Tensor]\n    ] = None,\n    limits: Optional[Union[List, torch.Tensor]] = None,\n    subset: Optional[List[int]] = None,\n    upper: Optional[Union[List[Optional[str]], str]] = \"hist\",\n    lower: Optional[Union[List[Optional[str]], str]] = None,\n    diag: Optional[Union[List[Optional[str]], str]] = \"hist\",\n    figsize: Tuple = (10, 10),\n    labels: Optional[List[str]] = None,\n    ticks: Optional[Union[List, torch.Tensor]] = None,\n    offdiag: Optional[Union[List[Optional[str]], str]] = None,\n    diag_kwargs: Optional[Union[List[Optional[Dict]], Dict]] = None,\n    upper_kwargs: Optional[Union[List[Optional[Dict]], Dict]] = None,\n    lower_kwargs: Optional[Union[List[Optional[Dict]], Dict]] = None,\n    fig_kwargs: Optional[Dict] = None,\n    fig: Optional[FigureBase] = None,\n    axes: Optional[Axes] = None,\n    **kwargs: Optional[Any],\n) -&gt; Tuple[FigureBase, Axes]:\n    \"\"\"\n    Plot samples in a 2D grid showing marginals and pairwise marginals.\n\n    Each of the diagonal plots can be interpreted as a 1D-marginal of the distribution\n    that the samples were drawn from. Each upper-diagonal plot can be interpreted as a\n    2D-marginal of the distribution.\n\n    Args:\n        samples: Samples used to build the histogram.\n        points: List of additional points to scatter.\n        limits: Array containing the plot xlim for each parameter dimension. If None,\n            just use the min and max of the passed samples\n        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot\n            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,\n            if they exist, the 4th, 5th and so on).\n        upper: Plotting style for upper diagonal, {hist, scatter, contour, kde,\n            None}.\n        lower: Plotting style for upper diagonal, {hist, scatter, contour, kde,\n            None}.\n        diag: Plotting style for diagonal, {hist, scatter, kde}.\n        figsize: Size of the entire figure.\n        labels: List of strings specifying the names of the parameters.\n        ticks: Position of the ticks.\n        offdiag: deprecated, use upper instead.\n        diag_kwargs: Additional arguments to adjust the diagonal plot,\n            see the source code in `_get_default_diag_kwarg()`\n        upper_kwargs: Additional arguments to adjust the upper diagonal plot,\n            see the source code in `_get_default_offdiag_kwarg()`\n        lower_kwargs: Additional arguments to adjust the lower diagonal plot,\n            see the source code in `_get_default_offdiag_kwarg()`\n        fig_kwargs: Additional arguments to adjust the overall figure,\n            see the source code in `_get_default_fig_kwargs()`\n        fig: matplotlib figure to plot on.\n        axes: matplotlib axes corresponding to fig.\n        **kwargs: Additional arguments to adjust the plot (deprecated).\n\n    Returns: figure and axis of posterior distribution plot\n    \"\"\"\n\n    # Backwards compatibility\n    if len(kwargs) &gt; 0:\n        warn(\n            \"**kwargs are deprecated, use fig_kwargs instead. \\n \\\n              Calling the to be deprecated pairplot function\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        fig, axes = pairplot_dep(\n            samples,\n            points,\n            limits,\n            subset,\n            offdiag,\n            diag,\n            figsize,\n            labels,\n            ticks,\n            upper,\n            fig,\n            axes,\n            **kwargs,\n        )\n        return fig, axes\n\n    samples, dim, limits = prepare_for_plot(samples, limits)\n\n    # prepate figure kwargs\n    fig_kwargs_filled = _get_default_fig_kwargs()\n    # update the defaults dictionary with user provided values\n    fig_kwargs_filled = _update(fig_kwargs_filled, fig_kwargs)\n\n    # checks.\n    if fig_kwargs_filled[\"legend\"]:\n        assert len(fig_kwargs_filled[\"samples_labels\"]) &gt;= len(\n            samples\n        ), \"Provide at least as many labels as samples.\"\n    if offdiag is not None:\n        warn(\"offdiag is deprecated, use upper or lower instead.\", stacklevel=2)\n        upper = offdiag\n\n    # Prepare diag\n    diag_list = to_list_string(diag, len(samples))\n    diag_kwargs_list = to_list_kwargs(diag_kwargs, len(samples))\n    diag_func = get_diag_funcs(diag_list)\n    diag_kwargs_filled = []\n    for i, (diag_i, diag_kwargs_i) in enumerate(zip(diag_list, diag_kwargs_list)):\n        diag_kwarg_filled_i = _get_default_diag_kwargs(diag_i, i)\n        # update the defaults dictionary with user provided values\n        diag_kwarg_filled_i = _update(diag_kwarg_filled_i, diag_kwargs_i)\n        diag_kwargs_filled.append(diag_kwarg_filled_i)\n\n    # Prepare upper\n    upper_list = to_list_string(upper, len(samples))\n    upper_kwargs_list = to_list_kwargs(upper_kwargs, len(samples))\n    upper_func = get_offdiag_funcs(upper_list)\n    upper_kwargs_filled = []\n    for i, (upper_i, upper_kwargs_i) in enumerate(zip(upper_list, upper_kwargs_list)):\n        upper_kwarg_filled_i = _get_default_offdiag_kwargs(upper_i, i)\n        # update the defaults dictionary with user provided values\n        upper_kwarg_filled_i = _update(upper_kwarg_filled_i, upper_kwargs_i)\n        upper_kwargs_filled.append(upper_kwarg_filled_i)\n\n    # Prepare lower\n    lower_list = to_list_string(lower, len(samples))\n    lower_kwargs_list = to_list_kwargs(lower_kwargs, len(samples))\n    lower_func = get_offdiag_funcs(lower_list)\n    lower_kwargs_filled = []\n    for i, (lower_i, lower_kwargs_i) in enumerate(zip(lower_list, lower_kwargs_list)):\n        lower_kwarg_filled_i = _get_default_offdiag_kwargs(lower_i, i)\n        # update the defaults dictionary with user provided values\n        lower_kwarg_filled_i = _update(lower_kwarg_filled_i, lower_kwargs_i)\n        lower_kwargs_filled.append(lower_kwarg_filled_i)\n\n    return _arrange_grid(\n        diag_func,\n        upper_func,\n        lower_func,\n        diag_kwargs_filled,\n        upper_kwargs_filled,\n        lower_kwargs_filled,\n        samples,\n        points,\n        limits,\n        subset,\n        figsize,\n        labels,\n        ticks,\n        fig,\n        axes,\n        fig_kwargs_filled,\n    )\n</code></pre>"},{"location":"reference/analysis/#sbi.analysis.plot.marginal_plot","title":"<code>marginal_plot(samples, points=None, limits=None, subset=None, diag='hist', figsize=(10, 2), labels=None, ticks=None, diag_kwargs=None, fig_kwargs=None, fig=None, axes=None, **kwargs)</code>","text":"<p>Plot samples in a row showing 1D marginals of selected dimensions.</p> <p>Each of the plots can be interpreted as a 1D-marginal of the distribution that the samples were drawn from.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Union[List[ndarray], List[Tensor], ndarray, Tensor]</code> <p>Samples used to build the histogram.</p> required <code>points</code> <code>Optional[Union[List[ndarray], List[Tensor], ndarray, Tensor]]</code> <p>List of additional points to scatter.</p> <code>None</code> <code>limits</code> <code>Optional[Union[List, Tensor]]</code> <p>Array containing the plot xlim for each parameter dimension. If None, just use the min and max of the passed samples</p> <code>None</code> <code>subset</code> <code>Optional[List[int]]</code> <p>List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and, if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on).</p> <code>None</code> <code>diag</code> <code>Optional[Union[List[Optional[str]], str]]</code> <p>Plotting style for 1D marginals, {hist, kde cond, None}.</p> <code>'hist'</code> <code>figsize</code> <code>Optional[Tuple]</code> <p>Size of the entire figure.</p> <code>(10, 2)</code> <code>labels</code> <code>Optional[List[str]]</code> <p>List of strings specifying the names of the parameters.</p> <code>None</code> <code>ticks</code> <code>Optional[Union[List, Tensor]]</code> <p>Position of the ticks.</p> <code>None</code> <code>diag_kwargs</code> <code>Optional[Union[List[Optional[Dict]], Dict]]</code> <p>Additional arguments to adjust the diagonal plot, see the source code in <code>_get_default_diag_kwarg()</code></p> <code>None</code> <code>fig_kwargs</code> <code>Optional[Dict]</code> <p>Additional arguments to adjust the overall figure, see the source code in <code>_get_default_fig_kwargs()</code></p> <code>None</code> <code>fig</code> <code>Optional[FigureBase]</code> <p>matplotlib figure to plot on.</p> <code>None</code> <code>axes</code> <code>Optional[Axes]</code> <p>matplotlib axes corresponding to fig.</p> <code>None</code> <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional arguments to adjust the plot (deprecated)</p> <code>{}</code> <p>Returns: figure and axis of posterior distribution plot</p> Source code in <code>sbi/analysis/plot.py</code> <pre><code>def marginal_plot(\n    samples: Union[List[np.ndarray], List[torch.Tensor], np.ndarray, torch.Tensor],\n    points: Optional[\n        Union[List[np.ndarray], List[torch.Tensor], np.ndarray, torch.Tensor]\n    ] = None,\n    limits: Optional[Union[List, torch.Tensor]] = None,\n    subset: Optional[List[int]] = None,\n    diag: Optional[Union[List[Optional[str]], str]] = \"hist\",\n    figsize: Optional[Tuple] = (10, 2),\n    labels: Optional[List[str]] = None,\n    ticks: Optional[Union[List, torch.Tensor]] = None,\n    diag_kwargs: Optional[Union[List[Optional[Dict]], Dict]] = None,\n    fig_kwargs: Optional[Dict] = None,\n    fig: Optional[FigureBase] = None,\n    axes: Optional[Axes] = None,\n    **kwargs: Optional[Any],\n) -&gt; Tuple[FigureBase, Axes]:\n    \"\"\"\n    Plot samples in a row showing 1D marginals of selected dimensions.\n\n    Each of the plots can be interpreted as a 1D-marginal of the distribution\n    that the samples were drawn from.\n\n    Args:\n        samples: Samples used to build the histogram.\n        points: List of additional points to scatter.\n        limits: Array containing the plot xlim for each parameter dimension. If None,\n            just use the min and max of the passed samples\n        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot\n            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,\n            if they exist, the 4th, 5th and so on).\n        diag: Plotting style for 1D marginals, {hist, kde cond, None}.\n        figsize: Size of the entire figure.\n        labels: List of strings specifying the names of the parameters.\n        ticks: Position of the ticks.\n        diag_kwargs: Additional arguments to adjust the diagonal plot,\n            see the source code in `_get_default_diag_kwarg()`\n        fig_kwargs: Additional arguments to adjust the overall figure,\n            see the source code in `_get_default_fig_kwargs()`\n        fig: matplotlib figure to plot on.\n        axes: matplotlib axes corresponding to fig.\n        **kwargs: Additional arguments to adjust the plot (deprecated)\n    Returns: figure and axis of posterior distribution plot\n    \"\"\"\n\n    # backwards compatibility\n    if len(kwargs) &gt; 0:\n        warn(\n            \"**kwargs are deprecated, use fig_kwargs instead.\\n\\\n              calling the to be deprecated marginal_plot function\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        fig, axes = marginal_plot_dep(\n            samples,\n            points,\n            limits,\n            subset,\n            diag,\n            figsize,\n            labels,\n            ticks,\n            fig,\n            axes,\n            **kwargs,\n        )\n        return fig, axes\n\n    samples, dim, limits = prepare_for_plot(samples, limits)\n\n    # prepare kwargs and functions of the subplots\n    diag_list = to_list_string(diag, len(samples))\n    diag_kwargs_list = to_list_kwargs(diag_kwargs, len(samples))\n    diag_func = get_diag_funcs(diag_list)\n    diag_kwargs_filled = []\n    for i, (diag_i, diag_kwargs_i) in enumerate(zip(diag_list, diag_kwargs_list)):\n        diag_kwarg_filled_i = _get_default_diag_kwargs(diag_i, i)\n        diag_kwarg_filled_i = _update(diag_kwarg_filled_i, diag_kwargs_i)\n        diag_kwargs_filled.append(diag_kwarg_filled_i)\n\n    # prepare fig_kwargs\n    fig_kwargs_filled = _get_default_fig_kwargs()\n    fig_kwargs_filled = _update(fig_kwargs_filled, fig_kwargs)\n\n    # generate plot\n    return _arrange_grid(\n        diag_func,\n        [None],\n        [None],\n        diag_kwargs_filled,\n        [None],\n        [None],\n        samples,\n        points,\n        limits,\n        subset,\n        figsize,\n        labels,\n        ticks,\n        fig,\n        axes,\n        fig_kwargs_filled,\n    )\n</code></pre>"},{"location":"reference/analysis/#sbi.analysis.plot.conditional_pairplot","title":"<code>conditional_pairplot(density, condition, limits, points=None, subset=None, resolution=50, figsize=(10, 10), labels=None, ticks=None, fig=None, axes=None, **kwargs)</code>","text":"<p>Plot conditional distribution given all other parameters.</p> <p>The conditionals can be interpreted as slices through the <code>density</code> at a location given by <code>condition</code>.</p> <p>For example: Say we have a 3D density with parameters \\(\\theta_0\\), \\(\\theta_1\\), \\(\\theta_2\\) and a condition \\(c\\) passed by the user in the <code>condition</code> argument. For the plot of \\(\\theta_0\\) on the diagonal, this will plot the conditional \\(p(\\theta_0 | \\theta_1=c[1], \\theta_2=c[2])\\). For the upper diagonal of \\(\\theta_1\\) and \\(\\theta_2\\), it will plot \\(p(\\theta_1, \\theta_2 | \\theta_0=c[0])\\). All other diagonals and upper-diagonals are built in the corresponding way.</p> <p>Parameters:</p> Name Type Description Default <code>density</code> <code>Any</code> <p>Probability density with a <code>log_prob()</code> method.</p> required <code>condition</code> <code>Tensor</code> <p>Condition that all but the one/two regarded parameters are fixed to. The condition should be of shape (1, dim_theta), i.e. it could e.g. be a sample from the posterior distribution.</p> required <code>limits</code> <code>Union[List, Tensor]</code> <p>Limits in between which each parameter will be evaluated.</p> required <code>points</code> <code>Optional[Union[List[ndarray], List[Tensor], ndarray, Tensor]]</code> <p>Additional points to scatter.</p> <code>None</code> <code>subset</code> <code>Optional[List[int]]</code> <p>List containing the dimensions to plot. E.g. subset=[1,3] will plot plot only the 1<sup>st</sup> and 3<sup>rd</sup> dimension but will discard the 0<sup>th</sup> and 2<sup>nd</sup> (and, if they exist, the 4<sup>th</sup>, 5<sup>th</sup> and so on)</p> <code>None</code> <code>resolution</code> <code>int</code> <p>Resolution of the grid at which we evaluate the <code>pdf</code>.</p> <code>50</code> <code>figsize</code> <code>Tuple</code> <p>Size of the entire figure.</p> <code>(10, 10)</code> <code>labels</code> <code>Optional[List[str]]</code> <p>List of strings specifying the names of the parameters.</p> <code>None</code> <code>ticks</code> <code>Optional[Union[List, Tensor]]</code> <p>Position of the ticks.</p> <code>None</code> <code>points_colors</code> <p>Colors of the <code>points</code>.</p> required <code>fig</code> <p>matplotlib figure to plot on.</p> <code>None</code> <code>axes</code> <p>matplotlib axes corresponding to fig.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to adjust the plot, e.g., <code>samples_colors</code>, <code>points_colors</code> and many more, see the source code in <code>_get_default_opts()</code> in <code>sbi.analysis.plot</code> for details.</p> <code>{}</code> <p>Returns: figure and axis of posterior distribution plot</p> Source code in <code>sbi/analysis/plot.py</code> <pre><code>def conditional_pairplot(\n    density: Any,\n    condition: torch.Tensor,\n    limits: Union[List, torch.Tensor],\n    points: Optional[\n        Union[List[np.ndarray], List[torch.Tensor], np.ndarray, torch.Tensor]\n    ] = None,\n    subset: Optional[List[int]] = None,\n    resolution: int = 50,\n    figsize: Tuple = (10, 10),\n    labels: Optional[List[str]] = None,\n    ticks: Optional[Union[List, torch.Tensor]] = None,\n    fig=None,\n    axes=None,\n    **kwargs,\n):\n    r\"\"\"\n    Plot conditional distribution given all other parameters.\n\n    The conditionals can be interpreted as slices through the `density` at a location\n    given by `condition`.\n\n    For example:\n    Say we have a 3D density with parameters $\\theta_0$, $\\theta_1$, $\\theta_2$ and\n    a condition $c$ passed by the user in the `condition` argument.\n    For the plot of $\\theta_0$ on the diagonal, this will plot the conditional\n    $p(\\theta_0 | \\theta_1=c[1], \\theta_2=c[2])$. For the upper\n    diagonal of $\\theta_1$ and $\\theta_2$, it will plot\n    $p(\\theta_1, \\theta_2 | \\theta_0=c[0])$. All other diagonals and upper-diagonals\n    are built in the corresponding way.\n\n    Args:\n        density: Probability density with a `log_prob()` method.\n        condition: Condition that all but the one/two regarded parameters are fixed to.\n            The condition should be of shape (1, dim_theta), i.e. it could e.g. be\n            a sample from the posterior distribution.\n        limits: Limits in between which each parameter will be evaluated.\n        points: Additional points to scatter.\n        subset: List containing the dimensions to plot. E.g. subset=[1,3] will plot\n            plot only the 1st and 3rd dimension but will discard the 0th and 2nd (and,\n            if they exist, the 4th, 5th and so on)\n        resolution: Resolution of the grid at which we evaluate the `pdf`.\n        figsize: Size of the entire figure.\n        labels: List of strings specifying the names of the parameters.\n        ticks: Position of the ticks.\n        points_colors: Colors of the `points`.\n\n        fig: matplotlib figure to plot on.\n        axes: matplotlib axes corresponding to fig.\n        **kwargs: Additional arguments to adjust the plot, e.g., `samples_colors`,\n            `points_colors` and many more, see the source code in `_get_default_opts()`\n            in `sbi.analysis.plot` for details.\n\n    Returns: figure and axis of posterior distribution plot\n    \"\"\"\n    device = density._device if hasattr(density, \"_device\") else \"cpu\"\n\n    # Setting these is required because _pairplot_scaffold will check if opts['diag'] is\n    # `None`. This would break if opts has no key 'diag'. Same for 'upper'.\n    diag = \"cond\"\n    offdiag = \"cond\"\n\n    opts = _get_default_opts()\n    # update the defaults dictionary by the current values of the variables (passed by\n    # the user)\n    opts = _update(opts, locals())\n    opts = _update(opts, kwargs)\n    opts[\"lower\"] = None\n\n    dim, limits, eps_margins = prepare_for_conditional_plot(condition, opts)\n    diag_func = get_conditional_diag_func(opts, limits, eps_margins, resolution)\n\n    def offdiag_func(row, col, **kwargs):\n        p_image = (\n            eval_conditional_density(\n                opts[\"density\"],\n                opts[\"condition\"].to(device),\n                limits.to(device),\n                row,\n                col,\n                resolution=resolution,\n                eps_margins1=eps_margins[row],\n                eps_margins2=eps_margins[col],\n            )\n            .to(\"cpu\")\n            .numpy()\n        )\n        plt.imshow(\n            p_image.T,\n            origin=\"lower\",\n            extent=(\n                limits[col, 0].item(),\n                limits[col, 1].item(),\n                limits[row, 0].item(),\n                limits[row, 1].item(),\n            ),\n            aspect=\"auto\",\n        )\n\n    return _arrange_plots(\n        diag_func, offdiag_func, dim, limits, points, opts, fig=fig, axes=axes\n    )\n</code></pre>"},{"location":"reference/analysis/#sbi.analysis.conditional_density.conditional_corrcoeff","title":"<code>conditional_corrcoeff(density, limits, condition, subset=None, resolution=50)</code>","text":"<p>Returns the conditional correlation matrix of a distribution.</p> <p>To compute the conditional distribution, we condition all but two parameters to values from <code>condition</code>, and then compute the Pearson correlation coefficient \\(\\rho\\) between the remaining two parameters under the distribution <code>density</code>. We do so for any pair of parameters specified in <code>subset</code>, thus creating a matrix containing conditional correlations between any pair of parameters.</p> <p>If <code>condition</code> is a batch of conditions, this function computes the conditional correlation matrix for each one of them and returns the mean.</p> <p>Parameters:</p> Name Type Description Default <code>density</code> <code>Any</code> <p>Probability density function with <code>.log_prob()</code> function.</p> required <code>limits</code> <code>Tensor</code> <p>Limits within which to evaluate the <code>density</code>.</p> required <code>condition</code> <code>Tensor</code> <p>Values to condition the <code>density</code> on. If a batch of conditions is passed, we compute the conditional correlation matrix for each of them and return the average conditional correlation matrix.</p> required <code>subset</code> <code>Optional[List[int]]</code> <p>Evaluate the conditional distribution only on a subset of dimensions. If <code>None</code> this function uses all dimensions.</p> <code>None</code> <code>resolution</code> <code>int</code> <p>Number of grid points on which the conditional distribution is evaluated. A higher value increases the accuracy of the estimated correlation but also increases the computational cost.</p> <code>50</code> <p>Returns: Average conditional correlation matrix of shape either <code>(num_dim, num_dim)</code> or <code>(len(subset), len(subset))</code> if <code>subset</code> was specified.</p> Source code in <code>sbi/analysis/conditional_density.py</code> <pre><code>def conditional_corrcoeff(\n    density: Any,\n    limits: Tensor,\n    condition: Tensor,\n    subset: Optional[List[int]] = None,\n    resolution: int = 50,\n) -&gt; Tensor:\n    r\"\"\"Returns the conditional correlation matrix of a distribution.\n\n    To compute the conditional distribution, we condition all but two parameters to\n    values from `condition`, and then compute the Pearson correlation\n    coefficient $\\rho$ between the remaining two parameters under the distribution\n    `density`. We do so for any pair of parameters specified in `subset`, thus\n    creating a matrix containing conditional correlations between any pair of\n    parameters.\n\n    If `condition` is a batch of conditions, this function computes the conditional\n    correlation matrix for each one of them and returns the mean.\n\n    Args:\n        density: Probability density function with `.log_prob()` function.\n        limits: Limits within which to evaluate the `density`.\n        condition: Values to condition the `density` on. If a batch of conditions is\n            passed, we compute the conditional correlation matrix for each of them and\n            return the average conditional correlation matrix.\n        subset: Evaluate the conditional distribution only on a subset of dimensions.\n            If `None` this function uses all dimensions.\n        resolution: Number of grid points on which the conditional distribution is\n            evaluated. A higher value increases the accuracy of the estimated\n            correlation but also increases the computational cost.\n\n    Returns: Average conditional correlation matrix of shape either `(num_dim, num_dim)`\n    or `(len(subset), len(subset))` if `subset` was specified.\n    \"\"\"\n\n    device = density._device if hasattr(density, \"_device\") else \"cpu\"\n\n    subset_ = subset if subset is not None else range(condition.shape[1])\n\n    correlation_matrices = []\n    for cond in condition:\n        correlation_matrices.append(\n            torch.stack([\n                compute_corrcoeff(\n                    eval_conditional_density(\n                        density,\n                        cond.to(device),\n                        limits.to(device),\n                        dim1=dim1,\n                        dim2=dim2,\n                        resolution=resolution,\n                    ),\n                    limits[[dim1, dim2]].to(device),\n                )\n                for dim1 in subset_\n                for dim2 in subset_\n                if dim1 &lt; dim2\n            ])\n        )\n\n    average_correlations = torch.mean(torch.stack(correlation_matrices), dim=0)\n\n    # `average_correlations` is still a vector containing the upper triangular entries.\n    # Below, assemble them into a matrix:\n    av_correlation_matrix = torch.zeros((len(subset_), len(subset_)), device=device)\n    triu_indices = torch.triu_indices(\n        row=len(subset_), col=len(subset_), offset=1, device=device\n    )\n    av_correlation_matrix[triu_indices[0], triu_indices[1]] = average_correlations\n\n    # Make the matrix symmetric by copying upper diagonal to lower diagonal.\n    av_correlation_matrix = torch.triu(av_correlation_matrix) + torch.tril(\n        av_correlation_matrix.T\n    )\n\n    av_correlation_matrix.fill_diagonal_(1.0)\n    return av_correlation_matrix\n</code></pre>"},{"location":"reference/inference/","title":"Inference","text":""},{"location":"reference/inference/#algorithms","title":"Algorithms","text":""},{"location":"reference/inference/#sbi.inference.snpe.snpe_a.SNPE_A","title":"<code>SNPE_A</code>","text":"<p>               Bases: <code>PosteriorEstimator</code></p> Source code in <code>sbi/inference/snpe/snpe_a.py</code> <pre><code>class SNPE_A(PosteriorEstimator):\n    def __init__(\n        self,\n        prior: Optional[Distribution] = None,\n        density_estimator: Union[str, Callable] = \"mdn_snpe_a\",\n        num_components: int = 10,\n        device: str = \"cpu\",\n        logging_level: Union[int, str] = \"WARNING\",\n        summary_writer: Optional[TensorboardSummaryWriter] = None,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"SNPE-A [1].\n\n        [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional\n            Density Estimation_, Papamakarios et al., NeurIPS 2016,\n            https://arxiv.org/abs/1605.06376.\n\n        This class implements SNPE-A. SNPE-A trains across multiple rounds with a\n        maximum-likelihood-loss. This will make training converge to the proposal\n        posterior instead of the true posterior. To correct for this, SNPE-A applies a\n        post-hoc correction after training. This correction has to be performed\n        analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the\n        last round. In the last round, SNPE-A can use a Mixture of Gaussians.\n\n        Args:\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them. Any\n                object with `.log_prob()`and `.sample()` (for example, a PyTorch\n                distribution) can be used.\n            density_estimator: If it is a string (only \"mdn_snpe_a\" is valid), use a\n                pre-configured mixture of densities network. Alternatively, a function\n                that builds a custom neural network can be provided. The function will\n                be called with the first batch of simulations (theta, x), which can\n                thus be used for shape inference and potentially for z-scoring. It\n                needs to return a PyTorch `nn.Module` implementing the density\n                estimator. The density estimator needs to provide the methods\n                `.log_prob` and `.sample()`. Note that until the last round only a\n                single (multivariate) Gaussian component is used for training (see\n                Algorithm 1 in [1]). In the last round, this component is replicated\n                `num_components` times, its parameters are perturbed with a very small\n                noise, and then the last training round is done with the expanded\n                Gaussian mixture as estimator for the proposal posterior.\n            num_components: Number of components of the mixture of Gaussians in the\n                last round. This overrides the `num_components` value passed to\n                `posterior_nn()`.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n            logging_level: Minimum severity of messages to log. One of the strings\n                INFO, WARNING, DEBUG, ERROR and CRITICAL.\n            summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n                file location (default is `&lt;current working directory&gt;/logs`.)\n            show_progress_bars: Whether to show a progressbar during training.\n        \"\"\"\n\n        # Catch invalid inputs.\n        if not ((density_estimator == \"mdn_snpe_a\") or callable(density_estimator)):\n            raise TypeError(\n                \"The `density_estimator` passed to SNPE_A needs to be a \"\n                \"callable or the string 'mdn_snpe_a'!\"\n            )\n\n        # `num_components` will be used to replicate the Gaussian in the last round.\n        self._num_components = num_components\n        self._ran_final_round = False\n\n        # WARNING: sneaky trick ahead. We proxy the parent's `train` here,\n        # requiring the signature to have `num_atoms`, save it for use below, and\n        # continue. It's sneaky because we are using the object (self) as a namespace\n        # to pass arguments between functions, and that's implicit state management.\n        kwargs = del_entries(\n            locals(),\n            entries=(\"self\", \"__class__\", \"num_components\"),\n        )\n        super().__init__(**kwargs)\n\n    def train(\n        self,\n        final_round: bool = False,\n        training_batch_size: int = 200,\n        learning_rate: float = 5e-4,\n        validation_fraction: float = 0.1,\n        stop_after_epochs: int = 20,\n        max_num_epochs: int = 2**31 - 1,\n        clip_max_norm: Optional[float] = 5.0,\n        calibration_kernel: Optional[Callable] = None,\n        resume_training: bool = False,\n        retrain_from_scratch: bool = False,\n        show_train_summary: bool = False,\n        dataloader_kwargs: Optional[Dict] = None,\n        component_perturbation: float = 5e-3,\n    ) -&gt; ConditionalDensityEstimator:\n        r\"\"\"Return density estimator that approximates the proposal posterior.\n\n        [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional\n            Density Estimation_, Papamakarios et al., NeurIPS 2016,\n            https://arxiv.org/abs/1605.06376.\n\n        Training is performed with maximum likelihood on samples from the latest round,\n        which leads the algorithm to converge to the proposal posterior.\n\n        Args:\n            final_round: Whether we are in the last round of training or not. For all\n                but the last round, Algorithm 1 from [1] is executed. In last the\n                round, Algorithm 2 from [1] is executed once.\n            training_batch_size: Training batch size.\n            learning_rate: Learning rate for Adam optimizer.\n            validation_fraction: The fraction of data to use for validation.\n            stop_after_epochs: The number of epochs to wait for improvement on the\n                validation set before terminating training.\n            max_num_epochs: Maximum number of epochs to run. If reached, we stop\n                training even when the validation loss is still decreasing. Otherwise,\n                we train until validation loss increases (see also `stop_after_epochs`).\n            clip_max_norm: Value at which to clip the total gradient norm in order to\n                prevent exploding gradients. Use None for no clipping.\n            calibration_kernel: A function to calibrate the loss with respect to the\n                simulations `x`. See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017.\n            resume_training: Can be used in case training time is limited, e.g. on a\n                cluster. If `True`, the split between train and validation set, the\n                optimizer, the number of epochs, and the best validation log-prob will\n                be restored from the last time `.train()` was called.\n            force_first_round_loss: If `True`, train with maximum likelihood,\n                i.e., potentially ignoring the correction for using a proposal\n                distribution different from the prior.\n            retrain_from_scratch: Whether to retrain the conditional density\n                estimator for the posterior from scratch each round. Not supported for\n                SNPE-A.\n            show_train_summary: Whether to print the number of epochs and validation\n                loss and leakage after the training.\n            dataloader_kwargs: Additional or updated kwargs to be passed to the training\n                and validation dataloaders (like, e.g., a collate_fn)\n            component_perturbation: The standard deviation applied to all weights and\n                biases when, in the last round, the Mixture of Gaussians is build from\n                a single Gaussian. This value can be problem-specific and also depends\n                on the number of mixture components.\n\n        Returns:\n            Density estimator that approximates the distribution $p(\\theta|x)$.\n        \"\"\"\n\n        assert not retrain_from_scratch, \"\"\"Retraining from scratch is not supported in\n            SNPE-A yet. The reason for this is that, if we reininitialized the density\n            estimator, the z-scoring would change, which would break the posthoc\n            correction. This is a pure implementation issue.\"\"\"\n\n        kwargs = del_entries(\n            locals(),\n            entries=(\n                \"self\",\n                \"__class__\",\n                \"final_round\",\n                \"component_perturbation\",\n            ),\n        )\n\n        # SNPE-A always discards the prior samples.\n        kwargs[\"discard_prior_samples\"] = True\n        kwargs[\"force_first_round_loss\"] = True\n\n        self._round = max(self._data_round_index)\n\n        if final_round:\n            # If there is (will be) only one round, train with Algorithm 2 from [1].\n            if self._round == 0:\n                self._build_neural_net = partial(\n                    self._build_neural_net, num_components=self._num_components\n                )\n            # Run Algorithm 2 from [1].\n            elif not self._ran_final_round:\n                # Now switch to the specified number of components. This method will\n                # only be used if `retrain_from_scratch=True`. Otherwise,\n                # the MDN will be built from replicating the single-component net for\n                # `num_component` times (via `_expand_mog()`).\n                self._build_neural_net = partial(\n                    self._build_neural_net, num_components=self._num_components\n                )\n\n                # Extend the MDN to the originally desired number of components.\n                self._expand_mog(eps=component_perturbation)\n            else:\n                warnings.warn(\n                    \"You have already run SNPE-A with `final_round=True`. Running it\"\n                    \"again with this setting will not allow computing the posthoc\"\n                    \"correction applied in SNPE-A. Thus, you will get an error when \"\n                    \"calling `.build_posterior()` after training.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        else:\n            # Run Algorithm 1 from [1].\n            # Wrap the function that builds the MDN such that we can make\n            # sure that there is only one component when running.\n            self._build_neural_net = partial(self._build_neural_net, num_components=1)\n\n        if final_round:\n            self._ran_final_round = True\n\n        return super().train(**kwargs)\n\n    def correct_for_proposal(\n        self,\n        density_estimator: Optional[TorchModule] = None,\n    ) -&gt; \"SNPE_A_MDN\":\n        r\"\"\"Build mixture of Gaussians that approximates the posterior.\n\n        Returns a `SNPE_A_MDN` object, which applies the posthoc-correction required in\n        SNPE-A.\n\n        Args:\n            density_estimator: The density estimator that the posterior is based on.\n                If `None`, use the latest neural density estimator that was trained.\n\n        Returns:\n            Posterior $p(\\theta|x)$  with `.sample()` and `.log_prob()` methods.\n        \"\"\"\n        if density_estimator is None:\n            density_estimator = deepcopy(\n                self._neural_net\n            )  # PosteriorEstimator.train() also returns a deepcopy, mimic this here\n            # If internal net is used device is defined.\n            device = self._device\n        else:\n            # Otherwise, infer it from the device of the net parameters.\n            device = str(next(density_estimator.parameters()).device)\n\n        # Set proposal of the density estimator.\n        # This also evokes the z-scoring correction if necessary.\n        if (\n            self._proposal_roundwise[-1] is self._prior\n            or self._proposal_roundwise[-1] is None\n        ):\n            proposal = self._prior\n            assert isinstance(\n                proposal, (MultivariateNormal, BoxUniform)\n            ), \"\"\"Prior must be `torch.distributions.MultivariateNormal` or `sbi.utils.\n                BoxUniform`\"\"\"\n        else:\n            assert isinstance(\n                self._proposal_roundwise[-1], DirectPosterior\n            ), \"\"\"The proposal you passed to `append_simulations` is neither the prior\n                nor a `DirectPosterior`. SNPE-A currently only supports these scenarios.\n                \"\"\"\n            proposal = self._proposal_roundwise[-1]\n\n        # Create the SNPE_A_MDN\n        wrapped_density_estimator = SNPE_A_MDN(\n            flow=density_estimator,  # type: ignore\n            proposal=proposal,\n            prior=self._prior,\n            device=device,\n        )\n        return wrapped_density_estimator\n\n    def build_posterior(\n        self,\n        density_estimator: Optional[TorchModule] = None,\n        prior: Optional[Distribution] = None,\n        **kwargs,\n    ) -&gt; \"DirectPosterior\":\n        r\"\"\"Build posterior from the neural density estimator.\n\n        This method first corrects the estimated density with `correct_for_proposal`\n        and then returns a `DirectPosterior`.\n\n        Args:\n            density_estimator: The density estimator that the posterior is based on.\n                If `None`, use the latest neural density estimator that was trained.\n            prior: Prior distribution.\n\n        Returns:\n            Posterior $p(\\theta|x)$  with `.sample()` and `.log_prob()` methods.\n        \"\"\"\n        if prior is None:\n            assert (\n                self._prior is not None\n            ), \"\"\"You did not pass a prior. You have to pass the prior either at\n                initialization `inference = SNPE_A(prior)` or to `.build_posterior\n                (prior=prior)`.\"\"\"\n            prior = self._prior\n\n        wrapped_density_estimator = self.correct_for_proposal(\n            density_estimator=density_estimator\n        )\n        self._posterior = super().build_posterior(\n            density_estimator=wrapped_density_estimator,\n            prior=prior,\n            **kwargs,\n        )\n        return deepcopy(self._posterior)  # type: ignore\n\n    def _log_prob_proposal_posterior(\n        self,\n        theta: Tensor,\n        x: Tensor,\n        masks: Tensor,\n        proposal: Optional[Any],\n    ) -&gt; Tensor:\n        \"\"\"Return the log-probability of the proposal posterior.\n\n        For SNPE-A this is the same as `self._neural_net.log_prob(theta, x)` in\n        `_loss()` to be found in `snpe_base.py`.\n\n        Args:\n            theta: Batch of parameters \u03b8.\n            x: Batch of data.\n            masks: Mask that is True for prior samples in the batch in order to train\n                them with prior loss.\n            proposal: Proposal distribution.\n\n        Returns: Log-probability of the proposal posterior.\n        \"\"\"\n        return self._neural_net.log_prob(theta, x)\n\n    def _expand_mog(self, eps: float = 1e-5):\n        \"\"\"\n        Replicate a singe Gaussian trained with Algorithm 1 before continuing\n        with Algorithm 2. The weights and biases of the associated MDN layers\n        are repeated `num_components` times, slightly perturbed to break the\n        symmetry such that the gradients in the subsequent training are not\n        all identical.\n\n        Args:\n            eps: Standard deviation for the random perturbation.\n        \"\"\"\n        assert isinstance(self._neural_net.net._distribution, MultivariateGaussianMDN)\n\n        # Increase the number of components\n        self._neural_net.net._distribution._num_components = self._num_components\n\n        # Expand the 1-dim Gaussian.\n        for name, param in self._neural_net.named_parameters():\n            if any(\n                key in name for key in [\"logits\", \"means\", \"unconstrained\", \"upper\"]\n            ):\n                if \"bias\" in name:\n                    param.data = param.data.repeat(self._num_components)\n                    param.data.add_(torch.randn_like(param.data) * eps)\n                    param.grad = None  # let autograd construct a new gradient\n                elif \"weight\" in name:\n                    param.data = param.data.repeat(self._num_components, 1)\n                    param.data.add_(torch.randn_like(param.data) * eps)\n                    param.grad = None  # let autograd construct a new gradient\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snpe.snpe_a.SNPE_A.__init__","title":"<code>__init__(prior=None, density_estimator='mdn_snpe_a', num_components=10, device='cpu', logging_level='WARNING', summary_writer=None, show_progress_bars=True)</code>","text":"<p>SNPE-A [1].</p> <p>[1] Fast epsilon-free Inference of Simulation Models with Bayesian Conditional     Density Estimation, Papamakarios et al., NeurIPS 2016,     https://arxiv.org/abs/1605.06376.</p> <p>This class implements SNPE-A. SNPE-A trains across multiple rounds with a maximum-likelihood-loss. This will make training converge to the proposal posterior instead of the true posterior. To correct for this, SNPE-A applies a post-hoc correction after training. This correction has to be performed analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the last round. In the last round, SNPE-A can use a Mixture of Gaussians.</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Optional[Distribution]</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch distribution) can be used.</p> <code>None</code> <code>density_estimator</code> <code>Union[str, Callable]</code> <p>If it is a string (only \u201cmdn_snpe_a\u201d is valid), use a pre-configured mixture of densities network. Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch <code>nn.Module</code> implementing the density estimator. The density estimator needs to provide the methods <code>.log_prob</code> and <code>.sample()</code>. Note that until the last round only a single (multivariate) Gaussian component is used for training (see Algorithm 1 in [1]). In the last round, this component is replicated <code>num_components</code> times, its parameters are perturbed with a very small noise, and then the last training round is done with the expanded Gaussian mixture as estimator for the proposal posterior.</p> <code>'mdn_snpe_a'</code> <code>num_components</code> <code>int</code> <p>Number of components of the mixture of Gaussians in the last round. This overrides the <code>num_components</code> value passed to <code>posterior_nn()</code>.</p> <code>10</code> <code>device</code> <code>str</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d.</p> <code>'cpu'</code> <code>logging_level</code> <code>Union[int, str]</code> <p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p> <code>'WARNING'</code> <code>summary_writer</code> <code>Optional[TensorboardSummaryWriter]</code> <p>A tensorboard <code>SummaryWriter</code> to control, among others, log file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during training.</p> <code>True</code> Source code in <code>sbi/inference/snpe/snpe_a.py</code> <pre><code>def __init__(\n    self,\n    prior: Optional[Distribution] = None,\n    density_estimator: Union[str, Callable] = \"mdn_snpe_a\",\n    num_components: int = 10,\n    device: str = \"cpu\",\n    logging_level: Union[int, str] = \"WARNING\",\n    summary_writer: Optional[TensorboardSummaryWriter] = None,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"SNPE-A [1].\n\n    [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional\n        Density Estimation_, Papamakarios et al., NeurIPS 2016,\n        https://arxiv.org/abs/1605.06376.\n\n    This class implements SNPE-A. SNPE-A trains across multiple rounds with a\n    maximum-likelihood-loss. This will make training converge to the proposal\n    posterior instead of the true posterior. To correct for this, SNPE-A applies a\n    post-hoc correction after training. This correction has to be performed\n    analytically. Thus, SNPE-A is limited to Gaussian distributions for all but the\n    last round. In the last round, SNPE-A can use a Mixture of Gaussians.\n\n    Args:\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. Any\n            object with `.log_prob()`and `.sample()` (for example, a PyTorch\n            distribution) can be used.\n        density_estimator: If it is a string (only \"mdn_snpe_a\" is valid), use a\n            pre-configured mixture of densities network. Alternatively, a function\n            that builds a custom neural network can be provided. The function will\n            be called with the first batch of simulations (theta, x), which can\n            thus be used for shape inference and potentially for z-scoring. It\n            needs to return a PyTorch `nn.Module` implementing the density\n            estimator. The density estimator needs to provide the methods\n            `.log_prob` and `.sample()`. Note that until the last round only a\n            single (multivariate) Gaussian component is used for training (see\n            Algorithm 1 in [1]). In the last round, this component is replicated\n            `num_components` times, its parameters are perturbed with a very small\n            noise, and then the last training round is done with the expanded\n            Gaussian mixture as estimator for the proposal posterior.\n        num_components: Number of components of the mixture of Gaussians in the\n            last round. This overrides the `num_components` value passed to\n            `posterior_nn()`.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n        logging_level: Minimum severity of messages to log. One of the strings\n            INFO, WARNING, DEBUG, ERROR and CRITICAL.\n        summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n            file location (default is `&lt;current working directory&gt;/logs`.)\n        show_progress_bars: Whether to show a progressbar during training.\n    \"\"\"\n\n    # Catch invalid inputs.\n    if not ((density_estimator == \"mdn_snpe_a\") or callable(density_estimator)):\n        raise TypeError(\n            \"The `density_estimator` passed to SNPE_A needs to be a \"\n            \"callable or the string 'mdn_snpe_a'!\"\n        )\n\n    # `num_components` will be used to replicate the Gaussian in the last round.\n    self._num_components = num_components\n    self._ran_final_round = False\n\n    # WARNING: sneaky trick ahead. We proxy the parent's `train` here,\n    # requiring the signature to have `num_atoms`, save it for use below, and\n    # continue. It's sneaky because we are using the object (self) as a namespace\n    # to pass arguments between functions, and that's implicit state management.\n    kwargs = del_entries(\n        locals(),\n        entries=(\"self\", \"__class__\", \"num_components\"),\n    )\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snpe.snpe_a.SNPE_A.build_posterior","title":"<code>build_posterior(density_estimator=None, prior=None, **kwargs)</code>","text":"<p>Build posterior from the neural density estimator.</p> <p>This method first corrects the estimated density with <code>correct_for_proposal</code> and then returns a <code>DirectPosterior</code>.</p> <p>Parameters:</p> Name Type Description Default <code>density_estimator</code> <code>Optional[TorchModule]</code> <p>The density estimator that the posterior is based on. If <code>None</code>, use the latest neural density estimator that was trained.</p> <code>None</code> <code>prior</code> <code>Optional[Distribution]</code> <p>Prior distribution.</p> <code>None</code> <p>Returns:</p> Type Description <code>DirectPosterior</code> <p>Posterior \\(p(\\theta|x)\\)  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p> Source code in <code>sbi/inference/snpe/snpe_a.py</code> <pre><code>def build_posterior(\n    self,\n    density_estimator: Optional[TorchModule] = None,\n    prior: Optional[Distribution] = None,\n    **kwargs,\n) -&gt; \"DirectPosterior\":\n    r\"\"\"Build posterior from the neural density estimator.\n\n    This method first corrects the estimated density with `correct_for_proposal`\n    and then returns a `DirectPosterior`.\n\n    Args:\n        density_estimator: The density estimator that the posterior is based on.\n            If `None`, use the latest neural density estimator that was trained.\n        prior: Prior distribution.\n\n    Returns:\n        Posterior $p(\\theta|x)$  with `.sample()` and `.log_prob()` methods.\n    \"\"\"\n    if prior is None:\n        assert (\n            self._prior is not None\n        ), \"\"\"You did not pass a prior. You have to pass the prior either at\n            initialization `inference = SNPE_A(prior)` or to `.build_posterior\n            (prior=prior)`.\"\"\"\n        prior = self._prior\n\n    wrapped_density_estimator = self.correct_for_proposal(\n        density_estimator=density_estimator\n    )\n    self._posterior = super().build_posterior(\n        density_estimator=wrapped_density_estimator,\n        prior=prior,\n        **kwargs,\n    )\n    return deepcopy(self._posterior)  # type: ignore\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snpe.snpe_a.SNPE_A.correct_for_proposal","title":"<code>correct_for_proposal(density_estimator=None)</code>","text":"<p>Build mixture of Gaussians that approximates the posterior.</p> <p>Returns a <code>SNPE_A_MDN</code> object, which applies the posthoc-correction required in SNPE-A.</p> <p>Parameters:</p> Name Type Description Default <code>density_estimator</code> <code>Optional[TorchModule]</code> <p>The density estimator that the posterior is based on. If <code>None</code>, use the latest neural density estimator that was trained.</p> <code>None</code> <p>Returns:</p> Type Description <code>SNPE_A_MDN</code> <p>Posterior \\(p(\\theta|x)\\)  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p> Source code in <code>sbi/inference/snpe/snpe_a.py</code> <pre><code>def correct_for_proposal(\n    self,\n    density_estimator: Optional[TorchModule] = None,\n) -&gt; \"SNPE_A_MDN\":\n    r\"\"\"Build mixture of Gaussians that approximates the posterior.\n\n    Returns a `SNPE_A_MDN` object, which applies the posthoc-correction required in\n    SNPE-A.\n\n    Args:\n        density_estimator: The density estimator that the posterior is based on.\n            If `None`, use the latest neural density estimator that was trained.\n\n    Returns:\n        Posterior $p(\\theta|x)$  with `.sample()` and `.log_prob()` methods.\n    \"\"\"\n    if density_estimator is None:\n        density_estimator = deepcopy(\n            self._neural_net\n        )  # PosteriorEstimator.train() also returns a deepcopy, mimic this here\n        # If internal net is used device is defined.\n        device = self._device\n    else:\n        # Otherwise, infer it from the device of the net parameters.\n        device = str(next(density_estimator.parameters()).device)\n\n    # Set proposal of the density estimator.\n    # This also evokes the z-scoring correction if necessary.\n    if (\n        self._proposal_roundwise[-1] is self._prior\n        or self._proposal_roundwise[-1] is None\n    ):\n        proposal = self._prior\n        assert isinstance(\n            proposal, (MultivariateNormal, BoxUniform)\n        ), \"\"\"Prior must be `torch.distributions.MultivariateNormal` or `sbi.utils.\n            BoxUniform`\"\"\"\n    else:\n        assert isinstance(\n            self._proposal_roundwise[-1], DirectPosterior\n        ), \"\"\"The proposal you passed to `append_simulations` is neither the prior\n            nor a `DirectPosterior`. SNPE-A currently only supports these scenarios.\n            \"\"\"\n        proposal = self._proposal_roundwise[-1]\n\n    # Create the SNPE_A_MDN\n    wrapped_density_estimator = SNPE_A_MDN(\n        flow=density_estimator,  # type: ignore\n        proposal=proposal,\n        prior=self._prior,\n        device=device,\n    )\n    return wrapped_density_estimator\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snpe.snpe_a.SNPE_A.train","title":"<code>train(final_round=False, training_batch_size=200, learning_rate=0.0005, validation_fraction=0.1, stop_after_epochs=20, max_num_epochs=2 ** 31 - 1, clip_max_norm=5.0, calibration_kernel=None, resume_training=False, retrain_from_scratch=False, show_train_summary=False, dataloader_kwargs=None, component_perturbation=0.005)</code>","text":"<p>Return density estimator that approximates the proposal posterior.</p> <p>[1] Fast epsilon-free Inference of Simulation Models with Bayesian Conditional     Density Estimation, Papamakarios et al., NeurIPS 2016,     https://arxiv.org/abs/1605.06376.</p> <p>Training is performed with maximum likelihood on samples from the latest round, which leads the algorithm to converge to the proposal posterior.</p> <p>Parameters:</p> Name Type Description Default <code>final_round</code> <code>bool</code> <p>Whether we are in the last round of training or not. For all but the last round, Algorithm 1 from [1] is executed. In last the round, Algorithm 2 from [1] is executed once.</p> <code>False</code> <code>training_batch_size</code> <code>int</code> <p>Training batch size.</p> <code>200</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for Adam optimizer.</p> <code>0.0005</code> <code>validation_fraction</code> <code>float</code> <p>The fraction of data to use for validation.</p> <code>0.1</code> <code>stop_after_epochs</code> <code>int</code> <p>The number of epochs to wait for improvement on the validation set before terminating training.</p> <code>20</code> <code>max_num_epochs</code> <code>int</code> <p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. Otherwise, we train until validation loss increases (see also <code>stop_after_epochs</code>).</p> <code>2 ** 31 - 1</code> <code>clip_max_norm</code> <code>Optional[float]</code> <p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p> <code>5.0</code> <code>calibration_kernel</code> <code>Optional[Callable]</code> <p>A function to calibrate the loss with respect to the simulations <code>x</code>. See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017.</p> <code>None</code> <code>resume_training</code> <code>bool</code> <p>Can be used in case training time is limited, e.g. on a cluster. If <code>True</code>, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time <code>.train()</code> was called.</p> <code>False</code> <code>force_first_round_loss</code> <p>If <code>True</code>, train with maximum likelihood, i.e., potentially ignoring the correction for using a proposal distribution different from the prior.</p> required <code>retrain_from_scratch</code> <code>bool</code> <p>Whether to retrain the conditional density estimator for the posterior from scratch each round. Not supported for SNPE-A.</p> <code>False</code> <code>show_train_summary</code> <code>bool</code> <p>Whether to print the number of epochs and validation loss and leakage after the training.</p> <code>False</code> <code>dataloader_kwargs</code> <code>Optional[Dict]</code> <p>Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn)</p> <code>None</code> <code>component_perturbation</code> <code>float</code> <p>The standard deviation applied to all weights and biases when, in the last round, the Mixture of Gaussians is build from a single Gaussian. This value can be problem-specific and also depends on the number of mixture components.</p> <code>0.005</code> <p>Returns:</p> Type Description <code>ConditionalDensityEstimator</code> <p>Density estimator that approximates the distribution \\(p(\\theta|x)\\).</p> Source code in <code>sbi/inference/snpe/snpe_a.py</code> <pre><code>def train(\n    self,\n    final_round: bool = False,\n    training_batch_size: int = 200,\n    learning_rate: float = 5e-4,\n    validation_fraction: float = 0.1,\n    stop_after_epochs: int = 20,\n    max_num_epochs: int = 2**31 - 1,\n    clip_max_norm: Optional[float] = 5.0,\n    calibration_kernel: Optional[Callable] = None,\n    resume_training: bool = False,\n    retrain_from_scratch: bool = False,\n    show_train_summary: bool = False,\n    dataloader_kwargs: Optional[Dict] = None,\n    component_perturbation: float = 5e-3,\n) -&gt; ConditionalDensityEstimator:\n    r\"\"\"Return density estimator that approximates the proposal posterior.\n\n    [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional\n        Density Estimation_, Papamakarios et al., NeurIPS 2016,\n        https://arxiv.org/abs/1605.06376.\n\n    Training is performed with maximum likelihood on samples from the latest round,\n    which leads the algorithm to converge to the proposal posterior.\n\n    Args:\n        final_round: Whether we are in the last round of training or not. For all\n            but the last round, Algorithm 1 from [1] is executed. In last the\n            round, Algorithm 2 from [1] is executed once.\n        training_batch_size: Training batch size.\n        learning_rate: Learning rate for Adam optimizer.\n        validation_fraction: The fraction of data to use for validation.\n        stop_after_epochs: The number of epochs to wait for improvement on the\n            validation set before terminating training.\n        max_num_epochs: Maximum number of epochs to run. If reached, we stop\n            training even when the validation loss is still decreasing. Otherwise,\n            we train until validation loss increases (see also `stop_after_epochs`).\n        clip_max_norm: Value at which to clip the total gradient norm in order to\n            prevent exploding gradients. Use None for no clipping.\n        calibration_kernel: A function to calibrate the loss with respect to the\n            simulations `x`. See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017.\n        resume_training: Can be used in case training time is limited, e.g. on a\n            cluster. If `True`, the split between train and validation set, the\n            optimizer, the number of epochs, and the best validation log-prob will\n            be restored from the last time `.train()` was called.\n        force_first_round_loss: If `True`, train with maximum likelihood,\n            i.e., potentially ignoring the correction for using a proposal\n            distribution different from the prior.\n        retrain_from_scratch: Whether to retrain the conditional density\n            estimator for the posterior from scratch each round. Not supported for\n            SNPE-A.\n        show_train_summary: Whether to print the number of epochs and validation\n            loss and leakage after the training.\n        dataloader_kwargs: Additional or updated kwargs to be passed to the training\n            and validation dataloaders (like, e.g., a collate_fn)\n        component_perturbation: The standard deviation applied to all weights and\n            biases when, in the last round, the Mixture of Gaussians is build from\n            a single Gaussian. This value can be problem-specific and also depends\n            on the number of mixture components.\n\n    Returns:\n        Density estimator that approximates the distribution $p(\\theta|x)$.\n    \"\"\"\n\n    assert not retrain_from_scratch, \"\"\"Retraining from scratch is not supported in\n        SNPE-A yet. The reason for this is that, if we reininitialized the density\n        estimator, the z-scoring would change, which would break the posthoc\n        correction. This is a pure implementation issue.\"\"\"\n\n    kwargs = del_entries(\n        locals(),\n        entries=(\n            \"self\",\n            \"__class__\",\n            \"final_round\",\n            \"component_perturbation\",\n        ),\n    )\n\n    # SNPE-A always discards the prior samples.\n    kwargs[\"discard_prior_samples\"] = True\n    kwargs[\"force_first_round_loss\"] = True\n\n    self._round = max(self._data_round_index)\n\n    if final_round:\n        # If there is (will be) only one round, train with Algorithm 2 from [1].\n        if self._round == 0:\n            self._build_neural_net = partial(\n                self._build_neural_net, num_components=self._num_components\n            )\n        # Run Algorithm 2 from [1].\n        elif not self._ran_final_round:\n            # Now switch to the specified number of components. This method will\n            # only be used if `retrain_from_scratch=True`. Otherwise,\n            # the MDN will be built from replicating the single-component net for\n            # `num_component` times (via `_expand_mog()`).\n            self._build_neural_net = partial(\n                self._build_neural_net, num_components=self._num_components\n            )\n\n            # Extend the MDN to the originally desired number of components.\n            self._expand_mog(eps=component_perturbation)\n        else:\n            warnings.warn(\n                \"You have already run SNPE-A with `final_round=True`. Running it\"\n                \"again with this setting will not allow computing the posthoc\"\n                \"correction applied in SNPE-A. Thus, you will get an error when \"\n                \"calling `.build_posterior()` after training.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    else:\n        # Run Algorithm 1 from [1].\n        # Wrap the function that builds the MDN such that we can make\n        # sure that there is only one component when running.\n        self._build_neural_net = partial(self._build_neural_net, num_components=1)\n\n    if final_round:\n        self._ran_final_round = True\n\n    return super().train(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snpe.snpe_c.SNPE_C","title":"<code>SNPE_C</code>","text":"<p>               Bases: <code>PosteriorEstimator</code></p> Source code in <code>sbi/inference/snpe/snpe_c.py</code> <pre><code>class SNPE_C(PosteriorEstimator):\n    def __init__(\n        self,\n        prior: Optional[Distribution] = None,\n        density_estimator: Union[str, Callable] = \"maf\",\n        device: str = \"cpu\",\n        logging_level: Union[int, str] = \"WARNING\",\n        summary_writer: Optional[TensorboardSummaryWriter] = None,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"SNPE-C / APT [1].\n\n        [1] _Automatic Posterior Transformation for Likelihood-free Inference_,\n            Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.\n\n        This class implements two loss variants of SNPE-C: the non-atomic and the atomic\n        version. The atomic loss of SNPE-C can be used for any density estimator,\n        i.e. also for normalizing flows. However, it suffers from leakage issues. On\n        the other hand, the non-atomic loss can only be used only if the proposal\n        distribution is a mixture of Gaussians, the density estimator is a mixture of\n        Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from\n        leakage issues. At the beginning of each round, we print whether the non-atomic\n        or the atomic version is used.\n\n        In this codebase, we will automatically switch to the non-atomic loss if the\n        following criteria are fulfilled:&lt;br/&gt;\n        - proposal is a `DirectPosterior` with density_estimator `mdn`, as built\n            with `sbi.neural_nets.posterior_nn()`.&lt;br/&gt;\n        - the density estimator is a `mdn`, as built with\n            `sbi.neural_nets.posterior_nn()`.&lt;br/&gt;\n        - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or\n            `isinstance(prior, sbi.utils.BoxUniform)`\n\n        Note that custom implementations of any of these densities (or estimators) will\n        not trigger the non-atomic loss, and the algorithm will fall back onto using\n        the atomic loss.\n\n        Args:\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them.\n            density_estimator: If it is a string, use a pre-configured network of the\n                provided type (one of nsf, maf, mdn, made). Alternatively, a function\n                that builds a custom neural network can be provided. The function will\n                be called with the first batch of simulations (theta, x), which can\n                thus be used for shape inference and potentially for z-scoring. It\n                needs to return a PyTorch `nn.Module` implementing the density\n                estimator. The density estimator needs to provide the methods\n                `.log_prob` and `.sample()`.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n            logging_level: Minimum severity of messages to log. One of the strings\n                INFO, WARNING, DEBUG, ERROR and CRITICAL.\n            summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n                file location (default is `&lt;current working directory&gt;/logs`.)\n            show_progress_bars: Whether to show a progressbar during training.\n        \"\"\"\n\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        super().__init__(**kwargs)\n\n    def train(\n        self,\n        num_atoms: int = 10,\n        training_batch_size: int = 200,\n        learning_rate: float = 5e-4,\n        validation_fraction: float = 0.1,\n        stop_after_epochs: int = 20,\n        max_num_epochs: int = 2**31 - 1,\n        clip_max_norm: Optional[float] = 5.0,\n        calibration_kernel: Optional[Callable] = None,\n        resume_training: bool = False,\n        force_first_round_loss: bool = False,\n        discard_prior_samples: bool = False,\n        use_combined_loss: bool = False,\n        retrain_from_scratch: bool = False,\n        show_train_summary: bool = False,\n        dataloader_kwargs: Optional[Dict] = None,\n    ) -&gt; nn.Module:\n        r\"\"\"Return density estimator that approximates the distribution $p(\\theta|x)$.\n\n        Args:\n            num_atoms: Number of atoms to use for classification.\n            training_batch_size: Training batch size.\n            learning_rate: Learning rate for Adam optimizer.\n            validation_fraction: The fraction of data to use for validation.\n            stop_after_epochs: The number of epochs to wait for improvement on the\n                validation set before terminating training.\n            max_num_epochs: Maximum number of epochs to run. If reached, we stop\n                training even when the validation loss is still decreasing. Otherwise,\n                we train until validation loss increases (see also `stop_after_epochs`).\n            clip_max_norm: Value at which to clip the total gradient norm in order to\n                prevent exploding gradients. Use None for no clipping.\n            calibration_kernel: A function to calibrate the loss with respect to the\n                simulations `x`. See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017.\n            resume_training: Can be used in case training time is limited, e.g. on a\n                cluster. If `True`, the split between train and validation set, the\n                optimizer, the number of epochs, and the best validation log-prob will\n                be restored from the last time `.train()` was called.\n            force_first_round_loss: If `True`, train with maximum likelihood,\n                i.e., potentially ignoring the correction for using a proposal\n                distribution different from the prior.\n            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n                from the prior. Training may be sped up by ignoring such less targeted\n                samples.\n            use_combined_loss: Whether to train the neural net also on prior samples\n                using maximum likelihood in addition to training it on all samples using\n                atomic loss. The extra MLE loss helps prevent density leaking with\n                bounded priors.\n            retrain_from_scratch: Whether to retrain the conditional density\n                estimator for the posterior from scratch each round.\n            show_train_summary: Whether to print the number of epochs and validation\n                loss and leakage after the training.\n            dataloader_kwargs: Additional or updated kwargs to be passed to the training\n                and validation dataloaders (like, e.g., a collate_fn)\n\n        Returns:\n            Density estimator that approximates the distribution $p(\\theta|x)$.\n        \"\"\"\n\n        # WARNING: sneaky trick ahead. We proxy the parent's `train` here,\n        # requiring the signature to have `num_atoms`, save it for use below, and\n        # continue. It's sneaky because we are using the object (self) as a namespace\n        # to pass arguments between functions, and that's implicit state management.\n        self._num_atoms = num_atoms\n        self._use_combined_loss = use_combined_loss\n        kwargs = del_entries(\n            locals(),\n            entries=(\"self\", \"__class__\", \"num_atoms\", \"use_combined_loss\"),\n        )\n\n        self._round = max(self._data_round_index)\n\n        if self._round &gt; 0:\n            # Set the proposal to the last proposal that was passed by the user. For\n            # atomic SNPE, it does not matter what the proposal is. For non-atomic\n            # SNPE, we only use the latest data that was passed, i.e. the one from the\n            # last proposal.\n            proposal = self._proposal_roundwise[-1]\n            self.use_non_atomic_loss = (\n                isinstance(proposal, DirectPosterior)\n                and isinstance(proposal.posterior_estimator.net._distribution, mdn)\n                and isinstance(self._neural_net.net._distribution, mdn)\n                and check_dist_class(\n                    self._prior, class_to_check=(Uniform, MultivariateNormal)\n                )[0]\n            )\n\n            algorithm = \"non-atomic\" if self.use_non_atomic_loss else \"atomic\"\n            print(f\"Using SNPE-C with {algorithm} loss\")\n\n            if self.use_non_atomic_loss:\n                # Take care of z-scoring, pre-compute and store prior terms.\n                self._set_state_for_mog_proposal()\n\n        return super().train(**kwargs)\n\n    def _set_state_for_mog_proposal(self) -&gt; None:\n        \"\"\"Set state variables that are used at each training step of non-atomic SNPE-C.\n\n        Three things are computed:\n        1) Check if z-scoring was requested. To do so, we check if the `_transform`\n            argument of the net had been a `CompositeTransform`. See pyknos mdn.py.\n        2) Define a (potentially standardized) prior. It's standardized if z-scoring\n            had been requested.\n        3) Compute (Precision * mean) for the prior. This quantity is used at every\n            training step if the prior is Gaussian.\n        \"\"\"\n\n        self.z_score_theta = isinstance(\n            self._neural_net.net._transform, CompositeTransform\n        )\n\n        self._set_maybe_z_scored_prior()\n\n        if isinstance(self._maybe_z_scored_prior, MultivariateNormal):\n            self.prec_m_prod_prior = torch.mv(\n                self._maybe_z_scored_prior.precision_matrix,  # type: ignore\n                self._maybe_z_scored_prior.loc,  # type: ignore\n            )\n\n    def _set_maybe_z_scored_prior(self) -&gt; None:\n        r\"\"\"Compute and store potentially standardized prior (if z-scoring was done).\n\n        The proposal posterior is:\n        $pp(\\theta|x) = 1/Z * q(\\theta|x) * prop(\\theta) / p(\\theta)$\n\n        Let's denote z-scored theta by `a`: a = (theta - mean) / std\n        Then pp'(a|x) = 1/Z_2 * q'(a|x) * prop'(a) / p'(a)$\n\n        The ' indicates that the evaluation occurs in standardized space. The constant\n        scaling factor has been absorbed into Z_2.\n        From the above equation, we see that we need to evaluate the prior **in\n        standardized space**. We build the standardized prior in this function.\n\n        The standardize transform that is applied to the samples theta does not use\n        the exact prior mean and std (due to implementation issues). Hence, the z-scored\n        prior will not be exactly have mean=0 and std=1.\n        \"\"\"\n\n        if self.z_score_theta:\n            scale = self._neural_net.net._transform._transforms[0]._scale\n            shift = self._neural_net.net._transform._transforms[0]._shift\n\n            # Following the definintion of the linear transform in\n            # `standardizing_transform` in `sbiutils.py`:\n            # shift=-mean / std\n            # scale=1 / std\n            # Solving these equations for mean and std:\n            estim_prior_std = 1 / scale\n            estim_prior_mean = -shift * estim_prior_std\n\n            # Compute the discrepancy of the true prior mean and std and the mean and\n            # std that was empirically estimated from samples.\n            # N(theta|m,s) = N((theta-m_e)/s_e|(m-m_e)/s_e, s/s_e)\n            # Above: m,s are true prior mean and std. m_e,s_e are estimated prior mean\n            # and std (estimated from samples and used to build standardize transform).\n            almost_zero_mean = (self._prior.mean - estim_prior_mean) / estim_prior_std\n            almost_one_std = torch.sqrt(self._prior.variance) / estim_prior_std\n\n            if isinstance(self._prior, MultivariateNormal):\n                self._maybe_z_scored_prior = MultivariateNormal(\n                    almost_zero_mean, torch.diag(almost_one_std)\n                )\n            else:\n                range_ = torch.sqrt(almost_one_std * 3.0)\n                self._maybe_z_scored_prior = BoxUniform(\n                    almost_zero_mean - range_, almost_zero_mean + range_\n                )\n        else:\n            self._maybe_z_scored_prior = self._prior\n\n    def _log_prob_proposal_posterior(\n        self,\n        theta: Tensor,\n        x: Tensor,\n        masks: Tensor,\n        proposal: DirectPosterior,\n    ) -&gt; Tensor:\n        \"\"\"Return the log-probability of the proposal posterior.\n\n        If the proposal is a MoG, the density estimator is a MoG, and the prior is\n        either Gaussian or uniform, we use non-atomic loss. Else, use atomic loss (which\n        suffers from leakage).\n\n        Args:\n            theta: Batch of parameters \u03b8.\n            x: Batch of data.\n            masks: Mask that is True for prior samples in the batch in order to train\n                them with prior loss.\n            proposal: Proposal distribution.\n\n        Returns: Log-probability of the proposal posterior.\n        \"\"\"\n\n        if self.use_non_atomic_loss:\n            if not (\n                hasattr(self._neural_net.net, \"_distribution\")\n                and isinstance(self._neural_net.net._distribution, mdn)\n            ):\n                raise ValueError(\n                    \"The density estimator must be a MDNtext for non-atomic loss.\"\n                )\n\n            return self._log_prob_proposal_posterior_mog(theta, x, proposal)\n        else:\n            if not hasattr(self._neural_net, \"log_prob\"):\n                raise ValueError(\n                    \"The neural estimator must have a log_prob method, for\\\n                                 atomic loss. It should at best follow the \\\n                                 sbi.neural_nets 'DensityEstiamtor' interface.\"\n                )\n            return self._log_prob_proposal_posterior_atomic(theta, x, masks)\n\n    def _log_prob_proposal_posterior_atomic(\n        self, theta: Tensor, x: Tensor, masks: Tensor\n    ):\n        \"\"\"Return log probability of the proposal posterior for atomic proposals.\n\n        We have two main options when evaluating the proposal posterior.\n            (1) Generate atoms from the proposal prior.\n            (2) Generate atoms from a more targeted distribution, such as the most\n                recent posterior.\n        If we choose the latter, it is likely beneficial not to do this in the first\n        round, since we would be sampling from a randomly-initialized neural density\n        estimator.\n\n        Args:\n            theta: Batch of parameters \u03b8.\n            x: Batch of data.\n            masks: Mask that is True for prior samples in the batch in order to train\n                them with prior loss.\n\n        Returns:\n            Log-probability of the proposal posterior.\n        \"\"\"\n        batch_size = theta.shape[0]\n\n        num_atoms = int(\n            clamp_and_warn(\"num_atoms\", self._num_atoms, min_val=2, max_val=batch_size)\n        )\n\n        # Each set of parameter atoms is evaluated using the same x,\n        # so we repeat rows of the data x, e.g. [1, 2] -&gt; [1, 1, 2, 2]\n        repeated_x = repeat_rows(x, num_atoms)\n\n        # To generate the full set of atoms for a given item in the batch,\n        # we sample without replacement num_atoms - 1 times from the rest\n        # of the theta in the batch.\n        probs = ones(batch_size, batch_size) * (1 - eye(batch_size)) / (batch_size - 1)\n\n        choices = torch.multinomial(probs, num_samples=num_atoms - 1, replacement=False)\n        contrasting_theta = theta[choices]\n\n        # We can now create our sets of atoms from the contrasting parameter sets\n        # we have generated.\n        atomic_theta = torch.cat((theta[:, None, :], contrasting_theta), dim=1).reshape(\n            batch_size * num_atoms, -1\n        )\n\n        # Get (batch_size * num_atoms) log prob prior evals.\n        log_prob_prior = self._prior.log_prob(atomic_theta)\n        log_prob_prior = log_prob_prior.reshape(batch_size, num_atoms)\n        assert_all_finite(log_prob_prior, \"prior eval\")\n\n        # Evaluate large batch giving (batch_size * num_atoms) log prob posterior evals.\n        atomic_theta = reshape_to_sample_batch_event(\n            atomic_theta, atomic_theta.shape[1:]\n        )\n        repeated_x = reshape_to_batch_event(\n            repeated_x, self._neural_net.condition_shape\n        )\n        log_prob_posterior = self._neural_net.log_prob(atomic_theta, repeated_x)\n        assert_all_finite(log_prob_posterior, \"posterior eval\")\n        log_prob_posterior = log_prob_posterior.reshape(batch_size, num_atoms)\n\n        # Compute unnormalized proposal posterior.\n        unnormalized_log_prob = log_prob_posterior - log_prob_prior\n\n        # Normalize proposal posterior across discrete set of atoms.\n        log_prob_proposal_posterior = unnormalized_log_prob[:, 0] - torch.logsumexp(\n            unnormalized_log_prob, dim=-1\n        )\n        assert_all_finite(log_prob_proposal_posterior, \"proposal posterior eval\")\n\n        # XXX This evaluates the posterior on _all_ prior samples\n        if self._use_combined_loss:\n            theta = reshape_to_sample_batch_event(theta, self._neural_net.input_shape)\n            x = reshape_to_batch_event(x, self._neural_net.condition_shape)\n            log_prob_posterior_non_atomic = self._neural_net.log_prob(theta, x)\n            # squeeze to remove sample dimension, which is always one during the loss\n            # evaluation of `SNPE_C` (because we have one theta vector per x vector).\n            log_prob_posterior_non_atomic = log_prob_posterior_non_atomic.squeeze(dim=0)\n            masks = masks.reshape(-1)\n            log_prob_proposal_posterior = (\n                masks * log_prob_posterior_non_atomic + log_prob_proposal_posterior\n            )\n\n        return log_prob_proposal_posterior\n\n    def _log_prob_proposal_posterior_mog(\n        self, theta: Tensor, x: Tensor, proposal: DirectPosterior\n    ) -&gt; Tensor:\n        \"\"\"Return log-probability of the proposal posterior for MoG proposal.\n\n        For MoG proposals and MoG density estimators, this can be done in closed form\n        and does not require atomic loss (i.e. there will be no leakage issues).\n\n        Notation:\n\n        m are mean vectors.\n        prec are precision matrices.\n        cov are covariance matrices.\n\n        _p at the end indicates that it is the proposal.\n        _d indicates that it is the density estimator.\n        _pp indicates the proposal posterior.\n\n        All tensors will have shapes (batch_dim, num_components, ...)\n\n        Args:\n            theta: Batch of parameters \u03b8.\n            x: Batch of data.\n            proposal: Proposal distribution.\n\n        Returns:\n            Log-probability of the proposal posterior.\n        \"\"\"\n\n        # Evaluate the proposal. MDNs do not have functionality to run the embedding_net\n        # and then get the mixture_components (**without** calling log_prob()). Hence,\n        # we call them separately here.\n        encoded_x = proposal.posterior_estimator.net._embedding_net(proposal.default_x)\n        dist = (\n            proposal.posterior_estimator.net._distribution\n        )  # defined to avoid ugly black formatting.\n        logits_p, m_p, prec_p, _, _ = dist.get_mixture_components(encoded_x)\n        norm_logits_p = logits_p - torch.logsumexp(logits_p, dim=-1, keepdim=True)\n\n        # Evaluate the density estimator.\n        encoded_x = self._neural_net.net._embedding_net(x)\n        dist = self._neural_net.net._distribution  # defined to avoid black formatting.\n        logits_d, m_d, prec_d, _, _ = dist.get_mixture_components(encoded_x)\n        norm_logits_d = logits_d - torch.logsumexp(logits_d, dim=-1, keepdim=True)\n\n        # z-score theta if it z-scoring had been requested.\n        theta = self._maybe_z_score_theta(theta)\n\n        # Compute the MoG parameters of the proposal posterior.\n        (\n            logits_pp,\n            m_pp,\n            prec_pp,\n            cov_pp,\n        ) = self._automatic_posterior_transformation(\n            norm_logits_p, m_p, prec_p, norm_logits_d, m_d, prec_d\n        )\n\n        # Compute the log_prob of theta under the product.\n        log_prob_proposal_posterior = mog_log_prob(theta, logits_pp, m_pp, prec_pp)\n        assert_all_finite(\n            log_prob_proposal_posterior,\n            \"\"\"the evaluation of the MoG proposal posterior. This is likely due to a\n            numerical instability in the training procedure. Please create an issue on\n            Github.\"\"\",\n        )\n\n        return log_prob_proposal_posterior\n\n    def _automatic_posterior_transformation(\n        self,\n        logits_p: Tensor,\n        means_p: Tensor,\n        precisions_p: Tensor,\n        logits_d: Tensor,\n        means_d: Tensor,\n        precisions_d: Tensor,\n    ):\n        r\"\"\"Returns the MoG parameters of the proposal posterior.\n\n        The proposal posterior is:\n        $pp(\\theta|x) = 1/Z * q(\\theta|x) * prop(\\theta) / p(\\theta)$\n        In words: proposal posterior = posterior estimate * proposal / prior.\n\n        If the posterior estimate and the proposal are MoG and the prior is either\n        Gaussian or uniform, we can solve this in closed-form. The is implemented in\n        this function.\n\n        This function implements Appendix A1 from Greenberg et al. 2019.\n\n        We have to build L*K components. How do we do this?\n        Example: proposal has two components, density estimator has three components.\n        Let's call the two components of the proposal i,j and the three components\n        of the density estimator x,y,z. We have to multiply every component of the\n        proposal with every component of the density estimator. So, what we do is:\n        1) for the proposal, build: i,i,i,j,j,j. Done with torch.repeat_interleave()\n        2) for the density estimator, build: x,y,z,x,y,z. Done with torch.repeat()\n        3) Multiply them with simple matrix operations.\n\n        Args:\n            logits_p: Component weight of each Gaussian of the proposal.\n            means_p: Mean of each Gaussian of the proposal.\n            precisions_p: Precision matrix of each Gaussian of the proposal.\n            logits_d: Component weight for each Gaussian of the density estimator.\n            means_d: Mean of each Gaussian of the density estimator.\n            precisions_d: Precision matrix of each Gaussian of the density estimator.\n\n        Returns: (Component weight, mean, precision matrix, covariance matrix) of each\n            Gaussian of the proposal posterior. Has L*K terms (proposal has L terms,\n            density estimator has K terms).\n        \"\"\"\n\n        precisions_pp, covariances_pp = self._precisions_proposal_posterior(\n            precisions_p, precisions_d\n        )\n\n        means_pp = self._means_proposal_posterior(\n            covariances_pp, means_p, precisions_p, means_d, precisions_d\n        )\n\n        logits_pp = self._logits_proposal_posterior(\n            means_pp,\n            precisions_pp,\n            covariances_pp,\n            logits_p,\n            means_p,\n            precisions_p,\n            logits_d,\n            means_d,\n            precisions_d,\n        )\n\n        return logits_pp, means_pp, precisions_pp, covariances_pp\n\n    def _precisions_proposal_posterior(\n        self, precisions_p: Tensor, precisions_d: Tensor\n    ):\n        \"\"\"Return the precisions and covariances of the proposal posterior.\n\n        Args:\n            precisions_p: Precision matrices of the proposal distribution.\n            precisions_d: Precision matrices of the density estimator.\n\n        Returns: (Precisions, Covariances) of the proposal posterior. L*K terms.\n        \"\"\"\n\n        num_comps_p = precisions_p.shape[1]\n        num_comps_d = precisions_d.shape[1]\n\n        precisions_p_rep = precisions_p.repeat_interleave(num_comps_d, dim=1)\n        precisions_d_rep = precisions_d.repeat(1, num_comps_p, 1, 1)\n\n        precisions_pp = precisions_p_rep + precisions_d_rep\n        if isinstance(self._maybe_z_scored_prior, MultivariateNormal):\n            precisions_pp -= self._maybe_z_scored_prior.precision_matrix\n\n        covariances_pp = torch.inverse(precisions_pp)\n\n        return precisions_pp, covariances_pp\n\n    def _means_proposal_posterior(\n        self,\n        covariances_pp: Tensor,\n        means_p: Tensor,\n        precisions_p: Tensor,\n        means_d: Tensor,\n        precisions_d: Tensor,\n    ):\n        \"\"\"Return the means of the proposal posterior.\n\n        means_pp = C_ix * (P_i * m_i + P_x * m_x - P_o * m_o).\n\n        Args:\n            covariances_pp: Covariance matrices of the proposal posterior.\n            means_p: Means of the proposal distribution.\n            precisions_p: Precision matrices of the proposal distribution.\n            means_d: Means of the density estimator.\n            precisions_d: Precision matrices of the density estimator.\n\n        Returns: Means of the proposal posterior. L*K terms.\n        \"\"\"\n\n        num_comps_p = precisions_p.shape[1]\n        num_comps_d = precisions_d.shape[1]\n\n        # First, compute the product P_i * m_i and P_j * m_j\n        prec_m_prod_p = batched_mixture_mv(precisions_p, means_p)\n        prec_m_prod_d = batched_mixture_mv(precisions_d, means_d)\n\n        # Repeat them to allow for matrix operations: same trick as for the precisions.\n        prec_m_prod_p_rep = prec_m_prod_p.repeat_interleave(num_comps_d, dim=1)\n        prec_m_prod_d_rep = prec_m_prod_d.repeat(1, num_comps_p, 1)\n\n        # Means = C_ij * (P_i * m_i + P_x * m_x - P_o * m_o).\n        summed_cov_m_prod_rep = prec_m_prod_p_rep + prec_m_prod_d_rep\n        if isinstance(self._maybe_z_scored_prior, MultivariateNormal):\n            summed_cov_m_prod_rep -= self.prec_m_prod_prior\n\n        means_pp = batched_mixture_mv(covariances_pp, summed_cov_m_prod_rep)\n\n        return means_pp\n\n    @staticmethod\n    def _logits_proposal_posterior(\n        means_pp: Tensor,\n        precisions_pp: Tensor,\n        covariances_pp: Tensor,\n        logits_p: Tensor,\n        means_p: Tensor,\n        precisions_p: Tensor,\n        logits_d: Tensor,\n        means_d: Tensor,\n        precisions_d: Tensor,\n    ):\n        \"\"\"Return the component weights (i.e. logits) of the proposal posterior.\n\n        Args:\n            means_pp: Means of the proposal posterior.\n            precisions_pp: Precision matrices of the proposal posterior.\n            covariances_pp: Covariance matrices of the proposal posterior.\n            logits_p: Component weights (i.e. logits) of the proposal distribution.\n            means_p: Means of the proposal distribution.\n            precisions_p: Precision matrices of the proposal distribution.\n            logits_d: Component weights (i.e. logits) of the density estimator.\n            means_d: Means of the density estimator.\n            precisions_d: Precision matrices of the density estimator.\n\n        Returns: Component weights of the proposal posterior. L*K terms.\n        \"\"\"\n\n        num_comps_p = precisions_p.shape[1]\n        num_comps_d = precisions_d.shape[1]\n\n        # Compute log(alpha_i * beta_j)\n        logits_p_rep = logits_p.repeat_interleave(num_comps_d, dim=1)\n        logits_d_rep = logits_d.repeat(1, num_comps_p)\n        logit_factors = logits_p_rep + logits_d_rep\n\n        # Compute sqrt(det()/(det()*det()))\n        logdet_covariances_pp = torch.logdet(covariances_pp)\n        logdet_covariances_p = -torch.logdet(precisions_p)\n        logdet_covariances_d = -torch.logdet(precisions_d)\n\n        # Repeat the proposal and density estimator terms such that there are LK terms.\n        # Same trick as has been used above.\n        logdet_covariances_p_rep = logdet_covariances_p.repeat_interleave(\n            num_comps_d, dim=1\n        )\n        logdet_covariances_d_rep = logdet_covariances_d.repeat(1, num_comps_p)\n\n        log_sqrt_det_ratio = 0.5 * (\n            logdet_covariances_pp\n            - (logdet_covariances_p_rep + logdet_covariances_d_rep)\n        )\n\n        # Compute for proposal, density estimator, and proposal posterior:\n        # mu_i.T * P_i * mu_i\n        exponent_p = batched_mixture_vmv(precisions_p, means_p)\n        exponent_d = batched_mixture_vmv(precisions_d, means_d)\n        exponent_pp = batched_mixture_vmv(precisions_pp, means_pp)\n\n        # Extend proposal and density estimator exponents to get LK terms.\n        exponent_p_rep = exponent_p.repeat_interleave(num_comps_d, dim=1)\n        exponent_d_rep = exponent_d.repeat(1, num_comps_p)\n        exponent = -0.5 * (exponent_p_rep + exponent_d_rep - exponent_pp)\n\n        logits_pp = logit_factors + log_sqrt_det_ratio + exponent\n\n        return logits_pp\n\n    def _maybe_z_score_theta(self, theta: Tensor) -&gt; Tensor:\n        \"\"\"Return potentially standardized theta if z-scoring was requested.\"\"\"\n\n        if self.z_score_theta:\n            theta, _ = self._neural_net.net._transform(theta)\n\n        return theta\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snpe.snpe_c.SNPE_C.__init__","title":"<code>__init__(prior=None, density_estimator='maf', device='cpu', logging_level='WARNING', summary_writer=None, show_progress_bars=True)</code>","text":"<p>SNPE-C / APT [1].</p> <p>[1] Automatic Posterior Transformation for Likelihood-free Inference,     Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</p> <p>This class implements two loss variants of SNPE-C: the non-atomic and the atomic version. The atomic loss of SNPE-C can be used for any density estimator, i.e. also for normalizing flows. However, it suffers from leakage issues. On the other hand, the non-atomic loss can only be used only if the proposal distribution is a mixture of Gaussians, the density estimator is a mixture of Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from leakage issues. At the beginning of each round, we print whether the non-atomic or the atomic version is used.</p> <p>In this codebase, we will automatically switch to the non-atomic loss if the following criteria are fulfilled: - proposal is a <code>DirectPosterior</code> with density_estimator <code>mdn</code>, as built     with <code>sbi.neural_nets.posterior_nn()</code>. - the density estimator is a <code>mdn</code>, as built with     <code>sbi.neural_nets.posterior_nn()</code>. - <code>isinstance(prior, MultivariateNormal)</code> (from <code>torch.distributions</code>) or     <code>isinstance(prior, sbi.utils.BoxUniform)</code></p> <p>Note that custom implementations of any of these densities (or estimators) will not trigger the non-atomic loss, and the algorithm will fall back onto using the atomic loss.</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Optional[Distribution]</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them.</p> <code>None</code> <code>density_estimator</code> <code>Union[str, Callable]</code> <p>If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch <code>nn.Module</code> implementing the density estimator. The density estimator needs to provide the methods <code>.log_prob</code> and <code>.sample()</code>.</p> <code>'maf'</code> <code>device</code> <code>str</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d.</p> <code>'cpu'</code> <code>logging_level</code> <code>Union[int, str]</code> <p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p> <code>'WARNING'</code> <code>summary_writer</code> <code>Optional[TensorboardSummaryWriter]</code> <p>A tensorboard <code>SummaryWriter</code> to control, among others, log file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during training.</p> <code>True</code> Source code in <code>sbi/inference/snpe/snpe_c.py</code> <pre><code>def __init__(\n    self,\n    prior: Optional[Distribution] = None,\n    density_estimator: Union[str, Callable] = \"maf\",\n    device: str = \"cpu\",\n    logging_level: Union[int, str] = \"WARNING\",\n    summary_writer: Optional[TensorboardSummaryWriter] = None,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"SNPE-C / APT [1].\n\n    [1] _Automatic Posterior Transformation for Likelihood-free Inference_,\n        Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.\n\n    This class implements two loss variants of SNPE-C: the non-atomic and the atomic\n    version. The atomic loss of SNPE-C can be used for any density estimator,\n    i.e. also for normalizing flows. However, it suffers from leakage issues. On\n    the other hand, the non-atomic loss can only be used only if the proposal\n    distribution is a mixture of Gaussians, the density estimator is a mixture of\n    Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from\n    leakage issues. At the beginning of each round, we print whether the non-atomic\n    or the atomic version is used.\n\n    In this codebase, we will automatically switch to the non-atomic loss if the\n    following criteria are fulfilled:&lt;br/&gt;\n    - proposal is a `DirectPosterior` with density_estimator `mdn`, as built\n        with `sbi.neural_nets.posterior_nn()`.&lt;br/&gt;\n    - the density estimator is a `mdn`, as built with\n        `sbi.neural_nets.posterior_nn()`.&lt;br/&gt;\n    - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or\n        `isinstance(prior, sbi.utils.BoxUniform)`\n\n    Note that custom implementations of any of these densities (or estimators) will\n    not trigger the non-atomic loss, and the algorithm will fall back onto using\n    the atomic loss.\n\n    Args:\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them.\n        density_estimator: If it is a string, use a pre-configured network of the\n            provided type (one of nsf, maf, mdn, made). Alternatively, a function\n            that builds a custom neural network can be provided. The function will\n            be called with the first batch of simulations (theta, x), which can\n            thus be used for shape inference and potentially for z-scoring. It\n            needs to return a PyTorch `nn.Module` implementing the density\n            estimator. The density estimator needs to provide the methods\n            `.log_prob` and `.sample()`.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n        logging_level: Minimum severity of messages to log. One of the strings\n            INFO, WARNING, DEBUG, ERROR and CRITICAL.\n        summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n            file location (default is `&lt;current working directory&gt;/logs`.)\n        show_progress_bars: Whether to show a progressbar during training.\n    \"\"\"\n\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snpe.snpe_c.SNPE_C.train","title":"<code>train(num_atoms=10, training_batch_size=200, learning_rate=0.0005, validation_fraction=0.1, stop_after_epochs=20, max_num_epochs=2 ** 31 - 1, clip_max_norm=5.0, calibration_kernel=None, resume_training=False, force_first_round_loss=False, discard_prior_samples=False, use_combined_loss=False, retrain_from_scratch=False, show_train_summary=False, dataloader_kwargs=None)</code>","text":"<p>Return density estimator that approximates the distribution \\(p(\\theta|x)\\).</p> <p>Parameters:</p> Name Type Description Default <code>num_atoms</code> <code>int</code> <p>Number of atoms to use for classification.</p> <code>10</code> <code>training_batch_size</code> <code>int</code> <p>Training batch size.</p> <code>200</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for Adam optimizer.</p> <code>0.0005</code> <code>validation_fraction</code> <code>float</code> <p>The fraction of data to use for validation.</p> <code>0.1</code> <code>stop_after_epochs</code> <code>int</code> <p>The number of epochs to wait for improvement on the validation set before terminating training.</p> <code>20</code> <code>max_num_epochs</code> <code>int</code> <p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. Otherwise, we train until validation loss increases (see also <code>stop_after_epochs</code>).</p> <code>2 ** 31 - 1</code> <code>clip_max_norm</code> <code>Optional[float]</code> <p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p> <code>5.0</code> <code>calibration_kernel</code> <code>Optional[Callable]</code> <p>A function to calibrate the loss with respect to the simulations <code>x</code>. See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017.</p> <code>None</code> <code>resume_training</code> <code>bool</code> <p>Can be used in case training time is limited, e.g. on a cluster. If <code>True</code>, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time <code>.train()</code> was called.</p> <code>False</code> <code>force_first_round_loss</code> <code>bool</code> <p>If <code>True</code>, train with maximum likelihood, i.e., potentially ignoring the correction for using a proposal distribution different from the prior.</p> <code>False</code> <code>discard_prior_samples</code> <code>bool</code> <p>Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples.</p> <code>False</code> <code>use_combined_loss</code> <code>bool</code> <p>Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors.</p> <code>False</code> <code>retrain_from_scratch</code> <code>bool</code> <p>Whether to retrain the conditional density estimator for the posterior from scratch each round.</p> <code>False</code> <code>show_train_summary</code> <code>bool</code> <p>Whether to print the number of epochs and validation loss and leakage after the training.</p> <code>False</code> <code>dataloader_kwargs</code> <code>Optional[Dict]</code> <p>Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn)</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>Density estimator that approximates the distribution \\(p(\\theta|x)\\).</p> Source code in <code>sbi/inference/snpe/snpe_c.py</code> <pre><code>def train(\n    self,\n    num_atoms: int = 10,\n    training_batch_size: int = 200,\n    learning_rate: float = 5e-4,\n    validation_fraction: float = 0.1,\n    stop_after_epochs: int = 20,\n    max_num_epochs: int = 2**31 - 1,\n    clip_max_norm: Optional[float] = 5.0,\n    calibration_kernel: Optional[Callable] = None,\n    resume_training: bool = False,\n    force_first_round_loss: bool = False,\n    discard_prior_samples: bool = False,\n    use_combined_loss: bool = False,\n    retrain_from_scratch: bool = False,\n    show_train_summary: bool = False,\n    dataloader_kwargs: Optional[Dict] = None,\n) -&gt; nn.Module:\n    r\"\"\"Return density estimator that approximates the distribution $p(\\theta|x)$.\n\n    Args:\n        num_atoms: Number of atoms to use for classification.\n        training_batch_size: Training batch size.\n        learning_rate: Learning rate for Adam optimizer.\n        validation_fraction: The fraction of data to use for validation.\n        stop_after_epochs: The number of epochs to wait for improvement on the\n            validation set before terminating training.\n        max_num_epochs: Maximum number of epochs to run. If reached, we stop\n            training even when the validation loss is still decreasing. Otherwise,\n            we train until validation loss increases (see also `stop_after_epochs`).\n        clip_max_norm: Value at which to clip the total gradient norm in order to\n            prevent exploding gradients. Use None for no clipping.\n        calibration_kernel: A function to calibrate the loss with respect to the\n            simulations `x`. See Lueckmann, Gon\u00e7alves et al., NeurIPS 2017.\n        resume_training: Can be used in case training time is limited, e.g. on a\n            cluster. If `True`, the split between train and validation set, the\n            optimizer, the number of epochs, and the best validation log-prob will\n            be restored from the last time `.train()` was called.\n        force_first_round_loss: If `True`, train with maximum likelihood,\n            i.e., potentially ignoring the correction for using a proposal\n            distribution different from the prior.\n        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n            from the prior. Training may be sped up by ignoring such less targeted\n            samples.\n        use_combined_loss: Whether to train the neural net also on prior samples\n            using maximum likelihood in addition to training it on all samples using\n            atomic loss. The extra MLE loss helps prevent density leaking with\n            bounded priors.\n        retrain_from_scratch: Whether to retrain the conditional density\n            estimator for the posterior from scratch each round.\n        show_train_summary: Whether to print the number of epochs and validation\n            loss and leakage after the training.\n        dataloader_kwargs: Additional or updated kwargs to be passed to the training\n            and validation dataloaders (like, e.g., a collate_fn)\n\n    Returns:\n        Density estimator that approximates the distribution $p(\\theta|x)$.\n    \"\"\"\n\n    # WARNING: sneaky trick ahead. We proxy the parent's `train` here,\n    # requiring the signature to have `num_atoms`, save it for use below, and\n    # continue. It's sneaky because we are using the object (self) as a namespace\n    # to pass arguments between functions, and that's implicit state management.\n    self._num_atoms = num_atoms\n    self._use_combined_loss = use_combined_loss\n    kwargs = del_entries(\n        locals(),\n        entries=(\"self\", \"__class__\", \"num_atoms\", \"use_combined_loss\"),\n    )\n\n    self._round = max(self._data_round_index)\n\n    if self._round &gt; 0:\n        # Set the proposal to the last proposal that was passed by the user. For\n        # atomic SNPE, it does not matter what the proposal is. For non-atomic\n        # SNPE, we only use the latest data that was passed, i.e. the one from the\n        # last proposal.\n        proposal = self._proposal_roundwise[-1]\n        self.use_non_atomic_loss = (\n            isinstance(proposal, DirectPosterior)\n            and isinstance(proposal.posterior_estimator.net._distribution, mdn)\n            and isinstance(self._neural_net.net._distribution, mdn)\n            and check_dist_class(\n                self._prior, class_to_check=(Uniform, MultivariateNormal)\n            )[0]\n        )\n\n        algorithm = \"non-atomic\" if self.use_non_atomic_loss else \"atomic\"\n        print(f\"Using SNPE-C with {algorithm} loss\")\n\n        if self.use_non_atomic_loss:\n            # Take care of z-scoring, pre-compute and store prior terms.\n            self._set_state_for_mog_proposal()\n\n    return super().train(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snle.snle_a.SNLE_A","title":"<code>SNLE_A</code>","text":"<p>               Bases: <code>LikelihoodEstimator</code></p> Source code in <code>sbi/inference/snle/snle_a.py</code> <pre><code>class SNLE_A(LikelihoodEstimator):\n    def __init__(\n        self,\n        prior: Optional[Distribution] = None,\n        density_estimator: Union[str, Callable] = \"maf\",\n        device: str = \"cpu\",\n        logging_level: Union[int, str] = \"WARNING\",\n        summary_writer: Optional[TensorboardSummaryWriter] = None,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"Sequential Neural Likelihood [1].\n\n        [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with\n        Autoregressive Flows_, Papamakarios et al., AISTATS 2019,\n        https://arxiv.org/abs/1805.07226\n\n        Args:\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them. If `None`, the\n                prior must be passed to `.build_posterior()`.\n            density_estimator: If it is a string, use a pre-configured network of the\n                provided type (one of nsf, maf, mdn, made). Alternatively, a function\n                that builds a custom neural network can be provided. The function will\n                be called with the first batch of simulations (theta, x), which can\n                thus be used for shape inference and potentially for z-scoring. It\n                needs to return a PyTorch `nn.Module` implementing the density\n                estimator. The density estimator needs to provide the methods\n                `.log_prob` and `.sample()`.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n            logging_level: Minimum severity of messages to log. One of the strings\n                INFO, WARNING, DEBUG, ERROR and CRITICAL.\n            summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n                file location (default is `&lt;current working directory&gt;/logs`.)\n            show_progress_bars: Whether to show a progressbar during simulation and\n                sampling.\n        \"\"\"\n\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snle.snle_a.SNLE_A.__init__","title":"<code>__init__(prior=None, density_estimator='maf', device='cpu', logging_level='WARNING', summary_writer=None, show_progress_bars=True)</code>","text":"<p>Sequential Neural Likelihood [1].</p> <p>[1] Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows_, Papamakarios et al., AISTATS 2019, https://arxiv.org/abs/1805.07226</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Optional[Distribution]</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the prior must be passed to <code>.build_posterior()</code>.</p> <code>None</code> <code>density_estimator</code> <code>Union[str, Callable]</code> <p>If it is a string, use a pre-configured network of the provided type (one of nsf, maf, mdn, made). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch <code>nn.Module</code> implementing the density estimator. The density estimator needs to provide the methods <code>.log_prob</code> and <code>.sample()</code>.</p> <code>'maf'</code> <code>device</code> <code>str</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d.</p> <code>'cpu'</code> <code>logging_level</code> <code>Union[int, str]</code> <p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p> <code>'WARNING'</code> <code>summary_writer</code> <code>Optional[TensorboardSummaryWriter]</code> <p>A tensorboard <code>SummaryWriter</code> to control, among others, log file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during simulation and sampling.</p> <code>True</code> Source code in <code>sbi/inference/snle/snle_a.py</code> <pre><code>def __init__(\n    self,\n    prior: Optional[Distribution] = None,\n    density_estimator: Union[str, Callable] = \"maf\",\n    device: str = \"cpu\",\n    logging_level: Union[int, str] = \"WARNING\",\n    summary_writer: Optional[TensorboardSummaryWriter] = None,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"Sequential Neural Likelihood [1].\n\n    [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with\n    Autoregressive Flows_, Papamakarios et al., AISTATS 2019,\n    https://arxiv.org/abs/1805.07226\n\n    Args:\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. If `None`, the\n            prior must be passed to `.build_posterior()`.\n        density_estimator: If it is a string, use a pre-configured network of the\n            provided type (one of nsf, maf, mdn, made). Alternatively, a function\n            that builds a custom neural network can be provided. The function will\n            be called with the first batch of simulations (theta, x), which can\n            thus be used for shape inference and potentially for z-scoring. It\n            needs to return a PyTorch `nn.Module` implementing the density\n            estimator. The density estimator needs to provide the methods\n            `.log_prob` and `.sample()`.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n        logging_level: Minimum severity of messages to log. One of the strings\n            INFO, WARNING, DEBUG, ERROR and CRITICAL.\n        summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n            file location (default is `&lt;current working directory&gt;/logs`.)\n        show_progress_bars: Whether to show a progressbar during simulation and\n            sampling.\n    \"\"\"\n\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_a.SNRE_A","title":"<code>SNRE_A</code>","text":"<p>               Bases: <code>RatioEstimator</code></p> Source code in <code>sbi/inference/snre/snre_a.py</code> <pre><code>class SNRE_A(RatioEstimator):\n    def __init__(\n        self,\n        prior: Optional[Distribution] = None,\n        classifier: Union[str, Callable] = \"resnet\",\n        device: str = \"cpu\",\n        logging_level: Union[int, str] = \"warning\",\n        summary_writer: Optional[TensorboardSummaryWriter] = None,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"AALR[1], here known as SNRE_A.\n\n        [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans\n            et al., ICML 2020, https://arxiv.org/abs/1903.04057\n\n        Args:\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them. If `None`, the\n                prior must be passed to `.build_posterior()`.\n            classifier: Classifier trained to approximate likelihood ratios. If it is\n                a string, use a pre-configured network of the provided type (one of\n                linear, mlp, resnet). Alternatively, a function that builds a custom\n                neural network can be provided. The function will be called with the\n                first batch of simulations (theta, x), which can thus be used for shape\n                inference and potentially for z-scoring. It needs to return a PyTorch\n                `nn.Module` implementing the classifier.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n            logging_level: Minimum severity of messages to log. One of the strings\n                INFO, WARNING, DEBUG, ERROR and CRITICAL.\n            summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n                file location (default is `&lt;current working directory&gt;/logs`.)\n            show_progress_bars: Whether to show a progressbar during simulation and\n                sampling.\n        \"\"\"\n\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        super().__init__(**kwargs)\n\n    def train(\n        self,\n        training_batch_size: int = 200,\n        learning_rate: float = 5e-4,\n        validation_fraction: float = 0.1,\n        stop_after_epochs: int = 20,\n        max_num_epochs: int = 2**31 - 1,\n        clip_max_norm: Optional[float] = 5.0,\n        resume_training: bool = False,\n        discard_prior_samples: bool = False,\n        retrain_from_scratch: bool = False,\n        show_train_summary: bool = False,\n        dataloader_kwargs: Optional[Dict] = None,\n        loss_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; nn.Module:\n        r\"\"\"Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n\n        Args:\n            training_batch_size: Training batch size.\n            learning_rate: Learning rate for Adam optimizer.\n            validation_fraction: The fraction of data to use for validation.\n            stop_after_epochs: The number of epochs to wait for improvement on the\n                validation set before terminating training.\n            max_num_epochs: Maximum number of epochs to run. If reached, we stop\n                training even when the validation loss is still decreasing. Otherwise,\n                we train until validation loss increases (see also `stop_after_epochs`).\n            clip_max_norm: Value at which to clip the total gradient norm in order to\n                prevent exploding gradients. Use None for no clipping.\n            resume_training: Can be used in case training time is limited, e.g. on a\n                cluster. If `True`, the split between train and validation set, the\n                optimizer, the number of epochs, and the best validation log-prob will\n                be restored from the last time `.train()` was called.\n            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n                from the prior. Training may be sped up by ignoring such less targeted\n                samples.\n            retrain_from_scratch: Whether to retrain the conditional density\n                estimator for the posterior from scratch each round.\n            show_train_summary: Whether to print the number of epochs and validation\n                loss and leakage after the training.\n            dataloader_kwargs: Additional or updated kwargs to be passed to the training\n                and validation dataloaders (like, e.g., a collate_fn)\n            loss_kwargs: Additional or updated kwargs to be passed to the self._loss fn.\n\n        Returns:\n            Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n        \"\"\"\n\n        # AALR is defined for `num_atoms=2`.\n        # Proxy to `super().__call__` to ensure right parameter.\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        return super().train(**kwargs, num_atoms=2)\n\n    def _loss(self, theta: Tensor, x: Tensor, num_atoms: int) -&gt; Tensor:\n        \"\"\"Returns the binary cross-entropy loss for the trained classifier.\n\n        The classifier takes as input a $(\\theta,x)$ pair. It is trained to predict 1\n        if the pair was sampled from the joint $p(\\theta,x)$, and to predict 0 if the\n        pair was sampled from the marginals $p(\\theta)p(x)$.\n        \"\"\"\n\n        assert theta.shape[0] == x.shape[0], \"Batch sizes for theta and x must match.\"\n        batch_size = theta.shape[0]\n\n        logits = self._classifier_logits(theta, x, num_atoms)\n        likelihood = torch.sigmoid(logits).squeeze()\n\n        # Alternating pairs where there is one sampled from the joint and one\n        # sampled from the marginals. The first element is sampled from the\n        # joint p(theta, x) and is labelled 1. The second element is sampled\n        # from the marginals p(theta)p(x) and is labelled 0. And so on.\n        labels = ones(2 * batch_size, device=self._device)  # two atoms\n        labels[1::2] = 0.0\n\n        # Binary cross entropy to learn the likelihood (AALR-specific)\n        return nn.BCELoss()(likelihood, labels)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_a.SNRE_A.__init__","title":"<code>__init__(prior=None, classifier='resnet', device='cpu', logging_level='warning', summary_writer=None, show_progress_bars=True)</code>","text":"<p>AALR[1], here known as SNRE_A.</p> <p>[1] Likelihood-free MCMC with Amortized Approximate Likelihood Ratios, Hermans     et al., ICML 2020, https://arxiv.org/abs/1903.04057</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Optional[Distribution]</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the prior must be passed to <code>.build_posterior()</code>.</p> <code>None</code> <code>classifier</code> <code>Union[str, Callable]</code> <p>Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch <code>nn.Module</code> implementing the classifier.</p> <code>'resnet'</code> <code>device</code> <code>str</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d.</p> <code>'cpu'</code> <code>logging_level</code> <code>Union[int, str]</code> <p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p> <code>'warning'</code> <code>summary_writer</code> <code>Optional[TensorboardSummaryWriter]</code> <p>A tensorboard <code>SummaryWriter</code> to control, among others, log file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during simulation and sampling.</p> <code>True</code> Source code in <code>sbi/inference/snre/snre_a.py</code> <pre><code>def __init__(\n    self,\n    prior: Optional[Distribution] = None,\n    classifier: Union[str, Callable] = \"resnet\",\n    device: str = \"cpu\",\n    logging_level: Union[int, str] = \"warning\",\n    summary_writer: Optional[TensorboardSummaryWriter] = None,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"AALR[1], here known as SNRE_A.\n\n    [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans\n        et al., ICML 2020, https://arxiv.org/abs/1903.04057\n\n    Args:\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. If `None`, the\n            prior must be passed to `.build_posterior()`.\n        classifier: Classifier trained to approximate likelihood ratios. If it is\n            a string, use a pre-configured network of the provided type (one of\n            linear, mlp, resnet). Alternatively, a function that builds a custom\n            neural network can be provided. The function will be called with the\n            first batch of simulations (theta, x), which can thus be used for shape\n            inference and potentially for z-scoring. It needs to return a PyTorch\n            `nn.Module` implementing the classifier.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n        logging_level: Minimum severity of messages to log. One of the strings\n            INFO, WARNING, DEBUG, ERROR and CRITICAL.\n        summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n            file location (default is `&lt;current working directory&gt;/logs`.)\n        show_progress_bars: Whether to show a progressbar during simulation and\n            sampling.\n    \"\"\"\n\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_a.SNRE_A.train","title":"<code>train(training_batch_size=200, learning_rate=0.0005, validation_fraction=0.1, stop_after_epochs=20, max_num_epochs=2 ** 31 - 1, clip_max_norm=5.0, resume_training=False, discard_prior_samples=False, retrain_from_scratch=False, show_train_summary=False, dataloader_kwargs=None, loss_kwargs=None)</code>","text":"<p>Return classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\).</p> <p>Parameters:</p> Name Type Description Default <code>training_batch_size</code> <code>int</code> <p>Training batch size.</p> <code>200</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for Adam optimizer.</p> <code>0.0005</code> <code>validation_fraction</code> <code>float</code> <p>The fraction of data to use for validation.</p> <code>0.1</code> <code>stop_after_epochs</code> <code>int</code> <p>The number of epochs to wait for improvement on the validation set before terminating training.</p> <code>20</code> <code>max_num_epochs</code> <code>int</code> <p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. Otherwise, we train until validation loss increases (see also <code>stop_after_epochs</code>).</p> <code>2 ** 31 - 1</code> <code>clip_max_norm</code> <code>Optional[float]</code> <p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p> <code>5.0</code> <code>resume_training</code> <code>bool</code> <p>Can be used in case training time is limited, e.g. on a cluster. If <code>True</code>, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time <code>.train()</code> was called.</p> <code>False</code> <code>discard_prior_samples</code> <code>bool</code> <p>Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples.</p> <code>False</code> <code>retrain_from_scratch</code> <code>bool</code> <p>Whether to retrain the conditional density estimator for the posterior from scratch each round.</p> <code>False</code> <code>show_train_summary</code> <code>bool</code> <p>Whether to print the number of epochs and validation loss and leakage after the training.</p> <code>False</code> <code>dataloader_kwargs</code> <code>Optional[Dict]</code> <p>Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn)</p> <code>None</code> <code>loss_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional or updated kwargs to be passed to the self._loss fn.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>Classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\).</p> Source code in <code>sbi/inference/snre/snre_a.py</code> <pre><code>def train(\n    self,\n    training_batch_size: int = 200,\n    learning_rate: float = 5e-4,\n    validation_fraction: float = 0.1,\n    stop_after_epochs: int = 20,\n    max_num_epochs: int = 2**31 - 1,\n    clip_max_norm: Optional[float] = 5.0,\n    resume_training: bool = False,\n    discard_prior_samples: bool = False,\n    retrain_from_scratch: bool = False,\n    show_train_summary: bool = False,\n    dataloader_kwargs: Optional[Dict] = None,\n    loss_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; nn.Module:\n    r\"\"\"Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n\n    Args:\n        training_batch_size: Training batch size.\n        learning_rate: Learning rate for Adam optimizer.\n        validation_fraction: The fraction of data to use for validation.\n        stop_after_epochs: The number of epochs to wait for improvement on the\n            validation set before terminating training.\n        max_num_epochs: Maximum number of epochs to run. If reached, we stop\n            training even when the validation loss is still decreasing. Otherwise,\n            we train until validation loss increases (see also `stop_after_epochs`).\n        clip_max_norm: Value at which to clip the total gradient norm in order to\n            prevent exploding gradients. Use None for no clipping.\n        resume_training: Can be used in case training time is limited, e.g. on a\n            cluster. If `True`, the split between train and validation set, the\n            optimizer, the number of epochs, and the best validation log-prob will\n            be restored from the last time `.train()` was called.\n        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n            from the prior. Training may be sped up by ignoring such less targeted\n            samples.\n        retrain_from_scratch: Whether to retrain the conditional density\n            estimator for the posterior from scratch each round.\n        show_train_summary: Whether to print the number of epochs and validation\n            loss and leakage after the training.\n        dataloader_kwargs: Additional or updated kwargs to be passed to the training\n            and validation dataloaders (like, e.g., a collate_fn)\n        loss_kwargs: Additional or updated kwargs to be passed to the self._loss fn.\n\n    Returns:\n        Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n    \"\"\"\n\n    # AALR is defined for `num_atoms=2`.\n    # Proxy to `super().__call__` to ensure right parameter.\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    return super().train(**kwargs, num_atoms=2)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_b.SNRE_B","title":"<code>SNRE_B</code>","text":"<p>               Bases: <code>RatioEstimator</code></p> Source code in <code>sbi/inference/snre/snre_b.py</code> <pre><code>class SNRE_B(RatioEstimator):\n    def __init__(\n        self,\n        prior: Optional[Distribution] = None,\n        classifier: Union[str, Callable] = \"resnet\",\n        device: str = \"cpu\",\n        logging_level: Union[int, str] = \"warning\",\n        summary_writer: Optional[TensorboardSummaryWriter] = None,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"SRE[1], here known as SNRE_B.\n\n        [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,\n            ICML 2020, https://arxiv.org/pdf/2002.03712\n\n        Args:\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them. If `None`, the\n                prior must be passed to `.build_posterior()`.\n            classifier: Classifier trained to approximate likelihood ratios. If it is\n                a string, use a pre-configured network of the provided type (one of\n                linear, mlp, resnet). Alternatively, a function that builds a custom\n                neural network can be provided. The function will be called with the\n                first batch of simulations (theta, x), which can thus be used for shape\n                inference and potentially for z-scoring. It needs to return a PyTorch\n                `nn.Module` implementing the classifier.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n            logging_level: Minimum severity of messages to log. One of the strings\n                INFO, WARNING, DEBUG, ERROR and CRITICAL.\n            summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n                file location (default is `&lt;current working directory&gt;/logs`.)\n            show_progress_bars: Whether to show a progressbar during simulation and\n                sampling.\n        \"\"\"\n\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        super().__init__(**kwargs)\n\n    def train(\n        self,\n        num_atoms: int = 10,\n        training_batch_size: int = 200,\n        learning_rate: float = 5e-4,\n        validation_fraction: float = 0.1,\n        stop_after_epochs: int = 20,\n        max_num_epochs: int = 2**31 - 1,\n        clip_max_norm: Optional[float] = 5.0,\n        resume_training: bool = False,\n        discard_prior_samples: bool = False,\n        retrain_from_scratch: bool = False,\n        show_train_summary: bool = False,\n        dataloader_kwargs: Optional[Dict] = None,\n    ) -&gt; nn.Module:\n        r\"\"\"Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n\n        Args:\n            num_atoms: Number of atoms to use for classification.\n            training_batch_size: Training batch size.\n            learning_rate: Learning rate for Adam optimizer.\n            validation_fraction: The fraction of data to use for validation.\n            stop_after_epochs: The number of epochs to wait for improvement on the\n                validation set before terminating training.\n            max_num_epochs: Maximum number of epochs to run. If reached, we stop\n                training even when the validation loss is still decreasing. Otherwise,\n                we train until validation loss increases (see also `stop_after_epochs`).\n            clip_max_norm: Value at which to clip the total gradient norm in order to\n                prevent exploding gradients. Use None for no clipping.\n            resume_training: Can be used in case training time is limited, e.g. on a\n                cluster. If `True`, the split between train and validation set, the\n                optimizer, the number of epochs, and the best validation log-prob will\n                be restored from the last time `.train()` was called.\n            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n                from the prior. Training may be sped up by ignoring such less targeted\n                samples.\n            retrain_from_scratch: Whether to retrain the conditional density\n                estimator for the posterior from scratch each round.\n            show_train_summary: Whether to print the number of epochs and validation\n                loss and leakage after the training.\n            dataloader_kwargs: Additional or updated kwargs to be passed to the training\n                and validation dataloaders (like, e.g., a collate_fn)\n\n        Returns:\n            Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n        \"\"\"\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        return super().train(**kwargs)\n\n    def _loss(self, theta: Tensor, x: Tensor, num_atoms: int) -&gt; Tensor:\n        r\"\"\"Return cross-entropy (via softmax activation) loss for 1-out-of-`num_atoms`\n        classification.\n\n        The classifier takes as input `num_atoms` $(\\theta,x)$ pairs. Out of these\n        pairs, one pair was sampled from the joint $p(\\theta,x)$ and all others from the\n        marginals $p(\\theta)p(x)$. The classifier is trained to predict which of the\n        pairs was sampled from the joint $p(\\theta,x)$.\n        \"\"\"\n\n        assert theta.shape[0] == x.shape[0], \"Batch sizes for theta and x must match.\"\n        batch_size = theta.shape[0]\n        logits = self._classifier_logits(theta, x, num_atoms)\n\n        # For 1-out-of-`num_atoms` classification each datapoint consists\n        # of `num_atoms` points, with one of them being the correct one.\n        # We have a batch of `batch_size` such datapoints.\n        logits = logits.reshape(batch_size, num_atoms)\n\n        # Index 0 is the theta-x-pair sampled from the joint p(theta,x) and hence the\n        # \"correct\" one for the 1-out-of-N classification.\n        log_prob = logits[:, 0] - torch.logsumexp(logits, dim=-1)\n\n        return -torch.mean(log_prob)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_b.SNRE_B.__init__","title":"<code>__init__(prior=None, classifier='resnet', device='cpu', logging_level='warning', summary_writer=None, show_progress_bars=True)</code>","text":"<p>SRE[1], here known as SNRE_B.</p> <p>[1] On Contrastive Learning for Likelihood-free Inference, Durkan et al.,     ICML 2020, https://arxiv.org/pdf/2002.03712</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Optional[Distribution]</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the prior must be passed to <code>.build_posterior()</code>.</p> <code>None</code> <code>classifier</code> <code>Union[str, Callable]</code> <p>Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch <code>nn.Module</code> implementing the classifier.</p> <code>'resnet'</code> <code>device</code> <code>str</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d.</p> <code>'cpu'</code> <code>logging_level</code> <code>Union[int, str]</code> <p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p> <code>'warning'</code> <code>summary_writer</code> <code>Optional[TensorboardSummaryWriter]</code> <p>A tensorboard <code>SummaryWriter</code> to control, among others, log file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during simulation and sampling.</p> <code>True</code> Source code in <code>sbi/inference/snre/snre_b.py</code> <pre><code>def __init__(\n    self,\n    prior: Optional[Distribution] = None,\n    classifier: Union[str, Callable] = \"resnet\",\n    device: str = \"cpu\",\n    logging_level: Union[int, str] = \"warning\",\n    summary_writer: Optional[TensorboardSummaryWriter] = None,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"SRE[1], here known as SNRE_B.\n\n    [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,\n        ICML 2020, https://arxiv.org/pdf/2002.03712\n\n    Args:\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. If `None`, the\n            prior must be passed to `.build_posterior()`.\n        classifier: Classifier trained to approximate likelihood ratios. If it is\n            a string, use a pre-configured network of the provided type (one of\n            linear, mlp, resnet). Alternatively, a function that builds a custom\n            neural network can be provided. The function will be called with the\n            first batch of simulations (theta, x), which can thus be used for shape\n            inference and potentially for z-scoring. It needs to return a PyTorch\n            `nn.Module` implementing the classifier.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n        logging_level: Minimum severity of messages to log. One of the strings\n            INFO, WARNING, DEBUG, ERROR and CRITICAL.\n        summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n            file location (default is `&lt;current working directory&gt;/logs`.)\n        show_progress_bars: Whether to show a progressbar during simulation and\n            sampling.\n    \"\"\"\n\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_b.SNRE_B.train","title":"<code>train(num_atoms=10, training_batch_size=200, learning_rate=0.0005, validation_fraction=0.1, stop_after_epochs=20, max_num_epochs=2 ** 31 - 1, clip_max_norm=5.0, resume_training=False, discard_prior_samples=False, retrain_from_scratch=False, show_train_summary=False, dataloader_kwargs=None)</code>","text":"<p>Return classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\).</p> <p>Parameters:</p> Name Type Description Default <code>num_atoms</code> <code>int</code> <p>Number of atoms to use for classification.</p> <code>10</code> <code>training_batch_size</code> <code>int</code> <p>Training batch size.</p> <code>200</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for Adam optimizer.</p> <code>0.0005</code> <code>validation_fraction</code> <code>float</code> <p>The fraction of data to use for validation.</p> <code>0.1</code> <code>stop_after_epochs</code> <code>int</code> <p>The number of epochs to wait for improvement on the validation set before terminating training.</p> <code>20</code> <code>max_num_epochs</code> <code>int</code> <p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. Otherwise, we train until validation loss increases (see also <code>stop_after_epochs</code>).</p> <code>2 ** 31 - 1</code> <code>clip_max_norm</code> <code>Optional[float]</code> <p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p> <code>5.0</code> <code>resume_training</code> <code>bool</code> <p>Can be used in case training time is limited, e.g. on a cluster. If <code>True</code>, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time <code>.train()</code> was called.</p> <code>False</code> <code>discard_prior_samples</code> <code>bool</code> <p>Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples.</p> <code>False</code> <code>retrain_from_scratch</code> <code>bool</code> <p>Whether to retrain the conditional density estimator for the posterior from scratch each round.</p> <code>False</code> <code>show_train_summary</code> <code>bool</code> <p>Whether to print the number of epochs and validation loss and leakage after the training.</p> <code>False</code> <code>dataloader_kwargs</code> <code>Optional[Dict]</code> <p>Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn)</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>Classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\).</p> Source code in <code>sbi/inference/snre/snre_b.py</code> <pre><code>def train(\n    self,\n    num_atoms: int = 10,\n    training_batch_size: int = 200,\n    learning_rate: float = 5e-4,\n    validation_fraction: float = 0.1,\n    stop_after_epochs: int = 20,\n    max_num_epochs: int = 2**31 - 1,\n    clip_max_norm: Optional[float] = 5.0,\n    resume_training: bool = False,\n    discard_prior_samples: bool = False,\n    retrain_from_scratch: bool = False,\n    show_train_summary: bool = False,\n    dataloader_kwargs: Optional[Dict] = None,\n) -&gt; nn.Module:\n    r\"\"\"Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n\n    Args:\n        num_atoms: Number of atoms to use for classification.\n        training_batch_size: Training batch size.\n        learning_rate: Learning rate for Adam optimizer.\n        validation_fraction: The fraction of data to use for validation.\n        stop_after_epochs: The number of epochs to wait for improvement on the\n            validation set before terminating training.\n        max_num_epochs: Maximum number of epochs to run. If reached, we stop\n            training even when the validation loss is still decreasing. Otherwise,\n            we train until validation loss increases (see also `stop_after_epochs`).\n        clip_max_norm: Value at which to clip the total gradient norm in order to\n            prevent exploding gradients. Use None for no clipping.\n        resume_training: Can be used in case training time is limited, e.g. on a\n            cluster. If `True`, the split between train and validation set, the\n            optimizer, the number of epochs, and the best validation log-prob will\n            be restored from the last time `.train()` was called.\n        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n            from the prior. Training may be sped up by ignoring such less targeted\n            samples.\n        retrain_from_scratch: Whether to retrain the conditional density\n            estimator for the posterior from scratch each round.\n        show_train_summary: Whether to print the number of epochs and validation\n            loss and leakage after the training.\n        dataloader_kwargs: Additional or updated kwargs to be passed to the training\n            and validation dataloaders (like, e.g., a collate_fn)\n\n    Returns:\n        Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n    \"\"\"\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    return super().train(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_c.SNRE_C","title":"<code>SNRE_C</code>","text":"<p>               Bases: <code>RatioEstimator</code></p> Source code in <code>sbi/inference/snre/snre_c.py</code> <pre><code>class SNRE_C(RatioEstimator):\n    def __init__(\n        self,\n        prior: Optional[Distribution] = None,\n        classifier: Union[str, Callable] = \"resnet\",\n        device: str = \"cpu\",\n        logging_level: Union[int, str] = \"warning\",\n        summary_writer: Optional[TensorboardSummaryWriter] = None,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"NRE-C[1] is a generalization of the non-sequential (amortized) versions of\n        SNRE_A and SNRE_B. We call the algorithm SNRE_C within `sbi`.\n\n        NRE-C:\n        (1) like SNRE_B, features a \"multiclass\" loss function where several marginally\n            drawn parameter-data pairs are contrasted against a jointly drawn pair.\n        (2) like AALR/NRE_A, i.e., the non-sequential version of SNRE_A, it encourages\n            the approximate ratio $p(\\theta,x)/p(\\theta)p(x)$, accessed through\n            `.potential()` within `sbi`, to be exact at optimum. This addresses the\n            issue that SNRE_B estimates this ratio only up to an arbitrary function\n            (normalizing constant) of the data $x$.\n\n        Just like for all ratio estimation algorithms, the sequential version of SNRE_C\n        will be estimated only up to a function (normalizing constant) of the data $x$\n        in rounds after the first.\n\n        [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,\n            NeurIPS 2022, https://arxiv.org/abs/2210.06170\n\n        Args:\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them. If `None`, the\n                prior must be passed to `.build_posterior()`.\n            classifier: Classifier trained to approximate likelihood ratios. If it is\n                a string, use a pre-configured network of the provided type (one of\n                linear, mlp, resnet). Alternatively, a function that builds a custom\n                neural network can be provided. The function will be called with the\n                first batch of simulations (theta, x), which can thus be used for shape\n                inference and potentially for z-scoring. It needs to return a PyTorch\n                `nn.Module` implementing the classifier.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n            logging_level: Minimum severity of messages to log. One of the strings\n                INFO, WARNING, DEBUG, ERROR and CRITICAL.\n            summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n                file location (default is `&lt;current working directory&gt;/logs`.)\n            show_progress_bars: Whether to show a progressbar during simulation and\n                sampling.\n        \"\"\"\n\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        super().__init__(**kwargs)\n\n    def train(\n        self,\n        num_classes: int = 5,\n        gamma: float = 1.0,\n        training_batch_size: int = 200,\n        learning_rate: float = 5e-4,\n        validation_fraction: float = 0.1,\n        stop_after_epochs: int = 20,\n        max_num_epochs: int = 2**31 - 1,\n        clip_max_norm: Optional[float] = 5.0,\n        resume_training: bool = False,\n        discard_prior_samples: bool = False,\n        retrain_from_scratch: bool = False,\n        show_train_summary: bool = False,\n        dataloader_kwargs: Optional[Dict] = None,\n    ) -&gt; nn.Module:\n        r\"\"\"Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n\n        Args:\n            num_classes: Number of theta to classify against, corresponds to $K$ in\n                _Contrastive Neural Ratio Estimation_. Minimum value is 1. Similar to\n                `num_atoms` for SNRE_B except SNRE_C has an additional independently\n                drawn sample. The total number of alternative parameters `NRE-C` \"sees\"\n                is $2K-1$ or `2 * num_classes - 1` divided between two loss terms.\n            gamma: Determines the relative weight of the sum of all $K$ dependently\n                drawn classes against the marginally drawn one. Specifically,\n                $p(y=k) :=p_K$, $p(y=0) := p_0$, $p_0 = 1 - K p_K$, and finally\n                $\\gamma := K p_K / p_0$.\n            training_batch_size: Training batch size.\n            learning_rate: Learning rate for Adam optimizer.\n            validation_fraction: The fraction of data to use for validation.\n            stop_after_epochs: The number of epochs to wait for improvement on the\n                validation set before terminating training.\n            max_num_epochs: Maximum number of epochs to run. If reached, we stop\n                training even when the validation loss is still decreasing. Otherwise,\n                we train until validation loss increases (see also `stop_after_epochs`).\n            clip_max_norm: Value at which to clip the total gradient norm in order to\n                prevent exploding gradients. Use None for no clipping.\n            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e`\n                during training. Expect errors, silent or explicit, when `False`.\n            resume_training: Can be used in case training time is limited, e.g. on a\n                cluster. If `True`, the split between train and validation set, the\n                optimizer, the number of epochs, and the best validation log-prob will\n                be restored from the last time `.train()` was called.\n            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n                from the prior. Training may be sped up by ignoring such less targeted\n                samples.\n            retrain_from_scratch: Whether to retrain the conditional density\n                estimator for the posterior from scratch each round.\n            show_train_summary: Whether to print the number of epochs and validation\n                loss and leakage after the training.\n            dataloader_kwargs: Additional or updated kwargs to be passed to the training\n                and validation dataloaders (like, e.g., a collate_fn)\n\n        Returns:\n            Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n        \"\"\"\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        kwargs[\"num_atoms\"] = kwargs.pop(\"num_classes\") + 1\n        kwargs[\"loss_kwargs\"] = {\"gamma\": kwargs.pop(\"gamma\")}\n        return super().train(**kwargs)\n\n    def _loss(\n        self, theta: Tensor, x: Tensor, num_atoms: int, gamma: float\n    ) -&gt; torch.Tensor:\n        r\"\"\"Return cross-entropy loss (via ''multi-class sigmoid'' activation) for\n        1-out-of-`K + 1` classification.\n\n        At optimum, this loss function returns the exact likelihood-to-evidence ratio\n        in the first round.\n        Details of loss computation are described in Contrastive Neural Ratio\n        Estimation[1]. The paper does not discuss the sequential case.\n\n        [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,\n            NeurIPS 2022, https://arxiv.org/abs/2210.06170\n        \"\"\"\n\n        # Reminder: K = num_classes\n        # The algorithm is written with K, so we convert back to K format rather than\n        # reasoning in num_atoms.\n        num_classes = num_atoms - 1\n        assert num_classes &gt;= 1, f\"num_classes = {num_classes} must be greater than 1.\"\n\n        assert theta.shape[0] == x.shape[0], \"Batch sizes for theta and x must match.\"\n        batch_size = theta.shape[0]\n\n        # We append a contrastive theta to the marginal case because we will remove\n        # the jointly drawn\n        # sample in the logits_marginal[:, 0] position. That makes the remaining sample\n        # marginally drawn.\n        # We have a batch of `batch_size` datapoints.\n        logits_marginal = self._classifier_logits(theta, x, num_classes + 1).reshape(\n            batch_size, num_classes + 1\n        )\n        logits_joint = self._classifier_logits(theta, x, num_classes).reshape(\n            batch_size, num_classes\n        )\n\n        dtype = logits_marginal.dtype\n        device = logits_marginal.device\n\n        # Index 0 is the theta-x-pair sampled from the joint p(theta,x) and hence\n        # we remove the jointly drawn sample from the logits_marginal\n        logits_marginal = logits_marginal[:, 1:]\n        # ... and retain it in the logits_joint. Now we have two arrays with K choices.\n\n        # To use logsumexp, we extend the denominator logits with loggamma\n        loggamma = torch.tensor(gamma, dtype=dtype, device=device).log()\n        logK = torch.tensor(num_classes, dtype=dtype, device=device).log()\n        denominator_marginal = torch.concat(\n            [loggamma + logits_marginal, logK.expand((batch_size, 1))],\n            dim=-1,\n        )\n        denominator_joint = torch.concat(\n            [loggamma + logits_joint, logK.expand((batch_size, 1))],\n            dim=-1,\n        )\n\n        # Compute the contributions to the loss from each term in the classification.\n        log_prob_marginal = logK - torch.logsumexp(denominator_marginal, dim=-1)\n        log_prob_joint = (\n            loggamma + logits_joint[:, 0] - torch.logsumexp(denominator_joint, dim=-1)\n        )\n\n        # relative weights. p_marginal := p_0, and p_joint := p_K * K from the notation.\n        p_marginal, p_joint = self._get_prior_probs_marginal_and_joint(gamma)\n        return -torch.mean(p_marginal * log_prob_marginal + p_joint * log_prob_joint)\n\n    @staticmethod\n    def _get_prior_probs_marginal_and_joint(gamma: float) -&gt; Tuple[float, float]:\n        r\"\"\"Return a tuple (p_marginal, p_joint) where `p_marginal := `$p_0$,\n        `p_joint := `$p_K \\cdot K$.\n\n        We let the joint (dependently drawn) class to be equally likely across K\n        options. The marginal class is therefore restricted to get the remaining\n        probability.\n        \"\"\"\n        p_joint = gamma / (1 + gamma)\n        p_marginal = 1 / (1 + gamma)\n        return p_marginal, p_joint\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_c.SNRE_C.__init__","title":"<code>__init__(prior=None, classifier='resnet', device='cpu', logging_level='warning', summary_writer=None, show_progress_bars=True)</code>","text":"<p>NRE-C[1] is a generalization of the non-sequential (amortized) versions of SNRE_A and SNRE_B. We call the algorithm SNRE_C within <code>sbi</code>.</p> <p>NRE-C: (1) like SNRE_B, features a \u201cmulticlass\u201d loss function where several marginally     drawn parameter-data pairs are contrasted against a jointly drawn pair. (2) like AALR/NRE_A, i.e., the non-sequential version of SNRE_A, it encourages     the approximate ratio \\(p(\\theta,x)/p(\\theta)p(x)\\), accessed through     <code>.potential()</code> within <code>sbi</code>, to be exact at optimum. This addresses the     issue that SNRE_B estimates this ratio only up to an arbitrary function     (normalizing constant) of the data \\(x\\).</p> <p>Just like for all ratio estimation algorithms, the sequential version of SNRE_C will be estimated only up to a function (normalizing constant) of the data \\(x\\) in rounds after the first.</p> <p>[1] Contrastive Neural Ratio Estimation, Benajmin Kurt Miller, et. al.,     NeurIPS 2022, https://arxiv.org/abs/2210.06170</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Optional[Distribution]</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the prior must be passed to <code>.build_posterior()</code>.</p> <code>None</code> <code>classifier</code> <code>Union[str, Callable]</code> <p>Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations (theta, x), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch <code>nn.Module</code> implementing the classifier.</p> <code>'resnet'</code> <code>device</code> <code>str</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d.</p> <code>'cpu'</code> <code>logging_level</code> <code>Union[int, str]</code> <p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p> <code>'warning'</code> <code>summary_writer</code> <code>Optional[TensorboardSummaryWriter]</code> <p>A tensorboard <code>SummaryWriter</code> to control, among others, log file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during simulation and sampling.</p> <code>True</code> Source code in <code>sbi/inference/snre/snre_c.py</code> <pre><code>def __init__(\n    self,\n    prior: Optional[Distribution] = None,\n    classifier: Union[str, Callable] = \"resnet\",\n    device: str = \"cpu\",\n    logging_level: Union[int, str] = \"warning\",\n    summary_writer: Optional[TensorboardSummaryWriter] = None,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"NRE-C[1] is a generalization of the non-sequential (amortized) versions of\n    SNRE_A and SNRE_B. We call the algorithm SNRE_C within `sbi`.\n\n    NRE-C:\n    (1) like SNRE_B, features a \"multiclass\" loss function where several marginally\n        drawn parameter-data pairs are contrasted against a jointly drawn pair.\n    (2) like AALR/NRE_A, i.e., the non-sequential version of SNRE_A, it encourages\n        the approximate ratio $p(\\theta,x)/p(\\theta)p(x)$, accessed through\n        `.potential()` within `sbi`, to be exact at optimum. This addresses the\n        issue that SNRE_B estimates this ratio only up to an arbitrary function\n        (normalizing constant) of the data $x$.\n\n    Just like for all ratio estimation algorithms, the sequential version of SNRE_C\n    will be estimated only up to a function (normalizing constant) of the data $x$\n    in rounds after the first.\n\n    [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,\n        NeurIPS 2022, https://arxiv.org/abs/2210.06170\n\n    Args:\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. If `None`, the\n            prior must be passed to `.build_posterior()`.\n        classifier: Classifier trained to approximate likelihood ratios. If it is\n            a string, use a pre-configured network of the provided type (one of\n            linear, mlp, resnet). Alternatively, a function that builds a custom\n            neural network can be provided. The function will be called with the\n            first batch of simulations (theta, x), which can thus be used for shape\n            inference and potentially for z-scoring. It needs to return a PyTorch\n            `nn.Module` implementing the classifier.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n        logging_level: Minimum severity of messages to log. One of the strings\n            INFO, WARNING, DEBUG, ERROR and CRITICAL.\n        summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n            file location (default is `&lt;current working directory&gt;/logs`.)\n        show_progress_bars: Whether to show a progressbar during simulation and\n            sampling.\n    \"\"\"\n\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.snre_c.SNRE_C.train","title":"<code>train(num_classes=5, gamma=1.0, training_batch_size=200, learning_rate=0.0005, validation_fraction=0.1, stop_after_epochs=20, max_num_epochs=2 ** 31 - 1, clip_max_norm=5.0, resume_training=False, discard_prior_samples=False, retrain_from_scratch=False, show_train_summary=False, dataloader_kwargs=None)</code>","text":"<p>Return classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\).</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of theta to classify against, corresponds to \\(K\\) in Contrastive Neural Ratio Estimation. Minimum value is 1. Similar to <code>num_atoms</code> for SNRE_B except SNRE_C has an additional independently drawn sample. The total number of alternative parameters <code>NRE-C</code> \u201csees\u201d is \\(2K-1\\) or <code>2 * num_classes - 1</code> divided between two loss terms.</p> <code>5</code> <code>gamma</code> <code>float</code> <p>Determines the relative weight of the sum of all \\(K\\) dependently drawn classes against the marginally drawn one. Specifically, \\(p(y=k) :=p_K\\), \\(p(y=0) := p_0\\), \\(p_0 = 1 - K p_K\\), and finally \\(\\gamma := K p_K / p_0\\).</p> <code>1.0</code> <code>training_batch_size</code> <code>int</code> <p>Training batch size.</p> <code>200</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for Adam optimizer.</p> <code>0.0005</code> <code>validation_fraction</code> <code>float</code> <p>The fraction of data to use for validation.</p> <code>0.1</code> <code>stop_after_epochs</code> <code>int</code> <p>The number of epochs to wait for improvement on the validation set before terminating training.</p> <code>20</code> <code>max_num_epochs</code> <code>int</code> <p>Maximum number of epochs to run. If reached, we stop training even when the validation loss is still decreasing. Otherwise, we train until validation loss increases (see also <code>stop_after_epochs</code>).</p> <code>2 ** 31 - 1</code> <code>clip_max_norm</code> <code>Optional[float]</code> <p>Value at which to clip the total gradient norm in order to prevent exploding gradients. Use None for no clipping.</p> <code>5.0</code> <code>exclude_invalid_x</code> <p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=\u00b1\u221e</code> during training. Expect errors, silent or explicit, when <code>False</code>.</p> required <code>resume_training</code> <code>bool</code> <p>Can be used in case training time is limited, e.g. on a cluster. If <code>True</code>, the split between train and validation set, the optimizer, the number of epochs, and the best validation log-prob will be restored from the last time <code>.train()</code> was called.</p> <code>False</code> <code>discard_prior_samples</code> <code>bool</code> <p>Whether to discard samples simulated in round 1, i.e. from the prior. Training may be sped up by ignoring such less targeted samples.</p> <code>False</code> <code>retrain_from_scratch</code> <code>bool</code> <p>Whether to retrain the conditional density estimator for the posterior from scratch each round.</p> <code>False</code> <code>show_train_summary</code> <code>bool</code> <p>Whether to print the number of epochs and validation loss and leakage after the training.</p> <code>False</code> <code>dataloader_kwargs</code> <code>Optional[Dict]</code> <p>Additional or updated kwargs to be passed to the training and validation dataloaders (like, e.g., a collate_fn)</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>Classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\).</p> Source code in <code>sbi/inference/snre/snre_c.py</code> <pre><code>def train(\n    self,\n    num_classes: int = 5,\n    gamma: float = 1.0,\n    training_batch_size: int = 200,\n    learning_rate: float = 5e-4,\n    validation_fraction: float = 0.1,\n    stop_after_epochs: int = 20,\n    max_num_epochs: int = 2**31 - 1,\n    clip_max_norm: Optional[float] = 5.0,\n    resume_training: bool = False,\n    discard_prior_samples: bool = False,\n    retrain_from_scratch: bool = False,\n    show_train_summary: bool = False,\n    dataloader_kwargs: Optional[Dict] = None,\n) -&gt; nn.Module:\n    r\"\"\"Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n\n    Args:\n        num_classes: Number of theta to classify against, corresponds to $K$ in\n            _Contrastive Neural Ratio Estimation_. Minimum value is 1. Similar to\n            `num_atoms` for SNRE_B except SNRE_C has an additional independently\n            drawn sample. The total number of alternative parameters `NRE-C` \"sees\"\n            is $2K-1$ or `2 * num_classes - 1` divided between two loss terms.\n        gamma: Determines the relative weight of the sum of all $K$ dependently\n            drawn classes against the marginally drawn one. Specifically,\n            $p(y=k) :=p_K$, $p(y=0) := p_0$, $p_0 = 1 - K p_K$, and finally\n            $\\gamma := K p_K / p_0$.\n        training_batch_size: Training batch size.\n        learning_rate: Learning rate for Adam optimizer.\n        validation_fraction: The fraction of data to use for validation.\n        stop_after_epochs: The number of epochs to wait for improvement on the\n            validation set before terminating training.\n        max_num_epochs: Maximum number of epochs to run. If reached, we stop\n            training even when the validation loss is still decreasing. Otherwise,\n            we train until validation loss increases (see also `stop_after_epochs`).\n        clip_max_norm: Value at which to clip the total gradient norm in order to\n            prevent exploding gradients. Use None for no clipping.\n        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e`\n            during training. Expect errors, silent or explicit, when `False`.\n        resume_training: Can be used in case training time is limited, e.g. on a\n            cluster. If `True`, the split between train and validation set, the\n            optimizer, the number of epochs, and the best validation log-prob will\n            be restored from the last time `.train()` was called.\n        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n            from the prior. Training may be sped up by ignoring such less targeted\n            samples.\n        retrain_from_scratch: Whether to retrain the conditional density\n            estimator for the posterior from scratch each round.\n        show_train_summary: Whether to print the number of epochs and validation\n            loss and leakage after the training.\n        dataloader_kwargs: Additional or updated kwargs to be passed to the training\n            and validation dataloaders (like, e.g., a collate_fn)\n\n    Returns:\n        Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n    \"\"\"\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    kwargs[\"num_atoms\"] = kwargs.pop(\"num_classes\") + 1\n    kwargs[\"loss_kwargs\"] = {\"gamma\": kwargs.pop(\"gamma\")}\n    return super().train(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.bnre.BNRE","title":"<code>BNRE</code>","text":"<p>               Bases: <code>SNRE_A</code></p> Source code in <code>sbi/inference/snre/bnre.py</code> <pre><code>class BNRE(SNRE_A):\n    def __init__(\n        self,\n        prior: Optional[Distribution] = None,\n        classifier: Union[str, Callable] = \"resnet\",\n        device: str = \"cpu\",\n        logging_level: Union[int, str] = \"warning\",\n        summary_writer: Optional[TensorboardSummaryWriter] = None,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"Balanced neural ratio estimation (BNRE)[1]. BNRE is a variation of NRE\n        aiming to produce more conservative posterior approximations\n\n        [1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G..\n        Towards Reliable Simulation-Based Inference with Balanced Neural Ratio\n        Estimation.\n        NeurIPS 2022. https://arxiv.org/abs/2208.13624\n\n        Args:\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them. If `None`, the\n                prior must be passed to `.build_posterior()`.\n            classifier: Classifier trained to approximate likelihood ratios. If it is\n                a string, use a pre-configured network of the provided type (one of\n                linear, mlp, resnet). Alternatively, a function that builds a custom\n                neural network can be provided. The function will be called with the\n                first batch of simulations $(\\theta, x)$, which can thus be used for\n                shape inference and potentially for z-scoring. It needs to return a\n                PyTorch `nn.Module` implementing the classifier.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n            logging_level: Minimum severity of messages to log. One of the strings\n                INFO, WARNING, DEBUG, ERROR and CRITICAL.\n            summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n                file location (default is `&lt;current working directory&gt;/logs`.)\n            show_progress_bars: Whether to show a progressbar during simulation and\n                sampling.\n        \"\"\"\n\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        super().__init__(**kwargs)\n\n    def train(\n        self,\n        regularization_strength: float = 100.0,\n        training_batch_size: int = 200,\n        learning_rate: float = 5e-4,\n        validation_fraction: float = 0.1,\n        stop_after_epochs: int = 20,\n        max_num_epochs: int = 2**31 - 1,\n        clip_max_norm: Optional[float] = 5.0,\n        resume_training: bool = False,\n        discard_prior_samples: bool = False,\n        retrain_from_scratch: bool = False,\n        show_train_summary: bool = False,\n        dataloader_kwargs: Optional[Dict] = None,\n    ) -&gt; nn.Module:\n        r\"\"\"Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n        Args:\n\n            regularization_strength: The multiplicative coefficient applied to the\n                balancing regularizer ($\\lambda$).\n            training_batch_size: Training batch size.\n            learning_rate: Learning rate for Adam optimizer.\n            validation_fraction: The fraction of data to use for validation.\n            stop_after_epochs: The number of epochs to wait for improvement on the\n                validation set before terminating training.\n            max_num_epochs: Maximum number of epochs to run. If reached, we stop\n                training even when the validation loss is still decreasing. Otherwise,\n                we train until validation loss increases (see also `stop_after_epochs`).\n            clip_max_norm: Value at which to clip the total gradient norm in order to\n                prevent exploding gradients. Use None for no clipping.\n            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e`\n                during training. Expect errors, silent or explicit, when `False`.\n            resume_training: Can be used in case training time is limited, e.g. on a\n                cluster. If `True`, the split between train and validation set, the\n                optimizer, the number of epochs, and the best validation log-prob will\n                be restored from the last time `.train()` was called.\n            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n                from the prior. Training may be sped up by ignoring such less targeted\n                samples.\n            retrain_from_scratch: Whether to retrain the conditional density\n                estimator for the posterior from scratch each round.\n            show_train_summary: Whether to print the number of epochs and validation\n                loss and leakage after the training.\n            dataloader_kwargs: Additional or updated kwargs to be passed to the training\n                and validation dataloaders (like, e.g., a collate_fn)\n        Returns:\n            Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n        \"\"\"\n        kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n        kwargs[\"loss_kwargs\"] = {\n            \"regularization_strength\": kwargs.pop(\"regularization_strength\")\n        }\n        return super().train(**kwargs)\n\n    def _loss(\n        self, theta: Tensor, x: Tensor, num_atoms: int, regularization_strength: float\n    ) -&gt; Tensor:\n        \"\"\"Returns the binary cross-entropy loss for the trained classifier.\n\n        The classifier takes as input a $(\\theta,x)$ pair. It is trained to predict 1\n        if the pair was sampled from the joint $p(\\theta,x)$, and to predict 0 if the\n        pair was sampled from the marginals $p(\\theta)p(x)$.\n        \"\"\"\n\n        assert theta.shape[0] == x.shape[0], \"Batch sizes for theta and x must match.\"\n        batch_size = theta.shape[0]\n\n        logits = self._classifier_logits(theta, x, num_atoms)\n        likelihood = torch.sigmoid(logits).squeeze()\n\n        # Alternating pairs where there is one sampled from the joint and one\n        # sampled from the marginals. The first element is sampled from the\n        # joint p(theta, x) and is labelled 1. The second element is sampled\n        # from the marginals p(theta)p(x) and is labelled 0. And so on.\n        labels = ones(2 * batch_size, device=self._device)  # two atoms\n        labels[1::2] = 0.0\n\n        # Binary cross entropy to learn the likelihood (AALR-specific)\n        bce = nn.BCELoss()(likelihood, labels)\n\n        # Balancing regularizer\n        regularizer = (\n            (torch.sigmoid(logits[0::2]) + torch.sigmoid(logits[1::2]) - 1)\n            .mean()\n            .square()\n        )\n\n        return bce + regularization_strength * regularizer\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.bnre.BNRE.__init__","title":"<code>__init__(prior=None, classifier='resnet', device='cpu', logging_level='warning', summary_writer=None, show_progress_bars=True)</code>","text":"<p>Balanced neural ratio estimation (BNRE)[1]. BNRE is a variation of NRE aiming to produce more conservative posterior approximations</p> <p>[1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G.. Towards Reliable Simulation-Based Inference with Balanced Neural Ratio Estimation. NeurIPS 2022. https://arxiv.org/abs/2208.13624</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Optional[Distribution]</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the prior must be passed to <code>.build_posterior()</code>.</p> <code>None</code> <code>classifier</code> <code>Union[str, Callable]</code> <p>Classifier trained to approximate likelihood ratios. If it is a string, use a pre-configured network of the provided type (one of linear, mlp, resnet). Alternatively, a function that builds a custom neural network can be provided. The function will be called with the first batch of simulations \\((\\theta, x)\\), which can thus be used for shape inference and potentially for z-scoring. It needs to return a PyTorch <code>nn.Module</code> implementing the classifier.</p> <code>'resnet'</code> <code>device</code> <code>str</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:{0, 1, \u2026}\u201d.</p> <code>'cpu'</code> <code>logging_level</code> <code>Union[int, str]</code> <p>Minimum severity of messages to log. One of the strings INFO, WARNING, DEBUG, ERROR and CRITICAL.</p> <code>'warning'</code> <code>summary_writer</code> <code>Optional[TensorboardSummaryWriter]</code> <p>A tensorboard <code>SummaryWriter</code> to control, among others, log file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during simulation and sampling.</p> <code>True</code> Source code in <code>sbi/inference/snre/bnre.py</code> <pre><code>def __init__(\n    self,\n    prior: Optional[Distribution] = None,\n    classifier: Union[str, Callable] = \"resnet\",\n    device: str = \"cpu\",\n    logging_level: Union[int, str] = \"warning\",\n    summary_writer: Optional[TensorboardSummaryWriter] = None,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"Balanced neural ratio estimation (BNRE)[1]. BNRE is a variation of NRE\n    aiming to produce more conservative posterior approximations\n\n    [1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G..\n    Towards Reliable Simulation-Based Inference with Balanced Neural Ratio\n    Estimation.\n    NeurIPS 2022. https://arxiv.org/abs/2208.13624\n\n    Args:\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. If `None`, the\n            prior must be passed to `.build_posterior()`.\n        classifier: Classifier trained to approximate likelihood ratios. If it is\n            a string, use a pre-configured network of the provided type (one of\n            linear, mlp, resnet). Alternatively, a function that builds a custom\n            neural network can be provided. The function will be called with the\n            first batch of simulations $(\\theta, x)$, which can thus be used for\n            shape inference and potentially for z-scoring. It needs to return a\n            PyTorch `nn.Module` implementing the classifier.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:{0, 1, ...}\".\n        logging_level: Minimum severity of messages to log. One of the strings\n            INFO, WARNING, DEBUG, ERROR and CRITICAL.\n        summary_writer: A tensorboard `SummaryWriter` to control, among others, log\n            file location (default is `&lt;current working directory&gt;/logs`.)\n        show_progress_bars: Whether to show a progressbar during simulation and\n            sampling.\n    \"\"\"\n\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.snre.bnre.BNRE.train","title":"<code>train(regularization_strength=100.0, training_batch_size=200, learning_rate=0.0005, validation_fraction=0.1, stop_after_epochs=20, max_num_epochs=2 ** 31 - 1, clip_max_norm=5.0, resume_training=False, discard_prior_samples=False, retrain_from_scratch=False, show_train_summary=False, dataloader_kwargs=None)</code>","text":"<p>Return classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\). Args:</p> <pre><code>regularization_strength: The multiplicative coefficient applied to the\n    balancing regularizer ($\\lambda$).\ntraining_batch_size: Training batch size.\nlearning_rate: Learning rate for Adam optimizer.\nvalidation_fraction: The fraction of data to use for validation.\nstop_after_epochs: The number of epochs to wait for improvement on the\n    validation set before terminating training.\nmax_num_epochs: Maximum number of epochs to run. If reached, we stop\n    training even when the validation loss is still decreasing. Otherwise,\n    we train until validation loss increases (see also `stop_after_epochs`).\nclip_max_norm: Value at which to clip the total gradient norm in order to\n    prevent exploding gradients. Use None for no clipping.\nexclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e`\n    during training. Expect errors, silent or explicit, when `False`.\nresume_training: Can be used in case training time is limited, e.g. on a\n    cluster. If `True`, the split between train and validation set, the\n    optimizer, the number of epochs, and the best validation log-prob will\n    be restored from the last time `.train()` was called.\ndiscard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n    from the prior. Training may be sped up by ignoring such less targeted\n    samples.\nretrain_from_scratch: Whether to retrain the conditional density\n    estimator for the posterior from scratch each round.\nshow_train_summary: Whether to print the number of epochs and validation\n    loss and leakage after the training.\ndataloader_kwargs: Additional or updated kwargs to be passed to the training\n    and validation dataloaders (like, e.g., a collate_fn)\n</code></pre> <p>Returns:     Classifier that approximates the ratio \\(p(\\theta,x)/p(\\theta)p(x)\\).</p> Source code in <code>sbi/inference/snre/bnre.py</code> <pre><code>def train(\n    self,\n    regularization_strength: float = 100.0,\n    training_batch_size: int = 200,\n    learning_rate: float = 5e-4,\n    validation_fraction: float = 0.1,\n    stop_after_epochs: int = 20,\n    max_num_epochs: int = 2**31 - 1,\n    clip_max_norm: Optional[float] = 5.0,\n    resume_training: bool = False,\n    discard_prior_samples: bool = False,\n    retrain_from_scratch: bool = False,\n    show_train_summary: bool = False,\n    dataloader_kwargs: Optional[Dict] = None,\n) -&gt; nn.Module:\n    r\"\"\"Return classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n    Args:\n\n        regularization_strength: The multiplicative coefficient applied to the\n            balancing regularizer ($\\lambda$).\n        training_batch_size: Training batch size.\n        learning_rate: Learning rate for Adam optimizer.\n        validation_fraction: The fraction of data to use for validation.\n        stop_after_epochs: The number of epochs to wait for improvement on the\n            validation set before terminating training.\n        max_num_epochs: Maximum number of epochs to run. If reached, we stop\n            training even when the validation loss is still decreasing. Otherwise,\n            we train until validation loss increases (see also `stop_after_epochs`).\n        clip_max_norm: Value at which to clip the total gradient norm in order to\n            prevent exploding gradients. Use None for no clipping.\n        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=\u00b1\u221e`\n            during training. Expect errors, silent or explicit, when `False`.\n        resume_training: Can be used in case training time is limited, e.g. on a\n            cluster. If `True`, the split between train and validation set, the\n            optimizer, the number of epochs, and the best validation log-prob will\n            be restored from the last time `.train()` was called.\n        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.\n            from the prior. Training may be sped up by ignoring such less targeted\n            samples.\n        retrain_from_scratch: Whether to retrain the conditional density\n            estimator for the posterior from scratch each round.\n        show_train_summary: Whether to print the number of epochs and validation\n            loss and leakage after the training.\n        dataloader_kwargs: Additional or updated kwargs to be passed to the training\n            and validation dataloaders (like, e.g., a collate_fn)\n    Returns:\n        Classifier that approximates the ratio $p(\\theta,x)/p(\\theta)p(x)$.\n    \"\"\"\n    kwargs = del_entries(locals(), entries=(\"self\", \"__class__\"))\n    kwargs[\"loss_kwargs\"] = {\n        \"regularization_strength\": kwargs.pop(\"regularization_strength\")\n    }\n    return super().train(**kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.mcabc.MCABC","title":"<code>MCABC</code>","text":"<p>               Bases: <code>ABCBASE</code></p> <p>Monte-Carlo Approximate Bayesian Computation (Rejection ABC).</p> Source code in <code>sbi/inference/abc/mcabc.py</code> <pre><code>class MCABC(ABCBASE):\n    \"\"\"Monte-Carlo Approximate Bayesian Computation (Rejection ABC).\"\"\"\n\n    def __init__(\n        self,\n        simulator: Callable,\n        prior,\n        distance: Union[str, Callable] = \"l2\",\n        requires_iid_data: Optional[None] = None,\n        distance_kwargs: Optional[Dict] = None,\n        num_workers: int = 1,\n        simulation_batch_size: int = 1,\n        distance_batch_size: int = -1,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].\n\n        [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.\n        (1999). Population growth of human Y chromosomes: a study of Y chromosome\n        microsatellites. Molecular biology and evolution, 16(12), 1791-1798.\n\n        Args:\n            simulator: A function that takes parameters $\\theta$ and maps them to\n                simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any\n                regular Python callable (i.e. function or class with `__call__` method)\n                can be used.\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them. Any\n                object with `.log_prob()`and `.sample()` (for example, a PyTorch\n                distribution) can be used.\n            distance: Distance function to compare observed and simulated data. Can be\n                a custom callable function or one of `l1`, `l2`, `mse`,\n                `mmd`, `wasserstein`.\n            requires_iid_data: Whether to allow conditioning on iid sampled data or not.\n                Typically, this information is inferred by the choice of the distance,\n                but in case a custom distance is used, this information is pivotal.\n            distance_kwargs: Configurations parameters for the distances. In particular\n                useful for the MMD and Wasserstein distance.\n            num_workers: Number of parallel workers to use for simulations.\n            simulation_batch_size: Number of parameter sets that the simulator\n                maps to data x at once. If None, we simulate all parameter sets at the\n                same time. If &gt;= 1, the simulator has to process data of shape\n                (simulation_batch_size, parameter_dimension).\n            distance_batch_size: Number of simulations that the distance function\n                evaluates against the reference observations at once. If -1, we evaluate\n                all simulations at the same time.\n        \"\"\"\n\n        super().__init__(\n            simulator=simulator,\n            prior=prior,\n            distance=distance,\n            requires_iid_data=requires_iid_data,\n            distance_kwargs=distance_kwargs,\n            num_workers=num_workers,\n            simulation_batch_size=simulation_batch_size,\n            distance_batch_size=distance_batch_size,\n            show_progress_bars=show_progress_bars,\n        )\n\n    def __call__(\n        self,\n        x_o: Union[Tensor, ndarray],\n        num_simulations: int,\n        eps: Optional[float] = None,\n        quantile: Optional[float] = None,\n        lra: bool = False,\n        sass: bool = False,\n        sass_fraction: float = 0.25,\n        sass_expansion_degree: int = 1,\n        kde: bool = False,\n        kde_kwargs: Optional[Dict[str, Any]] = None,\n        return_summary: bool = False,\n        num_iid_samples: int = 1,\n    ) -&gt; Union[Tuple[Tensor, dict], Tuple[KDEWrapper, dict], Tensor, KDEWrapper]:\n        r\"\"\"Run MCABC and return accepted parameters or KDE object fitted on them.\n\n        Args:\n            x_o: Observed data.\n            num_simulations: Number of simulations to run.\n            eps: Acceptance threshold $\\epsilon$ for distance between observed and\n                simulated data.\n            quantile: Upper quantile of smallest distances for which the corresponding\n                parameters are returned, e.g, q=0.01 will return the top 1%. Exactly\n                one of quantile or `eps` have to be passed.\n            lra: Whether to run linear regression adjustment as in Beaumont et al. 2002\n            sass: Whether to determine semi-automatic summary statistics as in\n                Fearnhead &amp; Prangle 2012.\n            sass_fraction: Fraction of simulation budget used for the initial sass run.\n            sass_expansion_degree: Degree of the polynomial feature expansion for the\n                sass regression, default 1 - no expansion.\n            kde: Whether to run KDE on the accepted parameters to return a KDE\n                object from which one can sample.\n            kde_kwargs: kwargs for performing KDE:\n                'bandwidth='; either a float, or a string naming a bandwidth\n                heuristics, e.g., 'cv' (cross validation), 'silvermann' or 'scott',\n                default 'cv'.\n                'transform': transform applied to the parameters before doing KDE.\n                'sample_weights': weights associated with samples. See 'get_kde' for\n                more details\n            return_summary: Whether to return the distances and data corresponding to\n                the accepted parameters.\n            num_iid_samples: Number of simulations per parameter. Choose\n                `num_iid_samples&gt;1`, if you have chosen a statistical distance that\n                evaluates sets of simulations against a set of reference observations\n                instead of a single data-point comparison.\n\n        Returns:\n            theta (if kde False): accepted parameters\n            kde (if kde True): KDE object based on accepted parameters from which one\n                can .sample() and .log_prob().\n            summary (if summary True): dictionary containing the accepted paramters (if\n                kde True), distances and simulated data x.\n        \"\"\"\n\n        # Exactly one of eps or quantile need to be passed.\n        assert (eps is not None) ^ (\n            quantile is not None\n        ), \"Eps or quantile must be passed, but not both.\"\n        if kde_kwargs is None:\n            kde_kwargs = {}\n\n        # Run SASS and change the simulator and x_o accordingly.\n        if sass:\n            num_pilot_simulations = int(sass_fraction * num_simulations)\n            self.logger.info(\n                \"Running SASS with %s pilot samples.\", num_pilot_simulations\n            )\n            num_simulations -= num_pilot_simulations\n\n            pilot_theta = self.prior.sample((num_pilot_simulations,))\n            pilot_x = self._batched_simulator(pilot_theta)\n\n            sass_transform = self.get_sass_transform(\n                pilot_theta, pilot_x, sass_expansion_degree\n            )\n\n            # Add sass transform to simulator and x_o.\n            def simulator(theta):\n                return sass_transform(self._batched_simulator(theta))\n\n            x_o = sass_transform(x_o)\n        else:\n            simulator = self._batched_simulator\n\n        # Simulate and calculate distances.\n        theta = self.prior.sample((num_simulations,))\n        theta_repeat = theta.repeat_interleave(num_iid_samples, dim=0)\n        x = simulator(theta_repeat)\n        x = x.reshape((\n            num_simulations,\n            num_iid_samples,\n            -1,\n        ))  # Dim(num_initial_pop, num_iid_samples, -1)\n\n        # Infer x shape to test and set x_o.\n        if not self.distance.requires_iid_data:\n            x = x.squeeze(1)\n            self.x_shape = x[0].shape\n            self.x_o = process_x(x_o, self.x_shape)\n        else:\n            self.x_shape = x[0, 0].shape\n            self.x_o = process_x(x_o, self.x_shape)\n\n        distances = self.distance(self.x_o, x)\n\n        # Select based on acceptance threshold epsilon.\n        if eps is not None:\n            is_accepted = distances &lt; eps\n            num_accepted = is_accepted.sum().item()\n            assert num_accepted &gt; 0, f\"No parameters accepted, eps={eps} too small\"\n\n            theta_accepted = theta[is_accepted]\n            distances_accepted = distances[is_accepted]\n            x_accepted = x[is_accepted]\n\n        # Select based on quantile on sorted distances.\n        elif quantile is not None:\n            num_top_samples = int(num_simulations * quantile)\n            sort_idx = torch.argsort(distances)\n            theta_accepted = theta[sort_idx][:num_top_samples]\n            distances_accepted = distances[sort_idx][:num_top_samples]\n            x_accepted = x[sort_idx][:num_top_samples]\n\n        else:\n            raise ValueError(\"One of epsilon or quantile has to be passed.\")\n\n        # Maybe adjust theta with LRA.\n        if lra:\n            self.logger.info(\"Running Linear regression adjustment.\")\n            final_theta = self.run_lra(theta_accepted, x_accepted, observation=self.x_o)\n        else:\n            final_theta = theta_accepted\n\n        if kde:\n            self.logger.info(\n                \"\"\"KDE on %s samples with bandwidth option\n                {kde_kwargs[\"bandwidth\"] if \"bandwidth\" in kde_kwargs else \"cv\"}.\n                Beware that KDE can give unreliable results when used with too few\n                samples and in high dimensions.\"\"\",\n                final_theta.shape[0],\n            )\n\n            kde_dist = get_kde(final_theta, **kde_kwargs)\n\n            if return_summary:\n                return (\n                    kde_dist,\n                    dict(theta=final_theta, distances=distances_accepted, x=x_accepted),\n                )\n            else:\n                return kde_dist\n        elif return_summary:\n            return final_theta, dict(distances=distances_accepted, x=x_accepted)\n        else:\n            return final_theta\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.mcabc.MCABC.__call__","title":"<code>__call__(x_o, num_simulations, eps=None, quantile=None, lra=False, sass=False, sass_fraction=0.25, sass_expansion_degree=1, kde=False, kde_kwargs=None, return_summary=False, num_iid_samples=1)</code>","text":"<p>Run MCABC and return accepted parameters or KDE object fitted on them.</p> <p>Parameters:</p> Name Type Description Default <code>x_o</code> <code>Union[Tensor, ndarray]</code> <p>Observed data.</p> required <code>num_simulations</code> <code>int</code> <p>Number of simulations to run.</p> required <code>eps</code> <code>Optional[float]</code> <p>Acceptance threshold \\(\\epsilon\\) for distance between observed and simulated data.</p> <code>None</code> <code>quantile</code> <code>Optional[float]</code> <p>Upper quantile of smallest distances for which the corresponding parameters are returned, e.g, q=0.01 will return the top 1%. Exactly one of quantile or <code>eps</code> have to be passed.</p> <code>None</code> <code>lra</code> <code>bool</code> <p>Whether to run linear regression adjustment as in Beaumont et al. 2002</p> <code>False</code> <code>sass</code> <code>bool</code> <p>Whether to determine semi-automatic summary statistics as in Fearnhead &amp; Prangle 2012.</p> <code>False</code> <code>sass_fraction</code> <code>float</code> <p>Fraction of simulation budget used for the initial sass run.</p> <code>0.25</code> <code>sass_expansion_degree</code> <code>int</code> <p>Degree of the polynomial feature expansion for the sass regression, default 1 - no expansion.</p> <code>1</code> <code>kde</code> <code>bool</code> <p>Whether to run KDE on the accepted parameters to return a KDE object from which one can sample.</p> <code>False</code> <code>kde_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>kwargs for performing KDE: \u2018bandwidth=\u2019; either a float, or a string naming a bandwidth heuristics, e.g., \u2018cv\u2019 (cross validation), \u2018silvermann\u2019 or \u2018scott\u2019, default \u2018cv\u2019. \u2018transform\u2019: transform applied to the parameters before doing KDE. \u2018sample_weights\u2019: weights associated with samples. See \u2018get_kde\u2019 for more details</p> <code>None</code> <code>return_summary</code> <code>bool</code> <p>Whether to return the distances and data corresponding to the accepted parameters.</p> <code>False</code> <code>num_iid_samples</code> <code>int</code> <p>Number of simulations per parameter. Choose <code>num_iid_samples&gt;1</code>, if you have chosen a statistical distance that evaluates sets of simulations against a set of reference observations instead of a single data-point comparison.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>theta</code> <code>if kde False</code> <p>accepted parameters</p> <code>kde</code> <code>if kde True</code> <p>KDE object based on accepted parameters from which one can .sample() and .log_prob().</p> <code>summary</code> <code>if summary True</code> <p>dictionary containing the accepted paramters (if kde True), distances and simulated data x.</p> Source code in <code>sbi/inference/abc/mcabc.py</code> <pre><code>def __call__(\n    self,\n    x_o: Union[Tensor, ndarray],\n    num_simulations: int,\n    eps: Optional[float] = None,\n    quantile: Optional[float] = None,\n    lra: bool = False,\n    sass: bool = False,\n    sass_fraction: float = 0.25,\n    sass_expansion_degree: int = 1,\n    kde: bool = False,\n    kde_kwargs: Optional[Dict[str, Any]] = None,\n    return_summary: bool = False,\n    num_iid_samples: int = 1,\n) -&gt; Union[Tuple[Tensor, dict], Tuple[KDEWrapper, dict], Tensor, KDEWrapper]:\n    r\"\"\"Run MCABC and return accepted parameters or KDE object fitted on them.\n\n    Args:\n        x_o: Observed data.\n        num_simulations: Number of simulations to run.\n        eps: Acceptance threshold $\\epsilon$ for distance between observed and\n            simulated data.\n        quantile: Upper quantile of smallest distances for which the corresponding\n            parameters are returned, e.g, q=0.01 will return the top 1%. Exactly\n            one of quantile or `eps` have to be passed.\n        lra: Whether to run linear regression adjustment as in Beaumont et al. 2002\n        sass: Whether to determine semi-automatic summary statistics as in\n            Fearnhead &amp; Prangle 2012.\n        sass_fraction: Fraction of simulation budget used for the initial sass run.\n        sass_expansion_degree: Degree of the polynomial feature expansion for the\n            sass regression, default 1 - no expansion.\n        kde: Whether to run KDE on the accepted parameters to return a KDE\n            object from which one can sample.\n        kde_kwargs: kwargs for performing KDE:\n            'bandwidth='; either a float, or a string naming a bandwidth\n            heuristics, e.g., 'cv' (cross validation), 'silvermann' or 'scott',\n            default 'cv'.\n            'transform': transform applied to the parameters before doing KDE.\n            'sample_weights': weights associated with samples. See 'get_kde' for\n            more details\n        return_summary: Whether to return the distances and data corresponding to\n            the accepted parameters.\n        num_iid_samples: Number of simulations per parameter. Choose\n            `num_iid_samples&gt;1`, if you have chosen a statistical distance that\n            evaluates sets of simulations against a set of reference observations\n            instead of a single data-point comparison.\n\n    Returns:\n        theta (if kde False): accepted parameters\n        kde (if kde True): KDE object based on accepted parameters from which one\n            can .sample() and .log_prob().\n        summary (if summary True): dictionary containing the accepted paramters (if\n            kde True), distances and simulated data x.\n    \"\"\"\n\n    # Exactly one of eps or quantile need to be passed.\n    assert (eps is not None) ^ (\n        quantile is not None\n    ), \"Eps or quantile must be passed, but not both.\"\n    if kde_kwargs is None:\n        kde_kwargs = {}\n\n    # Run SASS and change the simulator and x_o accordingly.\n    if sass:\n        num_pilot_simulations = int(sass_fraction * num_simulations)\n        self.logger.info(\n            \"Running SASS with %s pilot samples.\", num_pilot_simulations\n        )\n        num_simulations -= num_pilot_simulations\n\n        pilot_theta = self.prior.sample((num_pilot_simulations,))\n        pilot_x = self._batched_simulator(pilot_theta)\n\n        sass_transform = self.get_sass_transform(\n            pilot_theta, pilot_x, sass_expansion_degree\n        )\n\n        # Add sass transform to simulator and x_o.\n        def simulator(theta):\n            return sass_transform(self._batched_simulator(theta))\n\n        x_o = sass_transform(x_o)\n    else:\n        simulator = self._batched_simulator\n\n    # Simulate and calculate distances.\n    theta = self.prior.sample((num_simulations,))\n    theta_repeat = theta.repeat_interleave(num_iid_samples, dim=0)\n    x = simulator(theta_repeat)\n    x = x.reshape((\n        num_simulations,\n        num_iid_samples,\n        -1,\n    ))  # Dim(num_initial_pop, num_iid_samples, -1)\n\n    # Infer x shape to test and set x_o.\n    if not self.distance.requires_iid_data:\n        x = x.squeeze(1)\n        self.x_shape = x[0].shape\n        self.x_o = process_x(x_o, self.x_shape)\n    else:\n        self.x_shape = x[0, 0].shape\n        self.x_o = process_x(x_o, self.x_shape)\n\n    distances = self.distance(self.x_o, x)\n\n    # Select based on acceptance threshold epsilon.\n    if eps is not None:\n        is_accepted = distances &lt; eps\n        num_accepted = is_accepted.sum().item()\n        assert num_accepted &gt; 0, f\"No parameters accepted, eps={eps} too small\"\n\n        theta_accepted = theta[is_accepted]\n        distances_accepted = distances[is_accepted]\n        x_accepted = x[is_accepted]\n\n    # Select based on quantile on sorted distances.\n    elif quantile is not None:\n        num_top_samples = int(num_simulations * quantile)\n        sort_idx = torch.argsort(distances)\n        theta_accepted = theta[sort_idx][:num_top_samples]\n        distances_accepted = distances[sort_idx][:num_top_samples]\n        x_accepted = x[sort_idx][:num_top_samples]\n\n    else:\n        raise ValueError(\"One of epsilon or quantile has to be passed.\")\n\n    # Maybe adjust theta with LRA.\n    if lra:\n        self.logger.info(\"Running Linear regression adjustment.\")\n        final_theta = self.run_lra(theta_accepted, x_accepted, observation=self.x_o)\n    else:\n        final_theta = theta_accepted\n\n    if kde:\n        self.logger.info(\n            \"\"\"KDE on %s samples with bandwidth option\n            {kde_kwargs[\"bandwidth\"] if \"bandwidth\" in kde_kwargs else \"cv\"}.\n            Beware that KDE can give unreliable results when used with too few\n            samples and in high dimensions.\"\"\",\n            final_theta.shape[0],\n        )\n\n        kde_dist = get_kde(final_theta, **kde_kwargs)\n\n        if return_summary:\n            return (\n                kde_dist,\n                dict(theta=final_theta, distances=distances_accepted, x=x_accepted),\n            )\n        else:\n            return kde_dist\n    elif return_summary:\n        return final_theta, dict(distances=distances_accepted, x=x_accepted)\n    else:\n        return final_theta\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.mcabc.MCABC.__init__","title":"<code>__init__(simulator, prior, distance='l2', requires_iid_data=None, distance_kwargs=None, num_workers=1, simulation_batch_size=1, distance_batch_size=-1, show_progress_bars=True)</code>","text":"<p>Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</p> <p>[1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W. (1999). Population growth of human Y chromosomes: a study of Y chromosome microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</p> <p>Parameters:</p> Name Type Description Default <code>simulator</code> <code>Callable</code> <p>A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, <code>x</code>, \\(\\mathrm{sim}(\\theta)\\to x\\). Any regular Python callable (i.e. function or class with <code>__call__</code> method) can be used.</p> required <code>prior</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch distribution) can be used.</p> required <code>distance</code> <code>Union[str, Callable]</code> <p>Distance function to compare observed and simulated data. Can be a custom callable function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>, <code>mmd</code>, <code>wasserstein</code>.</p> <code>'l2'</code> <code>requires_iid_data</code> <code>Optional[None]</code> <p>Whether to allow conditioning on iid sampled data or not. Typically, this information is inferred by the choice of the distance, but in case a custom distance is used, this information is pivotal.</p> <code>None</code> <code>distance_kwargs</code> <code>Optional[Dict]</code> <p>Configurations parameters for the distances. In particular useful for the MMD and Wasserstein distance.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of parallel workers to use for simulations.</p> <code>1</code> <code>simulation_batch_size</code> <code>int</code> <p>Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If &gt;= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension).</p> <code>1</code> <code>distance_batch_size</code> <code>int</code> <p>Number of simulations that the distance function evaluates against the reference observations at once. If -1, we evaluate all simulations at the same time.</p> <code>-1</code> Source code in <code>sbi/inference/abc/mcabc.py</code> <pre><code>def __init__(\n    self,\n    simulator: Callable,\n    prior,\n    distance: Union[str, Callable] = \"l2\",\n    requires_iid_data: Optional[None] = None,\n    distance_kwargs: Optional[Dict] = None,\n    num_workers: int = 1,\n    simulation_batch_size: int = 1,\n    distance_batch_size: int = -1,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].\n\n    [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.\n    (1999). Population growth of human Y chromosomes: a study of Y chromosome\n    microsatellites. Molecular biology and evolution, 16(12), 1791-1798.\n\n    Args:\n        simulator: A function that takes parameters $\\theta$ and maps them to\n            simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any\n            regular Python callable (i.e. function or class with `__call__` method)\n            can be used.\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. Any\n            object with `.log_prob()`and `.sample()` (for example, a PyTorch\n            distribution) can be used.\n        distance: Distance function to compare observed and simulated data. Can be\n            a custom callable function or one of `l1`, `l2`, `mse`,\n            `mmd`, `wasserstein`.\n        requires_iid_data: Whether to allow conditioning on iid sampled data or not.\n            Typically, this information is inferred by the choice of the distance,\n            but in case a custom distance is used, this information is pivotal.\n        distance_kwargs: Configurations parameters for the distances. In particular\n            useful for the MMD and Wasserstein distance.\n        num_workers: Number of parallel workers to use for simulations.\n        simulation_batch_size: Number of parameter sets that the simulator\n            maps to data x at once. If None, we simulate all parameter sets at the\n            same time. If &gt;= 1, the simulator has to process data of shape\n            (simulation_batch_size, parameter_dimension).\n        distance_batch_size: Number of simulations that the distance function\n            evaluates against the reference observations at once. If -1, we evaluate\n            all simulations at the same time.\n    \"\"\"\n\n    super().__init__(\n        simulator=simulator,\n        prior=prior,\n        distance=distance,\n        requires_iid_data=requires_iid_data,\n        distance_kwargs=distance_kwargs,\n        num_workers=num_workers,\n        simulation_batch_size=simulation_batch_size,\n        distance_batch_size=distance_batch_size,\n        show_progress_bars=show_progress_bars,\n    )\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC","title":"<code>SMCABC</code>","text":"<p>               Bases: <code>ABCBASE</code></p> <p>Sequential Monte Carlo Approximate Bayesian Computation.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>class SMCABC(ABCBASE):\n    \"\"\"Sequential Monte Carlo Approximate Bayesian Computation.\"\"\"\n\n    def __init__(\n        self,\n        simulator: Callable,\n        prior: Distribution,\n        distance: Union[str, Callable] = \"l2\",\n        requires_iid_data: Optional[None] = None,\n        distance_kwargs: Optional[Dict] = None,\n        num_workers: int = 1,\n        simulation_batch_size: int = 1,\n        distance_batch_size: int = -1,\n        show_progress_bars: bool = True,\n        kernel: Optional[str] = \"gaussian\",\n        algorithm_variant: str = \"C\",\n    ):\n        r\"\"\"Sequential Monte Carlo Approximate Bayesian Computation.\n\n        We distinguish between three different SMC methods here:\n            - A: Toni et al. 2010 (Phd Thesis)\n            - B: Sisson et al. 2007 (with correction from 2009)\n            - C: Beaumont et al. 2009\n\n        In Toni et al. 2010 we find an overview of the differences on page 34:\n            - B: same as A except for resampling of weights if the effective sampling\n                size is too small.\n            - C: same as A except for calculation of the covariance of the perturbation\n                kernel: the kernel covariance is a scaled version of the covariance of\n                the previous population.\n\n        Args:\n            simulator: A function that takes parameters $\\theta$ and maps them to\n                simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any\n                regular Python callable (i.e. function or class with `__call__` method)\n                can be used.\n            prior: A probability distribution that expresses prior knowledge about the\n                parameters, e.g. which ranges are meaningful for them. Any\n                object with `.log_prob()`and `.sample()` (for example, a PyTorch\n                distribution) can be used.\n            distance: Distance function to compare observed and simulated data. Can be\n                a custom callable function or one of `l1`, `l2`, `mse`,\n                `mmd`, `wasserstein`.\n            requires_iid_data: Whether to allow conditioning on iid sampled data or not.\n                Typically, this information is inferred by the choice of the distance,\n                but in case a custom distance is used, this information is pivotal.\n            distance_kwargs: Configurations parameters for the distances. In particular\n                useful for the MMD and Wasserstein distance.\n            num_workers: Number of parallel workers to use for simulations.\n            simulation_batch_size: Number of parameter sets that the simulator\n                maps to data x at once. If None, we simulate all parameter sets at the\n                same time. If &gt;= 1, the simulator has to process data of shape\n                (simulation_batch_size, parameter_dimension).\n            distance_batch_size: Number of simulations that the distance function\n                evaluates against the reference observations at once. If -1, we evaluate\n                all simulations at the same time.\n            show_progress_bars: Whether to show a progressbar during simulation and\n                sampling.\n            kernel: Perturbation kernel.\n            algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.\n        \"\"\"\n\n        super().__init__(\n            simulator=simulator,\n            prior=prior,\n            distance=distance,\n            requires_iid_data=requires_iid_data,\n            distance_kwargs=distance_kwargs,\n            num_workers=num_workers,\n            simulation_batch_size=simulation_batch_size,\n            distance_batch_size=distance_batch_size,\n            show_progress_bars=show_progress_bars,\n        )\n\n        kernels = (\"gaussian\", \"uniform\")\n        assert (\n            kernel in kernels\n        ), f\"Kernel '{kernel}' not supported. Choose one from {kernels}.\"\n        self.kernel = kernel\n\n        algorithm_variants = (\"A\", \"B\", \"C\")\n        assert algorithm_variant in algorithm_variants, (\n            f\"SMCABC variant '{algorithm_variant}' not supported, choose one from\"\n            \" {algorithm_variants}.\"\n        )\n        self.algorithm_variant = algorithm_variant\n        self.distance_to_x0 = None\n        self.simulation_counter = 0\n        self.num_simulations = 0\n        self.kernel_variance = None\n\n        # Define simulator that keeps track of budget.\n        def simulate_with_budget(theta):\n            self.simulation_counter += theta.shape[0]\n            return self._batched_simulator(theta)\n\n        self._simulate_with_budget = simulate_with_budget\n\n    def __call__(\n        self,\n        x_o: Union[Tensor, ndarray],\n        num_particles: int,\n        num_initial_pop: int,\n        num_simulations: int,\n        epsilon_decay: float,\n        distance_based_decay: bool = False,\n        ess_min: Optional[float] = None,\n        kernel_variance_scale: float = 1.0,\n        use_last_pop_samples: bool = True,\n        return_summary: bool = False,\n        kde: bool = False,\n        kde_kwargs: Optional[Dict[str, Any]] = None,\n        kde_sample_weights: bool = False,\n        lra: bool = False,\n        lra_with_weights: bool = False,\n        sass: bool = False,\n        sass_fraction: float = 0.25,\n        sass_expansion_degree: int = 1,\n        num_iid_samples: int = 1,\n    ) -&gt; Union[Tensor, KDEWrapper, Tuple[Tensor, dict], Tuple[KDEWrapper, dict]]:\n        r\"\"\"Run SMCABC and return accepted parameters or KDE object fitted on them.\n\n        Args:\n            x_o: Observed data.\n            num_particles: Number of particles in each population.\n            num_initial_pop: Number of simulations used for initial population.\n            num_simulations: Total number of possible simulations.\n            epsilon_decay: Factor with which the acceptance threshold $\\epsilon$ decays.\n            distance_based_decay: Whether the $\\epsilon$ decay is constant over\n                populations or calculated from the previous populations distribution of\n                distances.\n            ess_min: Threshold of effective sampling size for resampling weights. Not\n                used when None (default).\n            kernel_variance_scale: Factor for scaling the perturbation kernel variance.\n            use_last_pop_samples: Whether to fill up the current population with\n                samples from the previous population when the budget is used up. If\n                False, the current population is discarded and the previous population\n                is returned.\n            lra: Whether to run linear regression adjustment as in Beaumont et al. 2002\n            lra_with_weights: Whether to run lra as weighted linear regression with SMC\n                weights\n            sass: Whether to determine semi-automatic summary statistics (sass) as in\n                Fearnhead &amp; Prangle 2012.\n            sass_fraction: Fraction of simulation budget used for the initial sass run.\n            sass_expansion_degree: Degree of the polynomial feature expansion for the\n                sass regression, default 1 - no expansion.\n            kde: Whether to run KDE on the accepted parameters to return a KDE\n                object from which one can sample.\n            kde_kwargs: kwargs for performing KDE:\n                'bandwidth='; either a float, or a string naming a bandwidth\n                heuristics, e.g., 'cv' (cross validation), 'silvermann' or 'scott',\n                default 'cv'.\n                'transform': transform applied to the parameters before doing KDE.\n                'sample_weights': weights associated with samples. See 'get_kde' for\n                more details\n            kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw\n                particles.\n            return_summary: Whether to return a dictionary with all accepted particles,\n                weights, etc. at the end.\n            num_iid_samples: Number of simulations per parameter. Choose\n                `num_iid_samples&gt;1`, if you have chosen a statistical distance that\n                evaluates sets of simulations against a set of reference observations\n                instead of a single data-point comparison.\n\n        Returns:\n            theta (if kde False): accepted parameters of the last population.\n            kde (if kde True): KDE object fitted on accepted parameters, from which one\n                can .sample() and .log_prob().\n            summary (if return_summary True): dictionary containing the accepted\n                paramters (if kde True), distances and simulated data x of all\n                populations.\n        \"\"\"\n\n        pop_idx = 0\n        self.num_simulations = num_simulations * num_iid_samples\n        if kde_kwargs is None:\n            kde_kwargs = {}\n        assert isinstance(epsilon_decay, float) and epsilon_decay &gt; 0.0\n        assert not (\n            self.distance.requires_iid_data and lra\n        ), \"Currently there is no support to run inference \"\n        \"on multiple observations together with lra.\"\n        assert not (\n            self.distance.requires_iid_data and sass\n        ), \"Currently there is no support to run inference \"\n        \"on multiple observations together with sass.\"\n\n        # Pilot run for SASS.\n        if sass:\n            num_pilot_simulations = int(sass_fraction * num_simulations)\n            self.logger.info(\n                \"Running SASS with %s pilot samples.\", num_pilot_simulations\n            )\n            sass_transform = self.run_sass_set_xo(\n                num_particles,\n                num_pilot_simulations,\n                x_o,\n                num_iid_samples,\n                lra,\n                sass_expansion_degree,\n            )\n            # Udpate simulator and xo\n            x_o = sass_transform(self.x_o)\n\n            def sass_simulator(theta):\n                self.simulation_counter += theta.shape[0]\n                return sass_transform(self._batched_simulator(theta))\n\n            self._simulate_with_budget = sass_simulator\n\n        # run initial population\n        particles, epsilon, distances, x = self._set_xo_and_sample_initial_population(\n            x_o, num_particles, num_initial_pop, num_iid_samples\n        )\n        log_weights = torch.log(1 / num_particles * torch.ones(num_particles))\n\n        self.logger.info((\n            \"population=%s, eps=%s, ess=%s, num_sims=%s\",\n            pop_idx,\n            epsilon,\n            1.0,\n            num_initial_pop,\n        ))\n\n        all_particles = [particles]\n        all_log_weights = [log_weights]\n        all_distances = [distances]\n        all_epsilons = [epsilon]\n        all_x = [x]\n\n        while self.simulation_counter &lt; self.num_simulations:\n            pop_idx += 1\n            # Decay based on quantile of distances from previous pop.\n            if distance_based_decay:\n                epsilon = self._get_next_epsilon(\n                    all_distances[pop_idx - 1], epsilon_decay\n                )\n            # Constant decay.\n            else:\n                epsilon *= epsilon_decay\n\n            # Get kernel variance from previous pop.\n            self.kernel_variance = self.get_kernel_variance(\n                all_particles[pop_idx - 1],\n                torch.exp(all_log_weights[pop_idx - 1]),\n                samples_per_dim=500,\n                kernel_variance_scale=kernel_variance_scale,\n            )\n            particles, log_weights, distances, x = self._sample_next_population(\n                particles=all_particles[pop_idx - 1],\n                log_weights=all_log_weights[pop_idx - 1],\n                distances=all_distances[pop_idx - 1],\n                epsilon=epsilon,\n                x=all_x[pop_idx - 1],\n                num_iid_samples=num_iid_samples,\n                use_last_pop_samples=use_last_pop_samples,\n            )\n\n            # Resample population if effective sampling size is too small.\n            if ess_min is not None:\n                particles, log_weights = self.resample_if_ess_too_small(\n                    particles, log_weights, ess_min, pop_idx\n                )\n\n            self.logger.info((\n                \"population=%s done: eps={epsilon:.6f}, num_sims=%s.\",\n                pop_idx,\n                epsilon,\n                self.simulation_counter,\n            ))\n\n            # collect results\n            all_particles.append(particles)\n            all_log_weights.append(log_weights)\n            all_distances.append(distances)\n            all_epsilons.append(epsilon)\n            all_x.append(x)\n\n        # Maybe run LRA and adjust weights.\n        if lra:\n            self.logger.info(\"Running Linear regression adjustment.\")\n            adjusted_particles, _ = self.run_lra_update_weights(\n                particles=all_particles[-1],\n                xs=all_x[-1],\n                observation=process_x(x_o),\n                log_weights=all_log_weights[-1],\n                lra_with_weights=lra_with_weights,\n            )\n            final_particles = adjusted_particles\n        else:\n            final_particles = all_particles[-1]\n\n        if kde:\n            self.logger.info(\n                \"\"\"KDE on %s samples with bandwidth option %s. Beware that KDE can give\n                unreliable results when used with too few samples and in high\n                dimensions.\"\"\",\n                final_particles.shape[0],\n                kde_kwargs.get(\"bandwidth\", \"cv\"),\n            )\n            # Maybe get particles weights from last population for weighted KDE.\n            if kde_sample_weights:\n                kde_kwargs[\"sample_weights\"] = all_log_weights[-1].exp()\n\n            kde_dist = get_kde(final_particles, **kde_kwargs)\n\n            if return_summary:\n                return (\n                    kde_dist,\n                    dict(\n                        particles=all_particles,\n                        weights=all_log_weights,\n                        epsilons=all_epsilons,\n                        distances=all_distances,\n                        xs=all_x,\n                    ),\n                )\n            else:\n                return kde_dist\n\n        if return_summary:\n            return (\n                final_particles,\n                dict(\n                    particles=all_particles,\n                    weights=all_log_weights,\n                    epsilons=all_epsilons,\n                    distances=all_distances,\n                    xs=all_x,\n                ),\n            )\n        else:\n            return final_particles\n\n    def _set_xo_and_sample_initial_population(\n        self,\n        x_o: Array,\n        num_particles: int,\n        num_initial_pop: int,\n        num_iid_samples: int,\n    ) -&gt; Tuple[Tensor, float, Tensor, Tensor]:\n        \"\"\"Return particles, epsilon and distances of initial population.\"\"\"\n\n        assert (\n            num_particles &lt;= num_initial_pop\n        ), \"number of initial round simulations must be greater than population size\"\n\n        assert (x_o.shape[0] == 1) or self.distance.requires_iid_data, (\n            \"Your data contain iid data-points, but the choice of \"\n            \"your distance does not allow multiple conditioning \"\n            \"observations.\"\n        )\n\n        theta = self.prior.sample((num_initial_pop,))\n\n        theta_repeat = theta.repeat_interleave(num_iid_samples, dim=0)\n        x = self._simulate_with_budget(theta_repeat)\n        x = x.reshape((\n            num_initial_pop,\n            num_iid_samples,\n            -1,\n        ))  # Dim(num_initial_pop, num_iid_samples, -1)\n\n        # Infer x shape to test and set x_o.\n        if not self.distance.requires_iid_data:\n            x = x.squeeze(1)\n            self.x_shape = x[0].shape\n        else:\n            self.x_shape = x[0, 0].shape\n        self.x_o = process_x(x_o, self.x_shape)\n\n        distances = self.distance(self.x_o, x)\n        sortidx = torch.argsort(distances)\n        particles = theta[sortidx][:num_particles]\n        # Take last accepted distance as epsilon.\n        initial_epsilon = distances[sortidx][num_particles - 1].item()\n\n        if not math.isfinite(initial_epsilon):\n            initial_epsilon = 1e8\n\n        return (\n            particles,\n            initial_epsilon,\n            distances[sortidx][:num_particles],\n            x[sortidx][:num_particles],\n        )\n\n    def _sample_next_population(\n        self,\n        particles: Tensor,\n        log_weights: Tensor,\n        distances: Tensor,\n        epsilon: float,\n        x: Tensor,\n        num_iid_samples: int,\n        use_last_pop_samples: bool = True,\n    ) -&gt; Tuple[Tensor, Tensor, Tensor, Tensor]:\n        \"\"\"Return particles, weights and distances of new population.\"\"\"\n\n        new_particles = []\n        new_log_weights = []\n        new_distances = []\n        new_x = []\n\n        num_accepted_particles = 0\n        num_particles = particles.shape[0]\n\n        while num_accepted_particles &lt; num_particles:\n            # Upperbound for batch size to not exceed simulation budget.\n            num_batch = min(\n                num_particles - num_accepted_particles,\n                self.num_simulations - self.simulation_counter,\n            )\n\n            # Sample from previous population and perturb.\n            particle_candidates = self._sample_and_perturb(\n                particles, torch.exp(log_weights), num_samples=num_batch\n            )\n            # Simulate and select based on distance.\n            candidates_repeated = particle_candidates.repeat_interleave(\n                num_iid_samples, dim=0\n            )\n            x_candidates = self._simulate_with_budget(candidates_repeated)\n            x_candidates = x_candidates.reshape((\n                num_batch,\n                num_iid_samples,\n                -1,\n            ))  # Dim(num_initial_pop, num_iid_samples, -1)\n            if not self.distance.requires_iid_data:\n                x_candidates = x_candidates.squeeze(1)\n\n            dists = self.distance(self.x_o, x_candidates)\n            is_accepted = dists &lt;= epsilon\n            num_accepted_batch = int(is_accepted.sum().item())\n\n            if num_accepted_batch &gt; 0:\n                new_particles.append(particle_candidates[is_accepted])\n                new_log_weights.append(\n                    self._calculate_new_log_weights(\n                        particle_candidates[is_accepted],\n                        particles,\n                        log_weights,\n                    )\n                )\n                new_distances.append(dists[is_accepted])\n                new_x.append(x_candidates[is_accepted])\n                num_accepted_particles += num_accepted_batch\n\n            # If simulation budget was exceeded and we still need particles, take\n            # previous population or fill up with previous population.\n            if (\n                self.simulation_counter &gt;= self.num_simulations\n                and num_accepted_particles &lt; num_particles\n            ):\n                if use_last_pop_samples:\n                    num_remaining = num_particles - num_accepted_particles\n                    self.logger.info(\n                        \"\"\"Simulation Budget exceeded, filling up with %s\n                        samples from last population.\"\"\",\n                        num_remaining,\n                    )\n                    # Some new particles have been accepted already, therefore\n                    # fill up the remaining once with old particles and weights.\n                    new_particles.append(particles[:num_remaining, :])\n                    # Recalculate weights with new particles.\n                    new_log_weights = [\n                        self._calculate_new_log_weights(\n                            torch.cat(new_particles),\n                            particles,\n                            log_weights,\n                        )\n                    ]\n                    new_distances.append(distances[:num_remaining])\n                    new_x.append(x[:num_remaining])\n                else:\n                    self.logger.info(\n                        \"Simulation Budget exceeded, returning previous population.\"\n                    )\n                    new_particles = [particles]\n                    new_log_weights = [log_weights]\n                    new_distances = [distances]\n                    new_x = [x]\n\n                break\n\n        # collect lists of tensors into tensors\n        new_particles = torch.cat(new_particles)\n        new_log_weights = torch.cat(new_log_weights)\n        new_distances = torch.cat(new_distances)\n        new_x = torch.cat(new_x)\n\n        # normalize the new weights\n        new_log_weights -= torch.logsumexp(new_log_weights, dim=0)\n\n        # Return sorted wrt distances.\n        sort_idx = torch.argsort(new_distances)\n\n        return (\n            new_particles[sort_idx],\n            new_log_weights[sort_idx],\n            new_distances[sort_idx],\n            new_x[sort_idx],\n        )\n\n    def _get_next_epsilon(self, distances: Tensor, quantile: float) -&gt; float:\n        \"\"\"Return epsilon for next round based on quantile of this round's distances.\n\n        Note: distances are made unique to avoid repeated distances from simulations\n        that result in the same observation.\n\n        Args:\n            distances: The distances accepted in this round.\n            quantile: Quantile in the distance distribution to determine new epsilon.\n\n        Returns:\n            epsilon: Epsilon for the next population.\n        \"\"\"\n        # Take unique distances to skip same distances simulations (return is sorted).\n        distances = torch.unique(distances)\n        # Cumsum as cdf proxy.\n        distances_cdf = torch.cumsum(distances, dim=0) / distances.sum()\n        # Take the q quantile of distances.\n        try:\n            qidx = torch.where(distances_cdf &gt;= quantile)[0][0]\n        except IndexError:\n            self.logger.warning((\n                \"\"\"Accepted unique distances=%s don't match quantile=%s. Selecting\n                    last distance.\"\"\",\n                distances,\n                quantile,\n            ))\n            qidx = -1\n\n        # The new epsilon is given by that distance.\n        return distances[qidx].item()\n\n    def _calculate_new_log_weights(\n        self,\n        new_particles: Tensor,\n        old_particles: Tensor,\n        old_log_weights: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"Return new log weights following formulas in publications A,B anc C.\"\"\"\n\n        # Prior can be batched across new particles.\n        prior_log_probs = self.prior.log_prob(new_particles)\n\n        # Contstruct function to get kernel log prob for given old particle.\n        # The kernel is centered on each old particle as in all three variants (A,B,C).\n        def kernel_log_prob(new_particle):\n            return self.get_new_kernel(old_particles).log_prob(new_particle)\n\n        # We still have to loop over particles here because\n        # the kernel log probs are already batched across old particles.\n        log_weighted_sum = torch.tensor(\n            [\n                torch.logsumexp(old_log_weights + kernel_log_prob(new_particle), dim=0)\n                for new_particle in new_particles\n            ],\n            dtype=torch.float32,\n        )\n        # new weights are prior probs over weighted sum:\n        return prior_log_probs - log_weighted_sum\n\n    @staticmethod\n    def sample_from_population_with_weights(\n        particles: Tensor, weights: Tensor, num_samples: int = 1\n    ) -&gt; Tensor:\n        \"\"\"Return samples from particles sampled with weights.\"\"\"\n\n        # define multinomial with weights as probs\n        multi = Multinomial(probs=weights)\n        # sample num samples, with replacement\n        samples = multi.sample(sample_shape=torch.Size((num_samples,)))\n        # get indices of success trials\n        indices = torch.where(samples)[1]\n        # return those indices from trace\n        return particles[indices]\n\n    def _sample_and_perturb(\n        self, particles: Tensor, weights: Tensor, num_samples: int = 1\n    ) -&gt; Tensor:\n        \"\"\"Sample and perturb batch of new parameters from trace.\n\n        Reject sampled and perturbed parameters outside of prior.\n        \"\"\"\n\n        num_accepted = 0\n        parameters = []\n        while num_accepted &lt; num_samples:\n            parms = self.sample_from_population_with_weights(\n                particles, weights, num_samples=num_samples - num_accepted\n            )\n\n            # Create kernel on params and perturb.\n            parms_perturbed = self.get_new_kernel(parms).sample()\n\n            is_within_prior = within_support(self.prior, parms_perturbed)\n            num_accepted += int(is_within_prior.sum().item())\n\n            if num_accepted &gt; 0:\n                parameters.append(parms_perturbed[is_within_prior])\n\n        return torch.cat(parameters)\n\n    def get_kernel_variance(\n        self,\n        particles: Tensor,\n        weights: Tensor,\n        samples_per_dim: int = 100,\n        kernel_variance_scale: float = 1.0,\n    ) -&gt; Tensor:\n        \"\"\"Return kernel variance for a given population of particles and weights.\"\"\"\n        if self.kernel == \"gaussian\":\n            # For variant C, Beaumont et al. 2009, the kernel variance comes from the\n            # previous population.\n            if self.algorithm_variant == \"C\":\n                # Calculate weighted covariance of particles.\n                population_cov = torch.tensor(\n                    np.atleast_2d(np.cov(particles, rowvar=False, aweights=weights)),\n                    dtype=torch.float32,\n                )\n                # Make sure variance is nonsingular.\n                try:\n                    torch.linalg.cholesky(kernel_variance_scale * population_cov)\n                except RuntimeError:\n                    self.logger.warning(\n                        \"\"\"\"Singular particle covariance, using unit covariance.\"\"\"\n                    )\n                    population_cov = torch.eye(particles.shape[1])\n                return kernel_variance_scale * population_cov\n            # While for Toni et al. and Sisson et al. it comes from the parameter\n            # ranges.\n            elif self.algorithm_variant in (\"A\", \"B\"):\n                particle_ranges = self.get_particle_ranges(\n                    particles, weights, samples_per_dim=samples_per_dim\n                )\n                return kernel_variance_scale * torch.diag(particle_ranges)\n            else:\n                raise ValueError(f\"Variant, '{self.algorithm_variant}' not supported.\")\n        elif self.kernel == \"uniform\":\n            # Variance spans the range of parameters for every dimension.\n            return kernel_variance_scale * self.get_particle_ranges(\n                particles, weights, samples_per_dim=samples_per_dim\n            )\n        else:\n            raise ValueError(f\"Kernel, '{self.kernel}' not supported.\")\n\n    def get_new_kernel(self, thetas: Tensor) -&gt; Distribution:\n        \"\"\"Return new kernel distribution for a given set of paramters.\"\"\"\n\n        if self.kernel == \"gaussian\":\n            assert self.kernel_variance is not None, \"get kernel variance first.\"\n            assert self.kernel_variance.ndim == 2\n            return MultivariateNormal(\n                loc=thetas, covariance_matrix=self.kernel_variance\n            )\n\n        elif self.kernel == \"uniform\":\n            low = thetas - self.kernel_variance\n            high = thetas + self.kernel_variance\n            # Move batch shape to event shape to get Uniform that is multivariate in\n            # parameter dimension.\n            return BoxUniform(low=low, high=high)\n        else:\n            raise ValueError(f\"Kernel, '{self.kernel}' not supported.\")\n\n    def resample_if_ess_too_small(\n        self,\n        particles: Tensor,\n        log_weights: Tensor,\n        ess_min: float,\n        pop_idx: int,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Return resampled particles and uniform weights if effectice sampling size is\n        too small.\n        \"\"\"\n\n        num_particles = particles.shape[0]\n        ess = (1 / torch.sum(torch.exp(2.0 * log_weights), dim=0)) / num_particles\n        # Resampling of weights for low ESS only for Sisson et al. 2007.\n        if ess &lt; ess_min:\n            self.logger.info(\"ESS=%s too low, resampling pop %s...\", ess, pop_idx)\n            # First resample, then set to uniform weights as in Sisson et al. 2007.\n            particles = self.sample_from_population_with_weights(\n                particles, torch.exp(log_weights), num_samples=num_particles\n            )\n            log_weights = torch.log(1 / num_particles * torch.ones(num_particles))\n\n        return particles, log_weights\n\n    def run_lra_update_weights(\n        self,\n        particles: Tensor,\n        xs: Tensor,\n        observation: Tensor,\n        log_weights: Tensor,\n        lra_with_weights: bool,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Return particles and weights adjusted with LRA.\n\n        Runs (weighted) linear regression from xs onto particles to adjust the\n        particles.\n\n        Updates the SMC weights according to the new particles.\n        \"\"\"\n\n        adjusted_particels = self.run_lra(\n            theta=particles,\n            x=xs,\n            observation=observation,\n            sample_weight=log_weights.exp() if lra_with_weights else None,\n        )\n\n        # Update SMC weights with LRA adjusted weights\n        adjusted_log_weights = self._calculate_new_log_weights(\n            new_particles=adjusted_particels,\n            old_particles=particles,\n            old_log_weights=log_weights,\n        )\n\n        return adjusted_particels, adjusted_log_weights\n\n    def run_sass_set_xo(\n        self,\n        num_particles: int,\n        num_pilot_simulations: int,\n        x_o,\n        num_iid_samples: int,\n        lra: bool = False,\n        sass_expansion_degree: int = 1,\n    ) -&gt; Callable:\n        \"\"\"Return transform for semi-automatic summary statistics.\n\n        Runs an single round of rejection abc with fixed budget and accepts\n        num_particles simulations to run the regression for sass.\n\n        Sets self.x_o once the x_shape can be derived from simulations.\n        \"\"\"\n        (\n            pilot_particles,\n            _,\n            _,\n            pilot_xs,\n        ) = self._set_xo_and_sample_initial_population(\n            x_o, num_particles, num_pilot_simulations, num_iid_samples\n        )\n        assert self.x_o is not None, \"x_o not set yet.\"\n\n        # Adjust with LRA.\n        if lra:\n            pilot_particles = self.run_lra(pilot_particles, pilot_xs, self.x_o)\n        sass_transform = self.get_sass_transform(\n            pilot_particles,\n            pilot_xs,\n            expansion_degree=sass_expansion_degree,\n            sample_weight=None,\n        )\n        return sass_transform\n\n    def get_particle_ranges(\n        self, particles: Tensor, weights: Tensor, samples_per_dim: int = 100\n    ) -&gt; Tensor:\n        \"\"\"Return range of particles in each parameter dimension.\"\"\"\n\n        # get weighted samples\n        samples = self.sample_from_population_with_weights(\n            particles,\n            weights,\n            num_samples=samples_per_dim * particles.shape[1],\n        )\n\n        # Variance spans the range of particles for every dimension.\n        particle_ranges = samples.max(0).values - samples.min(0).values\n        assert particle_ranges.ndim &lt; 2\n        return particle_ranges\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.__call__","title":"<code>__call__(x_o, num_particles, num_initial_pop, num_simulations, epsilon_decay, distance_based_decay=False, ess_min=None, kernel_variance_scale=1.0, use_last_pop_samples=True, return_summary=False, kde=False, kde_kwargs=None, kde_sample_weights=False, lra=False, lra_with_weights=False, sass=False, sass_fraction=0.25, sass_expansion_degree=1, num_iid_samples=1)</code>","text":"<p>Run SMCABC and return accepted parameters or KDE object fitted on them.</p> <p>Parameters:</p> Name Type Description Default <code>x_o</code> <code>Union[Tensor, ndarray]</code> <p>Observed data.</p> required <code>num_particles</code> <code>int</code> <p>Number of particles in each population.</p> required <code>num_initial_pop</code> <code>int</code> <p>Number of simulations used for initial population.</p> required <code>num_simulations</code> <code>int</code> <p>Total number of possible simulations.</p> required <code>epsilon_decay</code> <code>float</code> <p>Factor with which the acceptance threshold \\(\\epsilon\\) decays.</p> required <code>distance_based_decay</code> <code>bool</code> <p>Whether the \\(\\epsilon\\) decay is constant over populations or calculated from the previous populations distribution of distances.</p> <code>False</code> <code>ess_min</code> <code>Optional[float]</code> <p>Threshold of effective sampling size for resampling weights. Not used when None (default).</p> <code>None</code> <code>kernel_variance_scale</code> <code>float</code> <p>Factor for scaling the perturbation kernel variance.</p> <code>1.0</code> <code>use_last_pop_samples</code> <code>bool</code> <p>Whether to fill up the current population with samples from the previous population when the budget is used up. If False, the current population is discarded and the previous population is returned.</p> <code>True</code> <code>lra</code> <code>bool</code> <p>Whether to run linear regression adjustment as in Beaumont et al. 2002</p> <code>False</code> <code>lra_with_weights</code> <code>bool</code> <p>Whether to run lra as weighted linear regression with SMC weights</p> <code>False</code> <code>sass</code> <code>bool</code> <p>Whether to determine semi-automatic summary statistics (sass) as in Fearnhead &amp; Prangle 2012.</p> <code>False</code> <code>sass_fraction</code> <code>float</code> <p>Fraction of simulation budget used for the initial sass run.</p> <code>0.25</code> <code>sass_expansion_degree</code> <code>int</code> <p>Degree of the polynomial feature expansion for the sass regression, default 1 - no expansion.</p> <code>1</code> <code>kde</code> <code>bool</code> <p>Whether to run KDE on the accepted parameters to return a KDE object from which one can sample.</p> <code>False</code> <code>kde_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>kwargs for performing KDE: \u2018bandwidth=\u2019; either a float, or a string naming a bandwidth heuristics, e.g., \u2018cv\u2019 (cross validation), \u2018silvermann\u2019 or \u2018scott\u2019, default \u2018cv\u2019. \u2018transform\u2019: transform applied to the parameters before doing KDE. \u2018sample_weights\u2019: weights associated with samples. See \u2018get_kde\u2019 for more details</p> <code>None</code> <code>kde_sample_weights</code> <code>bool</code> <p>Whether perform weighted KDE with SMC weights or on raw particles.</p> <code>False</code> <code>return_summary</code> <code>bool</code> <p>Whether to return a dictionary with all accepted particles, weights, etc. at the end.</p> <code>False</code> <code>num_iid_samples</code> <code>int</code> <p>Number of simulations per parameter. Choose <code>num_iid_samples&gt;1</code>, if you have chosen a statistical distance that evaluates sets of simulations against a set of reference observations instead of a single data-point comparison.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>theta</code> <code>if kde False</code> <p>accepted parameters of the last population.</p> <code>kde</code> <code>if kde True</code> <p>KDE object fitted on accepted parameters, from which one can .sample() and .log_prob().</p> <code>summary</code> <code>if return_summary True</code> <p>dictionary containing the accepted paramters (if kde True), distances and simulated data x of all populations.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>def __call__(\n    self,\n    x_o: Union[Tensor, ndarray],\n    num_particles: int,\n    num_initial_pop: int,\n    num_simulations: int,\n    epsilon_decay: float,\n    distance_based_decay: bool = False,\n    ess_min: Optional[float] = None,\n    kernel_variance_scale: float = 1.0,\n    use_last_pop_samples: bool = True,\n    return_summary: bool = False,\n    kde: bool = False,\n    kde_kwargs: Optional[Dict[str, Any]] = None,\n    kde_sample_weights: bool = False,\n    lra: bool = False,\n    lra_with_weights: bool = False,\n    sass: bool = False,\n    sass_fraction: float = 0.25,\n    sass_expansion_degree: int = 1,\n    num_iid_samples: int = 1,\n) -&gt; Union[Tensor, KDEWrapper, Tuple[Tensor, dict], Tuple[KDEWrapper, dict]]:\n    r\"\"\"Run SMCABC and return accepted parameters or KDE object fitted on them.\n\n    Args:\n        x_o: Observed data.\n        num_particles: Number of particles in each population.\n        num_initial_pop: Number of simulations used for initial population.\n        num_simulations: Total number of possible simulations.\n        epsilon_decay: Factor with which the acceptance threshold $\\epsilon$ decays.\n        distance_based_decay: Whether the $\\epsilon$ decay is constant over\n            populations or calculated from the previous populations distribution of\n            distances.\n        ess_min: Threshold of effective sampling size for resampling weights. Not\n            used when None (default).\n        kernel_variance_scale: Factor for scaling the perturbation kernel variance.\n        use_last_pop_samples: Whether to fill up the current population with\n            samples from the previous population when the budget is used up. If\n            False, the current population is discarded and the previous population\n            is returned.\n        lra: Whether to run linear regression adjustment as in Beaumont et al. 2002\n        lra_with_weights: Whether to run lra as weighted linear regression with SMC\n            weights\n        sass: Whether to determine semi-automatic summary statistics (sass) as in\n            Fearnhead &amp; Prangle 2012.\n        sass_fraction: Fraction of simulation budget used for the initial sass run.\n        sass_expansion_degree: Degree of the polynomial feature expansion for the\n            sass regression, default 1 - no expansion.\n        kde: Whether to run KDE on the accepted parameters to return a KDE\n            object from which one can sample.\n        kde_kwargs: kwargs for performing KDE:\n            'bandwidth='; either a float, or a string naming a bandwidth\n            heuristics, e.g., 'cv' (cross validation), 'silvermann' or 'scott',\n            default 'cv'.\n            'transform': transform applied to the parameters before doing KDE.\n            'sample_weights': weights associated with samples. See 'get_kde' for\n            more details\n        kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw\n            particles.\n        return_summary: Whether to return a dictionary with all accepted particles,\n            weights, etc. at the end.\n        num_iid_samples: Number of simulations per parameter. Choose\n            `num_iid_samples&gt;1`, if you have chosen a statistical distance that\n            evaluates sets of simulations against a set of reference observations\n            instead of a single data-point comparison.\n\n    Returns:\n        theta (if kde False): accepted parameters of the last population.\n        kde (if kde True): KDE object fitted on accepted parameters, from which one\n            can .sample() and .log_prob().\n        summary (if return_summary True): dictionary containing the accepted\n            paramters (if kde True), distances and simulated data x of all\n            populations.\n    \"\"\"\n\n    pop_idx = 0\n    self.num_simulations = num_simulations * num_iid_samples\n    if kde_kwargs is None:\n        kde_kwargs = {}\n    assert isinstance(epsilon_decay, float) and epsilon_decay &gt; 0.0\n    assert not (\n        self.distance.requires_iid_data and lra\n    ), \"Currently there is no support to run inference \"\n    \"on multiple observations together with lra.\"\n    assert not (\n        self.distance.requires_iid_data and sass\n    ), \"Currently there is no support to run inference \"\n    \"on multiple observations together with sass.\"\n\n    # Pilot run for SASS.\n    if sass:\n        num_pilot_simulations = int(sass_fraction * num_simulations)\n        self.logger.info(\n            \"Running SASS with %s pilot samples.\", num_pilot_simulations\n        )\n        sass_transform = self.run_sass_set_xo(\n            num_particles,\n            num_pilot_simulations,\n            x_o,\n            num_iid_samples,\n            lra,\n            sass_expansion_degree,\n        )\n        # Udpate simulator and xo\n        x_o = sass_transform(self.x_o)\n\n        def sass_simulator(theta):\n            self.simulation_counter += theta.shape[0]\n            return sass_transform(self._batched_simulator(theta))\n\n        self._simulate_with_budget = sass_simulator\n\n    # run initial population\n    particles, epsilon, distances, x = self._set_xo_and_sample_initial_population(\n        x_o, num_particles, num_initial_pop, num_iid_samples\n    )\n    log_weights = torch.log(1 / num_particles * torch.ones(num_particles))\n\n    self.logger.info((\n        \"population=%s, eps=%s, ess=%s, num_sims=%s\",\n        pop_idx,\n        epsilon,\n        1.0,\n        num_initial_pop,\n    ))\n\n    all_particles = [particles]\n    all_log_weights = [log_weights]\n    all_distances = [distances]\n    all_epsilons = [epsilon]\n    all_x = [x]\n\n    while self.simulation_counter &lt; self.num_simulations:\n        pop_idx += 1\n        # Decay based on quantile of distances from previous pop.\n        if distance_based_decay:\n            epsilon = self._get_next_epsilon(\n                all_distances[pop_idx - 1], epsilon_decay\n            )\n        # Constant decay.\n        else:\n            epsilon *= epsilon_decay\n\n        # Get kernel variance from previous pop.\n        self.kernel_variance = self.get_kernel_variance(\n            all_particles[pop_idx - 1],\n            torch.exp(all_log_weights[pop_idx - 1]),\n            samples_per_dim=500,\n            kernel_variance_scale=kernel_variance_scale,\n        )\n        particles, log_weights, distances, x = self._sample_next_population(\n            particles=all_particles[pop_idx - 1],\n            log_weights=all_log_weights[pop_idx - 1],\n            distances=all_distances[pop_idx - 1],\n            epsilon=epsilon,\n            x=all_x[pop_idx - 1],\n            num_iid_samples=num_iid_samples,\n            use_last_pop_samples=use_last_pop_samples,\n        )\n\n        # Resample population if effective sampling size is too small.\n        if ess_min is not None:\n            particles, log_weights = self.resample_if_ess_too_small(\n                particles, log_weights, ess_min, pop_idx\n            )\n\n        self.logger.info((\n            \"population=%s done: eps={epsilon:.6f}, num_sims=%s.\",\n            pop_idx,\n            epsilon,\n            self.simulation_counter,\n        ))\n\n        # collect results\n        all_particles.append(particles)\n        all_log_weights.append(log_weights)\n        all_distances.append(distances)\n        all_epsilons.append(epsilon)\n        all_x.append(x)\n\n    # Maybe run LRA and adjust weights.\n    if lra:\n        self.logger.info(\"Running Linear regression adjustment.\")\n        adjusted_particles, _ = self.run_lra_update_weights(\n            particles=all_particles[-1],\n            xs=all_x[-1],\n            observation=process_x(x_o),\n            log_weights=all_log_weights[-1],\n            lra_with_weights=lra_with_weights,\n        )\n        final_particles = adjusted_particles\n    else:\n        final_particles = all_particles[-1]\n\n    if kde:\n        self.logger.info(\n            \"\"\"KDE on %s samples with bandwidth option %s. Beware that KDE can give\n            unreliable results when used with too few samples and in high\n            dimensions.\"\"\",\n            final_particles.shape[0],\n            kde_kwargs.get(\"bandwidth\", \"cv\"),\n        )\n        # Maybe get particles weights from last population for weighted KDE.\n        if kde_sample_weights:\n            kde_kwargs[\"sample_weights\"] = all_log_weights[-1].exp()\n\n        kde_dist = get_kde(final_particles, **kde_kwargs)\n\n        if return_summary:\n            return (\n                kde_dist,\n                dict(\n                    particles=all_particles,\n                    weights=all_log_weights,\n                    epsilons=all_epsilons,\n                    distances=all_distances,\n                    xs=all_x,\n                ),\n            )\n        else:\n            return kde_dist\n\n    if return_summary:\n        return (\n            final_particles,\n            dict(\n                particles=all_particles,\n                weights=all_log_weights,\n                epsilons=all_epsilons,\n                distances=all_distances,\n                xs=all_x,\n            ),\n        )\n    else:\n        return final_particles\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.__init__","title":"<code>__init__(simulator, prior, distance='l2', requires_iid_data=None, distance_kwargs=None, num_workers=1, simulation_batch_size=1, distance_batch_size=-1, show_progress_bars=True, kernel='gaussian', algorithm_variant='C')</code>","text":"<p>Sequential Monte Carlo Approximate Bayesian Computation.</p> We distinguish between three different SMC methods here <ul> <li>A: Toni et al. 2010 (Phd Thesis)</li> <li>B: Sisson et al. 2007 (with correction from 2009)</li> <li>C: Beaumont et al. 2009</li> </ul> <p>In Toni et al. 2010 we find an overview of the differences on page 34:     - B: same as A except for resampling of weights if the effective sampling         size is too small.     - C: same as A except for calculation of the covariance of the perturbation         kernel: the kernel covariance is a scaled version of the covariance of         the previous population.</p> <p>Parameters:</p> Name Type Description Default <code>simulator</code> <code>Callable</code> <p>A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, <code>x</code>, \\(\\mathrm{sim}(\\theta)\\to x\\). Any regular Python callable (i.e. function or class with <code>__call__</code> method) can be used.</p> required <code>prior</code> <code>Distribution</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch distribution) can be used.</p> required <code>distance</code> <code>Union[str, Callable]</code> <p>Distance function to compare observed and simulated data. Can be a custom callable function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>, <code>mmd</code>, <code>wasserstein</code>.</p> <code>'l2'</code> <code>requires_iid_data</code> <code>Optional[None]</code> <p>Whether to allow conditioning on iid sampled data or not. Typically, this information is inferred by the choice of the distance, but in case a custom distance is used, this information is pivotal.</p> <code>None</code> <code>distance_kwargs</code> <code>Optional[Dict]</code> <p>Configurations parameters for the distances. In particular useful for the MMD and Wasserstein distance.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of parallel workers to use for simulations.</p> <code>1</code> <code>simulation_batch_size</code> <code>int</code> <p>Number of parameter sets that the simulator maps to data x at once. If None, we simulate all parameter sets at the same time. If &gt;= 1, the simulator has to process data of shape (simulation_batch_size, parameter_dimension).</p> <code>1</code> <code>distance_batch_size</code> <code>int</code> <p>Number of simulations that the distance function evaluates against the reference observations at once. If -1, we evaluate all simulations at the same time.</p> <code>-1</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during simulation and sampling.</p> <code>True</code> <code>kernel</code> <code>Optional[str]</code> <p>Perturbation kernel.</p> <code>'gaussian'</code> <code>algorithm_variant</code> <code>str</code> <p>Indicating the choice of algorithm variant, A, B, or C.</p> <code>'C'</code> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>def __init__(\n    self,\n    simulator: Callable,\n    prior: Distribution,\n    distance: Union[str, Callable] = \"l2\",\n    requires_iid_data: Optional[None] = None,\n    distance_kwargs: Optional[Dict] = None,\n    num_workers: int = 1,\n    simulation_batch_size: int = 1,\n    distance_batch_size: int = -1,\n    show_progress_bars: bool = True,\n    kernel: Optional[str] = \"gaussian\",\n    algorithm_variant: str = \"C\",\n):\n    r\"\"\"Sequential Monte Carlo Approximate Bayesian Computation.\n\n    We distinguish between three different SMC methods here:\n        - A: Toni et al. 2010 (Phd Thesis)\n        - B: Sisson et al. 2007 (with correction from 2009)\n        - C: Beaumont et al. 2009\n\n    In Toni et al. 2010 we find an overview of the differences on page 34:\n        - B: same as A except for resampling of weights if the effective sampling\n            size is too small.\n        - C: same as A except for calculation of the covariance of the perturbation\n            kernel: the kernel covariance is a scaled version of the covariance of\n            the previous population.\n\n    Args:\n        simulator: A function that takes parameters $\\theta$ and maps them to\n            simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any\n            regular Python callable (i.e. function or class with `__call__` method)\n            can be used.\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. Any\n            object with `.log_prob()`and `.sample()` (for example, a PyTorch\n            distribution) can be used.\n        distance: Distance function to compare observed and simulated data. Can be\n            a custom callable function or one of `l1`, `l2`, `mse`,\n            `mmd`, `wasserstein`.\n        requires_iid_data: Whether to allow conditioning on iid sampled data or not.\n            Typically, this information is inferred by the choice of the distance,\n            but in case a custom distance is used, this information is pivotal.\n        distance_kwargs: Configurations parameters for the distances. In particular\n            useful for the MMD and Wasserstein distance.\n        num_workers: Number of parallel workers to use for simulations.\n        simulation_batch_size: Number of parameter sets that the simulator\n            maps to data x at once. If None, we simulate all parameter sets at the\n            same time. If &gt;= 1, the simulator has to process data of shape\n            (simulation_batch_size, parameter_dimension).\n        distance_batch_size: Number of simulations that the distance function\n            evaluates against the reference observations at once. If -1, we evaluate\n            all simulations at the same time.\n        show_progress_bars: Whether to show a progressbar during simulation and\n            sampling.\n        kernel: Perturbation kernel.\n        algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.\n    \"\"\"\n\n    super().__init__(\n        simulator=simulator,\n        prior=prior,\n        distance=distance,\n        requires_iid_data=requires_iid_data,\n        distance_kwargs=distance_kwargs,\n        num_workers=num_workers,\n        simulation_batch_size=simulation_batch_size,\n        distance_batch_size=distance_batch_size,\n        show_progress_bars=show_progress_bars,\n    )\n\n    kernels = (\"gaussian\", \"uniform\")\n    assert (\n        kernel in kernels\n    ), f\"Kernel '{kernel}' not supported. Choose one from {kernels}.\"\n    self.kernel = kernel\n\n    algorithm_variants = (\"A\", \"B\", \"C\")\n    assert algorithm_variant in algorithm_variants, (\n        f\"SMCABC variant '{algorithm_variant}' not supported, choose one from\"\n        \" {algorithm_variants}.\"\n    )\n    self.algorithm_variant = algorithm_variant\n    self.distance_to_x0 = None\n    self.simulation_counter = 0\n    self.num_simulations = 0\n    self.kernel_variance = None\n\n    # Define simulator that keeps track of budget.\n    def simulate_with_budget(theta):\n        self.simulation_counter += theta.shape[0]\n        return self._batched_simulator(theta)\n\n    self._simulate_with_budget = simulate_with_budget\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.get_kernel_variance","title":"<code>get_kernel_variance(particles, weights, samples_per_dim=100, kernel_variance_scale=1.0)</code>","text":"<p>Return kernel variance for a given population of particles and weights.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>def get_kernel_variance(\n    self,\n    particles: Tensor,\n    weights: Tensor,\n    samples_per_dim: int = 100,\n    kernel_variance_scale: float = 1.0,\n) -&gt; Tensor:\n    \"\"\"Return kernel variance for a given population of particles and weights.\"\"\"\n    if self.kernel == \"gaussian\":\n        # For variant C, Beaumont et al. 2009, the kernel variance comes from the\n        # previous population.\n        if self.algorithm_variant == \"C\":\n            # Calculate weighted covariance of particles.\n            population_cov = torch.tensor(\n                np.atleast_2d(np.cov(particles, rowvar=False, aweights=weights)),\n                dtype=torch.float32,\n            )\n            # Make sure variance is nonsingular.\n            try:\n                torch.linalg.cholesky(kernel_variance_scale * population_cov)\n            except RuntimeError:\n                self.logger.warning(\n                    \"\"\"\"Singular particle covariance, using unit covariance.\"\"\"\n                )\n                population_cov = torch.eye(particles.shape[1])\n            return kernel_variance_scale * population_cov\n        # While for Toni et al. and Sisson et al. it comes from the parameter\n        # ranges.\n        elif self.algorithm_variant in (\"A\", \"B\"):\n            particle_ranges = self.get_particle_ranges(\n                particles, weights, samples_per_dim=samples_per_dim\n            )\n            return kernel_variance_scale * torch.diag(particle_ranges)\n        else:\n            raise ValueError(f\"Variant, '{self.algorithm_variant}' not supported.\")\n    elif self.kernel == \"uniform\":\n        # Variance spans the range of parameters for every dimension.\n        return kernel_variance_scale * self.get_particle_ranges(\n            particles, weights, samples_per_dim=samples_per_dim\n        )\n    else:\n        raise ValueError(f\"Kernel, '{self.kernel}' not supported.\")\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.get_new_kernel","title":"<code>get_new_kernel(thetas)</code>","text":"<p>Return new kernel distribution for a given set of paramters.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>def get_new_kernel(self, thetas: Tensor) -&gt; Distribution:\n    \"\"\"Return new kernel distribution for a given set of paramters.\"\"\"\n\n    if self.kernel == \"gaussian\":\n        assert self.kernel_variance is not None, \"get kernel variance first.\"\n        assert self.kernel_variance.ndim == 2\n        return MultivariateNormal(\n            loc=thetas, covariance_matrix=self.kernel_variance\n        )\n\n    elif self.kernel == \"uniform\":\n        low = thetas - self.kernel_variance\n        high = thetas + self.kernel_variance\n        # Move batch shape to event shape to get Uniform that is multivariate in\n        # parameter dimension.\n        return BoxUniform(low=low, high=high)\n    else:\n        raise ValueError(f\"Kernel, '{self.kernel}' not supported.\")\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.get_particle_ranges","title":"<code>get_particle_ranges(particles, weights, samples_per_dim=100)</code>","text":"<p>Return range of particles in each parameter dimension.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>def get_particle_ranges(\n    self, particles: Tensor, weights: Tensor, samples_per_dim: int = 100\n) -&gt; Tensor:\n    \"\"\"Return range of particles in each parameter dimension.\"\"\"\n\n    # get weighted samples\n    samples = self.sample_from_population_with_weights(\n        particles,\n        weights,\n        num_samples=samples_per_dim * particles.shape[1],\n    )\n\n    # Variance spans the range of particles for every dimension.\n    particle_ranges = samples.max(0).values - samples.min(0).values\n    assert particle_ranges.ndim &lt; 2\n    return particle_ranges\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small","title":"<code>resample_if_ess_too_small(particles, log_weights, ess_min, pop_idx)</code>","text":"<p>Return resampled particles and uniform weights if effectice sampling size is too small.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>def resample_if_ess_too_small(\n    self,\n    particles: Tensor,\n    log_weights: Tensor,\n    ess_min: float,\n    pop_idx: int,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Return resampled particles and uniform weights if effectice sampling size is\n    too small.\n    \"\"\"\n\n    num_particles = particles.shape[0]\n    ess = (1 / torch.sum(torch.exp(2.0 * log_weights), dim=0)) / num_particles\n    # Resampling of weights for low ESS only for Sisson et al. 2007.\n    if ess &lt; ess_min:\n        self.logger.info(\"ESS=%s too low, resampling pop %s...\", ess, pop_idx)\n        # First resample, then set to uniform weights as in Sisson et al. 2007.\n        particles = self.sample_from_population_with_weights(\n            particles, torch.exp(log_weights), num_samples=num_particles\n        )\n        log_weights = torch.log(1 / num_particles * torch.ones(num_particles))\n\n    return particles, log_weights\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.run_lra_update_weights","title":"<code>run_lra_update_weights(particles, xs, observation, log_weights, lra_with_weights)</code>","text":"<p>Return particles and weights adjusted with LRA.</p> <p>Runs (weighted) linear regression from xs onto particles to adjust the particles.</p> <p>Updates the SMC weights according to the new particles.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>def run_lra_update_weights(\n    self,\n    particles: Tensor,\n    xs: Tensor,\n    observation: Tensor,\n    log_weights: Tensor,\n    lra_with_weights: bool,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Return particles and weights adjusted with LRA.\n\n    Runs (weighted) linear regression from xs onto particles to adjust the\n    particles.\n\n    Updates the SMC weights according to the new particles.\n    \"\"\"\n\n    adjusted_particels = self.run_lra(\n        theta=particles,\n        x=xs,\n        observation=observation,\n        sample_weight=log_weights.exp() if lra_with_weights else None,\n    )\n\n    # Update SMC weights with LRA adjusted weights\n    adjusted_log_weights = self._calculate_new_log_weights(\n        new_particles=adjusted_particels,\n        old_particles=particles,\n        old_log_weights=log_weights,\n    )\n\n    return adjusted_particels, adjusted_log_weights\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.run_sass_set_xo","title":"<code>run_sass_set_xo(num_particles, num_pilot_simulations, x_o, num_iid_samples, lra=False, sass_expansion_degree=1)</code>","text":"<p>Return transform for semi-automatic summary statistics.</p> <p>Runs an single round of rejection abc with fixed budget and accepts num_particles simulations to run the regression for sass.</p> <p>Sets self.x_o once the x_shape can be derived from simulations.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>def run_sass_set_xo(\n    self,\n    num_particles: int,\n    num_pilot_simulations: int,\n    x_o,\n    num_iid_samples: int,\n    lra: bool = False,\n    sass_expansion_degree: int = 1,\n) -&gt; Callable:\n    \"\"\"Return transform for semi-automatic summary statistics.\n\n    Runs an single round of rejection abc with fixed budget and accepts\n    num_particles simulations to run the regression for sass.\n\n    Sets self.x_o once the x_shape can be derived from simulations.\n    \"\"\"\n    (\n        pilot_particles,\n        _,\n        _,\n        pilot_xs,\n    ) = self._set_xo_and_sample_initial_population(\n        x_o, num_particles, num_pilot_simulations, num_iid_samples\n    )\n    assert self.x_o is not None, \"x_o not set yet.\"\n\n    # Adjust with LRA.\n    if lra:\n        pilot_particles = self.run_lra(pilot_particles, pilot_xs, self.x_o)\n    sass_transform = self.get_sass_transform(\n        pilot_particles,\n        pilot_xs,\n        expansion_degree=sass_expansion_degree,\n        sample_weight=None,\n    )\n    return sass_transform\n</code></pre>"},{"location":"reference/inference/#sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights","title":"<code>sample_from_population_with_weights(particles, weights, num_samples=1)</code>  <code>staticmethod</code>","text":"<p>Return samples from particles sampled with weights.</p> Source code in <code>sbi/inference/abc/smcabc.py</code> <pre><code>@staticmethod\ndef sample_from_population_with_weights(\n    particles: Tensor, weights: Tensor, num_samples: int = 1\n) -&gt; Tensor:\n    \"\"\"Return samples from particles sampled with weights.\"\"\"\n\n    # define multinomial with weights as probs\n    multi = Multinomial(probs=weights)\n    # sample num samples, with replacement\n    samples = multi.sample(sample_shape=torch.Size((num_samples,)))\n    # get indices of success trials\n    indices = torch.where(samples)[1]\n    # return those indices from trace\n    return particles[indices]\n</code></pre>"},{"location":"reference/inference/#helpers","title":"Helpers","text":""},{"location":"reference/inference/#sbi.inference.base.infer","title":"<code>infer(simulator, prior, method, num_simulations, num_workers=1, init_kwargs=None, train_kwargs=None, build_posterior_kwargs=None)</code>","text":"<p>Runs simulation-based inference and returns the posterior.</p> <p>This function provides a simple interface to run sbi. Inference is run for a single round and hence the returned posterior \\(p(\\theta|x)\\) can be sampled and evaluated for any \\(x\\) (i.e. it is amortized).</p> <p>The scope of this function is limited to the most essential features of sbi. For more flexibility (e.g. multi-round inference, different density estimators) please use the flexible interface described here: https://sbi-dev.github.io/sbi/tutorial/02_flexible_interface/</p> <p>Parameters:</p> Name Type Description Default <code>simulator</code> <code>Callable</code> <p>A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, <code>x</code>, \\(\\mathrm{sim}(\\theta)\\to x\\). Any regular Python callable (i.e. function or class with <code>__call__</code> method) can be used.</p> required <code>prior</code> <code>Distribution</code> <p>A probability distribution that expresses prior knowledge about the parameters, e.g. which ranges are meaningful for them. Any object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch distribution) can be used.</p> required <code>method</code> <code>str</code> <p>What inference method to use. Either of SNPE, SNLE or SNRE.</p> required <code>num_simulations</code> <code>int</code> <p>Number of simulation calls. More simulations means a longer runtime, but a better posterior estimate.</p> required <code>num_workers</code> <code>int</code> <p>Number of parallel workers to use for simulations.</p> <code>1</code> <code>init_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments for the inference method which are passed to <code>__init__</code>.</p> <code>None</code> <code>train_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments for training the density estimator.</p> <code>None</code> <code>build_posterior_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments for <code>build_posterior</code>.</p> <code>None</code> <p>Returns: Posterior over parameters conditional on observations (amortized).</p> Source code in <code>sbi/inference/base.py</code> <pre><code>def infer(\n    simulator: Callable,\n    prior: Distribution,\n    method: str,\n    num_simulations: int,\n    num_workers: int = 1,\n    init_kwargs: Optional[Dict] = None,\n    train_kwargs: Optional[Dict] = None,\n    build_posterior_kwargs: Optional[Dict] = None,\n) -&gt; NeuralPosterior:\n    r\"\"\"Runs simulation-based inference and returns the posterior.\n\n    This function provides a simple interface to run sbi. Inference is run for a single\n    round and hence the returned posterior $p(\\theta|x)$ can be sampled and evaluated\n    for any $x$ (i.e. it is amortized).\n\n    The scope of this function is limited to the most essential features of sbi. For\n    more flexibility (e.g. multi-round inference, different density estimators) please\n    use the flexible interface described here:\n    https://sbi-dev.github.io/sbi/tutorial/02_flexible_interface/\n\n    Args:\n        simulator: A function that takes parameters $\\theta$ and maps them to\n            simulations, or observations, `x`, $\\mathrm{sim}(\\theta)\\to x$. Any\n            regular Python callable (i.e. function or class with `__call__` method)\n            can be used.\n        prior: A probability distribution that expresses prior knowledge about the\n            parameters, e.g. which ranges are meaningful for them. Any\n            object with `.log_prob()`and `.sample()` (for example, a PyTorch\n            distribution) can be used.\n        method: What inference method to use. Either of SNPE, SNLE or SNRE.\n        num_simulations: Number of simulation calls. More simulations means a longer\n            runtime, but a better posterior estimate.\n        num_workers: Number of parallel workers to use for simulations.\n        init_kwargs: Additional keyword arguments for the inference method\n            which are passed to `__init__`.\n        train_kwargs: Additional keyword arguments for training the density estimator.\n        build_posterior_kwargs: Additional keyword arguments for `build_posterior`.\n\n    Returns: Posterior over parameters conditional on observations (amortized).\n    \"\"\"\n\n    try:\n        # Moved here to avoid circular imports at initialization.\n        import sbi.inference  # noqa: R0401\n\n        method_fun: Callable = getattr(sbi.inference, method.upper())\n    except AttributeError as err:\n        raise NameError(\n            \"Method not available. `method` must be one of 'SNPE', 'SNLE', 'SNRE'.\"\n        ) from err\n\n    if (\n        init_kwargs is not None\n        or build_posterior_kwargs is not None\n        or train_kwargs is not None\n    ):\n        warn(\n            \"We discourage the use the simple interface in more complicated settings. \"\n            \"Have a look into the flexible interface, e.g. in our tutorial \"\n            \"(https://sbi-dev.github.io/sbi/tutorial/02_flexible_interface/).\",\n            stacklevel=2,\n        )\n    # Set variables to empty dicts to be able to pass them\n    # to the functions later on (if necessary).\n    if build_posterior_kwargs is None:\n        build_posterior_kwargs = {}\n    if train_kwargs is None:\n        train_kwargs = {}\n    if init_kwargs is None:\n        init_kwargs = {}\n\n    prior, _, prior_returns_numpy = process_prior(prior)\n    simulator = process_simulator(simulator, prior, prior_returns_numpy)\n    check_sbi_inputs(simulator, prior)\n\n    inference = method_fun(prior=prior, **init_kwargs)\n    theta, x = simulate_for_sbi(\n        simulator=simulator,\n        proposal=prior,\n        num_simulations=num_simulations,\n        num_workers=num_workers,\n    )\n    _ = inference.append_simulations(theta, x).train(**train_kwargs)\n    posterior = inference.build_posterior(**build_posterior_kwargs)\n\n    return posterior\n</code></pre>"},{"location":"reference/inference/#sbi.inference.base.simulate_for_sbi","title":"<code>simulate_for_sbi(simulator, proposal, num_simulations, num_workers=1, simulation_batch_size=1, seed=None, show_progress_bar=True)</code>","text":"<p>Returns (\\(\\theta, x\\)) pairs obtained from sampling the proposal and simulating.</p> <p>This function performs two steps:</p> <ul> <li>Sample parameters \\(\\theta\\) from the <code>proposal</code>.</li> <li>Simulate these parameters to obtain \\(x\\).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>simulator</code> <code>Callable</code> <p>A function that takes parameters \\(\\theta\\) and maps them to simulations, or observations, <code>x</code>, \\(\\text{sim}(\\theta)\\to x\\). Any regular Python callable (i.e. function or class with <code>__call__</code> method) can be used. Note that the simulator should be able to handle numpy arrays for efficient parallelization. You can use <code>process_simulator</code> to ensure this.</p> required <code>proposal</code> <code>Any</code> <p>Probability distribution that the parameters \\(\\theta\\) are sampled from.</p> required <code>num_simulations</code> <code>int</code> <p>Number of simulations that are run.</p> required <code>num_workers</code> <code>int</code> <p>Number of parallel workers to use for simulations.</p> <code>1</code> <code>simulation_batch_size</code> <code>Union[int, None]</code> <p>Number of parameter sets of shape (simulation_batch_size, parameter_dimension) that the simulator receives per call. If None, we set simulation_batch_size=num_simulations and simulate all parameter sets with one call. Otherwise, we construct batches of parameter sets and distribute them among num_workers.</p> <code>1</code> <code>seed</code> <code>Optional[int]</code> <p>Seed for reproducibility.</p> <code>None</code> <code>show_progress_bar</code> <code>bool</code> <p>Whether to show a progress bar for simulating. This will not affect whether there will be a progressbar while drawing samples from the proposal.</p> <code>True</code> <p>Returns: Sampled parameters \\(\\theta\\) and simulation-outputs \\(x\\).</p> Source code in <code>sbi/inference/base.py</code> <pre><code>def simulate_for_sbi(\n    simulator: Callable,\n    proposal: Any,\n    num_simulations: int,\n    num_workers: int = 1,\n    simulation_batch_size: Union[int, None] = 1,\n    seed: Optional[int] = None,\n    show_progress_bar: bool = True,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Returns ($\\theta, x$) pairs obtained from sampling the proposal and simulating.\n\n    This function performs two steps:\n\n    - Sample parameters $\\theta$ from the `proposal`.\n    - Simulate these parameters to obtain $x$.\n\n    Args:\n        simulator: A function that takes parameters $\\theta$ and maps them to\n            simulations, or observations, `x`, $\\text{sim}(\\theta)\\to x$. Any\n            regular Python callable (i.e. function or class with `__call__` method)\n            can be used. Note that the simulator should be able to handle numpy\n            arrays for efficient parallelization. You can use\n            `process_simulator` to ensure this.\n        proposal: Probability distribution that the parameters $\\theta$ are sampled\n            from.\n        num_simulations: Number of simulations that are run.\n        num_workers: Number of parallel workers to use for simulations.\n        simulation_batch_size: Number of parameter sets of shape\n            (simulation_batch_size, parameter_dimension) that the simulator\n            receives per call. If None, we set\n            simulation_batch_size=num_simulations and simulate all parameter\n            sets with one call. Otherwise, we construct batches of parameter\n            sets and distribute them among num_workers.\n        seed: Seed for reproducibility.\n        show_progress_bar: Whether to show a progress bar for simulating. This will not\n            affect whether there will be a progressbar while drawing samples from the\n            proposal.\n\n    Returns: Sampled parameters $\\theta$ and simulation-outputs $x$.\n    \"\"\"\n\n    if num_simulations == 0:\n        theta = torch.tensor([], dtype=float32)\n        x = torch.tensor([], dtype=float32)\n\n    else:\n        # Cast theta to numpy for better joblib performance (seee #1175)\n        seed_all_backends(seed)\n        theta = proposal.sample((num_simulations,)).numpy()\n\n        # Parse the simulation_batch_size logic\n        if simulation_batch_size is None:\n            simulation_batch_size = num_simulations\n        else:\n            simulation_batch_size = min(simulation_batch_size, num_simulations)\n\n        # The batch size will be an approximation, since np.array_split does\n        # not take as argument the size of the batch but their total.\n        num_batches = num_simulations // simulation_batch_size\n\n        batches = np.array_split(theta, num_batches, axis=0)\n\n        if num_workers != 1:\n            batch_seeds = np.random.randint(low=0, high=1_000_000, size=(len(batches),))\n\n            # define seeded simulator.\n            def simulator_seeded(theta: ndarray, seed: int) -&gt; Tensor:\n                seed_all_backends(seed)\n                return simulator(theta)\n\n            simulation_outputs: list[Tensor] = [  # pyright: ignore\n                xx\n                for xx in tqdm(\n                    Parallel(return_as=\"generator\", n_jobs=num_workers)(\n                        delayed(simulator_seeded)(batch, seed)\n                        for batch, seed in zip(batches, batch_seeds)\n                    ),\n                    total=num_simulations,\n                    disable=not show_progress_bar,\n                )\n            ]\n\n        else:\n            simulation_outputs: list[Tensor] = []\n\n            for batch in tqdm(batches, disable=not show_progress_bar):\n                simulation_outputs.append(simulator(batch))\n\n        # Correctly format the output\n        x = torch.cat(simulation_outputs, dim=0)\n        theta = torch.as_tensor(theta, dtype=float32)\n\n    return theta, x\n</code></pre>"},{"location":"reference/inference/#sbi.utils.user_input_checks.process_prior","title":"<code>process_prior(prior, custom_prior_wrapper_kwargs=None)</code>","text":"<p>Return PyTorch distribution-like prior from user-provided prior.</p> <p>NOTE: If the prior argument is a sequence of PyTorch distributions, they will be interpreted as independent prior dimensions wrapped in a <code>MultipleIndependent</code> pytorch Distribution. In case the elements are not PyTorch distributions, make sure to use process_prior on each element in the list beforehand. See FAQ 7 for details.</p> <p>NOTE: returns a tuple (processed_prior, num_params, whether_prior_returns_numpy). The last two entries in the tuple can be passed on to <code>process_simulator</code> to prepare the simulator as well. For example, it will take care of casting parameters to numpy or adding a batch dimension to the simulator output, if needed.</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Union[Sequence[Distribution], Distribution, rv_frozen, multi_rv_frozen]</code> <p>Prior object with <code>.sample()</code> and <code>.log_prob()</code> as provided by the user, or a sequence of such objects.</p> required <code>custom_prior_wrapper_kwargs</code> <code>Optional[Dict]</code> <p>kwargs to be passed to the class that wraps a custom prior into a pytorch Distribution, e.g., for passing bounds for a prior with bounded support (lower_bound, upper_bound), or argument constraints. (arg_constraints), see pytorch.distributions.Distribution for more info.</p> <code>None</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If prior objects lacks <code>.sample()</code> or <code>.log_prob()</code>.</p> <p>Returns:</p> Name Type Description <code>prior</code> <code>Distribution</code> <p>Prior that emits samples and evaluates log prob as PyTorch Tensors.</p> <code>theta_numel</code> <code>int</code> <p>Number of parameters - elements in a single sample from the prior.</p> <code>prior_returns_numpy</code> <code>bool</code> <p>Whether the return type of the prior was a Numpy array.</p> Source code in <code>sbi/utils/user_input_checks.py</code> <pre><code>def process_prior(\n    prior: Union[Sequence[Distribution], Distribution, rv_frozen, multi_rv_frozen],\n    custom_prior_wrapper_kwargs: Optional[Dict] = None,\n) -&gt; Tuple[Distribution, int, bool]:\n    \"\"\"Return PyTorch distribution-like prior from user-provided prior.\n\n    NOTE: If the prior argument is a sequence of PyTorch distributions, they will be\n    interpreted as independent prior dimensions wrapped in a `MultipleIndependent`\n    pytorch Distribution. In case the elements are not PyTorch distributions, make sure\n    to use process_prior on each element in the list beforehand. See FAQ 7 for details.\n\n    NOTE: returns a tuple (processed_prior, num_params, whether_prior_returns_numpy).\n    The last two entries in the tuple can be passed on to `process_simulator` to prepare\n    the simulator as well. For example, it will take care of casting parameters to numpy\n    or adding a batch dimension to the simulator output, if needed.\n\n    Args:\n        prior: Prior object with `.sample()` and `.log_prob()` as provided by the user,\n            or a sequence of such objects.\n        custom_prior_wrapper_kwargs: kwargs to be passed to the class that wraps a\n            custom prior into a pytorch Distribution, e.g., for passing bounds for a\n            prior with bounded support (lower_bound, upper_bound), or argument\n            constraints.\n            (arg_constraints), see pytorch.distributions.Distribution for more info.\n\n    Raises:\n        AttributeError: If prior objects lacks `.sample()` or `.log_prob()`.\n\n    Returns:\n        prior: Prior that emits samples and evaluates log prob as PyTorch Tensors.\n        theta_numel: Number of parameters - elements in a single sample from the prior.\n        prior_returns_numpy: Whether the return type of the prior was a Numpy array.\n    \"\"\"\n\n    # If prior is a sequence, assume independent components and check as PyTorch prior.\n    if isinstance(prior, Sequence):\n        warnings.warn(\n            f\"Prior was provided as a sequence of {len(prior)} priors. They will be \"\n            \"interpreted as independent of each other and matched in order to the \"\n            \"components of the parameter.\",\n            stacklevel=2,\n        )\n        # process individual priors\n        prior = [process_prior(p, custom_prior_wrapper_kwargs)[0] for p in prior]\n        return process_pytorch_prior(MultipleIndependent(prior))\n\n    if isinstance(prior, Distribution):\n        return process_pytorch_prior(prior)\n\n    # If prior is given as `scipy.stats` object, wrap as PyTorch.\n    elif isinstance(prior, (rv_frozen, multi_rv_frozen)):\n        raise NotImplementedError(\n            \"Passing a prior as scipy.stats object is deprecated. \"\n            \"Please pass it as a PyTorch Distribution.\"\n        )\n\n    # Otherwise it is a custom prior - check for `.sample()` and `.log_prob()`.\n    else:\n        return process_custom_prior(prior, custom_prior_wrapper_kwargs)\n</code></pre>"},{"location":"reference/inference/#sbi.utils.user_input_checks.process_simulator","title":"<code>process_simulator(user_simulator, prior, is_numpy_simulator)</code>","text":"<p>Returns a simulator that meets the requirements for usage in sbi.</p> <p>Parameters:</p> Name Type Description Default <code>user_simulator</code> <code>Callable</code> <p>simulator provided by the user, possibly written in numpy.</p> required <code>prior</code> <code>Distribution</code> <p>prior as pytorch distribution or processed with <code>process_prior</code>.</p> required <code>is_numpy_simulator</code> <code>bool</code> <p>whether the simulator needs theta in numpy types, returned from <code>process_prior</code>.</p> required <p>Returns:</p> Name Type Description <code>simulator</code> <code>Callable</code> <p>processed simulator that returns <code>torch.Tensor</code> can handle batches of parameters.</p> Source code in <code>sbi/utils/user_input_checks.py</code> <pre><code>def process_simulator(\n    user_simulator: Callable,\n    prior: Distribution,\n    is_numpy_simulator: bool,\n) -&gt; Callable:\n    \"\"\"Returns a simulator that meets the requirements for usage in sbi.\n\n    Args:\n        user_simulator: simulator provided by the user, possibly written in numpy.\n        prior: prior as pytorch distribution or processed with `process_prior`.\n        is_numpy_simulator: whether the simulator needs theta in numpy types, returned\n            from `process_prior`.\n\n    Returns:\n        simulator: processed simulator that returns `torch.Tensor` can handle batches\n            of parameters.\n    \"\"\"\n\n    assert isinstance(user_simulator, Callable), \"Simulator must be a function.\"\n\n    joblib_simulator = wrap_as_joblib_efficient_simulator(\n        user_simulator, prior, is_numpy_simulator\n    )\n\n    batch_simulator = ensure_batched_simulator(joblib_simulator, prior)\n\n    return batch_simulator\n</code></pre>"},{"location":"reference/models/","title":"Neural networks","text":""},{"location":"reference/models/#sbi.neural_nets.factory.posterior_nn","title":"<code>posterior_nn(model, z_score_theta='independent', z_score_x='independent', hidden_features=50, num_transforms=5, num_bins=10, embedding_net=nn.Identity(), num_components=10, **kwargs)</code>","text":"<p>Returns a function that builds a density estimator for learning the posterior.</p> <p>This function will usually be used for SNPE. The returned function is to be passed to the inference class when using the flexible interface.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The type of density estimator that will be created. One of [<code>mdn</code>, <code>made</code>, <code>maf</code>, <code>maf_rqs</code>, <code>nsf</code>].</p> required <code>z_score_theta</code> <code>Optional[str]</code> <p>Whether to z-score parameters \\(\\theta\\) before passing them into the network, can take one of the following: - <code>none</code>, or None: do not z-score. - <code>independent</code>: z-score each dimension independently. - <code>structured</code>: treat dimensions as related, therefore compute mean and std over the entire batch, instead of per-dimension. Should be used when each sample is, for example, a time series or an image.</p> <code>'independent'</code> <code>z_score_x</code> <code>Optional[str]</code> <p>Whether to z-score simulation outputs \\(x\\) before passing them into the network, same options as z_score_theta.</p> <code>'independent'</code> <code>hidden_features</code> <code>int</code> <p>Number of hidden features.</p> <code>50</code> <code>num_transforms</code> <code>int</code> <p>Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a <code>maf</code> or a <code>nsf</code>). Ignored if density estimator is a <code>mdn</code> or <code>made</code>.</p> <code>5</code> <code>num_bins</code> <code>int</code> <p>Number of bins used for the splines in <code>nsf</code>. Ignored if density estimator not <code>nsf</code>.</p> <code>10</code> <code>embedding_net</code> <code>Module</code> <p>Optional embedding network for simulation outputs \\(x\\). This embedding net allows to learn features from potentially high-dimensional simulation outputs.</p> <code>Identity()</code> <code>num_components</code> <code>int</code> <p>Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn.</p> <code>10</code> <code>kwargs</code> <code>Any</code> <p>additional custom arguments passed to downstream build functions.</p> <code>{}</code> Source code in <code>sbi/neural_nets/factory.py</code> <pre><code>def posterior_nn(\n    model: str,\n    z_score_theta: Optional[str] = \"independent\",\n    z_score_x: Optional[str] = \"independent\",\n    hidden_features: int = 50,\n    num_transforms: int = 5,\n    num_bins: int = 10,\n    embedding_net: nn.Module = nn.Identity(),\n    num_components: int = 10,\n    **kwargs: Any,\n) -&gt; Callable:\n    r\"\"\"\n    Returns a function that builds a density estimator for learning the posterior.\n\n    This function will usually be used for SNPE. The returned function is to be passed\n    to the inference class when using the flexible interface.\n\n    Args:\n        model: The type of density estimator that will be created. One of [`mdn`,\n            `made`, `maf`, `maf_rqs`, `nsf`].\n        z_score_theta: Whether to z-score parameters $\\theta$ before passing them into\n            the network, can take one of the following:\n            - `none`, or None: do not z-score.\n            - `independent`: z-score each dimension independently.\n            - `structured`: treat dimensions as related, therefore compute mean and std\n            over the entire batch, instead of per-dimension. Should be used when each\n            sample is, for example, a time series or an image.\n        z_score_x: Whether to z-score simulation outputs $x$ before passing them into\n            the network, same options as z_score_theta.\n        hidden_features: Number of hidden features.\n        num_transforms: Number of transforms when a flow is used. Only relevant if\n            density estimator is a normalizing flow (i.e. currently either a `maf` or a\n            `nsf`). Ignored if density estimator is a `mdn` or `made`.\n        num_bins: Number of bins used for the splines in `nsf`. Ignored if density\n            estimator not `nsf`.\n        embedding_net: Optional embedding network for simulation outputs $x$. This\n            embedding net allows to learn features from potentially high-dimensional\n            simulation outputs.\n        num_components: Number of mixture components for a mixture of Gaussians.\n            Ignored if density estimator is not an mdn.\n        kwargs: additional custom arguments passed to downstream build functions.\n    \"\"\"\n\n    kwargs = dict(\n        zip(\n            (\n                \"z_score_x\",\n                \"z_score_y\",\n                \"hidden_features\",\n                \"num_transforms\",\n                \"num_bins\",\n                \"embedding_net\",\n                \"num_components\",\n            ),\n            (\n                z_score_theta,\n                z_score_x,\n                hidden_features,\n                num_transforms,\n                num_bins,\n                check_net_device(embedding_net, \"cpu\", embedding_net_warn_msg),\n                num_components,\n            ),\n        ),\n        **kwargs,\n    )\n\n    def build_fn_snpe_a(batch_theta, batch_x, num_components):\n        \"\"\"Build function for SNPE-A\n\n        Extract the number of components from the kwargs, such that they are exposed as\n        a kwargs, offering the possibility to later override this kwarg with\n        `functools.partial`. This is necessary in order to make sure that the MDN in\n        SNPE-A only has one component when running the Algorithm 1 part.\n        \"\"\"\n        return build_mdn(\n            batch_x=batch_theta,\n            batch_y=batch_x,\n            num_components=num_components,\n            **kwargs,\n        )\n\n    def build_fn(batch_theta, batch_x):\n        if model not in model_builders:\n            raise NotImplementedError(f\"Model {model} in not implemented\")\n\n        # The naming might be a bit confusing.\n        # batch_x are the latent variables, batch_y the conditioned variables.\n        # batch_theta are the parameters and batch_x the observable variables.\n        return model_builders[model](batch_x=batch_theta, batch_y=batch_x, **kwargs)\n\n    if model == \"mdn_snpe_a\":\n        if num_components != 10:\n            raise ValueError(\n                \"You set `num_components`. For SNPE-A, this has to be done at \"\n                \"instantiation of the inference object, i.e. \"\n                \"`inference = SNPE_A(..., num_components=20)`\"\n            )\n        kwargs.pop(\"num_components\")\n\n    return build_fn_snpe_a if model == \"mdn_snpe_a\" else build_fn\n</code></pre>"},{"location":"reference/models/#sbi.neural_nets.factory.likelihood_nn","title":"<code>likelihood_nn(model, z_score_theta='independent', z_score_x='independent', hidden_features=50, num_transforms=5, num_bins=10, embedding_net=nn.Identity(), num_components=10, **kwargs)</code>","text":"<p>Returns a function that builds a density estimator for learning the likelihood.</p> <p>This function will usually be used for SNLE. The returned function is to be passed to the inference class when using the flexible interface.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The type of density estimator that will be created. One of [<code>mdn</code>, <code>made</code>, <code>maf</code>, <code>maf_rqs</code>, <code>nsf</code>].</p> required <code>z_score_theta</code> <code>Optional[str]</code> <p>Whether to z-score parameters \\(\\theta\\) before passing them into the network, can take one of the following: - <code>none</code>, or None: do not z-score. - <code>independent</code>: z-score each dimension independently. - <code>structured</code>: treat dimensions as related, therefore compute mean and std over the entire batch, instead of per-dimension. Should be used when each sample is, for example, a time series or an image.</p> <code>'independent'</code> <code>z_score_x</code> <code>Optional[str]</code> <p>Whether to z-score simulation outputs \\(x\\) before passing them into the network, same options as z_score_theta.</p> <code>'independent'</code> <code>hidden_features</code> <code>int</code> <p>Number of hidden features.</p> <code>50</code> <code>num_transforms</code> <code>int</code> <p>Number of transforms when a flow is used. Only relevant if density estimator is a normalizing flow (i.e. currently either a <code>maf</code> or a <code>nsf</code>). Ignored if density estimator is a <code>mdn</code> or <code>made</code>.</p> <code>5</code> <code>num_bins</code> <code>int</code> <p>Number of bins used for the splines in <code>nsf</code>. Ignored if density estimator not <code>nsf</code>.</p> <code>10</code> <code>embedding_net</code> <code>Module</code> <p>Optional embedding network for parameters \\(\\theta\\).</p> <code>Identity()</code> <code>num_components</code> <code>int</code> <p>Number of mixture components for a mixture of Gaussians. Ignored if density estimator is not an mdn.</p> <code>10</code> <code>kwargs</code> <code>Any</code> <p>additional custom arguments passed to downstream build functions.</p> <code>{}</code> Source code in <code>sbi/neural_nets/factory.py</code> <pre><code>def likelihood_nn(\n    model: str,\n    z_score_theta: Optional[str] = \"independent\",\n    z_score_x: Optional[str] = \"independent\",\n    hidden_features: int = 50,\n    num_transforms: int = 5,\n    num_bins: int = 10,\n    embedding_net: nn.Module = nn.Identity(),\n    num_components: int = 10,\n    **kwargs: Any,\n) -&gt; Callable:\n    r\"\"\"\n    Returns a function that builds a density estimator for learning the likelihood.\n\n    This function will usually be used for SNLE. The returned function is to be passed\n    to the inference class when using the flexible interface.\n\n    Args:\n        model: The type of density estimator that will be created. One of [`mdn`,\n            `made`, `maf`, `maf_rqs`, `nsf`].\n        z_score_theta: Whether to z-score parameters $\\theta$ before passing them into\n            the network, can take one of the following:\n            - `none`, or None: do not z-score.\n            - `independent`: z-score each dimension independently.\n            - `structured`: treat dimensions as related, therefore compute mean and std\n            over the entire batch, instead of per-dimension. Should be used when each\n            sample is, for example, a time series or an image.\n        z_score_x: Whether to z-score simulation outputs $x$ before passing them into\n            the network, same options as z_score_theta.\n        hidden_features: Number of hidden features.\n        num_transforms: Number of transforms when a flow is used. Only relevant if\n            density estimator is a normalizing flow (i.e. currently either a `maf` or a\n            `nsf`). Ignored if density estimator is a `mdn` or `made`.\n        num_bins: Number of bins used for the splines in `nsf`. Ignored if density\n            estimator not `nsf`.\n        embedding_net: Optional embedding network for parameters $\\theta$.\n        num_components: Number of mixture components for a mixture of Gaussians.\n            Ignored if density estimator is not an mdn.\n        kwargs: additional custom arguments passed to downstream build functions.\n    \"\"\"\n\n    kwargs = dict(\n        zip(\n            (\n                \"z_score_x\",\n                \"z_score_y\",\n                \"hidden_features\",\n                \"num_transforms\",\n                \"num_bins\",\n                \"embedding_net\",\n                \"num_components\",\n            ),\n            (\n                z_score_x,\n                z_score_theta,\n                hidden_features,\n                num_transforms,\n                num_bins,\n                check_net_device(embedding_net, \"cpu\", embedding_net_warn_msg),\n                num_components,\n            ),\n        ),\n        **kwargs,\n    )\n\n    def build_fn(batch_theta, batch_x):\n        if model not in model_builders:\n            raise NotImplementedError(f\"Model {model} in not implemented\")\n\n        return model_builders[model](batch_x=batch_x, batch_y=batch_theta, **kwargs)\n\n    return build_fn\n</code></pre>"},{"location":"reference/models/#sbi.neural_nets.factory.classifier_nn","title":"<code>classifier_nn(model, z_score_theta='independent', z_score_x='independent', hidden_features=50, embedding_net_theta=nn.Identity(), embedding_net_x=nn.Identity(), **kwargs)</code>","text":"<p>Returns a function that builds a classifier for learning density ratios.</p> <p>This function will usually be used for SNRE. The returned function is to be passed to the inference class when using the flexible interface.</p> <p>Note that in the view of the SNRE classifier we build below, x=theta and y=x.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The type of classifier that will be created. One of [<code>linear</code>, <code>mlp</code>, <code>resnet</code>].</p> required <code>z_score_theta</code> <code>Optional[str]</code> <p>Whether to z-score parameters \\(\\theta\\) before passing them into the network, can take one of the following: - <code>none</code>, or None: do not z-score. - <code>independent</code>: z-score each dimension independently. - <code>structured</code>: treat dimensions as related, therefore compute mean and std over the entire batch, instead of per-dimension. Should be used when each sample is, for example, a time series or an image.</p> <code>'independent'</code> <code>z_score_x</code> <code>Optional[str]</code> <p>Whether to z-score simulation outputs \\(x\\) before passing them into the network, same options as z_score_theta.</p> <code>'independent'</code> <code>hidden_features</code> <code>int</code> <p>Number of hidden features.</p> <code>50</code> <code>embedding_net_theta</code> <code>Module</code> <p>Optional embedding network for parameters \\(\\theta\\).</p> <code>Identity()</code> <code>embedding_net_x</code> <code>Module</code> <p>Optional embedding network for simulation outputs \\(x\\). This embedding net allows to learn features from potentially high-dimensional simulation outputs.</p> <code>Identity()</code> <code>kwargs</code> <code>Any</code> <p>additional custom arguments passed to downstream build functions.</p> <code>{}</code> Source code in <code>sbi/neural_nets/factory.py</code> <pre><code>def classifier_nn(\n    model: str,\n    z_score_theta: Optional[str] = \"independent\",\n    z_score_x: Optional[str] = \"independent\",\n    hidden_features: int = 50,\n    embedding_net_theta: nn.Module = nn.Identity(),\n    embedding_net_x: nn.Module = nn.Identity(),\n    **kwargs: Any,\n) -&gt; Callable:\n    r\"\"\"\n    Returns a function that builds a classifier for learning density ratios.\n\n    This function will usually be used for SNRE. The returned function is to be passed\n    to the inference class when using the flexible interface.\n\n    Note that in the view of the SNRE classifier we build below, x=theta and y=x.\n\n    Args:\n        model: The type of classifier that will be created. One of [`linear`, `mlp`,\n            `resnet`].\n        z_score_theta: Whether to z-score parameters $\\theta$ before passing them into\n            the network, can take one of the following:\n            - `none`, or None: do not z-score.\n            - `independent`: z-score each dimension independently.\n            - `structured`: treat dimensions as related, therefore compute mean and std\n            over the entire batch, instead of per-dimension. Should be used when each\n            sample is, for example, a time series or an image.\n        z_score_x: Whether to z-score simulation outputs $x$ before passing them into\n            the network, same options as z_score_theta.\n        hidden_features: Number of hidden features.\n        embedding_net_theta:  Optional embedding network for parameters $\\theta$.\n        embedding_net_x:  Optional embedding network for simulation outputs $x$. This\n            embedding net allows to learn features from potentially high-dimensional\n            simulation outputs.\n        kwargs: additional custom arguments passed to downstream build functions.\n    \"\"\"\n\n    kwargs = dict(\n        zip(\n            (\n                \"z_score_x\",\n                \"z_score_y\",\n                \"hidden_features\",\n                \"embedding_net_x\",\n                \"embedding_net_y\",\n            ),\n            (\n                z_score_theta,\n                z_score_x,\n                hidden_features,\n                check_net_device(embedding_net_theta, \"cpu\", embedding_net_warn_msg),\n                check_net_device(embedding_net_x, \"cpu\", embedding_net_warn_msg),\n            ),\n        ),\n        **kwargs,\n    )\n\n    def build_fn(batch_theta, batch_x):\n        if model == \"linear\":\n            return build_linear_classifier(\n                batch_x=batch_theta, batch_y=batch_x, **kwargs\n            )\n        if model == \"mlp\":\n            return build_mlp_classifier(batch_x=batch_theta, batch_y=batch_x, **kwargs)\n        if model == \"resnet\":\n            return build_resnet_classifier(\n                batch_x=batch_theta, batch_y=batch_x, **kwargs\n            )\n        else:\n            raise NotImplementedError\n\n    return build_fn\n</code></pre>"},{"location":"reference/models/#sbi.neural_nets.density_estimators.ConditionalDensityEstimator","title":"<code>ConditionalDensityEstimator</code>","text":"<p>               Bases: <code>ConditionalEstimator</code></p> <p>Base class for density estimators.</p> <p>The density estimator class is a wrapper around neural networks that allows to evaluate the <code>log_prob</code>, <code>sample</code>, and provide the <code>loss</code> of \\(\\theta,x\\) pairs. Here \\(\\theta\\) would be the <code>input</code> and \\(x\\) would be the <code>condition</code>.</p> Note <p>We assume that the input to the density estimator is a tensor of shape (batch_size, input_size), where input_size is the dimensionality of the input. The condition is a tensor of shape (batch_size, *condition_shape), where condition_shape is the shape of the condition tensor.</p> Source code in <code>sbi/neural_nets/density_estimators/base.py</code> <pre><code>class ConditionalDensityEstimator(ConditionalEstimator):\n    r\"\"\"Base class for density estimators.\n\n    The density estimator class is a wrapper around neural networks that\n    allows to evaluate the `log_prob`, `sample`, and provide the `loss` of $\\theta,x$\n    pairs. Here $\\theta$ would be the `input` and $x$ would be the `condition`.\n\n    Note:\n        We assume that the input to the density estimator is a tensor of shape\n        (batch_size, input_size), where input_size is the dimensionality of the input.\n        The condition is a tensor of shape (batch_size, *condition_shape), where\n        condition_shape is the shape of the condition tensor.\n\n    \"\"\"\n\n    def __init__(\n        self, net: nn.Module, input_shape: torch.Size, condition_shape: torch.Size\n    ) -&gt; None:\n        r\"\"\"Base class for density estimators.\n\n        Args:\n            net: Neural network or any parameterized model that is used to estimate the\n                probability density of the input given a condition.\n            input_shape: Event shape of the input at which the density is being\n                evaluated (and which is also the event_shape of samples).\n            condition_shape: Shape of the condition.\n        \"\"\"\n        super().__init__(input_shape, condition_shape)\n        self.net = net\n\n    @property\n    def embedding_net(self) -&gt; Optional[nn.Module]:\n        r\"\"\"Return the embedding network if it exists.\"\"\"\n        return None\n\n    @abstractmethod\n    def log_prob(self, input: Tensor, condition: Tensor, **kwargs) -&gt; Tensor:\n        r\"\"\"Return the log probabilities of the inputs given a condition or multiple\n        i.e. batched conditions.\n\n        Args:\n            input: Inputs to evaluate the log probability on of shape\n                    `(sample_dim_input, batch_dim_input, *event_shape_input)`.\n            condition: Conditions of shape\n                `(batch_dim_condition, *event_shape_condition)`.\n\n        Raises:\n            RuntimeError: If batch_dim_input and batch_dim_condition do not match.\n\n        Returns:\n            Sample-wise log probabilities.\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def loss(self, input: Tensor, condition: Tensor, **kwargs) -&gt; Tensor:\n        r\"\"\"Return the loss for training the density estimator.\n\n        Args:\n            input: Inputs to evaluate the loss on of shape\n                `(batch_dim, *input_event_shape)`.\n            condition: Conditions of shape `(batch_dim, *event_shape_condition)`.\n\n        Returns:\n            Loss of shape (batch_dim,)\n        \"\"\"\n\n        pass\n\n    @abstractmethod\n    def sample(self, sample_shape: torch.Size, condition: Tensor, **kwargs) -&gt; Tensor:\n        r\"\"\"Return samples from the density estimator.\n\n        Args:\n            sample_shape: Shape of the samples to return.\n            condition: Conditions of shape `(batch_dim, *event_shape_condition)`.\n\n        Returns:\n            Samples of shape (*sample_shape, batch_dim, *event_shape_input).\n        \"\"\"\n\n        pass\n\n    def sample_and_log_prob(\n        self, sample_shape: torch.Size, condition: Tensor, **kwargs\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"Return samples and their density from the density estimator.\n\n        Args:\n            sample_shape: Shape of the samples to return.\n            condition: Conditions of shape `(batch_dim, *event_shape_condition)`.\n\n        Returns:\n            Samples and associated log probabilities.\n\n        Note:\n            For some density estimators, computing log_probs for samples is\n            more efficient than computing them separately. This method should\n            then be overwritten to provide a more efficient implementation.\n        \"\"\"\n\n        samples = self.sample(sample_shape, condition, **kwargs)\n        log_probs = self.log_prob(samples, condition, **kwargs)\n        return samples, log_probs\n</code></pre>"},{"location":"reference/models/#sbi.neural_nets.density_estimators.ConditionalDensityEstimator.embedding_net","title":"<code>embedding_net: Optional[nn.Module]</code>  <code>property</code>","text":"<p>Return the embedding network if it exists.</p>"},{"location":"reference/models/#sbi.neural_nets.density_estimators.ConditionalDensityEstimator.__init__","title":"<code>__init__(net, input_shape, condition_shape)</code>","text":"<p>Base class for density estimators.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>Neural network or any parameterized model that is used to estimate the probability density of the input given a condition.</p> required <code>input_shape</code> <code>Size</code> <p>Event shape of the input at which the density is being evaluated (and which is also the event_shape of samples).</p> required <code>condition_shape</code> <code>Size</code> <p>Shape of the condition.</p> required Source code in <code>sbi/neural_nets/density_estimators/base.py</code> <pre><code>def __init__(\n    self, net: nn.Module, input_shape: torch.Size, condition_shape: torch.Size\n) -&gt; None:\n    r\"\"\"Base class for density estimators.\n\n    Args:\n        net: Neural network or any parameterized model that is used to estimate the\n            probability density of the input given a condition.\n        input_shape: Event shape of the input at which the density is being\n            evaluated (and which is also the event_shape of samples).\n        condition_shape: Shape of the condition.\n    \"\"\"\n    super().__init__(input_shape, condition_shape)\n    self.net = net\n</code></pre>"},{"location":"reference/models/#sbi.neural_nets.density_estimators.ConditionalDensityEstimator.log_prob","title":"<code>log_prob(input, condition, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return the log probabilities of the inputs given a condition or multiple i.e. batched conditions.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Inputs to evaluate the log probability on of shape     <code>(sample_dim_input, batch_dim_input, *event_shape_input)</code>.</p> required <code>condition</code> <code>Tensor</code> <p>Conditions of shape <code>(batch_dim_condition, *event_shape_condition)</code>.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If batch_dim_input and batch_dim_condition do not match.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Sample-wise log probabilities.</p> Source code in <code>sbi/neural_nets/density_estimators/base.py</code> <pre><code>@abstractmethod\ndef log_prob(self, input: Tensor, condition: Tensor, **kwargs) -&gt; Tensor:\n    r\"\"\"Return the log probabilities of the inputs given a condition or multiple\n    i.e. batched conditions.\n\n    Args:\n        input: Inputs to evaluate the log probability on of shape\n                `(sample_dim_input, batch_dim_input, *event_shape_input)`.\n        condition: Conditions of shape\n            `(batch_dim_condition, *event_shape_condition)`.\n\n    Raises:\n        RuntimeError: If batch_dim_input and batch_dim_condition do not match.\n\n    Returns:\n        Sample-wise log probabilities.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/models/#sbi.neural_nets.density_estimators.ConditionalDensityEstimator.loss","title":"<code>loss(input, condition, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return the loss for training the density estimator.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Inputs to evaluate the loss on of shape <code>(batch_dim, *input_event_shape)</code>.</p> required <code>condition</code> <code>Tensor</code> <p>Conditions of shape <code>(batch_dim, *event_shape_condition)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss of shape (batch_dim,)</p> Source code in <code>sbi/neural_nets/density_estimators/base.py</code> <pre><code>@abstractmethod\ndef loss(self, input: Tensor, condition: Tensor, **kwargs) -&gt; Tensor:\n    r\"\"\"Return the loss for training the density estimator.\n\n    Args:\n        input: Inputs to evaluate the loss on of shape\n            `(batch_dim, *input_event_shape)`.\n        condition: Conditions of shape `(batch_dim, *event_shape_condition)`.\n\n    Returns:\n        Loss of shape (batch_dim,)\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/models/#sbi.neural_nets.density_estimators.ConditionalDensityEstimator.sample","title":"<code>sample(sample_shape, condition, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return samples from the density estimator.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Size</code> <p>Shape of the samples to return.</p> required <code>condition</code> <code>Tensor</code> <p>Conditions of shape <code>(batch_dim, *event_shape_condition)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Samples of shape (*sample_shape, batch_dim, *event_shape_input).</p> Source code in <code>sbi/neural_nets/density_estimators/base.py</code> <pre><code>@abstractmethod\ndef sample(self, sample_shape: torch.Size, condition: Tensor, **kwargs) -&gt; Tensor:\n    r\"\"\"Return samples from the density estimator.\n\n    Args:\n        sample_shape: Shape of the samples to return.\n        condition: Conditions of shape `(batch_dim, *event_shape_condition)`.\n\n    Returns:\n        Samples of shape (*sample_shape, batch_dim, *event_shape_input).\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/models/#sbi.neural_nets.density_estimators.ConditionalDensityEstimator.sample_and_log_prob","title":"<code>sample_and_log_prob(sample_shape, condition, **kwargs)</code>","text":"<p>Return samples and their density from the density estimator.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Size</code> <p>Shape of the samples to return.</p> required <code>condition</code> <code>Tensor</code> <p>Conditions of shape <code>(batch_dim, *event_shape_condition)</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Samples and associated log probabilities.</p> Note <p>For some density estimators, computing log_probs for samples is more efficient than computing them separately. This method should then be overwritten to provide a more efficient implementation.</p> Source code in <code>sbi/neural_nets/density_estimators/base.py</code> <pre><code>def sample_and_log_prob(\n    self, sample_shape: torch.Size, condition: Tensor, **kwargs\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Return samples and their density from the density estimator.\n\n    Args:\n        sample_shape: Shape of the samples to return.\n        condition: Conditions of shape `(batch_dim, *event_shape_condition)`.\n\n    Returns:\n        Samples and associated log probabilities.\n\n    Note:\n        For some density estimators, computing log_probs for samples is\n        more efficient than computing them separately. This method should\n        then be overwritten to provide a more efficient implementation.\n    \"\"\"\n\n    samples = self.sample(sample_shape, condition, **kwargs)\n    log_probs = self.log_prob(samples, condition, **kwargs)\n    return samples, log_probs\n</code></pre>"},{"location":"reference/posteriors/","title":"Posteriors","text":""},{"location":"reference/posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior","title":"<code>DirectPosterior</code>","text":"<p>               Bases: <code>NeuralPosterior</code></p> <p>Posterior \\(p(\\theta|x_o)\\) with <code>log_prob()</code> and <code>sample()</code> methods, only applicable to SNPE. SNPE trains a neural network to directly approximate the posterior distribution. However, for bounded priors, the neural network can have leakage: it puts non-zero mass in regions where the prior is zero. The <code>DirectPosterior</code> class wraps the trained network to deal with these cases. Specifically, this class offers the following functionality: - correct the calculation of the log probability such that it compensates for the   leakage. - reject samples that lie outside of the prior bounds. This class can not be used in combination with SNLE or SNRE.</p> Source code in <code>sbi/inference/posteriors/direct_posterior.py</code> <pre><code>class DirectPosterior(NeuralPosterior):\n    r\"\"\"Posterior $p(\\theta|x_o)$ with `log_prob()` and `sample()` methods, only\n    applicable to SNPE.&lt;br/&gt;&lt;br/&gt;\n    SNPE trains a neural network to directly approximate the posterior distribution.\n    However, for bounded priors, the neural network can have leakage: it puts non-zero\n    mass in regions where the prior is zero. The `DirectPosterior` class wraps the\n    trained network to deal with these cases.&lt;br/&gt;&lt;br/&gt;\n    Specifically, this class offers the following functionality:&lt;br/&gt;\n    - correct the calculation of the log probability such that it compensates for the\n      leakage.&lt;br/&gt;\n    - reject samples that lie outside of the prior bounds.&lt;br/&gt;&lt;br/&gt;\n    This class can not be used in combination with SNLE or SNRE.\n    \"\"\"\n\n    def __init__(\n        self,\n        posterior_estimator: ConditionalDensityEstimator,\n        prior: Distribution,\n        max_sampling_batch_size: int = 10_000,\n        device: Optional[str] = None,\n        x_shape: Optional[torch.Size] = None,\n        enable_transform: bool = True,\n    ):\n        \"\"\"\n        Args:\n            prior: Prior distribution with `.log_prob()` and `.sample()`.\n            posterior_estimator: The trained neural posterior.\n            max_sampling_batch_size: Batchsize of samples being drawn from\n                the proposal at every iteration.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:0\". If None,\n                `potential_fn.device` is used.\n            x_shape: Deprecated, should not be passed.\n            enable_transform: Whether to transform parameters to unconstrained space\n                during MAP optimization. When False, an identity transform will be\n                returned for `theta_transform`.\n        \"\"\"\n        # Because `DirectPosterior` does not take the `potential_fn` as input, it\n        # builds it itself. The `potential_fn` and `theta_transform` are used only for\n        # obtaining the MAP.\n        check_prior(prior)\n        potential_fn, theta_transform = posterior_estimator_based_potential(\n            posterior_estimator,\n            prior,\n            x_o=None,\n            enable_transform=enable_transform,\n        )\n\n        super().__init__(\n            potential_fn=potential_fn,\n            theta_transform=theta_transform,\n            device=device,\n            x_shape=x_shape,\n        )\n\n        self.prior = prior\n        self.posterior_estimator = posterior_estimator\n\n        self.max_sampling_batch_size = max_sampling_batch_size\n        self._leakage_density_correction_factor = None\n\n        self._purpose = \"\"\"It samples the posterior network and rejects samples that\n            lie outside of the prior bounds.\"\"\"\n\n    def sample(\n        self,\n        sample_shape: Shape = torch.Size(),\n        x: Optional[Tensor] = None,\n        max_sampling_batch_size: int = 10_000,\n        sample_with: Optional[str] = None,\n        show_progress_bars: bool = True,\n    ) -&gt; Tensor:\n        r\"\"\"Return samples from posterior distribution $p(\\theta|x)$.\n\n        Args:\n            sample_shape: Desired shape of samples that are drawn from posterior. If\n                sample_shape is multidimensional we simply draw `sample_shape.numel()`\n                samples and then reshape into the desired shape.\n            sample_with: This argument only exists to keep backward-compatibility with\n                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.\n            show_progress_bars: Whether to show sampling progress monitor.\n        \"\"\"\n\n        num_samples = torch.Size(sample_shape).numel()\n        x = self._x_else_default_x(x)\n        x = reshape_to_batch_event(\n            x, event_shape=self.posterior_estimator.condition_shape\n        )\n\n        max_sampling_batch_size = (\n            self.max_sampling_batch_size\n            if max_sampling_batch_size is None\n            else max_sampling_batch_size\n        )\n\n        if sample_with is not None:\n            raise ValueError(\n                f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting \"\n                f\"`sample_with` is no longer supported. You have to rerun \"\n                f\"`.build_posterior(sample_with={sample_with}).`\"\n            )\n\n        samples = rejection.accept_reject_sample(\n            proposal=self.posterior_estimator,\n            accept_reject_fn=lambda theta: within_support(self.prior, theta),\n            num_samples=num_samples,\n            show_progress_bars=show_progress_bars,\n            max_sampling_batch_size=max_sampling_batch_size,\n            proposal_sampling_kwargs={\"condition\": x},\n            alternative_method=\"build_posterior(..., sample_with='mcmc')\",\n        )[0]\n\n        return samples[:, 0]  # Remove batch dimension.\n\n    def sample_batched(\n        self,\n        sample_shape: Shape,\n        x: Tensor,\n        max_sampling_batch_size: int = 10_000,\n        show_progress_bars: bool = True,\n    ) -&gt; Tensor:\n        r\"\"\"Given a batch of observations [x_1, ..., x_B] this function samples from\n        posteriors $p(\\theta|x_1)$, ... ,$p(\\theta|x_B)$, in a batched (i.e. vectorized)\n        manner.\n\n        Args:\n            sample_shape: Desired shape of samples that are drawn from the posterior\n                given every observation.\n            x: A batch of observations, of shape `(batch_dim, event_shape_x)`.\n                `batch_dim` corresponds to the number of observations to be drawn.\n            max_sampling_batch_size: Maximum batch size for rejection sampling.\n            show_progress_bars: Whether to show sampling progress monitor.\n\n        Returns:\n            Samples from the posteriors of shape (*sample_shape, B, *input_shape)\n        \"\"\"\n        num_samples = torch.Size(sample_shape).numel()\n        condition_shape = self.posterior_estimator.condition_shape\n        x = reshape_to_batch_event(x, event_shape=condition_shape)\n\n        max_sampling_batch_size = (\n            self.max_sampling_batch_size\n            if max_sampling_batch_size is None\n            else max_sampling_batch_size\n        )\n\n        samples = rejection.accept_reject_sample(\n            proposal=self.posterior_estimator,\n            accept_reject_fn=lambda theta: within_support(self.prior, theta),\n            num_samples=num_samples,\n            show_progress_bars=show_progress_bars,\n            max_sampling_batch_size=max_sampling_batch_size,\n            proposal_sampling_kwargs={\"condition\": x},\n            alternative_method=\"build_posterior(..., sample_with='mcmc')\",\n        )[0]\n\n        return samples\n\n    def log_prob(\n        self,\n        theta: Tensor,\n        x: Optional[Tensor] = None,\n        norm_posterior: bool = True,\n        track_gradients: bool = False,\n        leakage_correction_params: Optional[dict] = None,\n    ) -&gt; Tensor:\n        r\"\"\"Returns the log-probability of the posterior $p(\\theta|x)$.\n\n        Args:\n            theta: Parameters $\\theta$.\n            norm_posterior: Whether to enforce a normalized posterior density.\n                Renormalization of the posterior is useful when some\n                probability falls out or leaks out of the prescribed prior support.\n                The normalizing factor is calculated via rejection sampling, so if you\n                need speedier but unnormalized log posterior estimates set here\n                `norm_posterior=False`. The returned log posterior is set to\n                -\u221e outside of the prior support regardless of this setting.\n            track_gradients: Whether the returned tensor supports tracking gradients.\n                This can be helpful for e.g. sensitivity analysis, but increases memory\n                consumption.\n            leakage_correction_params: A `dict` of keyword arguments to override the\n                default values of `leakage_correction()`. Possible options are:\n                `num_rejection_samples`, `force_update`, `show_progress_bars`, and\n                `rejection_sampling_batch_size`.\n                These parameters only have an effect if `norm_posterior=True`.\n\n        Returns:\n            `(len(\u03b8),)`-shaped log posterior probability $\\log p(\\theta|x)$ for \u03b8 in the\n            support of the prior, -\u221e (corresponding to 0 probability) outside.\n        \"\"\"\n        x = self._x_else_default_x(x)\n\n        theta = ensure_theta_batched(torch.as_tensor(theta))\n        theta_density_estimator = reshape_to_sample_batch_event(\n            theta, theta.shape[1:], leading_is_sample=True\n        )\n        x_density_estimator = reshape_to_batch_event(\n            x, event_shape=self.posterior_estimator.condition_shape\n        )\n        assert (\n            x_density_estimator.shape[0] == 1\n        ), \".log_prob() supports only `batchsize == 1`.\"\n\n        self.posterior_estimator.eval()\n\n        with torch.set_grad_enabled(track_gradients):\n            # Evaluate on device, move back to cpu for comparison with prior.\n            unnorm_log_prob = self.posterior_estimator.log_prob(\n                theta_density_estimator, condition=x_density_estimator\n            )\n            # `log_prob` supports only a single observation (i.e. `batchsize==1`).\n            # We now remove this additional dimension.\n            unnorm_log_prob = unnorm_log_prob.squeeze(dim=1)\n\n            # Force probability to be zero outside prior support.\n            in_prior_support = within_support(self.prior, theta)\n\n            masked_log_prob = torch.where(\n                in_prior_support,\n                unnorm_log_prob,\n                torch.tensor(float(\"-inf\"), dtype=torch.float32, device=self._device),\n            )\n\n            if leakage_correction_params is None:\n                leakage_correction_params = dict()  # use defaults\n            log_factor = (\n                log(self.leakage_correction(x=x, **leakage_correction_params))\n                if norm_posterior\n                else 0\n            )\n\n            return masked_log_prob - log_factor\n\n    def log_prob_batched(\n        self,\n        theta: Tensor,\n        x: Tensor,\n        norm_posterior: bool = True,\n        track_gradients: bool = False,\n        leakage_correction_params: Optional[dict] = None,\n    ) -&gt; Tensor:\n        \"\"\"Given a batch of observations [x_1, ..., x_B] and a batch of parameters \\\n            [$\\theta_1$,..., $\\theta_B$] this function evalautes the log-probabilities \\\n            of the posteriors $p(\\theta_1|x_1)$, ..., $p(\\theta_B|x_B)$ in a batched \\\n            (i.e. vectorized) manner.\n\n        Args:\n            theta: Batch of parameters $\\theta$ of shape \\\n                `(*sample_shape, batch_dim, *theta_shape)`.\n            x: Batch of observations $x$ of shape \\\n                `(batch_dim, *condition_shape)`.\n            norm_posterior: Whether to enforce a normalized posterior density.\n                Renormalization of the posterior is useful when some\n                probability falls out or leaks out of the prescribed prior support.\n                The normalizing factor is calculated via rejection sampling, so if you\n                need speedier but unnormalized log posterior estimates set here\n                `norm_posterior=False`. The returned log posterior is set to\n                -\u221e outside of the prior support regardless of this setting.\n            track_gradients: Whether the returned tensor supports tracking gradients.\n                This can be helpful for e.g. sensitivity analysis, but increases memory\n                consumption.\n            leakage_correction_params: A `dict` of keyword arguments to override the\n                default values of `leakage_correction()`. Possible options are:\n                `num_rejection_samples`, `force_update`, `show_progress_bars`, and\n                `rejection_sampling_batch_size`.\n                These parameters only have an effect if `norm_posterior=True`.\n\n        Returns:\n            `(len(\u03b8), B)`-shaped log posterior probability $\\\\log p(\\theta|x)$\\\\ for \u03b8 \\\n            in the support of the prior, -\u221e (corresponding to 0 probability) outside.\n        \"\"\"\n\n        theta = ensure_theta_batched(torch.as_tensor(theta))\n        event_shape = self.posterior_estimator.input_shape\n        theta_density_estimator = reshape_to_sample_batch_event(\n            theta, event_shape, leading_is_sample=True\n        )\n        x_density_estimator = reshape_to_batch_event(\n            x, event_shape=self.posterior_estimator.condition_shape\n        )\n\n        self.posterior_estimator.eval()\n\n        with torch.set_grad_enabled(track_gradients):\n            # Evaluate on device, move back to cpu for comparison with prior.\n            unnorm_log_prob = self.posterior_estimator.log_prob(\n                theta_density_estimator, condition=x_density_estimator\n            )\n\n            # Force probability to be zero outside prior support.\n            in_prior_support = within_support(self.prior, theta)\n\n            masked_log_prob = torch.where(\n                in_prior_support,\n                unnorm_log_prob,\n                torch.tensor(float(\"-inf\"), dtype=torch.float32, device=self._device),\n            )\n\n            if leakage_correction_params is None:\n                leakage_correction_params = dict()  # use defaults\n            log_factor = (\n                log(self.leakage_correction(x=x, **leakage_correction_params))\n                if norm_posterior\n                else 0\n            )\n\n            return masked_log_prob - log_factor\n\n    @torch.no_grad()\n    def leakage_correction(\n        self,\n        x: Tensor,\n        num_rejection_samples: int = 10_000,\n        force_update: bool = False,\n        show_progress_bars: bool = False,\n        rejection_sampling_batch_size: int = 10_000,\n    ) -&gt; Tensor:\n        r\"\"\"Return leakage correction factor for a leaky posterior density estimate.\n\n        The factor is estimated from the acceptance probability during rejection\n        sampling from the posterior.\n\n        This is to avoid re-estimating the acceptance probability from scratch\n        whenever `log_prob` is called and `norm_posterior=True`. Here, it\n        is estimated only once for `self.default_x` and saved for later. We\n        re-evaluate only whenever a new `x` is passed.\n\n        Arguments:\n            num_rejection_samples: Number of samples used to estimate correction factor.\n            show_progress_bars: Whether to show a progress bar during sampling.\n            rejection_sampling_batch_size: Batch size for rejection sampling.\n\n        Returns:\n            Saved or newly-estimated correction factor (as a scalar `Tensor`).\n        \"\"\"\n\n        def acceptance_at(x: Tensor) -&gt; Tensor:\n            # [1:] to remove batch-dimension for `reshape_to_batch_event`.\n            return rejection.accept_reject_sample(\n                proposal=self.posterior_estimator,\n                accept_reject_fn=lambda theta: within_support(self.prior, theta),\n                num_samples=num_rejection_samples,\n                show_progress_bars=show_progress_bars,\n                sample_for_correction_factor=True,\n                max_sampling_batch_size=rejection_sampling_batch_size,\n                proposal_sampling_kwargs={\n                    \"condition\": reshape_to_batch_event(\n                        x, event_shape=self.posterior_estimator.condition_shape\n                    )\n                },\n            )[1]\n\n        # Check if the provided x matches the default x (short-circuit on identity).\n        is_new_x = self.default_x is None or (\n            x is not self.default_x and (x != self.default_x).any()\n        )\n\n        not_saved_at_default_x = self._leakage_density_correction_factor is None\n\n        if is_new_x:  # Calculate at x; don't save.\n            return acceptance_at(x)\n        elif not_saved_at_default_x or force_update:  # Calculate at default_x; save.\n            assert self.default_x is not None\n            self._leakage_density_correction_factor = acceptance_at(self.default_x)\n\n        return self._leakage_density_correction_factor  # type: ignore\n\n    def map(\n        self,\n        x: Optional[Tensor] = None,\n        num_iter: int = 1_000,\n        num_to_optimize: int = 100,\n        learning_rate: float = 0.01,\n        init_method: Union[str, Tensor] = \"posterior\",\n        num_init_samples: int = 1_000,\n        save_best_every: int = 10,\n        show_progress_bars: bool = False,\n        force_update: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n        The method can be interrupted (Ctrl-C) when the user sees that the\n        log-probability converges. The best estimate will be saved in `self._map` and\n        can be accessed with `self.map()`. The MAP is obtained by running gradient\n        ascent from a given number of starting positions (samples from the posterior\n        with the highest log-probability). After the optimization is done, we select the\n        parameter set that has the highest log-probability after the optimization.\n\n        Warning: The default values used by this function are not well-tested. They\n        might require hand-tuning for the problem at hand.\n\n        For developers: if the prior is a `BoxUniform`, we carry out the optimization\n        in unbounded space and transform the result back into bounded space.\n\n        Args:\n            x: Deprecated - use `.set_default_x()` prior to `.map()`.\n            num_iter: Number of optimization steps that the algorithm takes\n                to find the MAP.\n            learning_rate: Learning rate of the optimizer.\n            init_method: How to select the starting parameters for the optimization. If\n                it is a string, it can be either [`posterior`, `prior`], which samples\n                the respective distribution `num_init_samples` times. If it is a\n                tensor, the tensor will be used as init locations.\n            num_init_samples: Draw this number of samples from the posterior and\n                evaluate the log-probability of all of them.\n            num_to_optimize: From the drawn `num_init_samples`, use the\n                `num_to_optimize` with highest log-probability as the initial points\n                for the optimization.\n            save_best_every: The best log-probability is computed, saved in the\n                `map`-attribute, and printed every `save_best_every`-th iteration.\n                Computing the best log-probability creates a significant overhead\n                (thus, the default is `10`.)\n            show_progress_bars: Whether to show a progressbar during sampling from the\n                posterior.\n            force_update: Whether to re-calculate the MAP when x is unchanged and\n                have a cached value.\n            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n                {'norm_posterior': True} for SNPE.\n\n        Returns:\n            The MAP estimate.\n        \"\"\"\n        return super().map(\n            x=x,\n            num_iter=num_iter,\n            num_to_optimize=num_to_optimize,\n            learning_rate=learning_rate,\n            init_method=init_method,\n            num_init_samples=num_init_samples,\n            save_best_every=save_best_every,\n            show_progress_bars=show_progress_bars,\n            force_update=force_update,\n        )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__","title":"<code>__init__(posterior_estimator, prior, max_sampling_batch_size=10000, device=None, x_shape=None, enable_transform=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Distribution</code> <p>Prior distribution with <code>.log_prob()</code> and <code>.sample()</code>.</p> required <code>posterior_estimator</code> <code>ConditionalDensityEstimator</code> <p>The trained neural posterior.</p> required <code>max_sampling_batch_size</code> <code>int</code> <p>Batchsize of samples being drawn from the proposal at every iteration.</p> <code>10000</code> <code>device</code> <code>Optional[str]</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:0\u201d. If None, <code>potential_fn.device</code> is used.</p> <code>None</code> <code>x_shape</code> <code>Optional[Size]</code> <p>Deprecated, should not be passed.</p> <code>None</code> <code>enable_transform</code> <code>bool</code> <p>Whether to transform parameters to unconstrained space during MAP optimization. When False, an identity transform will be returned for <code>theta_transform</code>.</p> <code>True</code> Source code in <code>sbi/inference/posteriors/direct_posterior.py</code> <pre><code>def __init__(\n    self,\n    posterior_estimator: ConditionalDensityEstimator,\n    prior: Distribution,\n    max_sampling_batch_size: int = 10_000,\n    device: Optional[str] = None,\n    x_shape: Optional[torch.Size] = None,\n    enable_transform: bool = True,\n):\n    \"\"\"\n    Args:\n        prior: Prior distribution with `.log_prob()` and `.sample()`.\n        posterior_estimator: The trained neural posterior.\n        max_sampling_batch_size: Batchsize of samples being drawn from\n            the proposal at every iteration.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:0\". If None,\n            `potential_fn.device` is used.\n        x_shape: Deprecated, should not be passed.\n        enable_transform: Whether to transform parameters to unconstrained space\n            during MAP optimization. When False, an identity transform will be\n            returned for `theta_transform`.\n    \"\"\"\n    # Because `DirectPosterior` does not take the `potential_fn` as input, it\n    # builds it itself. The `potential_fn` and `theta_transform` are used only for\n    # obtaining the MAP.\n    check_prior(prior)\n    potential_fn, theta_transform = posterior_estimator_based_potential(\n        posterior_estimator,\n        prior,\n        x_o=None,\n        enable_transform=enable_transform,\n    )\n\n    super().__init__(\n        potential_fn=potential_fn,\n        theta_transform=theta_transform,\n        device=device,\n        x_shape=x_shape,\n    )\n\n    self.prior = prior\n    self.posterior_estimator = posterior_estimator\n\n    self.max_sampling_batch_size = max_sampling_batch_size\n    self._leakage_density_correction_factor = None\n\n    self._purpose = \"\"\"It samples the posterior network and rejects samples that\n        lie outside of the prior bounds.\"\"\"\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction","title":"<code>leakage_correction(x, num_rejection_samples=10000, force_update=False, show_progress_bars=False, rejection_sampling_batch_size=10000)</code>","text":"<p>Return leakage correction factor for a leaky posterior density estimate.</p> <p>The factor is estimated from the acceptance probability during rejection sampling from the posterior.</p> <p>This is to avoid re-estimating the acceptance probability from scratch whenever <code>log_prob</code> is called and <code>norm_posterior=True</code>. Here, it is estimated only once for <code>self.default_x</code> and saved for later. We re-evaluate only whenever a new <code>x</code> is passed.</p> <p>Parameters:</p> Name Type Description Default <code>num_rejection_samples</code> <code>int</code> <p>Number of samples used to estimate correction factor.</p> <code>10000</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progress bar during sampling.</p> <code>False</code> <code>rejection_sampling_batch_size</code> <code>int</code> <p>Batch size for rejection sampling.</p> <code>10000</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Saved or newly-estimated correction factor (as a scalar <code>Tensor</code>).</p> Source code in <code>sbi/inference/posteriors/direct_posterior.py</code> <pre><code>@torch.no_grad()\ndef leakage_correction(\n    self,\n    x: Tensor,\n    num_rejection_samples: int = 10_000,\n    force_update: bool = False,\n    show_progress_bars: bool = False,\n    rejection_sampling_batch_size: int = 10_000,\n) -&gt; Tensor:\n    r\"\"\"Return leakage correction factor for a leaky posterior density estimate.\n\n    The factor is estimated from the acceptance probability during rejection\n    sampling from the posterior.\n\n    This is to avoid re-estimating the acceptance probability from scratch\n    whenever `log_prob` is called and `norm_posterior=True`. Here, it\n    is estimated only once for `self.default_x` and saved for later. We\n    re-evaluate only whenever a new `x` is passed.\n\n    Arguments:\n        num_rejection_samples: Number of samples used to estimate correction factor.\n        show_progress_bars: Whether to show a progress bar during sampling.\n        rejection_sampling_batch_size: Batch size for rejection sampling.\n\n    Returns:\n        Saved or newly-estimated correction factor (as a scalar `Tensor`).\n    \"\"\"\n\n    def acceptance_at(x: Tensor) -&gt; Tensor:\n        # [1:] to remove batch-dimension for `reshape_to_batch_event`.\n        return rejection.accept_reject_sample(\n            proposal=self.posterior_estimator,\n            accept_reject_fn=lambda theta: within_support(self.prior, theta),\n            num_samples=num_rejection_samples,\n            show_progress_bars=show_progress_bars,\n            sample_for_correction_factor=True,\n            max_sampling_batch_size=rejection_sampling_batch_size,\n            proposal_sampling_kwargs={\n                \"condition\": reshape_to_batch_event(\n                    x, event_shape=self.posterior_estimator.condition_shape\n                )\n            },\n        )[1]\n\n    # Check if the provided x matches the default x (short-circuit on identity).\n    is_new_x = self.default_x is None or (\n        x is not self.default_x and (x != self.default_x).any()\n    )\n\n    not_saved_at_default_x = self._leakage_density_correction_factor is None\n\n    if is_new_x:  # Calculate at x; don't save.\n        return acceptance_at(x)\n    elif not_saved_at_default_x or force_update:  # Calculate at default_x; save.\n        assert self.default_x is not None\n        self._leakage_density_correction_factor = acceptance_at(self.default_x)\n\n    return self._leakage_density_correction_factor  # type: ignore\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob","title":"<code>log_prob(theta, x=None, norm_posterior=True, track_gradients=False, leakage_correction_params=None)</code>","text":"<p>Returns the log-probability of the posterior \\(p(\\theta|x)\\).</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>Tensor</code> <p>Parameters \\(\\theta\\).</p> required <code>norm_posterior</code> <code>bool</code> <p>Whether to enforce a normalized posterior density. Renormalization of the posterior is useful when some probability falls out or leaks out of the prescribed prior support. The normalizing factor is calculated via rejection sampling, so if you need speedier but unnormalized log posterior estimates set here <code>norm_posterior=False</code>. The returned log posterior is set to -\u221e outside of the prior support regardless of this setting.</p> <code>True</code> <code>track_gradients</code> <code>bool</code> <p>Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption.</p> <code>False</code> <code>leakage_correction_params</code> <code>Optional[dict]</code> <p>A <code>dict</code> of keyword arguments to override the default values of <code>leakage_correction()</code>. Possible options are: <code>num_rejection_samples</code>, <code>force_update</code>, <code>show_progress_bars</code>, and <code>rejection_sampling_batch_size</code>. These parameters only have an effect if <code>norm_posterior=True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p><code>(len(\u03b8),)</code>-shaped log posterior probability \\(\\log p(\\theta|x)\\) for \u03b8 in the</p> <code>Tensor</code> <p>support of the prior, -\u221e (corresponding to 0 probability) outside.</p> Source code in <code>sbi/inference/posteriors/direct_posterior.py</code> <pre><code>def log_prob(\n    self,\n    theta: Tensor,\n    x: Optional[Tensor] = None,\n    norm_posterior: bool = True,\n    track_gradients: bool = False,\n    leakage_correction_params: Optional[dict] = None,\n) -&gt; Tensor:\n    r\"\"\"Returns the log-probability of the posterior $p(\\theta|x)$.\n\n    Args:\n        theta: Parameters $\\theta$.\n        norm_posterior: Whether to enforce a normalized posterior density.\n            Renormalization of the posterior is useful when some\n            probability falls out or leaks out of the prescribed prior support.\n            The normalizing factor is calculated via rejection sampling, so if you\n            need speedier but unnormalized log posterior estimates set here\n            `norm_posterior=False`. The returned log posterior is set to\n            -\u221e outside of the prior support regardless of this setting.\n        track_gradients: Whether the returned tensor supports tracking gradients.\n            This can be helpful for e.g. sensitivity analysis, but increases memory\n            consumption.\n        leakage_correction_params: A `dict` of keyword arguments to override the\n            default values of `leakage_correction()`. Possible options are:\n            `num_rejection_samples`, `force_update`, `show_progress_bars`, and\n            `rejection_sampling_batch_size`.\n            These parameters only have an effect if `norm_posterior=True`.\n\n    Returns:\n        `(len(\u03b8),)`-shaped log posterior probability $\\log p(\\theta|x)$ for \u03b8 in the\n        support of the prior, -\u221e (corresponding to 0 probability) outside.\n    \"\"\"\n    x = self._x_else_default_x(x)\n\n    theta = ensure_theta_batched(torch.as_tensor(theta))\n    theta_density_estimator = reshape_to_sample_batch_event(\n        theta, theta.shape[1:], leading_is_sample=True\n    )\n    x_density_estimator = reshape_to_batch_event(\n        x, event_shape=self.posterior_estimator.condition_shape\n    )\n    assert (\n        x_density_estimator.shape[0] == 1\n    ), \".log_prob() supports only `batchsize == 1`.\"\n\n    self.posterior_estimator.eval()\n\n    with torch.set_grad_enabled(track_gradients):\n        # Evaluate on device, move back to cpu for comparison with prior.\n        unnorm_log_prob = self.posterior_estimator.log_prob(\n            theta_density_estimator, condition=x_density_estimator\n        )\n        # `log_prob` supports only a single observation (i.e. `batchsize==1`).\n        # We now remove this additional dimension.\n        unnorm_log_prob = unnorm_log_prob.squeeze(dim=1)\n\n        # Force probability to be zero outside prior support.\n        in_prior_support = within_support(self.prior, theta)\n\n        masked_log_prob = torch.where(\n            in_prior_support,\n            unnorm_log_prob,\n            torch.tensor(float(\"-inf\"), dtype=torch.float32, device=self._device),\n        )\n\n        if leakage_correction_params is None:\n            leakage_correction_params = dict()  # use defaults\n        log_factor = (\n            log(self.leakage_correction(x=x, **leakage_correction_params))\n            if norm_posterior\n            else 0\n        )\n\n        return masked_log_prob - log_factor\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob_batched","title":"<code>log_prob_batched(theta, x, norm_posterior=True, track_gradients=False, leakage_correction_params=None)</code>","text":"<p>Given a batch of observations [x_1, \u2026, x_B] and a batch of parameters             [$  heta_1$,\u2026, $  heta_B$] this function evalautes the log-probabilities             of the posteriors \\(p(        heta_1|x_1)\\), \u2026, \\(p(  heta_B|x_B)\\) in a batched             (i.e. vectorized) manner.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>Tensor</code> <p>Batch of parameters $        heta$ of shape                 <code>(*sample_shape, batch_dim, *theta_shape)</code>.</p> required <code>x</code> <code>Tensor</code> <p>Batch of observations \\(x\\) of shape                 <code>(batch_dim, *condition_shape)</code>.</p> required <code>norm_posterior</code> <code>bool</code> <p>Whether to enforce a normalized posterior density. Renormalization of the posterior is useful when some probability falls out or leaks out of the prescribed prior support. The normalizing factor is calculated via rejection sampling, so if you need speedier but unnormalized log posterior estimates set here <code>norm_posterior=False</code>. The returned log posterior is set to -\u221e outside of the prior support regardless of this setting.</p> <code>True</code> <code>track_gradients</code> <code>bool</code> <p>Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption.</p> <code>False</code> <code>leakage_correction_params</code> <code>Optional[dict]</code> <p>A <code>dict</code> of keyword arguments to override the default values of <code>leakage_correction()</code>. Possible options are: <code>num_rejection_samples</code>, <code>force_update</code>, <code>show_progress_bars</code>, and <code>rejection_sampling_batch_size</code>. These parameters only have an effect if <code>norm_posterior=True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p><code>(len(\u03b8), B)</code>-shaped log posterior probability \\(\\log p(     heta|x)\\) for \u03b8             in the support of the prior, -\u221e (corresponding to 0 probability) outside.</p> Source code in <code>sbi/inference/posteriors/direct_posterior.py</code> <pre><code>def log_prob_batched(\n    self,\n    theta: Tensor,\n    x: Tensor,\n    norm_posterior: bool = True,\n    track_gradients: bool = False,\n    leakage_correction_params: Optional[dict] = None,\n) -&gt; Tensor:\n    \"\"\"Given a batch of observations [x_1, ..., x_B] and a batch of parameters \\\n        [$\\theta_1$,..., $\\theta_B$] this function evalautes the log-probabilities \\\n        of the posteriors $p(\\theta_1|x_1)$, ..., $p(\\theta_B|x_B)$ in a batched \\\n        (i.e. vectorized) manner.\n\n    Args:\n        theta: Batch of parameters $\\theta$ of shape \\\n            `(*sample_shape, batch_dim, *theta_shape)`.\n        x: Batch of observations $x$ of shape \\\n            `(batch_dim, *condition_shape)`.\n        norm_posterior: Whether to enforce a normalized posterior density.\n            Renormalization of the posterior is useful when some\n            probability falls out or leaks out of the prescribed prior support.\n            The normalizing factor is calculated via rejection sampling, so if you\n            need speedier but unnormalized log posterior estimates set here\n            `norm_posterior=False`. The returned log posterior is set to\n            -\u221e outside of the prior support regardless of this setting.\n        track_gradients: Whether the returned tensor supports tracking gradients.\n            This can be helpful for e.g. sensitivity analysis, but increases memory\n            consumption.\n        leakage_correction_params: A `dict` of keyword arguments to override the\n            default values of `leakage_correction()`. Possible options are:\n            `num_rejection_samples`, `force_update`, `show_progress_bars`, and\n            `rejection_sampling_batch_size`.\n            These parameters only have an effect if `norm_posterior=True`.\n\n    Returns:\n        `(len(\u03b8), B)`-shaped log posterior probability $\\\\log p(\\theta|x)$\\\\ for \u03b8 \\\n        in the support of the prior, -\u221e (corresponding to 0 probability) outside.\n    \"\"\"\n\n    theta = ensure_theta_batched(torch.as_tensor(theta))\n    event_shape = self.posterior_estimator.input_shape\n    theta_density_estimator = reshape_to_sample_batch_event(\n        theta, event_shape, leading_is_sample=True\n    )\n    x_density_estimator = reshape_to_batch_event(\n        x, event_shape=self.posterior_estimator.condition_shape\n    )\n\n    self.posterior_estimator.eval()\n\n    with torch.set_grad_enabled(track_gradients):\n        # Evaluate on device, move back to cpu for comparison with prior.\n        unnorm_log_prob = self.posterior_estimator.log_prob(\n            theta_density_estimator, condition=x_density_estimator\n        )\n\n        # Force probability to be zero outside prior support.\n        in_prior_support = within_support(self.prior, theta)\n\n        masked_log_prob = torch.where(\n            in_prior_support,\n            unnorm_log_prob,\n            torch.tensor(float(\"-inf\"), dtype=torch.float32, device=self._device),\n        )\n\n        if leakage_correction_params is None:\n            leakage_correction_params = dict()  # use defaults\n        log_factor = (\n            log(self.leakage_correction(x=x, **leakage_correction_params))\n            if norm_posterior\n            else 0\n        )\n\n        return masked_log_prob - log_factor\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior.map","title":"<code>map(x=None, num_iter=1000, num_to_optimize=100, learning_rate=0.01, init_method='posterior', num_init_samples=1000, save_best_every=10, show_progress_bars=False, force_update=False)</code>","text":"<p>Returns the maximum-a-posteriori estimate (MAP).</p> <p>The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in <code>self._map</code> and can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization.</p> <p>Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand.</p> <p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization in unbounded space and transform the result back into bounded space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p> <code>None</code> <code>num_iter</code> <code>int</code> <p>Number of optimization steps that the algorithm takes to find the MAP.</p> <code>1000</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer.</p> <code>0.01</code> <code>init_method</code> <code>Union[str, Tensor]</code> <p>How to select the starting parameters for the optimization. If it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples the respective distribution <code>num_init_samples</code> times. If it is a tensor, the tensor will be used as init locations.</p> <code>'posterior'</code> <code>num_init_samples</code> <code>int</code> <p>Draw this number of samples from the posterior and evaluate the log-probability of all of them.</p> <code>1000</code> <code>num_to_optimize</code> <code>int</code> <p>From the drawn <code>num_init_samples</code>, use the <code>num_to_optimize</code> with highest log-probability as the initial points for the optimization.</p> <code>100</code> <code>save_best_every</code> <code>int</code> <p>The best log-probability is computed, saved in the <code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration. Computing the best log-probability creates a significant overhead (thus, the default is <code>10</code>.)</p> <code>10</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during sampling from the posterior.</p> <code>False</code> <code>force_update</code> <code>bool</code> <p>Whether to re-calculate the MAP when x is unchanged and have a cached value.</p> <code>False</code> <code>log_prob_kwargs</code> <p>Will be empty for SNLE and SNRE. Will contain {\u2018norm_posterior\u2019: True} for SNPE.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MAP estimate.</p> Source code in <code>sbi/inference/posteriors/direct_posterior.py</code> <pre><code>def map(\n    self,\n    x: Optional[Tensor] = None,\n    num_iter: int = 1_000,\n    num_to_optimize: int = 100,\n    learning_rate: float = 0.01,\n    init_method: Union[str, Tensor] = \"posterior\",\n    num_init_samples: int = 1_000,\n    save_best_every: int = 10,\n    show_progress_bars: bool = False,\n    force_update: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n    The method can be interrupted (Ctrl-C) when the user sees that the\n    log-probability converges. The best estimate will be saved in `self._map` and\n    can be accessed with `self.map()`. The MAP is obtained by running gradient\n    ascent from a given number of starting positions (samples from the posterior\n    with the highest log-probability). After the optimization is done, we select the\n    parameter set that has the highest log-probability after the optimization.\n\n    Warning: The default values used by this function are not well-tested. They\n    might require hand-tuning for the problem at hand.\n\n    For developers: if the prior is a `BoxUniform`, we carry out the optimization\n    in unbounded space and transform the result back into bounded space.\n\n    Args:\n        x: Deprecated - use `.set_default_x()` prior to `.map()`.\n        num_iter: Number of optimization steps that the algorithm takes\n            to find the MAP.\n        learning_rate: Learning rate of the optimizer.\n        init_method: How to select the starting parameters for the optimization. If\n            it is a string, it can be either [`posterior`, `prior`], which samples\n            the respective distribution `num_init_samples` times. If it is a\n            tensor, the tensor will be used as init locations.\n        num_init_samples: Draw this number of samples from the posterior and\n            evaluate the log-probability of all of them.\n        num_to_optimize: From the drawn `num_init_samples`, use the\n            `num_to_optimize` with highest log-probability as the initial points\n            for the optimization.\n        save_best_every: The best log-probability is computed, saved in the\n            `map`-attribute, and printed every `save_best_every`-th iteration.\n            Computing the best log-probability creates a significant overhead\n            (thus, the default is `10`.)\n        show_progress_bars: Whether to show a progressbar during sampling from the\n            posterior.\n        force_update: Whether to re-calculate the MAP when x is unchanged and\n            have a cached value.\n        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n            {'norm_posterior': True} for SNPE.\n\n    Returns:\n        The MAP estimate.\n    \"\"\"\n    return super().map(\n        x=x,\n        num_iter=num_iter,\n        num_to_optimize=num_to_optimize,\n        learning_rate=learning_rate,\n        init_method=init_method,\n        num_init_samples=num_init_samples,\n        save_best_every=save_best_every,\n        show_progress_bars=show_progress_bars,\n        force_update=force_update,\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior.sample","title":"<code>sample(sample_shape=torch.Size(), x=None, max_sampling_batch_size=10000, sample_with=None, show_progress_bars=True)</code>","text":"<p>Return samples from posterior distribution \\(p(\\theta|x)\\).</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Shape</code> <p>Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code> samples and then reshape into the desired shape.</p> <code>Size()</code> <code>sample_with</code> <code>Optional[str]</code> <p>This argument only exists to keep backward-compatibility with <code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show sampling progress monitor.</p> <code>True</code> Source code in <code>sbi/inference/posteriors/direct_posterior.py</code> <pre><code>def sample(\n    self,\n    sample_shape: Shape = torch.Size(),\n    x: Optional[Tensor] = None,\n    max_sampling_batch_size: int = 10_000,\n    sample_with: Optional[str] = None,\n    show_progress_bars: bool = True,\n) -&gt; Tensor:\n    r\"\"\"Return samples from posterior distribution $p(\\theta|x)$.\n\n    Args:\n        sample_shape: Desired shape of samples that are drawn from posterior. If\n            sample_shape is multidimensional we simply draw `sample_shape.numel()`\n            samples and then reshape into the desired shape.\n        sample_with: This argument only exists to keep backward-compatibility with\n            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.\n        show_progress_bars: Whether to show sampling progress monitor.\n    \"\"\"\n\n    num_samples = torch.Size(sample_shape).numel()\n    x = self._x_else_default_x(x)\n    x = reshape_to_batch_event(\n        x, event_shape=self.posterior_estimator.condition_shape\n    )\n\n    max_sampling_batch_size = (\n        self.max_sampling_batch_size\n        if max_sampling_batch_size is None\n        else max_sampling_batch_size\n    )\n\n    if sample_with is not None:\n        raise ValueError(\n            f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting \"\n            f\"`sample_with` is no longer supported. You have to rerun \"\n            f\"`.build_posterior(sample_with={sample_with}).`\"\n        )\n\n    samples = rejection.accept_reject_sample(\n        proposal=self.posterior_estimator,\n        accept_reject_fn=lambda theta: within_support(self.prior, theta),\n        num_samples=num_samples,\n        show_progress_bars=show_progress_bars,\n        max_sampling_batch_size=max_sampling_batch_size,\n        proposal_sampling_kwargs={\"condition\": x},\n        alternative_method=\"build_posterior(..., sample_with='mcmc')\",\n    )[0]\n\n    return samples[:, 0]  # Remove batch dimension.\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior.sample_batched","title":"<code>sample_batched(sample_shape, x, max_sampling_batch_size=10000, show_progress_bars=True)</code>","text":"<p>Given a batch of observations [x_1, \u2026, x_B] this function samples from posteriors \\(p(\\theta|x_1)\\), \u2026 ,\\(p(\\theta|x_B)\\), in a batched (i.e. vectorized) manner.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Shape</code> <p>Desired shape of samples that are drawn from the posterior given every observation.</p> required <code>x</code> <code>Tensor</code> <p>A batch of observations, of shape <code>(batch_dim, event_shape_x)</code>. <code>batch_dim</code> corresponds to the number of observations to be drawn.</p> required <code>max_sampling_batch_size</code> <code>int</code> <p>Maximum batch size for rejection sampling.</p> <code>10000</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show sampling progress monitor.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Samples from the posteriors of shape (*sample_shape, B, *input_shape)</p> Source code in <code>sbi/inference/posteriors/direct_posterior.py</code> <pre><code>def sample_batched(\n    self,\n    sample_shape: Shape,\n    x: Tensor,\n    max_sampling_batch_size: int = 10_000,\n    show_progress_bars: bool = True,\n) -&gt; Tensor:\n    r\"\"\"Given a batch of observations [x_1, ..., x_B] this function samples from\n    posteriors $p(\\theta|x_1)$, ... ,$p(\\theta|x_B)$, in a batched (i.e. vectorized)\n    manner.\n\n    Args:\n        sample_shape: Desired shape of samples that are drawn from the posterior\n            given every observation.\n        x: A batch of observations, of shape `(batch_dim, event_shape_x)`.\n            `batch_dim` corresponds to the number of observations to be drawn.\n        max_sampling_batch_size: Maximum batch size for rejection sampling.\n        show_progress_bars: Whether to show sampling progress monitor.\n\n    Returns:\n        Samples from the posteriors of shape (*sample_shape, B, *input_shape)\n    \"\"\"\n    num_samples = torch.Size(sample_shape).numel()\n    condition_shape = self.posterior_estimator.condition_shape\n    x = reshape_to_batch_event(x, event_shape=condition_shape)\n\n    max_sampling_batch_size = (\n        self.max_sampling_batch_size\n        if max_sampling_batch_size is None\n        else max_sampling_batch_size\n    )\n\n    samples = rejection.accept_reject_sample(\n        proposal=self.posterior_estimator,\n        accept_reject_fn=lambda theta: within_support(self.prior, theta),\n        num_samples=num_samples,\n        show_progress_bars=show_progress_bars,\n        max_sampling_batch_size=max_sampling_batch_size,\n        proposal_sampling_kwargs={\"condition\": x},\n        alternative_method=\"build_posterior(..., sample_with='mcmc')\",\n    )[0]\n\n    return samples\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior","title":"<code>ImportanceSamplingPosterior</code>","text":"<p>               Bases: <code>NeuralPosterior</code></p> <p>Provides importance sampling to sample from the posterior. SNLE or SNRE train neural networks to approximate the likelihood(-ratios). <code>ImportanceSamplingPosterior</code> allows to estimate the posterior log-probability by estimating the normlalization constant with importance sampling. It also allows to perform importance sampling (with <code>.sample()</code>) and to draw approximate samples with sampling-importance-resampling (SIR) (with <code>.sir_sample()</code>)</p> Source code in <code>sbi/inference/posteriors/importance_posterior.py</code> <pre><code>class ImportanceSamplingPosterior(NeuralPosterior):\n    r\"\"\"Provides importance sampling to sample from the posterior.&lt;br/&gt;&lt;br/&gt;\n    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).\n    `ImportanceSamplingPosterior` allows to estimate the posterior log-probability by\n    estimating the normlalization constant with importance sampling. It also allows to\n    perform importance sampling (with `.sample()`) and to draw approximate samples with\n    sampling-importance-resampling (SIR) (with `.sir_sample()`)\n    \"\"\"\n\n    def __init__(\n        self,\n        potential_fn: Union[Callable, BasePotential],\n        proposal: Any,\n        theta_transform: Optional[TorchTransform] = None,\n        method: str = \"sir\",\n        oversampling_factor: int = 32,\n        max_sampling_batch_size: int = 10_000,\n        device: Optional[str] = None,\n        x_shape: Optional[torch.Size] = None,\n    ):\n        \"\"\"\n        Args:\n            potential_fn: The potential function from which to draw samples. Must be a\n                `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.\n            proposal: The proposal distribution.\n            theta_transform: Transformation that is applied to parameters. Is not used\n                during but only when calling `.map()`.\n            method: Either of [`sir`|`importance`]. This sets the behavior of the\n                `.sample()` method. With `sir`, approximate posterior samples are\n                generated with sampling importance resampling (SIR). With\n                `importance`, the `.sample()` method returns a tuple of samples and\n                corresponding importance weights.\n            oversampling_factor: Number of proposed samples from which only one is\n                selected based on its importance weight.\n            max_sampling_batch_size: The batch size of samples being drawn from the\n                proposal at every iteration.\n            device: Device on which to sample, e.g., \"cpu\", \"cuda\" or \"cuda:0\". If\n                None, `potential_fn.device` is used.\n            x_shape: Deprecated, should not be passed.\n        \"\"\"\n        super().__init__(\n            potential_fn,\n            theta_transform=theta_transform,\n            device=device,\n            x_shape=x_shape,\n        )\n\n        self.proposal = proposal\n        self._normalization_constant = None\n        self.method = method\n\n        self.oversampling_factor = oversampling_factor\n        self.max_sampling_batch_size = max_sampling_batch_size\n\n        self._purpose = (\n            \"It provides sampling-importance resampling (SIR) to .sample() from the \"\n            \"posterior and can evaluate the _unnormalized_ posterior density with \"\n            \".log_prob().\"\n        )\n\n    def log_prob(\n        self,\n        theta: Tensor,\n        x: Optional[Tensor] = None,\n        track_gradients: bool = False,\n        normalization_constant_params: Optional[dict] = None,\n    ) -&gt; Tensor:\n        r\"\"\"Returns the log-probability of theta under the posterior.\n\n        The normalization constant is estimated with importance sampling.\n\n        Args:\n            theta: Parameters $\\theta$.\n            track_gradients: Whether the returned tensor supports tracking gradients.\n                This can be helpful for e.g. sensitivity analysis, but increases memory\n                consumption.\n            normalization_constant_params: Parameters passed on to\n                `estimate_normalization_constant()`.\n\n        Returns:\n            `len($\\theta$)`-shaped log-probability.\n        \"\"\"\n        x = self._x_else_default_x(x)\n        self.potential_fn.set_x(x)\n\n        theta = ensure_theta_batched(torch.as_tensor(theta))\n\n        with torch.set_grad_enabled(track_gradients):\n            potential_values = self.potential_fn(\n                theta.to(self._device), track_gradients=track_gradients\n            )\n\n            if normalization_constant_params is None:\n                normalization_constant_params = dict()  # use defaults\n            normalization_constant = self.estimate_normalization_constant(\n                x, **normalization_constant_params\n            )\n\n            return (potential_values - torch.log(normalization_constant)).to(\n                self._device\n            )\n\n    @torch.no_grad()\n    def estimate_normalization_constant(\n        self, x: Tensor, num_samples: int = 10_000, force_update: bool = False\n    ) -&gt; Tensor:\n        \"\"\"Returns the normalization constant via importance sampling.\n\n        Args:\n            num_samples: Number of importance samples used for the estimate.\n            force_update: Whether to re-calculate the normlization constant when x is\n                unchanged and have a cached value.\n        \"\"\"\n        # Check if the provided x matches the default x (short-circuit on identity).\n        is_new_x = self.default_x is None or (\n            x is not self.default_x and (x != self.default_x).any()\n        )\n\n        not_saved_at_default_x = self._normalization_constant is None\n\n        if is_new_x:  # Calculate at x; don't save.\n            _, log_importance_weights = importance_sample(\n                self.potential_fn,\n                proposal=self.proposal,\n                num_samples=num_samples,\n            )\n            return torch.mean(torch.exp(log_importance_weights))\n        elif not_saved_at_default_x or force_update:  # Calculate at default_x; save.\n            assert self.default_x is not None\n            _, log_importance_weights = importance_sample(\n                self.potential_fn,\n                proposal=self.proposal,\n                num_samples=num_samples,\n            )\n            self._normalization_constant = torch.mean(torch.exp(log_importance_weights))\n\n        return self._normalization_constant.to(self._device)  # type: ignore\n\n    def sample(\n        self,\n        sample_shape: Shape = torch.Size(),\n        x: Optional[Tensor] = None,\n        method: Optional[str] = None,\n        oversampling_factor: int = 32,\n        max_sampling_batch_size: int = 10_000,\n        sample_with: Optional[str] = None,\n        show_progress_bars: bool = False,\n    ) -&gt; Union[Tensor, Tuple[Tensor, Tensor]]:\n        \"\"\"Return samples from the approximate posterior distribution.\n\n        Args:\n            sample_shape: Shape of samples that are drawn from posterior.\n            x: Observed data.\n            method: Either of [`sir`|`importance`]. This sets the behavior of the\n                `.sample()` method. With `sir`, approximate posterior samples are\n                generated with sampling importance resampling (SIR). With\n                `importance`, the `.sample()` method returns a tuple of samples and\n                corresponding importance weights.\n            oversampling_factor: Number of proposed samples from which only one is\n                selected based on its importance weight.\n            max_sampling_batch_size: The batch size of samples being drawn from the\n                proposal at every iteration.\n            show_progress_bars: Whether to show a progressbar during sampling.\n        \"\"\"\n\n        method = self.method if method is None else method\n\n        if sample_with is not None:\n            raise ValueError(\n                f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting \"\n                f\"`sample_with` is no longer supported. You have to rerun \"\n                f\"`.build_posterior(sample_with={sample_with}).`\"\n            )\n\n        self.potential_fn.set_x(self._x_else_default_x(x))\n\n        if method == \"sir\":\n            return self._sir_sample(\n                sample_shape,\n                oversampling_factor=oversampling_factor,\n                max_sampling_batch_size=max_sampling_batch_size,\n                show_progress_bars=show_progress_bars,\n            )\n        elif method == \"importance\":\n            return self._importance_sample(sample_shape)\n        else:\n            raise NameError\n\n    def sample_batched(\n        self,\n        sample_shape: Shape,\n        x: Tensor,\n        max_sampling_batch_size: int = 10000,\n        show_progress_bars: bool = True,\n    ) -&gt; Tensor:\n        raise NotImplementedError(\n            \"Batched sampling is not implemented for ImportanceSamplingPosterior. \\\n            Alternatively you can use `sample` in a loop \\\n            [posterior.sample(theta, x_o) for x_o in x].\"\n        )\n\n    def _importance_sample(\n        self,\n        sample_shape: Shape = torch.Size(),\n        show_progress_bars: bool = False,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Returns samples from the proposal and log of their importance weights.\n\n        Args:\n            sample_shape: Desired shape of samples that are drawn from posterior.\n            sample_with: This argument only exists to keep backward-compatibility with\n                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.\n            show_progress_bars: Whether to show sampling progress monitor.\n\n        Returns:\n            Samples and logarithm of corresponding importance weights.\n        \"\"\"\n        num_samples = torch.Size(sample_shape).numel()\n        samples, log_importance_weights = importance_sample(\n            self.potential_fn,\n            proposal=self.proposal,\n            num_samples=num_samples,\n            show_progress_bars=show_progress_bars,\n        )\n\n        samples = samples.reshape((*sample_shape, -1)).to(self._device)\n        return samples, log_importance_weights.to(self._device)\n\n    def _sir_sample(\n        self,\n        sample_shape: Shape = torch.Size(),\n        oversampling_factor: int = 32,\n        max_sampling_batch_size: int = 10_000,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"Returns approximate samples from posterior $p(\\theta|x)$ via SIR.\n\n        Args:\n            sample_shape: Desired shape of samples that are drawn from posterior. If\n                sample_shape is multidimensional we simply draw `sample_shape.numel()`\n                samples and then reshape into the desired shape.\n            x: Observed data.\n            sample_with: This argument only exists to keep backward-compatibility with\n                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.\n            oversampling_factor: Number of proposed samples form which only one is\n                selected based on its importance weight.\n            max_sampling_batch_size: The batchsize of samples being drawn from\n                the proposal at every iteration. Used only in `sir_sample()`.\n            show_progress_bars: Whether to show sampling progress monitor.\n\n        Returns:\n            Samples from posterior.\n        \"\"\"\n        # Replace arguments that were not passed with their default.\n        oversampling_factor = (\n            self.oversampling_factor\n            if oversampling_factor is None\n            else oversampling_factor\n        )\n        max_sampling_batch_size = (\n            self.max_sampling_batch_size\n            if max_sampling_batch_size is None\n            else max_sampling_batch_size\n        )\n\n        num_samples = torch.Size(sample_shape).numel()\n        samples = sampling_importance_resampling(\n            self.potential_fn,\n            proposal=self.proposal,\n            num_samples=num_samples,\n            num_candidate_samples=oversampling_factor,\n            show_progress_bars=show_progress_bars,\n            max_sampling_batch_size=max_sampling_batch_size,\n            device=self._device,\n        )\n\n        return samples.reshape((*sample_shape, -1)).to(self._device)\n\n    def map(\n        self,\n        x: Optional[Tensor] = None,\n        num_iter: int = 1_000,\n        num_to_optimize: int = 100,\n        learning_rate: float = 0.01,\n        init_method: Union[str, Tensor] = \"proposal\",\n        num_init_samples: int = 1_000,\n        save_best_every: int = 10,\n        show_progress_bars: bool = False,\n        force_update: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n        The method can be interrupted (Ctrl-C) when the user sees that the\n        log-probability converges. The best estimate will be saved in `self._map` and\n        can be accessed with `self.map()`. The MAP is obtained by running gradient\n        ascent from a given number of starting positions (samples from the posterior\n        with the highest log-probability). After the optimization is done, we select the\n        parameter set that has the highest log-probability after the optimization.\n\n        Warning: The default values used by this function are not well-tested. They\n        might require hand-tuning for the problem at hand.\n\n        For developers: if the prior is a `BoxUniform`, we carry out the optimization\n        in unbounded space and transform the result back into bounded space.\n\n        Args:\n            x: Deprecated - use `.set_default_x()` prior to `.map()`.\n            num_iter: Number of optimization steps that the algorithm takes\n                to find the MAP.\n            learning_rate: Learning rate of the optimizer.\n            init_method: How to select the starting parameters for the optimization. If\n                it is a string, it can be either [`posterior`, `prior`], which samples\n                the respective distribution `num_init_samples` times. If it is a\n                tensor, the tensor will be used as init locations.\n            num_init_samples: Draw this number of samples from the posterior and\n                evaluate the log-probability of all of them.\n            num_to_optimize: From the drawn `num_init_samples`, use the\n                `num_to_optimize` with highest log-probability as the initial points\n                for the optimization.\n            save_best_every: The best log-probability is computed, saved in the\n                `map`-attribute, and printed every `save_best_every`-th iteration.\n                Computing the best log-probability creates a significant overhead\n                (thus, the default is `10`.)\n            show_progress_bars: Whether to show a progressbar during sampling from the\n                posterior.\n            force_update: Whether to re-calculate the MAP when x is unchanged and\n                have a cached value.\n            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n                {'norm_posterior': True} for SNPE.\n\n        Returns:\n            The MAP estimate.\n        \"\"\"\n        return super().map(\n            x=x,\n            num_iter=num_iter,\n            num_to_optimize=num_to_optimize,\n            learning_rate=learning_rate,\n            init_method=init_method,\n            num_init_samples=num_init_samples,\n            save_best_every=save_best_every,\n            show_progress_bars=show_progress_bars,\n            force_update=force_update,\n        )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.__init__","title":"<code>__init__(potential_fn, proposal, theta_transform=None, method='sir', oversampling_factor=32, max_sampling_batch_size=10000, device=None, x_shape=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>potential_fn</code> <code>Union[Callable, BasePotential]</code> <p>The potential function from which to draw samples. Must be a <code>BasePotential</code> or a <code>Callable</code> which takes <code>theta</code> and <code>x_o</code> as inputs.</p> required <code>proposal</code> <code>Any</code> <p>The proposal distribution.</p> required <code>theta_transform</code> <code>Optional[TorchTransform]</code> <p>Transformation that is applied to parameters. Is not used during but only when calling <code>.map()</code>.</p> <code>None</code> <code>method</code> <code>str</code> <p>Either of [<code>sir</code>|<code>importance</code>]. This sets the behavior of the <code>.sample()</code> method. With <code>sir</code>, approximate posterior samples are generated with sampling importance resampling (SIR). With <code>importance</code>, the <code>.sample()</code> method returns a tuple of samples and corresponding importance weights.</p> <code>'sir'</code> <code>oversampling_factor</code> <code>int</code> <p>Number of proposed samples from which only one is selected based on its importance weight.</p> <code>32</code> <code>max_sampling_batch_size</code> <code>int</code> <p>The batch size of samples being drawn from the proposal at every iteration.</p> <code>10000</code> <code>device</code> <code>Optional[str]</code> <p>Device on which to sample, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:0\u201d. If None, <code>potential_fn.device</code> is used.</p> <code>None</code> <code>x_shape</code> <code>Optional[Size]</code> <p>Deprecated, should not be passed.</p> <code>None</code> Source code in <code>sbi/inference/posteriors/importance_posterior.py</code> <pre><code>def __init__(\n    self,\n    potential_fn: Union[Callable, BasePotential],\n    proposal: Any,\n    theta_transform: Optional[TorchTransform] = None,\n    method: str = \"sir\",\n    oversampling_factor: int = 32,\n    max_sampling_batch_size: int = 10_000,\n    device: Optional[str] = None,\n    x_shape: Optional[torch.Size] = None,\n):\n    \"\"\"\n    Args:\n        potential_fn: The potential function from which to draw samples. Must be a\n            `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.\n        proposal: The proposal distribution.\n        theta_transform: Transformation that is applied to parameters. Is not used\n            during but only when calling `.map()`.\n        method: Either of [`sir`|`importance`]. This sets the behavior of the\n            `.sample()` method. With `sir`, approximate posterior samples are\n            generated with sampling importance resampling (SIR). With\n            `importance`, the `.sample()` method returns a tuple of samples and\n            corresponding importance weights.\n        oversampling_factor: Number of proposed samples from which only one is\n            selected based on its importance weight.\n        max_sampling_batch_size: The batch size of samples being drawn from the\n            proposal at every iteration.\n        device: Device on which to sample, e.g., \"cpu\", \"cuda\" or \"cuda:0\". If\n            None, `potential_fn.device` is used.\n        x_shape: Deprecated, should not be passed.\n    \"\"\"\n    super().__init__(\n        potential_fn,\n        theta_transform=theta_transform,\n        device=device,\n        x_shape=x_shape,\n    )\n\n    self.proposal = proposal\n    self._normalization_constant = None\n    self.method = method\n\n    self.oversampling_factor = oversampling_factor\n    self.max_sampling_batch_size = max_sampling_batch_size\n\n    self._purpose = (\n        \"It provides sampling-importance resampling (SIR) to .sample() from the \"\n        \"posterior and can evaluate the _unnormalized_ posterior density with \"\n        \".log_prob().\"\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.estimate_normalization_constant","title":"<code>estimate_normalization_constant(x, num_samples=10000, force_update=False)</code>","text":"<p>Returns the normalization constant via importance sampling.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of importance samples used for the estimate.</p> <code>10000</code> <code>force_update</code> <code>bool</code> <p>Whether to re-calculate the normlization constant when x is unchanged and have a cached value.</p> <code>False</code> Source code in <code>sbi/inference/posteriors/importance_posterior.py</code> <pre><code>@torch.no_grad()\ndef estimate_normalization_constant(\n    self, x: Tensor, num_samples: int = 10_000, force_update: bool = False\n) -&gt; Tensor:\n    \"\"\"Returns the normalization constant via importance sampling.\n\n    Args:\n        num_samples: Number of importance samples used for the estimate.\n        force_update: Whether to re-calculate the normlization constant when x is\n            unchanged and have a cached value.\n    \"\"\"\n    # Check if the provided x matches the default x (short-circuit on identity).\n    is_new_x = self.default_x is None or (\n        x is not self.default_x and (x != self.default_x).any()\n    )\n\n    not_saved_at_default_x = self._normalization_constant is None\n\n    if is_new_x:  # Calculate at x; don't save.\n        _, log_importance_weights = importance_sample(\n            self.potential_fn,\n            proposal=self.proposal,\n            num_samples=num_samples,\n        )\n        return torch.mean(torch.exp(log_importance_weights))\n    elif not_saved_at_default_x or force_update:  # Calculate at default_x; save.\n        assert self.default_x is not None\n        _, log_importance_weights = importance_sample(\n            self.potential_fn,\n            proposal=self.proposal,\n            num_samples=num_samples,\n        )\n        self._normalization_constant = torch.mean(torch.exp(log_importance_weights))\n\n    return self._normalization_constant.to(self._device)  # type: ignore\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.log_prob","title":"<code>log_prob(theta, x=None, track_gradients=False, normalization_constant_params=None)</code>","text":"<p>Returns the log-probability of theta under the posterior.</p> <p>The normalization constant is estimated with importance sampling.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>Tensor</code> <p>Parameters \\(\\theta\\).</p> required <code>track_gradients</code> <code>bool</code> <p>Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption.</p> <code>False</code> <code>normalization_constant_params</code> <code>Optional[dict]</code> <p>Parameters passed on to <code>estimate_normalization_constant()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p><code>len($\\theta$)</code>-shaped log-probability.</p> Source code in <code>sbi/inference/posteriors/importance_posterior.py</code> <pre><code>def log_prob(\n    self,\n    theta: Tensor,\n    x: Optional[Tensor] = None,\n    track_gradients: bool = False,\n    normalization_constant_params: Optional[dict] = None,\n) -&gt; Tensor:\n    r\"\"\"Returns the log-probability of theta under the posterior.\n\n    The normalization constant is estimated with importance sampling.\n\n    Args:\n        theta: Parameters $\\theta$.\n        track_gradients: Whether the returned tensor supports tracking gradients.\n            This can be helpful for e.g. sensitivity analysis, but increases memory\n            consumption.\n        normalization_constant_params: Parameters passed on to\n            `estimate_normalization_constant()`.\n\n    Returns:\n        `len($\\theta$)`-shaped log-probability.\n    \"\"\"\n    x = self._x_else_default_x(x)\n    self.potential_fn.set_x(x)\n\n    theta = ensure_theta_batched(torch.as_tensor(theta))\n\n    with torch.set_grad_enabled(track_gradients):\n        potential_values = self.potential_fn(\n            theta.to(self._device), track_gradients=track_gradients\n        )\n\n        if normalization_constant_params is None:\n            normalization_constant_params = dict()  # use defaults\n        normalization_constant = self.estimate_normalization_constant(\n            x, **normalization_constant_params\n        )\n\n        return (potential_values - torch.log(normalization_constant)).to(\n            self._device\n        )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.map","title":"<code>map(x=None, num_iter=1000, num_to_optimize=100, learning_rate=0.01, init_method='proposal', num_init_samples=1000, save_best_every=10, show_progress_bars=False, force_update=False)</code>","text":"<p>Returns the maximum-a-posteriori estimate (MAP).</p> <p>The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in <code>self._map</code> and can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization.</p> <p>Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand.</p> <p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization in unbounded space and transform the result back into bounded space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p> <code>None</code> <code>num_iter</code> <code>int</code> <p>Number of optimization steps that the algorithm takes to find the MAP.</p> <code>1000</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer.</p> <code>0.01</code> <code>init_method</code> <code>Union[str, Tensor]</code> <p>How to select the starting parameters for the optimization. If it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples the respective distribution <code>num_init_samples</code> times. If it is a tensor, the tensor will be used as init locations.</p> <code>'proposal'</code> <code>num_init_samples</code> <code>int</code> <p>Draw this number of samples from the posterior and evaluate the log-probability of all of them.</p> <code>1000</code> <code>num_to_optimize</code> <code>int</code> <p>From the drawn <code>num_init_samples</code>, use the <code>num_to_optimize</code> with highest log-probability as the initial points for the optimization.</p> <code>100</code> <code>save_best_every</code> <code>int</code> <p>The best log-probability is computed, saved in the <code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration. Computing the best log-probability creates a significant overhead (thus, the default is <code>10</code>.)</p> <code>10</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during sampling from the posterior.</p> <code>False</code> <code>force_update</code> <code>bool</code> <p>Whether to re-calculate the MAP when x is unchanged and have a cached value.</p> <code>False</code> <code>log_prob_kwargs</code> <p>Will be empty for SNLE and SNRE. Will contain {\u2018norm_posterior\u2019: True} for SNPE.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MAP estimate.</p> Source code in <code>sbi/inference/posteriors/importance_posterior.py</code> <pre><code>def map(\n    self,\n    x: Optional[Tensor] = None,\n    num_iter: int = 1_000,\n    num_to_optimize: int = 100,\n    learning_rate: float = 0.01,\n    init_method: Union[str, Tensor] = \"proposal\",\n    num_init_samples: int = 1_000,\n    save_best_every: int = 10,\n    show_progress_bars: bool = False,\n    force_update: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n    The method can be interrupted (Ctrl-C) when the user sees that the\n    log-probability converges. The best estimate will be saved in `self._map` and\n    can be accessed with `self.map()`. The MAP is obtained by running gradient\n    ascent from a given number of starting positions (samples from the posterior\n    with the highest log-probability). After the optimization is done, we select the\n    parameter set that has the highest log-probability after the optimization.\n\n    Warning: The default values used by this function are not well-tested. They\n    might require hand-tuning for the problem at hand.\n\n    For developers: if the prior is a `BoxUniform`, we carry out the optimization\n    in unbounded space and transform the result back into bounded space.\n\n    Args:\n        x: Deprecated - use `.set_default_x()` prior to `.map()`.\n        num_iter: Number of optimization steps that the algorithm takes\n            to find the MAP.\n        learning_rate: Learning rate of the optimizer.\n        init_method: How to select the starting parameters for the optimization. If\n            it is a string, it can be either [`posterior`, `prior`], which samples\n            the respective distribution `num_init_samples` times. If it is a\n            tensor, the tensor will be used as init locations.\n        num_init_samples: Draw this number of samples from the posterior and\n            evaluate the log-probability of all of them.\n        num_to_optimize: From the drawn `num_init_samples`, use the\n            `num_to_optimize` with highest log-probability as the initial points\n            for the optimization.\n        save_best_every: The best log-probability is computed, saved in the\n            `map`-attribute, and printed every `save_best_every`-th iteration.\n            Computing the best log-probability creates a significant overhead\n            (thus, the default is `10`.)\n        show_progress_bars: Whether to show a progressbar during sampling from the\n            posterior.\n        force_update: Whether to re-calculate the MAP when x is unchanged and\n            have a cached value.\n        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n            {'norm_posterior': True} for SNPE.\n\n    Returns:\n        The MAP estimate.\n    \"\"\"\n    return super().map(\n        x=x,\n        num_iter=num_iter,\n        num_to_optimize=num_to_optimize,\n        learning_rate=learning_rate,\n        init_method=init_method,\n        num_init_samples=num_init_samples,\n        save_best_every=save_best_every,\n        show_progress_bars=show_progress_bars,\n        force_update=force_update,\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.sample","title":"<code>sample(sample_shape=torch.Size(), x=None, method=None, oversampling_factor=32, max_sampling_batch_size=10000, sample_with=None, show_progress_bars=False)</code>","text":"<p>Return samples from the approximate posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Shape</code> <p>Shape of samples that are drawn from posterior.</p> <code>Size()</code> <code>x</code> <code>Optional[Tensor]</code> <p>Observed data.</p> <code>None</code> <code>method</code> <code>Optional[str]</code> <p>Either of [<code>sir</code>|<code>importance</code>]. This sets the behavior of the <code>.sample()</code> method. With <code>sir</code>, approximate posterior samples are generated with sampling importance resampling (SIR). With <code>importance</code>, the <code>.sample()</code> method returns a tuple of samples and corresponding importance weights.</p> <code>None</code> <code>oversampling_factor</code> <code>int</code> <p>Number of proposed samples from which only one is selected based on its importance weight.</p> <code>32</code> <code>max_sampling_batch_size</code> <code>int</code> <p>The batch size of samples being drawn from the proposal at every iteration.</p> <code>10000</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during sampling.</p> <code>False</code> Source code in <code>sbi/inference/posteriors/importance_posterior.py</code> <pre><code>def sample(\n    self,\n    sample_shape: Shape = torch.Size(),\n    x: Optional[Tensor] = None,\n    method: Optional[str] = None,\n    oversampling_factor: int = 32,\n    max_sampling_batch_size: int = 10_000,\n    sample_with: Optional[str] = None,\n    show_progress_bars: bool = False,\n) -&gt; Union[Tensor, Tuple[Tensor, Tensor]]:\n    \"\"\"Return samples from the approximate posterior distribution.\n\n    Args:\n        sample_shape: Shape of samples that are drawn from posterior.\n        x: Observed data.\n        method: Either of [`sir`|`importance`]. This sets the behavior of the\n            `.sample()` method. With `sir`, approximate posterior samples are\n            generated with sampling importance resampling (SIR). With\n            `importance`, the `.sample()` method returns a tuple of samples and\n            corresponding importance weights.\n        oversampling_factor: Number of proposed samples from which only one is\n            selected based on its importance weight.\n        max_sampling_batch_size: The batch size of samples being drawn from the\n            proposal at every iteration.\n        show_progress_bars: Whether to show a progressbar during sampling.\n    \"\"\"\n\n    method = self.method if method is None else method\n\n    if sample_with is not None:\n        raise ValueError(\n            f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting \"\n            f\"`sample_with` is no longer supported. You have to rerun \"\n            f\"`.build_posterior(sample_with={sample_with}).`\"\n        )\n\n    self.potential_fn.set_x(self._x_else_default_x(x))\n\n    if method == \"sir\":\n        return self._sir_sample(\n            sample_shape,\n            oversampling_factor=oversampling_factor,\n            max_sampling_batch_size=max_sampling_batch_size,\n            show_progress_bars=show_progress_bars,\n        )\n    elif method == \"importance\":\n        return self._importance_sample(sample_shape)\n    else:\n        raise NameError\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior","title":"<code>MCMCPosterior</code>","text":"<p>               Bases: <code>NeuralPosterior</code></p> <p>Provides MCMC to sample from the posterior. SNLE or SNRE train neural networks to approximate the likelihood(-ratios). <code>MCMCPosterior</code> allows to sample from the posterior with MCMC.</p> Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code> <pre><code>class MCMCPosterior(NeuralPosterior):\n    r\"\"\"Provides MCMC to sample from the posterior.&lt;br/&gt;&lt;br/&gt;\n    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).\n    `MCMCPosterior` allows to sample from the posterior with MCMC.\n    \"\"\"\n\n    def __init__(\n        self,\n        potential_fn: Union[Callable, BasePotential],\n        proposal: Any,\n        theta_transform: Optional[TorchTransform] = None,\n        method: str = \"slice_np_vectorized\",\n        thin: int = -1,\n        warmup_steps: int = 200,\n        num_chains: int = 20,\n        init_strategy: str = \"resample\",\n        init_strategy_parameters: Optional[Dict[str, Any]] = None,\n        init_strategy_num_candidates: Optional[int] = None,\n        num_workers: int = 1,\n        mp_context: str = \"spawn\",\n        device: Optional[str] = None,\n        x_shape: Optional[torch.Size] = None,\n    ):\n        \"\"\"\n        Args:\n            potential_fn: The potential function from which to draw samples. Must be a\n                `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.\n            proposal: Proposal distribution that is used to initialize the MCMC chain.\n            theta_transform: Transformation that will be applied during sampling.\n                Allows to perform MCMC in unconstrained space.\n            method: Method used for MCMC sampling, one of `slice_np`,\n                `slice_np_vectorized`, `hmc_pyro`, `nuts_pyro`, `slice_pymc`,\n                `hmc_pymc`, `nuts_pymc`. `slice_np` is a custom\n                numpy implementation of slice sampling. `slice_np_vectorized` is\n                identical to `slice_np`, but if `num_chains&gt;1`, the chains are\n                vectorized for `slice_np_vectorized` whereas they are run sequentially\n                for `slice_np`. The samplers ending on `_pyro` are using Pyro, and\n                likewise the samplers ending on `_pymc` are using PyMC.\n            thin: The thinning factor for the chain, default 1 (no thinning).\n            warmup_steps: The initial number of samples to discard.\n            num_chains: The number of chains. Should generally be at most\n                `num_workers - 1`.\n            init_strategy: The initialisation strategy for chains; `proposal` will draw\n                init locations from `proposal`, whereas `sir` will use Sequential-\n                Importance-Resampling (SIR). SIR initially samples\n                `init_strategy_num_candidates` from the `proposal`, evaluates all of\n                them under the `potential_fn` and `proposal`, and then resamples the\n                initial locations with weights proportional to `exp(potential_fn -\n                proposal.log_prob`. `resample` is the same as `sir` but\n                uses `exp(potential_fn)` as weights.\n            init_strategy_parameters: Dictionary of keyword arguments passed to the\n                init strategy, e.g., for `init_strategy=sir` this could be\n                `num_candidate_samples`, i.e., the number of candidates to find init\n                locations (internal default is `1000`), or `device`.\n            init_strategy_num_candidates: Number of candidates to find init\n                 locations in `init_strategy=sir` (deprecated, use\n                 init_strategy_parameters instead).\n            num_workers: number of cpu cores used to parallelize mcmc\n            mp_context: Multiprocessing start method, either `\"fork\"` or `\"spawn\"`\n                (default), used by Pyro and PyMC samplers. `\"fork\"` can be significantly\n                faster than `\"spawn\"` but is only supported on POSIX-based systems\n                (e.g. Linux and macOS, not Windows).\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:0\". If None,\n                `potential_fn.device` is used.\n            x_shape: Deprecated, should not be passed.\n        \"\"\"\n        if method == \"slice\":\n            warn(\n                \"The Pyro-based slice sampler is deprecated, and the method `slice` \"\n                \"has been changed to `slice_np`, i.e., the custom \"\n                \"numpy-based slice sampler.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            method = \"slice_np\"\n\n        thin = _process_thin_default(thin)\n\n        super().__init__(\n            potential_fn,\n            theta_transform=theta_transform,\n            device=device,\n            x_shape=x_shape,\n        )\n\n        self.proposal = proposal\n        self.method = method\n        self.thin = thin\n        self.warmup_steps = warmup_steps\n        self.num_chains = num_chains\n        self.init_strategy = init_strategy\n        self.init_strategy_parameters = init_strategy_parameters or {}\n        self.num_workers = num_workers\n        self.mp_context = mp_context\n        self._posterior_sampler = None\n        # Hardcode parameter name to reduce clutter kwargs.\n        self.param_name = \"theta\"\n\n        if init_strategy_num_candidates is not None:\n            warn(\n                \"Passing `init_strategy_num_candidates` is deprecated as of sbi \"\n                \"v0.19.0. Instead, use e.g., `init_strategy_parameters \"\n                f\"={'num_candidate_samples': 1000}`\",\n                stacklevel=2,\n            )\n            self.init_strategy_parameters[\"num_candidate_samples\"] = (\n                init_strategy_num_candidates\n            )\n\n        self.potential_ = self._prepare_potential(method)\n\n        self._purpose = (\n            \"It provides MCMC to .sample() from the posterior and \"\n            \"can evaluate the _unnormalized_ posterior density with .log_prob().\"\n        )\n\n    @property\n    def mcmc_method(self) -&gt; str:\n        \"\"\"Returns MCMC method.\"\"\"\n        return self._mcmc_method\n\n    @mcmc_method.setter\n    def mcmc_method(self, method: str) -&gt; None:\n        \"\"\"See `set_mcmc_method`.\"\"\"\n        self.set_mcmc_method(method)\n\n    @property\n    def posterior_sampler(self):\n        \"\"\"Returns sampler created by `sample`.\"\"\"\n        return self._posterior_sampler\n\n    def set_mcmc_method(self, method: str) -&gt; \"NeuralPosterior\":\n        \"\"\"Sets sampling method to for MCMC and returns `NeuralPosterior`.\n\n        Args:\n            method: Method to use.\n\n        Returns:\n            `NeuralPosterior` for chainable calls.\n        \"\"\"\n        self._mcmc_method = method\n        return self\n\n    def log_prob(\n        self, theta: Tensor, x: Optional[Tensor] = None, track_gradients: bool = False\n    ) -&gt; Tensor:\n        r\"\"\"Returns the log-probability of theta under the posterior.\n\n        Args:\n            theta: Parameters $\\theta$.\n            track_gradients: Whether the returned tensor supports tracking gradients.\n                This can be helpful for e.g. sensitivity analysis, but increases memory\n                consumption.\n\n        Returns:\n            `len($\\theta$)`-shaped log-probability.\n        \"\"\"\n        warn(\n            \"`.log_prob()` is deprecated for methods that can only evaluate the \"\n            \"log-probability up to a normalizing constant. Use `.potential()` instead.\",\n            stacklevel=2,\n        )\n        warn(\"The log-probability is unnormalized!\", stacklevel=2)\n\n        self.potential_fn.set_x(self._x_else_default_x(x))\n\n        theta = ensure_theta_batched(torch.as_tensor(theta))\n        return self.potential_fn(\n            theta.to(self._device), track_gradients=track_gradients\n        )\n\n    def sample(\n        self,\n        sample_shape: Shape = torch.Size(),\n        x: Optional[Tensor] = None,\n        method: Optional[str] = None,\n        thin: Optional[int] = None,\n        warmup_steps: Optional[int] = None,\n        num_chains: Optional[int] = None,\n        init_strategy: Optional[str] = None,\n        init_strategy_parameters: Optional[Dict[str, Any]] = None,\n        init_strategy_num_candidates: Optional[int] = None,\n        mcmc_parameters: Optional[Dict] = None,\n        mcmc_method: Optional[str] = None,\n        sample_with: Optional[str] = None,\n        num_workers: Optional[int] = None,\n        mp_context: Optional[str] = None,\n        show_progress_bars: bool = True,\n    ) -&gt; Tensor:\n        r\"\"\"Return samples from posterior distribution $p(\\theta|x)$ with MCMC.\n\n        Check the `__init__()` method for a description of all arguments as well as\n        their default values.\n\n        Args:\n            sample_shape: Desired shape of samples that are drawn from posterior. If\n                sample_shape is multidimensional we simply draw `sample_shape.numel()`\n                samples and then reshape into the desired shape.\n            mcmc_parameters: Dictionary that is passed only to support the API of\n                `sbi` v0.17.2 or older.\n            mcmc_method: This argument only exists to keep backward-compatibility with\n                `sbi` v0.17.2 or older. Please use `method` instead.\n            sample_with: This argument only exists to keep backward-compatibility with\n                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.\n            show_progress_bars: Whether to show sampling progress monitor.\n\n        Returns:\n            Samples from posterior.\n        \"\"\"\n\n        self.potential_fn.set_x(self._x_else_default_x(x))\n\n        # Replace arguments that were not passed with their default.\n        method = self.method if method is None else method\n        thin = self.thin if thin is None else thin\n        warmup_steps = self.warmup_steps if warmup_steps is None else warmup_steps\n        num_chains = self.num_chains if num_chains is None else num_chains\n        init_strategy = self.init_strategy if init_strategy is None else init_strategy\n        num_workers = self.num_workers if num_workers is None else num_workers\n        mp_context = self.mp_context if mp_context is None else mp_context\n        init_strategy_parameters = (\n            self.init_strategy_parameters\n            if init_strategy_parameters is None\n            else init_strategy_parameters\n        )\n        if init_strategy_num_candidates is not None:\n            warn(\n                \"Passing `init_strategy_num_candidates` is deprecated as of sbi \"\n                \"v0.19.0. Instead, use e.g., \"\n                f\"`init_strategy_parameters={'num_candidate_samples': 1000}`\",\n                stacklevel=2,\n            )\n            self.init_strategy_parameters[\"num_candidate_samples\"] = (\n                init_strategy_num_candidates\n            )\n        if sample_with is not None:\n            raise ValueError(\n                f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting \"\n                \"`sample_with` is no longer supported. You have to rerun \"\n                f\"`.build_posterior(sample_with={sample_with}).`\"\n            )\n        if mcmc_method is not None:\n            warn(\n                \"You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this \"\n                \"is deprecated and will be removed in a future release. Use `method` \"\n                \"instead of `mcmc_method`.\",\n                stacklevel=2,\n            )\n            method = mcmc_method\n        if mcmc_parameters:\n            warn(\n                \"You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this \"\n                \"is deprecated and will be removed in a future release. Instead, pass \"\n                \"the variable to `.sample()` directly, e.g. \"\n                \"`posterior.sample((1,), num_chains=5)`.\",\n                stacklevel=2,\n            )\n        # The following lines are only for backwards compatibility with sbi v0.17.2 or\n        # older.\n        m_p = mcmc_parameters or {}  # define to shorten the variable name\n        method = _maybe_use_dict_entry(method, \"mcmc_method\", m_p)\n        thin = _maybe_use_dict_entry(thin, \"thin\", m_p)\n        warmup_steps = _maybe_use_dict_entry(warmup_steps, \"warmup_steps\", m_p)\n        num_chains = _maybe_use_dict_entry(num_chains, \"num_chains\", m_p)\n        init_strategy = _maybe_use_dict_entry(init_strategy, \"init_strategy\", m_p)\n        self.potential_ = self._prepare_potential(method)  # type: ignore\n\n        initial_params = self._get_initial_params(\n            init_strategy,  # type: ignore\n            num_chains,  # type: ignore\n            num_workers,\n            show_progress_bars,\n            **init_strategy_parameters,\n        )\n        num_samples = torch.Size(sample_shape).numel()\n\n        track_gradients = method in (\"hmc_pyro\", \"nuts_pyro\", \"hmc_pymc\", \"nuts_pymc\")\n        with torch.set_grad_enabled(track_gradients):\n            if method in (\"slice_np\", \"slice_np_vectorized\"):\n                transformed_samples = self._slice_np_mcmc(\n                    num_samples=num_samples,\n                    potential_function=self.potential_,\n                    initial_params=initial_params,\n                    thin=thin,  # type: ignore\n                    warmup_steps=warmup_steps,  # type: ignore\n                    vectorized=(method == \"slice_np_vectorized\"),\n                    interchangeable_chains=True,\n                    num_workers=num_workers,\n                    show_progress_bars=show_progress_bars,\n                )\n            elif method in (\"hmc_pyro\", \"nuts_pyro\"):\n                transformed_samples = self._pyro_mcmc(\n                    num_samples=num_samples,\n                    potential_function=self.potential_,\n                    initial_params=initial_params,\n                    mcmc_method=method,  # type: ignore\n                    thin=thin,  # type: ignore\n                    warmup_steps=warmup_steps,  # type: ignore\n                    num_chains=num_chains,\n                    show_progress_bars=show_progress_bars,\n                    mp_context=mp_context,\n                )\n            elif method in (\"hmc_pymc\", \"nuts_pymc\", \"slice_pymc\"):\n                transformed_samples = self._pymc_mcmc(\n                    num_samples=num_samples,\n                    potential_function=self.potential_,\n                    initial_params=initial_params,\n                    mcmc_method=method,  # type: ignore\n                    thin=thin,  # type: ignore\n                    warmup_steps=warmup_steps,  # type: ignore\n                    num_chains=num_chains,\n                    show_progress_bars=show_progress_bars,\n                    mp_context=mp_context,\n                )\n            else:\n                raise NameError(f\"The sampling method {method} is not implemented!\")\n\n        samples = self.theta_transform.inv(transformed_samples)\n        # NOTE: Currently MCMCPosteriors will require a single dimension for the\n        # parameter dimension. With recent ConditionalDensity(Ratio) estimators, we\n        # can have multiple dimensions for the parameter dimension.\n        samples = samples.reshape((*sample_shape, -1))  # type: ignore\n\n        return samples\n\n    def sample_batched(\n        self,\n        sample_shape: Shape,\n        x: Tensor,\n        method: Optional[str] = None,\n        thin: Optional[int] = None,\n        warmup_steps: Optional[int] = None,\n        num_chains: Optional[int] = None,\n        init_strategy: Optional[str] = None,\n        init_strategy_parameters: Optional[Dict[str, Any]] = None,\n        num_workers: Optional[int] = None,\n        mp_context: Optional[str] = None,\n        show_progress_bars: bool = True,\n    ) -&gt; Tensor:\n        r\"\"\"Given a batch of observations [x_1, ..., x_B] this function samples from\n        posteriors $p(\\theta|x_1)$, ... ,$p(\\theta|x_B)$, in a batched (i.e. vectorized)\n        manner.\n\n        Check the `__init__()` method for a description of all arguments as well as\n        their default values.\n\n        Args:\n            sample_shape: Desired shape of samples that are drawn from the posterior\n                given every observation.\n            x: A batch of observations, of shape `(batch_dim, event_shape_x)`.\n                `batch_dim` corresponds to the number of observations to be\n                drawn.\n            method: Method used for MCMC sampling, e.g., \"slice_np_vectorized\".\n            thin: The thinning factor for the chain, default 1 (no thinning).\n            warmup_steps: The initial number of samples to discard.\n            num_chains: The number of chains used for each `x` passed in the batch.\n            init_strategy: The initialisation strategy for chains.\n            init_strategy_parameters: Dictionary of keyword arguments passed to\n                the init strategy.\n            num_workers: number of cpu cores used to parallelize initial\n                parameter generation and mcmc sampling.\n            mp_context: Multiprocessing start method, either `\"fork\"` or `\"spawn\"`\n            show_progress_bars: Whether to show sampling progress monitor.\n\n        Returns:\n            Samples from the posteriors of shape (*sample_shape, B, *input_shape)\n        \"\"\"\n\n        # Replace arguments that were not passed with their default.\n        method = self.method if method is None else method\n        thin = self.thin if thin is None else thin\n        warmup_steps = self.warmup_steps if warmup_steps is None else warmup_steps\n        num_chains = self.num_chains if num_chains is None else num_chains\n        init_strategy = self.init_strategy if init_strategy is None else init_strategy\n        num_workers = self.num_workers if num_workers is None else num_workers\n        mp_context = self.mp_context if mp_context is None else mp_context\n        init_strategy_parameters = (\n            self.init_strategy_parameters\n            if init_strategy_parameters is None\n            else init_strategy_parameters\n        )\n\n        assert (\n            method == \"slice_np_vectorized\"\n        ), \"Batched sampling only supported for vectorized samplers!\"\n\n        # warn if num_chains is larger than num requested samples\n        if num_chains &gt; torch.Size(sample_shape).numel():\n            warnings.warn(\n                \"The passed number of MCMC chains is larger than the number of \"\n                f\"requested samples: {num_chains} &gt; {torch.Size(sample_shape).numel()},\"\n                f\" resetting it to {torch.Size(sample_shape).numel()}.\",\n                stacklevel=2,\n            )\n            num_chains = torch.Size(sample_shape).numel()\n\n        # custom shape handling to make sure to match the batch size of x and theta\n        # without unnecessary combinations.\n        if len(x.shape) == 1:\n            x = x.unsqueeze(0)\n        batch_size = x.shape[0]\n\n        x = reshape_to_batch_event(x, event_shape=x.shape[1:])\n\n        # For batched sampling, we want `num_chains` for each observation in the batch.\n        # Here we repeat the observations ABC -&gt; AAABBBCCC, so that the chains are\n        # in the order of the observations.\n        x_ = x.repeat_interleave(num_chains, dim=0)\n\n        self.potential_fn.set_x(x_, x_is_iid=False)\n        self.potential_ = self._prepare_potential(method)  # type: ignore\n\n        # For each observation in the batch, we have num_chains independent chains.\n        num_chains_extended = batch_size * num_chains\n        if num_chains_extended &gt; 100:\n            warnings.warn(\n                \"Note that for batched sampling, we use num_chains many chains for each\"\n                \" x in the batch. With the given settings, this results in a large \"\n                f\"number large number of chains ({num_chains_extended}), which can be \"\n                \"slow and memory-intensive for vectorized MCMC. Consider reducing the \"\n                \"number of chains.\",\n                stacklevel=2,\n            )\n        init_strategy_parameters[\"num_return_samples\"] = num_chains_extended\n        initial_params = self._get_initial_params_batched(\n            x,\n            init_strategy,  # type: ignore\n            num_chains,  # type: ignore\n            num_workers,\n            show_progress_bars,\n            **init_strategy_parameters,\n        )\n        # We need num_samples from each posterior in the batch\n        num_samples = torch.Size(sample_shape).numel() * batch_size\n\n        with torch.set_grad_enabled(False):\n            transformed_samples = self._slice_np_mcmc(\n                num_samples=num_samples,\n                potential_function=self.potential_,\n                initial_params=initial_params,\n                thin=thin,  # type: ignore\n                warmup_steps=warmup_steps,  # type: ignore\n                vectorized=(method == \"slice_np_vectorized\"),\n                interchangeable_chains=False,\n                num_workers=num_workers,\n                show_progress_bars=show_progress_bars,\n            )\n\n        # (num_chains_extended, samples_per_chain, *input_shape)\n        samples_per_chain: Tensor = self.theta_transform.inv(transformed_samples)  # type: ignore\n        dim_theta = samples_per_chain.shape[-1]\n        # We need to collect samples for each x from the respective chains.\n        # However, using samples.reshape(*sample_shape, batch_size, dim_theta)\n        # does not combine the samples in the right order, since this mixes\n        # samples that belong to different `x`. The following permute is a\n        # workaround to reshape the samples in the right order.\n        samples_per_x = samples_per_chain.reshape((\n            batch_size,\n            # We are flattening the sample shape here using -1 because we might have\n            # generated more samples than requested (more chains, or multiple of\n            # chains not matching sample_shape)\n            -1,\n            dim_theta,\n        )).permute(1, 0, -1)\n\n        # Shape is now (-1, batch_size, dim_theta)\n        # We can now select the number of requested samples\n        samples = samples_per_x[: torch.Size(sample_shape).numel()]\n        # and reshape into (*sample_shape, batch_size, dim_theta)\n        samples = samples.reshape((*sample_shape, batch_size, dim_theta))\n        return samples\n\n    def _build_mcmc_init_fn(\n        self,\n        proposal: Any,\n        potential_fn: Callable,\n        transform: torch_tf.Transform,\n        init_strategy: str,\n        **kwargs,\n    ) -&gt; Callable:\n        \"\"\"Return function that, when called, creates an initial parameter set for MCMC.\n\n        Args:\n            proposal: Proposal distribution.\n            potential_fn: Potential function that the candidate samples are weighted\n                with.\n            init_strategy: Specifies the initialization method. Either of\n                [`proposal`|`sir`|`resample`|`latest_sample`].\n            kwargs: Passed on to init function. This way, init specific keywords can\n                be set through `mcmc_parameters`. Unused arguments will be absorbed by\n                the intitialization method.\n\n        Returns: Initialization function.\n        \"\"\"\n        if init_strategy == \"proposal\" or init_strategy == \"prior\":\n            if init_strategy == \"prior\":\n                warn(\n                    \"You set `init_strategy=prior`. As of sbi v0.18.0, this is \"\n                    \"deprecated and it will be removed in a future release. Use \"\n                    \"`init_strategy=proposal` instead.\",\n                    stacklevel=2,\n                )\n            return lambda: proposal_init(proposal, transform=transform, **kwargs)\n        elif init_strategy == \"sir\":\n            warn(\n                \"As of sbi v0.19.0, the behavior of the SIR initialization for MCMC \"\n                \"has changed. If you wish to restore the behavior of sbi v0.18.0, set \"\n                \"`init_strategy='resample'.`\",\n                stacklevel=2,\n            )\n            return lambda: sir_init(\n                proposal, potential_fn, transform=transform, **kwargs\n            )\n        elif init_strategy == \"resample\":\n            return lambda: resample_given_potential_fn(\n                proposal, potential_fn, transform=transform, **kwargs\n            )\n        elif init_strategy == \"latest_sample\":\n            latest_sample = IterateParameters(self._mcmc_init_params, **kwargs)\n            return latest_sample\n        else:\n            raise NotImplementedError\n\n    def _get_initial_params(\n        self,\n        init_strategy: str,\n        num_chains: int,\n        num_workers: int,\n        show_progress_bars: bool,\n        **kwargs,\n    ) -&gt; Tensor:\n        \"\"\"Return initial parameters for MCMC obtained with given init strategy.\n\n        Parallelizes across CPU cores only for resample and SIR.\n\n        Args:\n            init_strategy: Specifies the initialization method. Either of\n                [`proposal`|`sir`|`resample`|`latest_sample`].\n            num_chains: number of MCMC chains, generates initial params for each\n            num_workers: number of CPU cores for parallization\n            show_progress_bars: whether to show progress bars for SIR init\n            kwargs: Passed on to `_build_mcmc_init_fn`.\n\n        Returns:\n            Tensor: initial parameters, one for each chain\n        \"\"\"\n        # Build init function\n        init_fn = self._build_mcmc_init_fn(\n            self.proposal,\n            self.potential_fn,\n            transform=self.theta_transform,\n            init_strategy=init_strategy,  # type: ignore\n            **kwargs,\n        )\n\n        # Parallelize inits for resampling only.\n        if num_workers &gt; 1 and (init_strategy == \"resample\" or init_strategy == \"sir\"):\n\n            def seeded_init_fn(seed):\n                torch.manual_seed(seed)\n                return init_fn()\n\n            seeds = torch.randint(high=2**31, size=(num_chains,))\n\n            # Generate initial params parallelized over num_workers.\n            initial_params = list(\n                tqdm(\n                    Parallel(return_as=\"generator\", n_jobs=num_workers)(\n                        delayed(seeded_init_fn)(seed) for seed in seeds\n                    ),\n                    total=len(seeds),\n                    desc=f\"\"\"Generating {num_chains} MCMC inits with\n                            {num_workers} workers.\"\"\",\n                    disable=not show_progress_bars,\n                )\n            )\n            initial_params = torch.cat(initial_params)  # type: ignore\n        else:\n            initial_params = torch.cat(\n                [init_fn() for _ in range(num_chains)]  # type: ignore\n            )\n        return initial_params\n\n    def _get_initial_params_batched(\n        self,\n        x: torch.Tensor,\n        init_strategy: str,\n        num_chains_per_x: int,\n        num_workers: int,\n        show_progress_bars: bool,\n        **kwargs,\n    ) -&gt; Tensor:\n        \"\"\"Return initial parameters for MCMC for a batch of `x`, obtained with given\n           init strategy.\n\n        Parallelizes across CPU cores only for resample and SIR.\n\n        Args:\n            x: Batch of observations to create different initial parameters for.\n            init_strategy: Specifies the initialization method. Either of\n                [`proposal`|`sir`|`resample`|`latest_sample`].\n            num_chains_per_x: number of MCMC chains for each x, generates initial params\n                for each x\n            num_workers: number of CPU cores for parallization\n            show_progress_bars: whether to show progress bars for SIR init\n            kwargs: Passed on to `_build_mcmc_init_fn`.\n\n        Returns:\n            Tensor: initial parameters, one for each chain\n        \"\"\"\n\n        potential_ = deepcopy(self.potential_fn)\n        initial_params = []\n        init_fn = self._build_mcmc_init_fn(\n            self.proposal,\n            potential_fn=potential_,\n            transform=self.theta_transform,\n            init_strategy=init_strategy,  # type: ignore\n            **kwargs,\n        )\n        for xi in x:\n            # Build init function\n            potential_.set_x(xi)\n\n            # Parallelize inits for resampling or sir.\n            if num_workers &gt; 1 and (\n                init_strategy == \"resample\" or init_strategy == \"sir\"\n            ):\n\n                def seeded_init_fn(seed):\n                    torch.manual_seed(seed)\n                    return init_fn()\n\n                seeds = torch.randint(high=2**31, size=(num_chains_per_x,))\n\n                # Generate initial params parallelized over num_workers.\n                initial_params = initial_params + list(\n                    tqdm(\n                        Parallel(return_as=\"generator\", n_jobs=num_workers)(\n                            delayed(seeded_init_fn)(seed) for seed in seeds\n                        ),\n                        total=len(seeds),\n                        desc=f\"\"\"Generating {num_chains_per_x} MCMC inits with\n                                {num_workers} workers.\"\"\",\n                        disable=not show_progress_bars,\n                    )\n                )\n\n            else:\n                initial_params = initial_params + [\n                    init_fn() for _ in range(num_chains_per_x)\n                ]  # type: ignore\n\n        initial_params = torch.cat(initial_params)\n        return initial_params\n\n    def _slice_np_mcmc(\n        self,\n        num_samples: int,\n        potential_function: Callable,\n        initial_params: Tensor,\n        thin: int,\n        warmup_steps: int,\n        vectorized: bool = False,\n        interchangeable_chains=True,\n        num_workers: int = 1,\n        init_width: Union[float, ndarray] = 0.01,\n        show_progress_bars: bool = True,\n    ) -&gt; Tensor:\n        \"\"\"Custom implementation of slice sampling using Numpy.\n\n        Args:\n            num_samples: Desired number of samples.\n            potential_function: A callable **class**.\n            initial_params: Initial parameters for MCMC chain.\n            thin: Thinning (subsampling) factor, default 1 (no thinning).\n            warmup_steps: Initial number of samples to discard.\n            vectorized: Whether to use a vectorized implementation of the\n                `SliceSampler`.\n            interchangeable_chains: Whether chains are interchangeable, i.e., whether\n                we can mix samples between chains.\n            num_workers: Number of CPU cores to use.\n            init_width: Inital width of brackets.\n            show_progress_bars: Whether to show a progressbar during sampling;\n                can only be turned off for vectorized sampler.\n\n        Returns:\n            Tensor of shape (num_samples, shape_of_single_theta).\n        \"\"\"\n\n        num_chains, dim_samples = initial_params.shape\n\n        if not vectorized:\n            SliceSamplerMultiChain = SliceSamplerSerial\n        else:\n            SliceSamplerMultiChain = SliceSamplerVectorized\n\n        def multi_obs_potential(params):\n            # Params are of shape (num_chains * num_obs, event).\n            all_potentials = potential_function(params)  # Shape: (num_chains, num_obs)\n            return all_potentials.flatten()\n\n        posterior_sampler = SliceSamplerMultiChain(\n            init_params=tensor2numpy(initial_params),\n            log_prob_fn=multi_obs_potential,\n            num_chains=num_chains,\n            thin=thin,\n            verbose=show_progress_bars,\n            num_workers=num_workers,\n            init_width=init_width,\n        )\n        warmup_ = warmup_steps * thin\n        num_samples_ = ceil((num_samples * thin) / num_chains)\n        # Run mcmc including warmup\n        samples = posterior_sampler.run(warmup_ + num_samples_)\n        samples = samples[:, warmup_steps:, :]  # discard warmup steps\n        samples = torch.from_numpy(samples)  # chains x samples x dim\n\n        # Save posterior sampler.\n        self._posterior_sampler = posterior_sampler\n\n        # Save sample as potential next init (if init_strategy == 'latest_sample').\n        self._mcmc_init_params = samples[:, -1, :].reshape(num_chains, dim_samples)\n\n        # Update: If chains are interchangeable, return concatenated samples. Otherwise\n        # return samples per chain.\n        if interchangeable_chains:\n            # Collect samples from all chains.\n            samples = samples.reshape(-1, dim_samples)[:num_samples]\n\n        return samples.type(torch.float32).to(self._device)\n\n    def _pyro_mcmc(\n        self,\n        num_samples: int,\n        potential_function: Callable,\n        initial_params: Tensor,\n        mcmc_method: str = \"nuts_pyro\",\n        thin: int = -1,\n        warmup_steps: int = 200,\n        num_chains: Optional[int] = 1,\n        show_progress_bars: bool = True,\n        mp_context: str = \"spawn\",\n    ) -&gt; Tensor:\n        r\"\"\"Return samples obtained using Pyro's HMC or NUTS sampler.\n\n        Args:\n            num_samples: Desired number of samples.\n            potential_function: A callable **class**. A class, but not a function,\n                is picklable for Pyro MCMC to use it across chains in parallel,\n                even when the potential function requires evaluating a neural network.\n            initial_params: Initial parameters for MCMC chain.\n            mcmc_method: Pyro MCMC method to use, either `\"hmc_pyro\"` or\n                `\"nuts_pyro\"` (default).\n            thin: Thinning (subsampling) factor, default 1 (no thinning).\n            warmup_steps: Initial number of samples to discard.\n            num_chains: Whether to sample in parallel. If None, use all but one CPU.\n            show_progress_bars: Whether to show a progressbar during sampling.\n\n        Returns:\n            Tensor of shape (num_samples, shape_of_single_theta).\n        \"\"\"\n        thin = _process_thin_default(thin)\n        num_chains = mp.cpu_count() - 1 if num_chains is None else num_chains\n        kernels = dict(hmc_pyro=HMC, nuts_pyro=NUTS)\n\n        sampler = MCMC(\n            kernel=kernels[mcmc_method](potential_fn=potential_function),\n            num_samples=ceil((thin * num_samples) / num_chains),\n            warmup_steps=warmup_steps,\n            initial_params={self.param_name: initial_params},\n            num_chains=num_chains,\n            mp_context=mp_context,\n            disable_progbar=not show_progress_bars,\n            transforms={},\n        )\n        sampler.run()\n        samples = next(iter(sampler.get_samples().values())).reshape(\n            -1,\n            initial_params.shape[1],  # .shape[1] = dim of theta\n        )\n\n        # Save posterior sampler.\n        self._posterior_sampler = sampler\n\n        samples = samples[::thin][:num_samples]\n\n        return samples.detach()\n\n    def _pymc_mcmc(\n        self,\n        num_samples: int,\n        potential_function: Callable,\n        initial_params: Tensor,\n        mcmc_method: str = \"nuts_pymc\",\n        thin: int = -1,\n        warmup_steps: int = 200,\n        num_chains: Optional[int] = 1,\n        show_progress_bars: bool = True,\n        mp_context: str = \"spawn\",\n    ) -&gt; Tensor:\n        r\"\"\"Return samples obtained using PyMC's HMC, NUTS or slice samplers.\n\n        Args:\n            num_samples: Desired number of samples.\n            potential_function: A callable **class**. A class, but not a function,\n                is picklable for PyMC MCMC to use it across chains in parallel,\n                even when the potential function requires evaluating a neural network.\n            initial_params: Initial parameters for MCMC chain.\n            mcmc_method: mcmc_method: Pyro MCMC method to use, either `\"hmc_pymc\"` or\n                `\"slice_pymc\"`, or `\"nuts_pymc\"` (default).\n            thin: Thinning (subsampling) factor, default 1 (no thinning).\n            warmup_steps: Initial number of samples to discard.\n            num_chains: Whether to sample in parallel. If None, use all but one CPU.\n            show_progress_bars: Whether to show a progressbar during sampling.\n\n        Returns:\n            Tensor of shape (num_samples, shape_of_single_theta).\n        \"\"\"\n        thin = _process_thin_default(thin)\n        num_chains = mp.cpu_count() - 1 if num_chains is None else num_chains\n        steps = dict(slice_pymc=\"slice\", hmc_pymc=\"hmc\", nuts_pymc=\"nuts\")\n\n        sampler = PyMCSampler(\n            potential_fn=potential_function,\n            step=steps[mcmc_method],\n            initvals=tensor2numpy(initial_params),\n            draws=ceil((thin * num_samples) / num_chains),\n            tune=warmup_steps,\n            chains=num_chains,\n            mp_ctx=mp_context,\n            progressbar=show_progress_bars,\n            param_name=self.param_name,\n            device=self._device,\n        )\n        samples = sampler.run()\n        samples = torch.from_numpy(samples).to(dtype=torch.float32, device=self._device)\n        samples = samples.reshape(-1, initial_params.shape[1])\n\n        # Save posterior sampler.\n        self._posterior_sampler = sampler\n\n        samples = samples[::thin][:num_samples]\n\n        return samples\n\n    def _prepare_potential(self, method: str) -&gt; Callable:\n        \"\"\"Combines potential and transform and takes care of gradients and pyro.\n\n        Args:\n            method: Which MCMC method to use.\n\n        Returns:\n            A potential function that is ready to be used in MCMC.\n        \"\"\"\n        if method in (\"hmc_pyro\", \"nuts_pyro\"):\n            track_gradients = True\n            pyro = True\n        elif method in (\"hmc_pymc\", \"nuts_pymc\"):\n            track_gradients = True\n            pyro = False\n        elif method in (\"slice_np\", \"slice_np_vectorized\", \"slice_pymc\"):\n            track_gradients = False\n            pyro = False\n        else:\n            if \"hmc\" in method or \"nuts\" in method:\n                warn(\n                    \"The kwargs 'hmc' and 'nuts' are deprecated. Use 'hmc_pyro', \"\n                    \"'nuts_pyro', 'hmc_pymc', or 'nuts_pymc' instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            raise NotImplementedError(f\"MCMC method {method} is not implemented.\")\n\n        prepared_potential = partial(\n            transformed_potential,\n            potential_fn=self.potential_fn,\n            theta_transform=self.theta_transform,\n            device=self._device,\n            track_gradients=track_gradients,\n        )\n        if pyro:\n            prepared_potential = partial(\n                pyro_potential_wrapper, potential=prepared_potential\n            )\n\n        return prepared_potential\n\n    def map(\n        self,\n        x: Optional[Tensor] = None,\n        num_iter: int = 1_000,\n        num_to_optimize: int = 100,\n        learning_rate: float = 0.01,\n        init_method: Union[str, Tensor] = \"proposal\",\n        num_init_samples: int = 1_000,\n        save_best_every: int = 10,\n        show_progress_bars: bool = False,\n        force_update: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n        The method can be interrupted (Ctrl-C) when the user sees that the\n        log-probability converges. The best estimate will be saved in `self._map` and\n        can be accessed with `self.map()`. The MAP is obtained by running gradient\n        ascent from a given number of starting positions (samples from the posterior\n        with the highest log-probability). After the optimization is done, we select the\n        parameter set that has the highest log-probability after the optimization.\n\n        Warning: The default values used by this function are not well-tested. They\n        might require hand-tuning for the problem at hand.\n\n        For developers: if the prior is a `BoxUniform`, we carry out the optimization\n        in unbounded space and transform the result back into bounded space.\n\n        Args:\n            x: Deprecated - use `.set_default_x()` prior to `.map()`.\n            num_iter: Number of optimization steps that the algorithm takes\n                to find the MAP.\n            learning_rate: Learning rate of the optimizer.\n            init_method: How to select the starting parameters for the optimization. If\n                it is a string, it can be either [`posterior`, `prior`], which samples\n                the respective distribution `num_init_samples` times. If it is a\n                tensor, the tensor will be used as init locations.\n            num_init_samples: Draw this number of samples from the posterior and\n                evaluate the log-probability of all of them.\n            num_to_optimize: From the drawn `num_init_samples`, use the\n                `num_to_optimize` with highest log-probability as the initial points\n                for the optimization.\n            save_best_every: The best log-probability is computed, saved in the\n                `map`-attribute, and printed every `save_best_every`-th iteration.\n                Computing the best log-probability creates a significant overhead\n                (thus, the default is `10`.)\n            show_progress_bars: Whether to show a progressbar during sampling from\n                the posterior.\n            force_update: Whether to re-calculate the MAP when x is unchanged and\n                have a cached value.\n            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n                {'norm_posterior': True} for SNPE.\n\n        Returns:\n            The MAP estimate.\n        \"\"\"\n        return super().map(\n            x=x,\n            num_iter=num_iter,\n            num_to_optimize=num_to_optimize,\n            learning_rate=learning_rate,\n            init_method=init_method,\n            num_init_samples=num_init_samples,\n            save_best_every=save_best_every,\n            show_progress_bars=show_progress_bars,\n            force_update=force_update,\n        )\n\n    def get_arviz_inference_data(self) -&gt; InferenceData:\n        \"\"\"Returns arviz InferenceData object constructed most recent samples.\n\n        Note: the InferenceData is constructed using the posterior samples generated in\n        most recent call to `.sample(...)`.\n\n        For Pyro and PyMC samplers, InferenceData will contain diagnostics, but for\n        sbi slice samplers, only the samples are added.\n\n        Returns:\n            inference_data: Arviz InferenceData object.\n        \"\"\"\n        assert (\n            self._posterior_sampler is not None\n        ), \"\"\"No samples have been generated, call .sample() first.\"\"\"\n\n        sampler: Union[\n            MCMC, SliceSamplerSerial, SliceSamplerVectorized, PyMCSampler\n        ] = self._posterior_sampler\n\n        # If Pyro sampler and samples not transformed, use arviz' from_pyro.\n        if isinstance(sampler, (HMC, NUTS)) and isinstance(\n            self.theta_transform, torch_tf.IndependentTransform\n        ):\n            inference_data = az.from_pyro(sampler)\n        # If PyMC sampler and samples not transformed, get cached InferenceData.\n        elif isinstance(sampler, PyMCSampler) and isinstance(\n            self.theta_transform, torch_tf.IndependentTransform\n        ):\n            inference_data = sampler.get_inference_data()\n\n        # otherwise get samples from sampler and transform to original space.\n        else:\n            transformed_samples = sampler.get_samples(group_by_chain=True)\n            # Pyro samplers returns dicts, get values.\n            if isinstance(transformed_samples, Dict):\n                # popitem gets last items, [1] get the values as tensor.\n                transformed_samples = transformed_samples.popitem()[1]\n            # Our slice samplers return numpy arrays.\n            elif isinstance(transformed_samples, ndarray):\n                transformed_samples = torch.from_numpy(transformed_samples).type(\n                    torch.float32\n                )\n            # For MultipleIndependent priors transforms first dim must be batch dim.\n            # thus, reshape back and forth to have batch dim in front.\n            samples_shape = transformed_samples.shape\n            samples = self.theta_transform.inv(  # type: ignore\n                transformed_samples.reshape(-1, samples_shape[-1])\n            ).reshape(  # type: ignore\n                *samples_shape\n            )\n\n            inference_data = az.convert_to_inference_data({\n                f\"{self.param_name}\": samples\n            })\n\n        return inference_data\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.mcmc_method","title":"<code>mcmc_method: str</code>  <code>property</code> <code>writable</code>","text":"<p>Returns MCMC method.</p>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.posterior_sampler","title":"<code>posterior_sampler</code>  <code>property</code>","text":"<p>Returns sampler created by <code>sample</code>.</p>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__init__","title":"<code>__init__(potential_fn, proposal, theta_transform=None, method='slice_np_vectorized', thin=-1, warmup_steps=200, num_chains=20, init_strategy='resample', init_strategy_parameters=None, init_strategy_num_candidates=None, num_workers=1, mp_context='spawn', device=None, x_shape=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>potential_fn</code> <code>Union[Callable, BasePotential]</code> <p>The potential function from which to draw samples. Must be a <code>BasePotential</code> or a <code>Callable</code> which takes <code>theta</code> and <code>x_o</code> as inputs.</p> required <code>proposal</code> <code>Any</code> <p>Proposal distribution that is used to initialize the MCMC chain.</p> required <code>theta_transform</code> <code>Optional[TorchTransform]</code> <p>Transformation that will be applied during sampling. Allows to perform MCMC in unconstrained space.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method used for MCMC sampling, one of <code>slice_np</code>, <code>slice_np_vectorized</code>, <code>hmc_pyro</code>, <code>nuts_pyro</code>, <code>slice_pymc</code>, <code>hmc_pymc</code>, <code>nuts_pymc</code>. <code>slice_np</code> is a custom numpy implementation of slice sampling. <code>slice_np_vectorized</code> is identical to <code>slice_np</code>, but if <code>num_chains&gt;1</code>, the chains are vectorized for <code>slice_np_vectorized</code> whereas they are run sequentially for <code>slice_np</code>. The samplers ending on <code>_pyro</code> are using Pyro, and likewise the samplers ending on <code>_pymc</code> are using PyMC.</p> <code>'slice_np_vectorized'</code> <code>thin</code> <code>int</code> <p>The thinning factor for the chain, default 1 (no thinning).</p> <code>-1</code> <code>warmup_steps</code> <code>int</code> <p>The initial number of samples to discard.</p> <code>200</code> <code>num_chains</code> <code>int</code> <p>The number of chains. Should generally be at most <code>num_workers - 1</code>.</p> <code>20</code> <code>init_strategy</code> <code>str</code> <p>The initialisation strategy for chains; <code>proposal</code> will draw init locations from <code>proposal</code>, whereas <code>sir</code> will use Sequential- Importance-Resampling (SIR). SIR initially samples <code>init_strategy_num_candidates</code> from the <code>proposal</code>, evaluates all of them under the <code>potential_fn</code> and <code>proposal</code>, and then resamples the initial locations with weights proportional to <code>exp(potential_fn - proposal.log_prob</code>. <code>resample</code> is the same as <code>sir</code> but uses <code>exp(potential_fn)</code> as weights.</p> <code>'resample'</code> <code>init_strategy_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of keyword arguments passed to the init strategy, e.g., for <code>init_strategy=sir</code> this could be <code>num_candidate_samples</code>, i.e., the number of candidates to find init locations (internal default is <code>1000</code>), or <code>device</code>.</p> <code>None</code> <code>init_strategy_num_candidates</code> <code>Optional[int]</code> <p>Number of candidates to find init  locations in <code>init_strategy=sir</code> (deprecated, use  init_strategy_parameters instead).</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>number of cpu cores used to parallelize mcmc</p> <code>1</code> <code>mp_context</code> <code>str</code> <p>Multiprocessing start method, either <code>\"fork\"</code> or <code>\"spawn\"</code> (default), used by Pyro and PyMC samplers. <code>\"fork\"</code> can be significantly faster than <code>\"spawn\"</code> but is only supported on POSIX-based systems (e.g. Linux and macOS, not Windows).</p> <code>'spawn'</code> <code>device</code> <code>Optional[str]</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:0\u201d. If None, <code>potential_fn.device</code> is used.</p> <code>None</code> <code>x_shape</code> <code>Optional[Size]</code> <p>Deprecated, should not be passed.</p> <code>None</code> Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code> <pre><code>def __init__(\n    self,\n    potential_fn: Union[Callable, BasePotential],\n    proposal: Any,\n    theta_transform: Optional[TorchTransform] = None,\n    method: str = \"slice_np_vectorized\",\n    thin: int = -1,\n    warmup_steps: int = 200,\n    num_chains: int = 20,\n    init_strategy: str = \"resample\",\n    init_strategy_parameters: Optional[Dict[str, Any]] = None,\n    init_strategy_num_candidates: Optional[int] = None,\n    num_workers: int = 1,\n    mp_context: str = \"spawn\",\n    device: Optional[str] = None,\n    x_shape: Optional[torch.Size] = None,\n):\n    \"\"\"\n    Args:\n        potential_fn: The potential function from which to draw samples. Must be a\n            `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.\n        proposal: Proposal distribution that is used to initialize the MCMC chain.\n        theta_transform: Transformation that will be applied during sampling.\n            Allows to perform MCMC in unconstrained space.\n        method: Method used for MCMC sampling, one of `slice_np`,\n            `slice_np_vectorized`, `hmc_pyro`, `nuts_pyro`, `slice_pymc`,\n            `hmc_pymc`, `nuts_pymc`. `slice_np` is a custom\n            numpy implementation of slice sampling. `slice_np_vectorized` is\n            identical to `slice_np`, but if `num_chains&gt;1`, the chains are\n            vectorized for `slice_np_vectorized` whereas they are run sequentially\n            for `slice_np`. The samplers ending on `_pyro` are using Pyro, and\n            likewise the samplers ending on `_pymc` are using PyMC.\n        thin: The thinning factor for the chain, default 1 (no thinning).\n        warmup_steps: The initial number of samples to discard.\n        num_chains: The number of chains. Should generally be at most\n            `num_workers - 1`.\n        init_strategy: The initialisation strategy for chains; `proposal` will draw\n            init locations from `proposal`, whereas `sir` will use Sequential-\n            Importance-Resampling (SIR). SIR initially samples\n            `init_strategy_num_candidates` from the `proposal`, evaluates all of\n            them under the `potential_fn` and `proposal`, and then resamples the\n            initial locations with weights proportional to `exp(potential_fn -\n            proposal.log_prob`. `resample` is the same as `sir` but\n            uses `exp(potential_fn)` as weights.\n        init_strategy_parameters: Dictionary of keyword arguments passed to the\n            init strategy, e.g., for `init_strategy=sir` this could be\n            `num_candidate_samples`, i.e., the number of candidates to find init\n            locations (internal default is `1000`), or `device`.\n        init_strategy_num_candidates: Number of candidates to find init\n             locations in `init_strategy=sir` (deprecated, use\n             init_strategy_parameters instead).\n        num_workers: number of cpu cores used to parallelize mcmc\n        mp_context: Multiprocessing start method, either `\"fork\"` or `\"spawn\"`\n            (default), used by Pyro and PyMC samplers. `\"fork\"` can be significantly\n            faster than `\"spawn\"` but is only supported on POSIX-based systems\n            (e.g. Linux and macOS, not Windows).\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:0\". If None,\n            `potential_fn.device` is used.\n        x_shape: Deprecated, should not be passed.\n    \"\"\"\n    if method == \"slice\":\n        warn(\n            \"The Pyro-based slice sampler is deprecated, and the method `slice` \"\n            \"has been changed to `slice_np`, i.e., the custom \"\n            \"numpy-based slice sampler.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        method = \"slice_np\"\n\n    thin = _process_thin_default(thin)\n\n    super().__init__(\n        potential_fn,\n        theta_transform=theta_transform,\n        device=device,\n        x_shape=x_shape,\n    )\n\n    self.proposal = proposal\n    self.method = method\n    self.thin = thin\n    self.warmup_steps = warmup_steps\n    self.num_chains = num_chains\n    self.init_strategy = init_strategy\n    self.init_strategy_parameters = init_strategy_parameters or {}\n    self.num_workers = num_workers\n    self.mp_context = mp_context\n    self._posterior_sampler = None\n    # Hardcode parameter name to reduce clutter kwargs.\n    self.param_name = \"theta\"\n\n    if init_strategy_num_candidates is not None:\n        warn(\n            \"Passing `init_strategy_num_candidates` is deprecated as of sbi \"\n            \"v0.19.0. Instead, use e.g., `init_strategy_parameters \"\n            f\"={'num_candidate_samples': 1000}`\",\n            stacklevel=2,\n        )\n        self.init_strategy_parameters[\"num_candidate_samples\"] = (\n            init_strategy_num_candidates\n        )\n\n    self.potential_ = self._prepare_potential(method)\n\n    self._purpose = (\n        \"It provides MCMC to .sample() from the posterior and \"\n        \"can evaluate the _unnormalized_ posterior density with .log_prob().\"\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.get_arviz_inference_data","title":"<code>get_arviz_inference_data()</code>","text":"<p>Returns arviz InferenceData object constructed most recent samples.</p> <p>Note: the InferenceData is constructed using the posterior samples generated in most recent call to <code>.sample(...)</code>.</p> <p>For Pyro and PyMC samplers, InferenceData will contain diagnostics, but for sbi slice samplers, only the samples are added.</p> <p>Returns:</p> Name Type Description <code>inference_data</code> <code>InferenceData</code> <p>Arviz InferenceData object.</p> Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code> <pre><code>def get_arviz_inference_data(self) -&gt; InferenceData:\n    \"\"\"Returns arviz InferenceData object constructed most recent samples.\n\n    Note: the InferenceData is constructed using the posterior samples generated in\n    most recent call to `.sample(...)`.\n\n    For Pyro and PyMC samplers, InferenceData will contain diagnostics, but for\n    sbi slice samplers, only the samples are added.\n\n    Returns:\n        inference_data: Arviz InferenceData object.\n    \"\"\"\n    assert (\n        self._posterior_sampler is not None\n    ), \"\"\"No samples have been generated, call .sample() first.\"\"\"\n\n    sampler: Union[\n        MCMC, SliceSamplerSerial, SliceSamplerVectorized, PyMCSampler\n    ] = self._posterior_sampler\n\n    # If Pyro sampler and samples not transformed, use arviz' from_pyro.\n    if isinstance(sampler, (HMC, NUTS)) and isinstance(\n        self.theta_transform, torch_tf.IndependentTransform\n    ):\n        inference_data = az.from_pyro(sampler)\n    # If PyMC sampler and samples not transformed, get cached InferenceData.\n    elif isinstance(sampler, PyMCSampler) and isinstance(\n        self.theta_transform, torch_tf.IndependentTransform\n    ):\n        inference_data = sampler.get_inference_data()\n\n    # otherwise get samples from sampler and transform to original space.\n    else:\n        transformed_samples = sampler.get_samples(group_by_chain=True)\n        # Pyro samplers returns dicts, get values.\n        if isinstance(transformed_samples, Dict):\n            # popitem gets last items, [1] get the values as tensor.\n            transformed_samples = transformed_samples.popitem()[1]\n        # Our slice samplers return numpy arrays.\n        elif isinstance(transformed_samples, ndarray):\n            transformed_samples = torch.from_numpy(transformed_samples).type(\n                torch.float32\n            )\n        # For MultipleIndependent priors transforms first dim must be batch dim.\n        # thus, reshape back and forth to have batch dim in front.\n        samples_shape = transformed_samples.shape\n        samples = self.theta_transform.inv(  # type: ignore\n            transformed_samples.reshape(-1, samples_shape[-1])\n        ).reshape(  # type: ignore\n            *samples_shape\n        )\n\n        inference_data = az.convert_to_inference_data({\n            f\"{self.param_name}\": samples\n        })\n\n    return inference_data\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.log_prob","title":"<code>log_prob(theta, x=None, track_gradients=False)</code>","text":"<p>Returns the log-probability of theta under the posterior.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>Tensor</code> <p>Parameters \\(\\theta\\).</p> required <code>track_gradients</code> <code>bool</code> <p>Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p><code>len($\\theta$)</code>-shaped log-probability.</p> Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code> <pre><code>def log_prob(\n    self, theta: Tensor, x: Optional[Tensor] = None, track_gradients: bool = False\n) -&gt; Tensor:\n    r\"\"\"Returns the log-probability of theta under the posterior.\n\n    Args:\n        theta: Parameters $\\theta$.\n        track_gradients: Whether the returned tensor supports tracking gradients.\n            This can be helpful for e.g. sensitivity analysis, but increases memory\n            consumption.\n\n    Returns:\n        `len($\\theta$)`-shaped log-probability.\n    \"\"\"\n    warn(\n        \"`.log_prob()` is deprecated for methods that can only evaluate the \"\n        \"log-probability up to a normalizing constant. Use `.potential()` instead.\",\n        stacklevel=2,\n    )\n    warn(\"The log-probability is unnormalized!\", stacklevel=2)\n\n    self.potential_fn.set_x(self._x_else_default_x(x))\n\n    theta = ensure_theta_batched(torch.as_tensor(theta))\n    return self.potential_fn(\n        theta.to(self._device), track_gradients=track_gradients\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.map","title":"<code>map(x=None, num_iter=1000, num_to_optimize=100, learning_rate=0.01, init_method='proposal', num_init_samples=1000, save_best_every=10, show_progress_bars=False, force_update=False)</code>","text":"<p>Returns the maximum-a-posteriori estimate (MAP).</p> <p>The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in <code>self._map</code> and can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization.</p> <p>Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand.</p> <p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization in unbounded space and transform the result back into bounded space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p> <code>None</code> <code>num_iter</code> <code>int</code> <p>Number of optimization steps that the algorithm takes to find the MAP.</p> <code>1000</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer.</p> <code>0.01</code> <code>init_method</code> <code>Union[str, Tensor]</code> <p>How to select the starting parameters for the optimization. If it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples the respective distribution <code>num_init_samples</code> times. If it is a tensor, the tensor will be used as init locations.</p> <code>'proposal'</code> <code>num_init_samples</code> <code>int</code> <p>Draw this number of samples from the posterior and evaluate the log-probability of all of them.</p> <code>1000</code> <code>num_to_optimize</code> <code>int</code> <p>From the drawn <code>num_init_samples</code>, use the <code>num_to_optimize</code> with highest log-probability as the initial points for the optimization.</p> <code>100</code> <code>save_best_every</code> <code>int</code> <p>The best log-probability is computed, saved in the <code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration. Computing the best log-probability creates a significant overhead (thus, the default is <code>10</code>.)</p> <code>10</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during sampling from the posterior.</p> <code>False</code> <code>force_update</code> <code>bool</code> <p>Whether to re-calculate the MAP when x is unchanged and have a cached value.</p> <code>False</code> <code>log_prob_kwargs</code> <p>Will be empty for SNLE and SNRE. Will contain {\u2018norm_posterior\u2019: True} for SNPE.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MAP estimate.</p> Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code> <pre><code>def map(\n    self,\n    x: Optional[Tensor] = None,\n    num_iter: int = 1_000,\n    num_to_optimize: int = 100,\n    learning_rate: float = 0.01,\n    init_method: Union[str, Tensor] = \"proposal\",\n    num_init_samples: int = 1_000,\n    save_best_every: int = 10,\n    show_progress_bars: bool = False,\n    force_update: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n    The method can be interrupted (Ctrl-C) when the user sees that the\n    log-probability converges. The best estimate will be saved in `self._map` and\n    can be accessed with `self.map()`. The MAP is obtained by running gradient\n    ascent from a given number of starting positions (samples from the posterior\n    with the highest log-probability). After the optimization is done, we select the\n    parameter set that has the highest log-probability after the optimization.\n\n    Warning: The default values used by this function are not well-tested. They\n    might require hand-tuning for the problem at hand.\n\n    For developers: if the prior is a `BoxUniform`, we carry out the optimization\n    in unbounded space and transform the result back into bounded space.\n\n    Args:\n        x: Deprecated - use `.set_default_x()` prior to `.map()`.\n        num_iter: Number of optimization steps that the algorithm takes\n            to find the MAP.\n        learning_rate: Learning rate of the optimizer.\n        init_method: How to select the starting parameters for the optimization. If\n            it is a string, it can be either [`posterior`, `prior`], which samples\n            the respective distribution `num_init_samples` times. If it is a\n            tensor, the tensor will be used as init locations.\n        num_init_samples: Draw this number of samples from the posterior and\n            evaluate the log-probability of all of them.\n        num_to_optimize: From the drawn `num_init_samples`, use the\n            `num_to_optimize` with highest log-probability as the initial points\n            for the optimization.\n        save_best_every: The best log-probability is computed, saved in the\n            `map`-attribute, and printed every `save_best_every`-th iteration.\n            Computing the best log-probability creates a significant overhead\n            (thus, the default is `10`.)\n        show_progress_bars: Whether to show a progressbar during sampling from\n            the posterior.\n        force_update: Whether to re-calculate the MAP when x is unchanged and\n            have a cached value.\n        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n            {'norm_posterior': True} for SNPE.\n\n    Returns:\n        The MAP estimate.\n    \"\"\"\n    return super().map(\n        x=x,\n        num_iter=num_iter,\n        num_to_optimize=num_to_optimize,\n        learning_rate=learning_rate,\n        init_method=init_method,\n        num_init_samples=num_init_samples,\n        save_best_every=save_best_every,\n        show_progress_bars=show_progress_bars,\n        force_update=force_update,\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample","title":"<code>sample(sample_shape=torch.Size(), x=None, method=None, thin=None, warmup_steps=None, num_chains=None, init_strategy=None, init_strategy_parameters=None, init_strategy_num_candidates=None, mcmc_parameters=None, mcmc_method=None, sample_with=None, num_workers=None, mp_context=None, show_progress_bars=True)</code>","text":"<p>Return samples from posterior distribution \\(p(\\theta|x)\\) with MCMC.</p> <p>Check the <code>__init__()</code> method for a description of all arguments as well as their default values.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Shape</code> <p>Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code> samples and then reshape into the desired shape.</p> <code>Size()</code> <code>mcmc_parameters</code> <code>Optional[Dict]</code> <p>Dictionary that is passed only to support the API of <code>sbi</code> v0.17.2 or older.</p> <code>None</code> <code>mcmc_method</code> <code>Optional[str]</code> <p>This argument only exists to keep backward-compatibility with <code>sbi</code> v0.17.2 or older. Please use <code>method</code> instead.</p> <code>None</code> <code>sample_with</code> <code>Optional[str]</code> <p>This argument only exists to keep backward-compatibility with <code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show sampling progress monitor.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Samples from posterior.</p> Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code> <pre><code>def sample(\n    self,\n    sample_shape: Shape = torch.Size(),\n    x: Optional[Tensor] = None,\n    method: Optional[str] = None,\n    thin: Optional[int] = None,\n    warmup_steps: Optional[int] = None,\n    num_chains: Optional[int] = None,\n    init_strategy: Optional[str] = None,\n    init_strategy_parameters: Optional[Dict[str, Any]] = None,\n    init_strategy_num_candidates: Optional[int] = None,\n    mcmc_parameters: Optional[Dict] = None,\n    mcmc_method: Optional[str] = None,\n    sample_with: Optional[str] = None,\n    num_workers: Optional[int] = None,\n    mp_context: Optional[str] = None,\n    show_progress_bars: bool = True,\n) -&gt; Tensor:\n    r\"\"\"Return samples from posterior distribution $p(\\theta|x)$ with MCMC.\n\n    Check the `__init__()` method for a description of all arguments as well as\n    their default values.\n\n    Args:\n        sample_shape: Desired shape of samples that are drawn from posterior. If\n            sample_shape is multidimensional we simply draw `sample_shape.numel()`\n            samples and then reshape into the desired shape.\n        mcmc_parameters: Dictionary that is passed only to support the API of\n            `sbi` v0.17.2 or older.\n        mcmc_method: This argument only exists to keep backward-compatibility with\n            `sbi` v0.17.2 or older. Please use `method` instead.\n        sample_with: This argument only exists to keep backward-compatibility with\n            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.\n        show_progress_bars: Whether to show sampling progress monitor.\n\n    Returns:\n        Samples from posterior.\n    \"\"\"\n\n    self.potential_fn.set_x(self._x_else_default_x(x))\n\n    # Replace arguments that were not passed with their default.\n    method = self.method if method is None else method\n    thin = self.thin if thin is None else thin\n    warmup_steps = self.warmup_steps if warmup_steps is None else warmup_steps\n    num_chains = self.num_chains if num_chains is None else num_chains\n    init_strategy = self.init_strategy if init_strategy is None else init_strategy\n    num_workers = self.num_workers if num_workers is None else num_workers\n    mp_context = self.mp_context if mp_context is None else mp_context\n    init_strategy_parameters = (\n        self.init_strategy_parameters\n        if init_strategy_parameters is None\n        else init_strategy_parameters\n    )\n    if init_strategy_num_candidates is not None:\n        warn(\n            \"Passing `init_strategy_num_candidates` is deprecated as of sbi \"\n            \"v0.19.0. Instead, use e.g., \"\n            f\"`init_strategy_parameters={'num_candidate_samples': 1000}`\",\n            stacklevel=2,\n        )\n        self.init_strategy_parameters[\"num_candidate_samples\"] = (\n            init_strategy_num_candidates\n        )\n    if sample_with is not None:\n        raise ValueError(\n            f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting \"\n            \"`sample_with` is no longer supported. You have to rerun \"\n            f\"`.build_posterior(sample_with={sample_with}).`\"\n        )\n    if mcmc_method is not None:\n        warn(\n            \"You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this \"\n            \"is deprecated and will be removed in a future release. Use `method` \"\n            \"instead of `mcmc_method`.\",\n            stacklevel=2,\n        )\n        method = mcmc_method\n    if mcmc_parameters:\n        warn(\n            \"You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this \"\n            \"is deprecated and will be removed in a future release. Instead, pass \"\n            \"the variable to `.sample()` directly, e.g. \"\n            \"`posterior.sample((1,), num_chains=5)`.\",\n            stacklevel=2,\n        )\n    # The following lines are only for backwards compatibility with sbi v0.17.2 or\n    # older.\n    m_p = mcmc_parameters or {}  # define to shorten the variable name\n    method = _maybe_use_dict_entry(method, \"mcmc_method\", m_p)\n    thin = _maybe_use_dict_entry(thin, \"thin\", m_p)\n    warmup_steps = _maybe_use_dict_entry(warmup_steps, \"warmup_steps\", m_p)\n    num_chains = _maybe_use_dict_entry(num_chains, \"num_chains\", m_p)\n    init_strategy = _maybe_use_dict_entry(init_strategy, \"init_strategy\", m_p)\n    self.potential_ = self._prepare_potential(method)  # type: ignore\n\n    initial_params = self._get_initial_params(\n        init_strategy,  # type: ignore\n        num_chains,  # type: ignore\n        num_workers,\n        show_progress_bars,\n        **init_strategy_parameters,\n    )\n    num_samples = torch.Size(sample_shape).numel()\n\n    track_gradients = method in (\"hmc_pyro\", \"nuts_pyro\", \"hmc_pymc\", \"nuts_pymc\")\n    with torch.set_grad_enabled(track_gradients):\n        if method in (\"slice_np\", \"slice_np_vectorized\"):\n            transformed_samples = self._slice_np_mcmc(\n                num_samples=num_samples,\n                potential_function=self.potential_,\n                initial_params=initial_params,\n                thin=thin,  # type: ignore\n                warmup_steps=warmup_steps,  # type: ignore\n                vectorized=(method == \"slice_np_vectorized\"),\n                interchangeable_chains=True,\n                num_workers=num_workers,\n                show_progress_bars=show_progress_bars,\n            )\n        elif method in (\"hmc_pyro\", \"nuts_pyro\"):\n            transformed_samples = self._pyro_mcmc(\n                num_samples=num_samples,\n                potential_function=self.potential_,\n                initial_params=initial_params,\n                mcmc_method=method,  # type: ignore\n                thin=thin,  # type: ignore\n                warmup_steps=warmup_steps,  # type: ignore\n                num_chains=num_chains,\n                show_progress_bars=show_progress_bars,\n                mp_context=mp_context,\n            )\n        elif method in (\"hmc_pymc\", \"nuts_pymc\", \"slice_pymc\"):\n            transformed_samples = self._pymc_mcmc(\n                num_samples=num_samples,\n                potential_function=self.potential_,\n                initial_params=initial_params,\n                mcmc_method=method,  # type: ignore\n                thin=thin,  # type: ignore\n                warmup_steps=warmup_steps,  # type: ignore\n                num_chains=num_chains,\n                show_progress_bars=show_progress_bars,\n                mp_context=mp_context,\n            )\n        else:\n            raise NameError(f\"The sampling method {method} is not implemented!\")\n\n    samples = self.theta_transform.inv(transformed_samples)\n    # NOTE: Currently MCMCPosteriors will require a single dimension for the\n    # parameter dimension. With recent ConditionalDensity(Ratio) estimators, we\n    # can have multiple dimensions for the parameter dimension.\n    samples = samples.reshape((*sample_shape, -1))  # type: ignore\n\n    return samples\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample_batched","title":"<code>sample_batched(sample_shape, x, method=None, thin=None, warmup_steps=None, num_chains=None, init_strategy=None, init_strategy_parameters=None, num_workers=None, mp_context=None, show_progress_bars=True)</code>","text":"<p>Given a batch of observations [x_1, \u2026, x_B] this function samples from posteriors \\(p(\\theta|x_1)\\), \u2026 ,\\(p(\\theta|x_B)\\), in a batched (i.e. vectorized) manner.</p> <p>Check the <code>__init__()</code> method for a description of all arguments as well as their default values.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Shape</code> <p>Desired shape of samples that are drawn from the posterior given every observation.</p> required <code>x</code> <code>Tensor</code> <p>A batch of observations, of shape <code>(batch_dim, event_shape_x)</code>. <code>batch_dim</code> corresponds to the number of observations to be drawn.</p> required <code>method</code> <code>Optional[str]</code> <p>Method used for MCMC sampling, e.g., \u201cslice_np_vectorized\u201d.</p> <code>None</code> <code>thin</code> <code>Optional[int]</code> <p>The thinning factor for the chain, default 1 (no thinning).</p> <code>None</code> <code>warmup_steps</code> <code>Optional[int]</code> <p>The initial number of samples to discard.</p> <code>None</code> <code>num_chains</code> <code>Optional[int]</code> <p>The number of chains used for each <code>x</code> passed in the batch.</p> <code>None</code> <code>init_strategy</code> <code>Optional[str]</code> <p>The initialisation strategy for chains.</p> <code>None</code> <code>init_strategy_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of keyword arguments passed to the init strategy.</p> <code>None</code> <code>num_workers</code> <code>Optional[int]</code> <p>number of cpu cores used to parallelize initial parameter generation and mcmc sampling.</p> <code>None</code> <code>mp_context</code> <code>Optional[str]</code> <p>Multiprocessing start method, either <code>\"fork\"</code> or <code>\"spawn\"</code></p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show sampling progress monitor.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Samples from the posteriors of shape (*sample_shape, B, *input_shape)</p> Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code> <pre><code>def sample_batched(\n    self,\n    sample_shape: Shape,\n    x: Tensor,\n    method: Optional[str] = None,\n    thin: Optional[int] = None,\n    warmup_steps: Optional[int] = None,\n    num_chains: Optional[int] = None,\n    init_strategy: Optional[str] = None,\n    init_strategy_parameters: Optional[Dict[str, Any]] = None,\n    num_workers: Optional[int] = None,\n    mp_context: Optional[str] = None,\n    show_progress_bars: bool = True,\n) -&gt; Tensor:\n    r\"\"\"Given a batch of observations [x_1, ..., x_B] this function samples from\n    posteriors $p(\\theta|x_1)$, ... ,$p(\\theta|x_B)$, in a batched (i.e. vectorized)\n    manner.\n\n    Check the `__init__()` method for a description of all arguments as well as\n    their default values.\n\n    Args:\n        sample_shape: Desired shape of samples that are drawn from the posterior\n            given every observation.\n        x: A batch of observations, of shape `(batch_dim, event_shape_x)`.\n            `batch_dim` corresponds to the number of observations to be\n            drawn.\n        method: Method used for MCMC sampling, e.g., \"slice_np_vectorized\".\n        thin: The thinning factor for the chain, default 1 (no thinning).\n        warmup_steps: The initial number of samples to discard.\n        num_chains: The number of chains used for each `x` passed in the batch.\n        init_strategy: The initialisation strategy for chains.\n        init_strategy_parameters: Dictionary of keyword arguments passed to\n            the init strategy.\n        num_workers: number of cpu cores used to parallelize initial\n            parameter generation and mcmc sampling.\n        mp_context: Multiprocessing start method, either `\"fork\"` or `\"spawn\"`\n        show_progress_bars: Whether to show sampling progress monitor.\n\n    Returns:\n        Samples from the posteriors of shape (*sample_shape, B, *input_shape)\n    \"\"\"\n\n    # Replace arguments that were not passed with their default.\n    method = self.method if method is None else method\n    thin = self.thin if thin is None else thin\n    warmup_steps = self.warmup_steps if warmup_steps is None else warmup_steps\n    num_chains = self.num_chains if num_chains is None else num_chains\n    init_strategy = self.init_strategy if init_strategy is None else init_strategy\n    num_workers = self.num_workers if num_workers is None else num_workers\n    mp_context = self.mp_context if mp_context is None else mp_context\n    init_strategy_parameters = (\n        self.init_strategy_parameters\n        if init_strategy_parameters is None\n        else init_strategy_parameters\n    )\n\n    assert (\n        method == \"slice_np_vectorized\"\n    ), \"Batched sampling only supported for vectorized samplers!\"\n\n    # warn if num_chains is larger than num requested samples\n    if num_chains &gt; torch.Size(sample_shape).numel():\n        warnings.warn(\n            \"The passed number of MCMC chains is larger than the number of \"\n            f\"requested samples: {num_chains} &gt; {torch.Size(sample_shape).numel()},\"\n            f\" resetting it to {torch.Size(sample_shape).numel()}.\",\n            stacklevel=2,\n        )\n        num_chains = torch.Size(sample_shape).numel()\n\n    # custom shape handling to make sure to match the batch size of x and theta\n    # without unnecessary combinations.\n    if len(x.shape) == 1:\n        x = x.unsqueeze(0)\n    batch_size = x.shape[0]\n\n    x = reshape_to_batch_event(x, event_shape=x.shape[1:])\n\n    # For batched sampling, we want `num_chains` for each observation in the batch.\n    # Here we repeat the observations ABC -&gt; AAABBBCCC, so that the chains are\n    # in the order of the observations.\n    x_ = x.repeat_interleave(num_chains, dim=0)\n\n    self.potential_fn.set_x(x_, x_is_iid=False)\n    self.potential_ = self._prepare_potential(method)  # type: ignore\n\n    # For each observation in the batch, we have num_chains independent chains.\n    num_chains_extended = batch_size * num_chains\n    if num_chains_extended &gt; 100:\n        warnings.warn(\n            \"Note that for batched sampling, we use num_chains many chains for each\"\n            \" x in the batch. With the given settings, this results in a large \"\n            f\"number large number of chains ({num_chains_extended}), which can be \"\n            \"slow and memory-intensive for vectorized MCMC. Consider reducing the \"\n            \"number of chains.\",\n            stacklevel=2,\n        )\n    init_strategy_parameters[\"num_return_samples\"] = num_chains_extended\n    initial_params = self._get_initial_params_batched(\n        x,\n        init_strategy,  # type: ignore\n        num_chains,  # type: ignore\n        num_workers,\n        show_progress_bars,\n        **init_strategy_parameters,\n    )\n    # We need num_samples from each posterior in the batch\n    num_samples = torch.Size(sample_shape).numel() * batch_size\n\n    with torch.set_grad_enabled(False):\n        transformed_samples = self._slice_np_mcmc(\n            num_samples=num_samples,\n            potential_function=self.potential_,\n            initial_params=initial_params,\n            thin=thin,  # type: ignore\n            warmup_steps=warmup_steps,  # type: ignore\n            vectorized=(method == \"slice_np_vectorized\"),\n            interchangeable_chains=False,\n            num_workers=num_workers,\n            show_progress_bars=show_progress_bars,\n        )\n\n    # (num_chains_extended, samples_per_chain, *input_shape)\n    samples_per_chain: Tensor = self.theta_transform.inv(transformed_samples)  # type: ignore\n    dim_theta = samples_per_chain.shape[-1]\n    # We need to collect samples for each x from the respective chains.\n    # However, using samples.reshape(*sample_shape, batch_size, dim_theta)\n    # does not combine the samples in the right order, since this mixes\n    # samples that belong to different `x`. The following permute is a\n    # workaround to reshape the samples in the right order.\n    samples_per_x = samples_per_chain.reshape((\n        batch_size,\n        # We are flattening the sample shape here using -1 because we might have\n        # generated more samples than requested (more chains, or multiple of\n        # chains not matching sample_shape)\n        -1,\n        dim_theta,\n    )).permute(1, 0, -1)\n\n    # Shape is now (-1, batch_size, dim_theta)\n    # We can now select the number of requested samples\n    samples = samples_per_x[: torch.Size(sample_shape).numel()]\n    # and reshape into (*sample_shape, batch_size, dim_theta)\n    samples = samples.reshape((*sample_shape, batch_size, dim_theta))\n    return samples\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_mcmc_method","title":"<code>set_mcmc_method(method)</code>","text":"<p>Sets sampling method to for MCMC and returns <code>NeuralPosterior</code>.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Method to use.</p> required <p>Returns:</p> Type Description <code>NeuralPosterior</code> <p><code>NeuralPosterior</code> for chainable calls.</p> Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code> <pre><code>def set_mcmc_method(self, method: str) -&gt; \"NeuralPosterior\":\n    \"\"\"Sets sampling method to for MCMC and returns `NeuralPosterior`.\n\n    Args:\n        method: Method to use.\n\n    Returns:\n        `NeuralPosterior` for chainable calls.\n    \"\"\"\n    self._mcmc_method = method\n    return self\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.rejection_posterior.RejectionPosterior","title":"<code>RejectionPosterior</code>","text":"<p>               Bases: <code>NeuralPosterior</code></p> <p>Provides rejection sampling to sample from the posterior. SNLE or SNRE train neural networks to approximate the likelihood(-ratios). <code>RejectionPosterior</code> allows to sample from the posterior with rejection sampling.</p> Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code> <pre><code>class RejectionPosterior(NeuralPosterior):\n    r\"\"\"Provides rejection sampling to sample from the posterior.&lt;br/&gt;&lt;br/&gt;\n    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).\n    `RejectionPosterior` allows to sample from the posterior with rejection sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        potential_fn: Union[Callable, BasePotential],\n        proposal: Any,\n        theta_transform: Optional[TorchTransform] = None,\n        max_sampling_batch_size: int = 10_000,\n        num_samples_to_find_max: int = 10_000,\n        num_iter_to_find_max: int = 100,\n        m: float = 1.2,\n        device: Optional[str] = None,\n        x_shape: Optional[torch.Size] = None,\n    ):\n        \"\"\"\n        Args:\n            potential_fn: The potential function from which to draw samples. Must be a\n                `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.\n            proposal: The proposal distribution.\n            theta_transform: Transformation that is applied to parameters. Is not used\n                during but only when calling `.map()`.\n            max_sampling_batch_size: The batchsize of samples being drawn from\n                the proposal at every iteration.\n            num_samples_to_find_max: The number of samples that are used to find the\n                maximum of the `potential_fn / proposal` ratio.\n            num_iter_to_find_max: The number of gradient ascent iterations to find the\n                maximum of the `potential_fn / proposal` ratio.\n            m: Multiplier to the `potential_fn / proposal` ratio.\n            device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:0\". If None,\n                `potential_fn.device` is used.\n            x_shape: Deprecated, should not be passed.\n        \"\"\"\n        super().__init__(\n            potential_fn,\n            theta_transform=theta_transform,\n            device=device,\n            x_shape=x_shape,\n        )\n\n        self.proposal = proposal\n        self.max_sampling_batch_size = max_sampling_batch_size\n        self.num_samples_to_find_max = num_samples_to_find_max\n        self.num_iter_to_find_max = num_iter_to_find_max\n        self.m = m\n\n        self._purpose = (\n            \"It provides rejection sampling to .sample() from the posterior and \"\n            \"can evaluate the _unnormalized_ posterior density with .log_prob().\"\n        )\n\n    def log_prob(\n        self, theta: Tensor, x: Optional[Tensor] = None, track_gradients: bool = False\n    ) -&gt; Tensor:\n        r\"\"\"Returns the log-probability of theta under the posterior.\n\n        Args:\n            theta: Parameters $\\theta$.\n            track_gradients: Whether the returned tensor supports tracking gradients.\n                This can be helpful for e.g. sensitivity analysis, but increases memory\n                consumption.\n\n        Returns:\n            `len($\\theta$)`-shaped log-probability.\n        \"\"\"\n        warn(\n            \"`.log_prob()` is deprecated for methods that can only evaluate the \"\n            \"log-probability up to a normalizing constant. Use `.potential()` instead.\",\n            stacklevel=2,\n        )\n        warn(\"The log-probability is unnormalized!\", stacklevel=2)\n\n        self.potential_fn.set_x(self._x_else_default_x(x))\n\n        theta = ensure_theta_batched(torch.as_tensor(theta))\n        return self.potential_fn(\n            theta.to(self._device), track_gradients=track_gradients\n        )\n\n    def sample(\n        self,\n        sample_shape: Shape = torch.Size(),\n        x: Optional[Tensor] = None,\n        max_sampling_batch_size: Optional[int] = None,\n        num_samples_to_find_max: Optional[int] = None,\n        num_iter_to_find_max: Optional[int] = None,\n        m: Optional[float] = None,\n        sample_with: Optional[str] = None,\n        show_progress_bars: bool = True,\n    ):\n        r\"\"\"Return samples from posterior $p(\\theta|x)$ via rejection sampling.\n\n        Args:\n            sample_shape: Desired shape of samples that are drawn from posterior. If\n                sample_shape is multidimensional we simply draw `sample_shape.numel()`\n                samples and then reshape into the desired shape.\n            sample_with: This argument only exists to keep backward-compatibility with\n                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.\n            show_progress_bars: Whether to show sampling progress monitor.\n\n        Returns:\n            Samples from posterior.\n        \"\"\"\n        num_samples = torch.Size(sample_shape).numel()\n        self.potential_fn.set_x(self._x_else_default_x(x))\n\n        potential = partial(self.potential_fn, track_gradients=True)\n\n        if sample_with is not None:\n            raise ValueError(\n                f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting \"\n                f\"`sample_with` is no longer supported. You have to rerun \"\n                f\"`.build_posterior(sample_with={sample_with}).`\"\n            )\n        # Replace arguments that were not passed with their default.\n        max_sampling_batch_size = (\n            self.max_sampling_batch_size\n            if max_sampling_batch_size is None\n            else max_sampling_batch_size\n        )\n        num_samples_to_find_max = (\n            self.num_samples_to_find_max\n            if num_samples_to_find_max is None\n            else num_samples_to_find_max\n        )\n        num_iter_to_find_max = (\n            self.num_iter_to_find_max\n            if num_iter_to_find_max is None\n            else num_iter_to_find_max\n        )\n        m = self.m if m is None else m\n\n        samples, _ = rejection_sample(\n            potential,\n            proposal=self.proposal,\n            num_samples=num_samples,\n            show_progress_bars=show_progress_bars,\n            warn_acceptance=0.01,\n            max_sampling_batch_size=max_sampling_batch_size,\n            num_samples_to_find_max=num_samples_to_find_max,\n            num_iter_to_find_max=num_iter_to_find_max,\n            m=m,\n            device=self._device,\n        )\n\n        return samples.reshape((*sample_shape, -1))\n\n    def sample_batched(\n        self,\n        sample_shape: Shape,\n        x: Tensor,\n        max_sampling_batch_size: int = 10000,\n        show_progress_bars: bool = True,\n    ) -&gt; Tensor:\n        raise NotImplementedError(\n            \"Batched sampling is not implemented for RejectionPosterior. \\\n            Alternatively you can use `sample` in a loop \\\n            [posterior.sample(theta, x_o) for x_o in x].\"\n        )\n\n    def map(\n        self,\n        x: Optional[Tensor] = None,\n        num_iter: int = 1_000,\n        num_to_optimize: int = 100,\n        learning_rate: float = 0.01,\n        init_method: Union[str, Tensor] = \"proposal\",\n        num_init_samples: int = 1_000,\n        save_best_every: int = 10,\n        show_progress_bars: bool = False,\n        force_update: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n        The method can be interrupted (Ctrl-C) when the user sees that the\n        log-probability converges. The best estimate will be saved in `self._map` and\n        can be accessed with `self.map()`. The MAP is obtained by running gradient\n        ascent from a given number of starting positions (samples from the posterior\n        with the highest log-probability). After the optimization is done, we select the\n        parameter set that has the highest log-probability after the optimization.\n\n        Warning: The default values used by this function are not well-tested. They\n        might require hand-tuning for the problem at hand.\n\n        For developers: if the prior is a `BoxUniform`, we carry out the optimization\n        in unbounded space and transform the result back into bounded space.\n\n        Args:\n            x: Deprecated - use `.set_default_x()` prior to `.map()`.\n            num_iter: Number of optimization steps that the algorithm takes\n                to find the MAP.\n            learning_rate: Learning rate of the optimizer.\n            init_method: How to select the starting parameters for the optimization. If\n                it is a string, it can be either [`posterior`, `prior`], which samples\n                the respective distribution `num_init_samples` times. If it is a\n                tensor, the tensor will be used as init locations.\n            num_init_samples: Draw this number of samples from the posterior and\n                evaluate the log-probability of all of them.\n            num_to_optimize: From the drawn `num_init_samples`, use the\n                `num_to_optimize` with highest log-probability as the initial points\n                for the optimization.\n            save_best_every: The best log-probability is computed, saved in the\n                `map`-attribute, and printed every `save_best_every`-th iteration.\n                Computing the best log-probability creates a significant overhead\n                (thus, the default is `10`.)\n            show_progress_bars: Whether to show a progressbar during sampling from\n                the posterior.\n            force_update: Whether to re-calculate the MAP when x is unchanged and\n                have a cached value.\n            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n                {'norm_posterior': True} for SNPE.\n\n        Returns:\n            The MAP estimate.\n        \"\"\"\n        return super().map(\n            x=x,\n            num_iter=num_iter,\n            num_to_optimize=num_to_optimize,\n            learning_rate=learning_rate,\n            init_method=init_method,\n            num_init_samples=num_init_samples,\n            save_best_every=save_best_every,\n            show_progress_bars=show_progress_bars,\n            force_update=force_update,\n        )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.__init__","title":"<code>__init__(potential_fn, proposal, theta_transform=None, max_sampling_batch_size=10000, num_samples_to_find_max=10000, num_iter_to_find_max=100, m=1.2, device=None, x_shape=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>potential_fn</code> <code>Union[Callable, BasePotential]</code> <p>The potential function from which to draw samples. Must be a <code>BasePotential</code> or a <code>Callable</code> which takes <code>theta</code> and <code>x_o</code> as inputs.</p> required <code>proposal</code> <code>Any</code> <p>The proposal distribution.</p> required <code>theta_transform</code> <code>Optional[TorchTransform]</code> <p>Transformation that is applied to parameters. Is not used during but only when calling <code>.map()</code>.</p> <code>None</code> <code>max_sampling_batch_size</code> <code>int</code> <p>The batchsize of samples being drawn from the proposal at every iteration.</p> <code>10000</code> <code>num_samples_to_find_max</code> <code>int</code> <p>The number of samples that are used to find the maximum of the <code>potential_fn / proposal</code> ratio.</p> <code>10000</code> <code>num_iter_to_find_max</code> <code>int</code> <p>The number of gradient ascent iterations to find the maximum of the <code>potential_fn / proposal</code> ratio.</p> <code>100</code> <code>m</code> <code>float</code> <p>Multiplier to the <code>potential_fn / proposal</code> ratio.</p> <code>1.2</code> <code>device</code> <code>Optional[str]</code> <p>Training device, e.g., \u201ccpu\u201d, \u201ccuda\u201d or \u201ccuda:0\u201d. If None, <code>potential_fn.device</code> is used.</p> <code>None</code> <code>x_shape</code> <code>Optional[Size]</code> <p>Deprecated, should not be passed.</p> <code>None</code> Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code> <pre><code>def __init__(\n    self,\n    potential_fn: Union[Callable, BasePotential],\n    proposal: Any,\n    theta_transform: Optional[TorchTransform] = None,\n    max_sampling_batch_size: int = 10_000,\n    num_samples_to_find_max: int = 10_000,\n    num_iter_to_find_max: int = 100,\n    m: float = 1.2,\n    device: Optional[str] = None,\n    x_shape: Optional[torch.Size] = None,\n):\n    \"\"\"\n    Args:\n        potential_fn: The potential function from which to draw samples. Must be a\n            `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.\n        proposal: The proposal distribution.\n        theta_transform: Transformation that is applied to parameters. Is not used\n            during but only when calling `.map()`.\n        max_sampling_batch_size: The batchsize of samples being drawn from\n            the proposal at every iteration.\n        num_samples_to_find_max: The number of samples that are used to find the\n            maximum of the `potential_fn / proposal` ratio.\n        num_iter_to_find_max: The number of gradient ascent iterations to find the\n            maximum of the `potential_fn / proposal` ratio.\n        m: Multiplier to the `potential_fn / proposal` ratio.\n        device: Training device, e.g., \"cpu\", \"cuda\" or \"cuda:0\". If None,\n            `potential_fn.device` is used.\n        x_shape: Deprecated, should not be passed.\n    \"\"\"\n    super().__init__(\n        potential_fn,\n        theta_transform=theta_transform,\n        device=device,\n        x_shape=x_shape,\n    )\n\n    self.proposal = proposal\n    self.max_sampling_batch_size = max_sampling_batch_size\n    self.num_samples_to_find_max = num_samples_to_find_max\n    self.num_iter_to_find_max = num_iter_to_find_max\n    self.m = m\n\n    self._purpose = (\n        \"It provides rejection sampling to .sample() from the posterior and \"\n        \"can evaluate the _unnormalized_ posterior density with .log_prob().\"\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.log_prob","title":"<code>log_prob(theta, x=None, track_gradients=False)</code>","text":"<p>Returns the log-probability of theta under the posterior.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>Tensor</code> <p>Parameters \\(\\theta\\).</p> required <code>track_gradients</code> <code>bool</code> <p>Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis, but increases memory consumption.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p><code>len($\\theta$)</code>-shaped log-probability.</p> Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code> <pre><code>def log_prob(\n    self, theta: Tensor, x: Optional[Tensor] = None, track_gradients: bool = False\n) -&gt; Tensor:\n    r\"\"\"Returns the log-probability of theta under the posterior.\n\n    Args:\n        theta: Parameters $\\theta$.\n        track_gradients: Whether the returned tensor supports tracking gradients.\n            This can be helpful for e.g. sensitivity analysis, but increases memory\n            consumption.\n\n    Returns:\n        `len($\\theta$)`-shaped log-probability.\n    \"\"\"\n    warn(\n        \"`.log_prob()` is deprecated for methods that can only evaluate the \"\n        \"log-probability up to a normalizing constant. Use `.potential()` instead.\",\n        stacklevel=2,\n    )\n    warn(\"The log-probability is unnormalized!\", stacklevel=2)\n\n    self.potential_fn.set_x(self._x_else_default_x(x))\n\n    theta = ensure_theta_batched(torch.as_tensor(theta))\n    return self.potential_fn(\n        theta.to(self._device), track_gradients=track_gradients\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.map","title":"<code>map(x=None, num_iter=1000, num_to_optimize=100, learning_rate=0.01, init_method='proposal', num_init_samples=1000, save_best_every=10, show_progress_bars=False, force_update=False)</code>","text":"<p>Returns the maximum-a-posteriori estimate (MAP).</p> <p>The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in <code>self._map</code> and can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization.</p> <p>Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand.</p> <p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization in unbounded space and transform the result back into bounded space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p> <code>None</code> <code>num_iter</code> <code>int</code> <p>Number of optimization steps that the algorithm takes to find the MAP.</p> <code>1000</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer.</p> <code>0.01</code> <code>init_method</code> <code>Union[str, Tensor]</code> <p>How to select the starting parameters for the optimization. If it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples the respective distribution <code>num_init_samples</code> times. If it is a tensor, the tensor will be used as init locations.</p> <code>'proposal'</code> <code>num_init_samples</code> <code>int</code> <p>Draw this number of samples from the posterior and evaluate the log-probability of all of them.</p> <code>1000</code> <code>num_to_optimize</code> <code>int</code> <p>From the drawn <code>num_init_samples</code>, use the <code>num_to_optimize</code> with highest log-probability as the initial points for the optimization.</p> <code>100</code> <code>save_best_every</code> <code>int</code> <p>The best log-probability is computed, saved in the <code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration. Computing the best log-probability creates a significant overhead (thus, the default is <code>10</code>.)</p> <code>10</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during sampling from the posterior.</p> <code>False</code> <code>force_update</code> <code>bool</code> <p>Whether to re-calculate the MAP when x is unchanged and have a cached value.</p> <code>False</code> <code>log_prob_kwargs</code> <p>Will be empty for SNLE and SNRE. Will contain {\u2018norm_posterior\u2019: True} for SNPE.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MAP estimate.</p> Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code> <pre><code>def map(\n    self,\n    x: Optional[Tensor] = None,\n    num_iter: int = 1_000,\n    num_to_optimize: int = 100,\n    learning_rate: float = 0.01,\n    init_method: Union[str, Tensor] = \"proposal\",\n    num_init_samples: int = 1_000,\n    save_best_every: int = 10,\n    show_progress_bars: bool = False,\n    force_update: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n    The method can be interrupted (Ctrl-C) when the user sees that the\n    log-probability converges. The best estimate will be saved in `self._map` and\n    can be accessed with `self.map()`. The MAP is obtained by running gradient\n    ascent from a given number of starting positions (samples from the posterior\n    with the highest log-probability). After the optimization is done, we select the\n    parameter set that has the highest log-probability after the optimization.\n\n    Warning: The default values used by this function are not well-tested. They\n    might require hand-tuning for the problem at hand.\n\n    For developers: if the prior is a `BoxUniform`, we carry out the optimization\n    in unbounded space and transform the result back into bounded space.\n\n    Args:\n        x: Deprecated - use `.set_default_x()` prior to `.map()`.\n        num_iter: Number of optimization steps that the algorithm takes\n            to find the MAP.\n        learning_rate: Learning rate of the optimizer.\n        init_method: How to select the starting parameters for the optimization. If\n            it is a string, it can be either [`posterior`, `prior`], which samples\n            the respective distribution `num_init_samples` times. If it is a\n            tensor, the tensor will be used as init locations.\n        num_init_samples: Draw this number of samples from the posterior and\n            evaluate the log-probability of all of them.\n        num_to_optimize: From the drawn `num_init_samples`, use the\n            `num_to_optimize` with highest log-probability as the initial points\n            for the optimization.\n        save_best_every: The best log-probability is computed, saved in the\n            `map`-attribute, and printed every `save_best_every`-th iteration.\n            Computing the best log-probability creates a significant overhead\n            (thus, the default is `10`.)\n        show_progress_bars: Whether to show a progressbar during sampling from\n            the posterior.\n        force_update: Whether to re-calculate the MAP when x is unchanged and\n            have a cached value.\n        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n            {'norm_posterior': True} for SNPE.\n\n    Returns:\n        The MAP estimate.\n    \"\"\"\n    return super().map(\n        x=x,\n        num_iter=num_iter,\n        num_to_optimize=num_to_optimize,\n        learning_rate=learning_rate,\n        init_method=init_method,\n        num_init_samples=num_init_samples,\n        save_best_every=save_best_every,\n        show_progress_bars=show_progress_bars,\n        force_update=force_update,\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.sample","title":"<code>sample(sample_shape=torch.Size(), x=None, max_sampling_batch_size=None, num_samples_to_find_max=None, num_iter_to_find_max=None, m=None, sample_with=None, show_progress_bars=True)</code>","text":"<p>Return samples from posterior \\(p(\\theta|x)\\) via rejection sampling.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Shape</code> <p>Desired shape of samples that are drawn from posterior. If sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code> samples and then reshape into the desired shape.</p> <code>Size()</code> <code>sample_with</code> <code>Optional[str]</code> <p>This argument only exists to keep backward-compatibility with <code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p> <code>None</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show sampling progress monitor.</p> <code>True</code> <p>Returns:</p> Type Description <p>Samples from posterior.</p> Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code> <pre><code>def sample(\n    self,\n    sample_shape: Shape = torch.Size(),\n    x: Optional[Tensor] = None,\n    max_sampling_batch_size: Optional[int] = None,\n    num_samples_to_find_max: Optional[int] = None,\n    num_iter_to_find_max: Optional[int] = None,\n    m: Optional[float] = None,\n    sample_with: Optional[str] = None,\n    show_progress_bars: bool = True,\n):\n    r\"\"\"Return samples from posterior $p(\\theta|x)$ via rejection sampling.\n\n    Args:\n        sample_shape: Desired shape of samples that are drawn from posterior. If\n            sample_shape is multidimensional we simply draw `sample_shape.numel()`\n            samples and then reshape into the desired shape.\n        sample_with: This argument only exists to keep backward-compatibility with\n            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.\n        show_progress_bars: Whether to show sampling progress monitor.\n\n    Returns:\n        Samples from posterior.\n    \"\"\"\n    num_samples = torch.Size(sample_shape).numel()\n    self.potential_fn.set_x(self._x_else_default_x(x))\n\n    potential = partial(self.potential_fn, track_gradients=True)\n\n    if sample_with is not None:\n        raise ValueError(\n            f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting \"\n            f\"`sample_with` is no longer supported. You have to rerun \"\n            f\"`.build_posterior(sample_with={sample_with}).`\"\n        )\n    # Replace arguments that were not passed with their default.\n    max_sampling_batch_size = (\n        self.max_sampling_batch_size\n        if max_sampling_batch_size is None\n        else max_sampling_batch_size\n    )\n    num_samples_to_find_max = (\n        self.num_samples_to_find_max\n        if num_samples_to_find_max is None\n        else num_samples_to_find_max\n    )\n    num_iter_to_find_max = (\n        self.num_iter_to_find_max\n        if num_iter_to_find_max is None\n        else num_iter_to_find_max\n    )\n    m = self.m if m is None else m\n\n    samples, _ = rejection_sample(\n        potential,\n        proposal=self.proposal,\n        num_samples=num_samples,\n        show_progress_bars=show_progress_bars,\n        warn_acceptance=0.01,\n        max_sampling_batch_size=max_sampling_batch_size,\n        num_samples_to_find_max=num_samples_to_find_max,\n        num_iter_to_find_max=num_iter_to_find_max,\n        m=m,\n        device=self._device,\n    )\n\n    return samples.reshape((*sample_shape, -1))\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior","title":"<code>VIPosterior</code>","text":"<p>               Bases: <code>NeuralPosterior</code></p> <p>Provides VI (Variational Inference) to sample from the posterior. SNLE or SNRE train neural networks to approximate the likelihood(-ratios). <code>VIPosterior</code> allows to learn a tractable variational posterior \\(q(\\theta)\\) which approximates the true posterior \\(p(\\theta|x_o)\\). After this second training stage, we can produce approximate posterior samples, by just sampling from q with no additional cost. For additional information see [1] and [2]. References: [1] Variational methods for simulation-based inference, Manuel Gl\u00f6ckler, Michael Deistler, Jakob Macke, 2022, https://openreview.net/forum?id=kZ0UYdhqkNY [2] Sequential Neural Posterior and Likelihood Approximation, Samuel Wiqvist, Jes Frellsen, Umberto Picchini, 2021, https://arxiv.org/abs/2102.06522</p> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>class VIPosterior(NeuralPosterior):\n    r\"\"\"Provides VI (Variational Inference) to sample from the posterior.&lt;br/&gt;&lt;br/&gt;\n    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).\n    `VIPosterior` allows to learn a tractable variational posterior $q(\\theta)$ which\n    approximates the true posterior $p(\\theta|x_o)$. After this second training stage,\n    we can produce approximate posterior samples, by just sampling from q with no\n    additional cost. For additional information see [1] and [2].&lt;br/&gt;&lt;br/&gt;\n    References:&lt;br/&gt;\n    [1] Variational methods for simulation-based inference, Manuel Gl\u00f6ckler, Michael\n    Deistler, Jakob Macke, 2022, https://openreview.net/forum?id=kZ0UYdhqkNY&lt;br/&gt;\n    [2] Sequential Neural Posterior and Likelihood Approximation, Samuel Wiqvist, Jes\n    Frellsen, Umberto Picchini, 2021, https://arxiv.org/abs/2102.06522\n    \"\"\"\n\n    def __init__(\n        self,\n        potential_fn: Union[Callable, BasePotential],\n        prior: Optional[TorchDistribution] = None,\n        q: Union[str, PyroTransformedDistribution, \"VIPosterior\", Callable] = \"maf\",\n        theta_transform: Optional[TorchTransform] = None,\n        vi_method: str = \"rKL\",\n        device: str = \"cpu\",\n        x_shape: Optional[torch.Size] = None,\n        parameters: Iterable = [],\n        modules: Iterable = [],\n    ):\n        \"\"\"\n        Args:\n            potential_fn: The potential function from which to draw samples. Must be a\n                `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.\n            prior: This is the prior distribution. Note that this is only\n                used to check/construct the variational distribution or within some\n                quality metrics. Please make sure that this matches with the prior\n                within the potential_fn. If `None` is given, we will try to infer it\n                from potential_fn or q, if this fails we raise an Error.\n            q: Variational distribution, either string, `TransformedDistribution`, or a\n                `VIPosterior` object. This specifies a parametric class of distribution\n                over which the best possible posterior approximation is searched. For\n                string input, we currently support [nsf, scf, maf, mcf, gaussian,\n                gaussian_diag]. You can also specify your own variational family by\n                passing a pyro `TransformedDistribution`.\n                Additionally, we allow a `Callable`, which allows you the pass a\n                `builder` function, which if called returns a distribution. This may be\n                useful for setting the hyperparameters e.g. `num_transfroms` within the\n                `get_flow_builder` method specifying the number of transformations\n                within a normalizing flow. If q is already a `VIPosterior`, then the\n                arguments will be copied from it (relevant for multi-round training).\n            theta_transform: Maps form prior support to unconstrained space. The\n                inverse is used here to ensure that the posterior support is equal to\n                that of the prior.\n            vi_method: This specifies the variational methods which are used to fit q to\n                the posterior. We currently support [rKL, fKL, IW, alpha]. Note that\n                some of the divergences are `mode seeking` i.e. they underestimate\n                variance and collapse on multimodal targets (`rKL`, `alpha` for alpha &gt;\n                1) and some are `mass covering` i.e. they overestimate variance but\n                typically cover all modes (`fKL`, `IW`, `alpha` for alpha &lt; 1).\n            device: Training device, e.g., `cpu`, `cuda` or `cuda:0`. We will ensure\n                that all other objects are also on this device.\n            x_shape: Deprecated, should not be passed.\n            parameters: List of parameters of the variational posterior. This is only\n                required for user-defined q i.e. if q does not have a `parameters`\n                attribute.\n            modules: List of modules of the variational posterior. This is only\n                required for user-defined q i.e. if q does not have a `modules`\n                attribute.\n        \"\"\"\n        super().__init__(potential_fn, theta_transform, device, x_shape=x_shape)\n\n        # Especially the prior may be on another device -&gt; move it...\n        self._device = device\n        self.potential_fn.device = device\n        move_all_tensor_to_device(self.potential_fn, device)\n\n        # Get prior and previous builds\n        if prior is not None:\n            self._prior = prior\n        elif hasattr(self.potential_fn, \"prior\") and isinstance(\n            self.potential_fn.prior, Distribution\n        ):\n            self._prior = self.potential_fn.prior\n        elif isinstance(q, VIPosterior) and isinstance(q._prior, Distribution):\n            self._prior = q._prior\n        else:\n            raise ValueError(\n                \"We could not find a suitable prior distribution within `potential_fn` \"\n                \"or `q` (if a VIPosterior is given). Please explicitly specify a prior.\"\n            )\n        move_all_tensor_to_device(self._prior, device)\n        self._optimizer = None\n\n        # In contrast to MCMC we want to project into constrained space.\n        if theta_transform is None:\n            self.link_transform = mcmc_transform(self._prior).inv\n        else:\n            self.link_transform = theta_transform.inv\n\n        # This will set the variational distribution and VI method\n        self.set_q(q, parameters=parameters, modules=modules)\n        self.set_vi_method(vi_method)\n\n        self._purpose = (\n            \"It provides Variational inference to .sample() from the posterior and \"\n            \"can evaluate the _normalized_ posterior density with .log_prob().\"\n        )\n\n    @property\n    def q(self) -&gt; Distribution:\n        \"\"\"Returns the variational posterior.\"\"\"\n        return self._q\n\n    @q.setter\n    def q(\n        self,\n        q: Union[str, Distribution, \"VIPosterior\", Callable],\n    ) -&gt; None:\n        \"\"\"Sets the variational distribution. If the distribution does not admit access\n        through `parameters` and `modules` function, please use `set_q` if you want to\n        explicitly specify the parameters and modules.\n\n\n        Args:\n            q: Variational distribution, either string, distribution, or a VIPosterior\n                object. This specifies a parametric class of distribution over which\n                the best possible posterior approximation is searched. For string input,\n                we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of\n                course, you can also specify your own variational family by passing a\n                `parameterized` distribution object i.e. a torch.distributions\n                Distribution with methods `parameters` returning an iterable of all\n                parameters (you can pass them within the paramters/modules attribute).\n                Additionally, we allow a `Callable`, which allows you the pass a\n                `builder` function, which if called returns an distribution. This may be\n                useful for setting the hyperparameters e.g. `num_transfroms:int` by\n                using the `get_flow_builder` method specifying the hyperparameters. If q\n                is already a `VIPosterior`, then the arguments will be copied from it\n                (relevant for multi-round training).\n\n\n        \"\"\"\n        self.set_q(q)\n\n    def set_q(\n        self,\n        q: Union[str, PyroTransformedDistribution, \"VIPosterior\", Callable],\n        parameters: Iterable = [],\n        modules: Iterable = [],\n    ) -&gt; None:\n        \"\"\"Defines the variational family.\n\n        You can specify over which parameters/modules we optimize. This is required for\n        custom distributions which e.g. do not inherit nn.Modules or has the function\n        `parameters` or `modules` to give direct access to trainable parameters.\n        Further, you can pass a function, which constructs a variational distribution\n        if called.\n\n        Args:\n            q: Variational distribution, either string, distribution, or a VIPosterior\n                object. This specifies a parametric class of distribution over which\n                the best possible posterior approximation is searched. For string input,\n                we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of\n                course, you can also specify your own variational family by passing a\n                `parameterized` distribution object i.e. a torch.distributions\n                Distribution with methods `parameters` returning an iterable of all\n                parameters (you can pass them within the paramters/modules attribute).\n                Additionally, we allow a `Callable`, which allows you the pass a\n                `builder` function, which if called returns an distribution. This may be\n                useful for setting the hyperparameters e.g. `num_transfroms:int` by\n                using the `get_flow_builder` method specifying the hyperparameters. If q\n                is already a `VIPosterior`, then the arguments will be copied from it\n                (relevant for multi-round training).\n            parameters: List of parameters associated with the distribution object.\n            modules: List of modules associated with the distribution object.\n\n        \"\"\"\n        self._q_arg = (q, parameters, modules)\n        if isinstance(q, Distribution):\n            q = adapt_variational_distribution(\n                q,\n                self._prior,\n                self.link_transform,\n                parameters=parameters,\n                modules=modules,\n            )\n            make_object_deepcopy_compatible(q)\n            self_custom_q_init_cache = deepcopy(q)\n            self._q_build_fn = lambda *args, **kwargs: self_custom_q_init_cache\n            self._trained_on = None\n        elif isinstance(q, (str, Callable)):\n            if isinstance(q, str):\n                self._q_build_fn = get_flow_builder(q)\n            else:\n                self._q_build_fn = q\n\n            q = self._q_build_fn(\n                self._prior.event_shape,\n                self.link_transform,\n                device=self._device,\n            )\n            make_object_deepcopy_compatible(q)\n            self._trained_on = None\n        elif isinstance(q, VIPosterior):\n            self._q_build_fn = q._q_build_fn\n            self._trained_on = q._trained_on\n            self.vi_method = q.vi_method  # type: ignore\n            self._device = q._device\n            self._prior = q._prior\n            self._x = q._x\n            self._q_arg = q._q_arg\n            make_object_deepcopy_compatible(q.q)\n            q = deepcopy(q.q)\n        move_all_tensor_to_device(q, self._device)\n        assert isinstance(\n            q, Distribution\n        ), \"\"\"Something went wrong when initializing the variational distribution.\n            Please create an issue on github https://github.com/mackelab/sbi/issues\"\"\"\n        check_variational_distribution(q, self._prior)\n        self._q = q\n\n    @property\n    def vi_method(self) -&gt; str:\n        \"\"\"Variational inference method e.g. one of [rKL, fKL, IW, alpha].\"\"\"\n        return self._vi_method\n\n    @vi_method.setter\n    def vi_method(self, method: str) -&gt; None:\n        \"\"\"See `set_vi_method`.\"\"\"\n        self.set_vi_method(method)\n\n    def set_vi_method(self, method: str) -&gt; \"VIPosterior\":\n        \"\"\"Sets variational inference method.\n\n        Args:\n            method: One of [rKL, fKL, IW, alpha].\n\n        Returns:\n            `VIPosterior` for chainable calls.\n        \"\"\"\n        self._vi_method = method\n        self._optimizer_builder = get_VI_method(method)\n        return self\n\n    def sample(\n        self,\n        sample_shape: Shape = torch.Size(),\n        x: Optional[Tensor] = None,\n        **kwargs,\n    ) -&gt; Tensor:\n        \"\"\"Samples from the variational posterior distribution.\n\n        Args:\n            sample_shape: Shape of samples\n\n        Returns:\n            Samples from posterior.\n        \"\"\"\n        x = self._x_else_default_x(x)\n        if self._trained_on is None or (x != self._trained_on).all():\n            raise AttributeError(\n                f\"The variational posterior was not fit on the specified `default_x` \"\n                f\"{x}. Please train using `posterior.train()`.\"\n            )\n        samples = self.q.sample(torch.Size(sample_shape))\n        return samples.reshape((*sample_shape, samples.shape[-1]))\n\n    def sample_batched(\n        self,\n        sample_shape: Shape,\n        x: Tensor,\n        max_sampling_batch_size: int = 10000,\n        show_progress_bars: bool = True,\n    ) -&gt; Tensor:\n        raise NotImplementedError(\n            \"Batched sampling is not implemented for VIPosterior. \\\n            Alternatively you can use `sample` in a loop \\\n            [posterior.sample(theta, x_o) for x_o in x].\"\n        )\n\n    def log_prob(\n        self,\n        theta: Tensor,\n        x: Optional[Tensor] = None,\n        track_gradients: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Returns the log-probability of theta under the variational posterior.\n\n        Args:\n            theta: Parameters\n            track_gradients: Whether the returned tensor supports tracking gradients.\n                This can be helpful for e.g. sensitivity analysis but increases memory\n                consumption.\n\n        Returns:\n            `len($\\theta$)`-shaped log-probability.\n        \"\"\"\n        x = self._x_else_default_x(x)\n        if self._trained_on is None or (x != self._trained_on).all():\n            raise AttributeError(\n                f\"The variational posterior was not fit using observation {x}.\\\n                     Please train.\"\n            )\n        with torch.set_grad_enabled(track_gradients):\n            theta = ensure_theta_batched(torch.as_tensor(theta))\n            return self.q.log_prob(theta)\n\n    def train(\n        self,\n        x: Optional[TorchTensor] = None,\n        n_particles: int = 256,\n        learning_rate: float = 1e-3,\n        gamma: float = 0.999,\n        max_num_iters: int = 2000,\n        min_num_iters: int = 10,\n        clip_value: float = 10.0,\n        warm_up_rounds: int = 100,\n        retrain_from_scratch: bool = False,\n        reset_optimizer: bool = False,\n        show_progress_bar: bool = True,\n        check_for_convergence: bool = True,\n        quality_control: bool = True,\n        quality_control_metric: str = \"psis\",\n        **kwargs,\n    ) -&gt; \"VIPosterior\":\n        \"\"\"This method trains the variational posterior.\n\n        Args:\n            x: The observation.\n            n_particles: Number of samples to approximate expectations within the\n                variational bounds. The larger the more accurate are gradient\n                estimates, but the computational cost per iteration increases.\n            learning_rate: Learning rate of the optimizer.\n            gamma: Learning rate decay per iteration. We use an exponential decay\n                scheduler.\n            max_num_iters: Maximum number of iterations.\n            min_num_iters: Minimum number of iterations.\n            clip_value: Gradient clipping value, decreasing may help if you see invalid\n                values.\n            warm_up_rounds: Initialize the posterior as the prior.\n            retrain_from_scratch: Retrain the variational distributions from scratch.\n            reset_optimizer: Reset the divergence optimizer\n            show_progress_bar: If any progress report should be displayed.\n            quality_control: If False quality control is skipped.\n            quality_control_metric: Which metric to use for evaluating the quality.\n            kwargs: Hyperparameters check corresponding `DivergenceOptimizer` for detail\n                eps: Determines sensitivity of convergence check.\n                retain_graph: Boolean which decides whether to retain the computation\n                    graph. This may be required for some `exotic` user-specified q's.\n                optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See\n                    `DivergenceOptimizer` for details.\n                scheduler: A PyTorch learning rate scheduler. See\n                    `DivergenceOptimizer` for details.\n                alpha: Only used if vi_method=`alpha`. Determines the alpha divergence.\n                K: Only used if vi_method=`IW`. Determines the number of importance\n                    weighted particles.\n                stick_the_landing: If one should use the STL estimator (only for rKL,\n                    IW, alpha).\n                dreg: If one should use the DREG estimator (only for rKL, IW, alpha).\n                weight_transform: Callable applied to importance weights (only for fKL)\n        Returns:\n            VIPosterior: `VIPosterior` (can be used to chain calls).\n        \"\"\"\n        # Update optimizer with current arguments.\n        if self._optimizer is not None:\n            self._optimizer.update({**locals(), **kwargs})\n\n        # Init q and the optimizer if necessary\n        if retrain_from_scratch:\n            self.q = self._q_build_fn()  # type: ignore\n            self._optimizer = self._optimizer_builder(\n                self.potential_fn,\n                self.q,\n                lr=learning_rate,\n                clip_value=clip_value,\n                gamma=gamma,\n                n_particles=n_particles,\n                prior=self._prior,\n                **kwargs,\n            )\n\n        if (\n            reset_optimizer\n            or self._optimizer is None\n            or not isinstance(self._optimizer, self._optimizer_builder)\n        ):\n            self._optimizer = self._optimizer_builder(\n                self.potential_fn,\n                self.q,\n                lr=learning_rate,\n                clip_value=clip_value,\n                gamma=gamma,\n                n_particles=n_particles,\n                prior=self._prior,\n                **kwargs,\n            )\n\n        # Check context\n        x = atleast_2d_float32_tensor(self._x_else_default_x(x)).to(  # type: ignore\n            self._device\n        )\n\n        already_trained = self._trained_on is not None and (x == self._trained_on).all()\n\n        # Optimize\n        optimizer = self._optimizer\n        optimizer.to(self._device)\n        optimizer.reset_loss_stats()\n\n        if show_progress_bar:\n            iters = tqdm(range(max_num_iters))\n        else:\n            iters = range(max_num_iters)\n\n        # Warmup before training\n        if reset_optimizer or (not optimizer.warm_up_was_done and not already_trained):\n            if show_progress_bar:\n                iters.set_description(  # type: ignore\n                    \"Warmup phase, this may take a few seconds...\"\n                )\n            optimizer.warm_up(warm_up_rounds)\n\n        for i in iters:\n            optimizer.step(x)\n            mean_loss, std_loss = optimizer.get_loss_stats()\n            # Update progress bar\n            if show_progress_bar:\n                assert isinstance(iters, tqdm)\n                iters.set_description(  # type: ignore\n                    f\"Loss: {np.round(float(mean_loss), 2)}\"\n                    f\"Std: {np.round(float(std_loss), 2)}\"\n                )\n            # Check for convergence\n            if check_for_convergence and i &gt; min_num_iters and optimizer.converged():\n                if show_progress_bar:\n                    print(f\"\\nConverged with loss: {np.round(float(mean_loss), 2)}\")\n                break\n        # Training finished:\n        self._trained_on = x\n\n        # Evaluate quality\n        if quality_control:\n            try:\n                self.evaluate(quality_control_metric=quality_control_metric)\n            except Exception as e:\n                print(\n                    f\"Quality control showed a low quality of the variational \"\n                    f\"posterior. We are automatically retraining the variational \"\n                    f\"posterior from scratch with a smaller learning rate. \"\n                    f\"Alternatively, if you want to skip quality control, please \"\n                    f\"retrain with `VIPosterior.train(..., quality_control=False)`. \"\n                    f\"\\nThe error that occured is: {e}\"\n                )\n                self.train(\n                    learning_rate=learning_rate * 0.1,\n                    retrain_from_scratch=True,\n                    reset_optimizer=True,\n                )\n\n        return self\n\n    def evaluate(self, quality_control_metric: str = \"psis\", N: int = int(5e4)) -&gt; None:\n        \"\"\"This function will evaluate the quality of the variational posterior\n        distribution. We currently support two different metrics of type `psis`, which\n        checks the quality based on the tails of importance weights (there should not be\n        much with a large one), or `prop` which checks the proportionality between q\n        and potential_fn.\n\n        NOTE: In our experience `prop` is sensitive to distinguish ``good`` from ``ok``\n        whereas `psis` is more sensitive in distinguishing `very bad` from `ok`.\n\n        Args:\n            quality_control_metric: The metric of choice, we currently support [psis,\n                prop, prop_prior].\n            N: Number of samples which is used to evaluate the metric.\n        \"\"\"\n        quality_control_fn, quality_control_msg = get_quality_metric(\n            quality_control_metric\n        )\n        metric = round(float(quality_control_fn(self, N=N)), 3)\n        print(f\"Quality Score: {metric} \" + quality_control_msg)\n\n    def map(\n        self,\n        x: Optional[TorchTensor] = None,\n        num_iter: int = 1_000,\n        num_to_optimize: int = 100,\n        learning_rate: float = 0.01,\n        init_method: Union[str, TorchTensor] = \"proposal\",\n        num_init_samples: int = 10_000,\n        save_best_every: int = 10,\n        show_progress_bars: bool = False,\n        force_update: bool = False,\n    ) -&gt; Tensor:\n        r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n        The method can be interrupted (Ctrl-C) when the user sees that the\n        log-probability converges. The best estimate will be saved in `self._map` and\n        can be accessed with `self.map()`. The MAP is obtained by running gradient\n        ascent from a given number of starting positions (samples from the posterior\n        with the highest log-probability). After the optimization is done, we select the\n        parameter set that has the highest log-probability after the optimization.\n\n        Warning: The default values used by this function are not well-tested. They\n        might require hand-tuning for the problem at hand.\n\n        For developers: if the prior is a `BoxUniform`, we carry out the optimization\n        in unbounded space and transform the result back into bounded space.\n\n        Args:\n            x: Deprecated - use `.set_default_x()` prior to `.map()`.\n            num_iter: Number of optimization steps that the algorithm takes\n                to find the MAP.\n            learning_rate: Learning rate of the optimizer.\n            init_method: How to select the starting parameters for the optimization. If\n                it is a string, it can be either [`posterior`, `prior`], which samples\n                the respective distribution `num_init_samples` times. If it is a\n                tensor, the tensor will be used as init locations.\n            num_init_samples: Draw this number of samples from the posterior and\n                evaluate the log-probability of all of them.\n            num_to_optimize: From the drawn `num_init_samples`, use the\n                `num_to_optimize` with highest log-probability as the initial points\n                for the optimization.\n            save_best_every: The best log-probability is computed, saved in the\n                `map`-attribute, and printed every `save_best_every`-th iteration.\n                Computing the best log-probability creates a significant overhead\n                (thus, the default is `10`.)\n            show_progress_bars: Whether to show a progressbar during sampling from\n                the posterior.\n            force_update: Whether to re-calculate the MAP when x is unchanged and\n                have a cached value.\n            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n                {'norm_posterior': True} for SNPE.\n\n        Returns:\n            The MAP estimate.\n        \"\"\"\n        self.proposal = self.q\n        return super().map(\n            x=x,\n            num_iter=num_iter,\n            num_to_optimize=num_to_optimize,\n            learning_rate=learning_rate,\n            init_method=init_method,\n            num_init_samples=num_init_samples,\n            save_best_every=save_best_every,\n            show_progress_bars=show_progress_bars,\n            force_update=force_update,\n        )\n\n    def __deepcopy__(self, memo: Optional[Dict] = None) -&gt; \"VIPosterior\":\n        \"\"\"This method is called when using `copy.deepcopy` on the object.\n\n        It defines how the object is copied. We need to overwrite this method, since the\n        default implementation does use __getstate__ and __setstate__ which we overwrite\n        to enable pickling (and in particular the necessary modifications are\n        incompatible deep copying).\n\n        Args:\n            memo (Optional[Dict], optional): Deep copy internal memo. Defaults to None.\n\n        Returns:\n            VIPosterior: Deep copy of the VIPosterior.\n        \"\"\"\n        if memo is None:\n            memo = {}\n        # Create a new instance of the class\n        cls = self.__class__\n        result = cls.__new__(cls)\n        # Add to memo\n        memo[id(self)] = result\n        # Copy attributes\n        for k, v in self.__dict__.items():\n            setattr(result, k, copy.deepcopy(v, memo))\n        return result\n\n    def __getstate__(self) -&gt; Dict:\n        \"\"\"This method is called when pickling the object.\n\n        It defines what is pickled. We need to overwrite this method, since some parts\n        due not support pickle protocols (e.g. due to local functions, etc.).\n\n        Returns:\n            Dict: All attributes of the VIPosterior.\n        \"\"\"\n        self._optimizer = None\n        self.__deepcopy__ = None  # type: ignore\n        self._q_build_fn = None\n        self._q.__deepcopy__ = None  # type: ignore\n        return self.__dict__\n\n    def __setstate__(self, state_dict: Dict):\n        \"\"\"This method is called when unpickling the object.\n\n        Especially, we need to restore the removed attributes and ensure that the object\n        e.g. remains deep copy compatible.\n\n        Args:\n            state_dict: Given state dictionary, we will restore the object from it.\n        \"\"\"\n        self.__dict__ = state_dict\n        q = deepcopy(self._q)\n        # Restore removed attributes\n        self.set_q(*self._q_arg)\n        self._q = q\n        make_object_deepcopy_compatible(self)\n        make_object_deepcopy_compatible(self.q)\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.q","title":"<code>q: Distribution</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the variational posterior.</p>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.vi_method","title":"<code>vi_method: str</code>  <code>property</code> <code>writable</code>","text":"<p>Variational inference method e.g. one of [rKL, fKL, IW, alpha].</p>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.__deepcopy__","title":"<code>__deepcopy__(memo=None)</code>","text":"<p>This method is called when using <code>copy.deepcopy</code> on the object.</p> <p>It defines how the object is copied. We need to overwrite this method, since the default implementation does use getstate and setstate which we overwrite to enable pickling (and in particular the necessary modifications are incompatible deep copying).</p> <p>Parameters:</p> Name Type Description Default <code>memo</code> <code>Optional[Dict]</code> <p>Deep copy internal memo. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>VIPosterior</code> <code>VIPosterior</code> <p>Deep copy of the VIPosterior.</p> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def __deepcopy__(self, memo: Optional[Dict] = None) -&gt; \"VIPosterior\":\n    \"\"\"This method is called when using `copy.deepcopy` on the object.\n\n    It defines how the object is copied. We need to overwrite this method, since the\n    default implementation does use __getstate__ and __setstate__ which we overwrite\n    to enable pickling (and in particular the necessary modifications are\n    incompatible deep copying).\n\n    Args:\n        memo (Optional[Dict], optional): Deep copy internal memo. Defaults to None.\n\n    Returns:\n        VIPosterior: Deep copy of the VIPosterior.\n    \"\"\"\n    if memo is None:\n        memo = {}\n    # Create a new instance of the class\n    cls = self.__class__\n    result = cls.__new__(cls)\n    # Add to memo\n    memo[id(self)] = result\n    # Copy attributes\n    for k, v in self.__dict__.items():\n        setattr(result, k, copy.deepcopy(v, memo))\n    return result\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.__getstate__","title":"<code>__getstate__()</code>","text":"<p>This method is called when pickling the object.</p> <p>It defines what is pickled. We need to overwrite this method, since some parts due not support pickle protocols (e.g. due to local functions, etc.).</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>All attributes of the VIPosterior.</p> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def __getstate__(self) -&gt; Dict:\n    \"\"\"This method is called when pickling the object.\n\n    It defines what is pickled. We need to overwrite this method, since some parts\n    due not support pickle protocols (e.g. due to local functions, etc.).\n\n    Returns:\n        Dict: All attributes of the VIPosterior.\n    \"\"\"\n    self._optimizer = None\n    self.__deepcopy__ = None  # type: ignore\n    self._q_build_fn = None\n    self._q.__deepcopy__ = None  # type: ignore\n    return self.__dict__\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.__init__","title":"<code>__init__(potential_fn, prior=None, q='maf', theta_transform=None, vi_method='rKL', device='cpu', x_shape=None, parameters=[], modules=[])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>potential_fn</code> <code>Union[Callable, BasePotential]</code> <p>The potential function from which to draw samples. Must be a <code>BasePotential</code> or a <code>Callable</code> which takes <code>theta</code> and <code>x_o</code> as inputs.</p> required <code>prior</code> <code>Optional[TorchDistribution]</code> <p>This is the prior distribution. Note that this is only used to check/construct the variational distribution or within some quality metrics. Please make sure that this matches with the prior within the potential_fn. If <code>None</code> is given, we will try to infer it from potential_fn or q, if this fails we raise an Error.</p> <code>None</code> <code>q</code> <code>Union[str, PyroTransformedDistribution, VIPosterior, Callable]</code> <p>Variational distribution, either string, <code>TransformedDistribution</code>, or a <code>VIPosterior</code> object. This specifies a parametric class of distribution over which the best possible posterior approximation is searched. For string input, we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. You can also specify your own variational family by passing a pyro <code>TransformedDistribution</code>. Additionally, we allow a <code>Callable</code>, which allows you the pass a <code>builder</code> function, which if called returns a distribution. This may be useful for setting the hyperparameters e.g. <code>num_transfroms</code> within the <code>get_flow_builder</code> method specifying the number of transformations within a normalizing flow. If q is already a <code>VIPosterior</code>, then the arguments will be copied from it (relevant for multi-round training).</p> <code>'maf'</code> <code>theta_transform</code> <code>Optional[TorchTransform]</code> <p>Maps form prior support to unconstrained space. The inverse is used here to ensure that the posterior support is equal to that of the prior.</p> <code>None</code> <code>vi_method</code> <code>str</code> <p>This specifies the variational methods which are used to fit q to the posterior. We currently support [rKL, fKL, IW, alpha]. Note that some of the divergences are <code>mode seeking</code> i.e. they underestimate variance and collapse on multimodal targets (<code>rKL</code>, <code>alpha</code> for alpha &gt; 1) and some are <code>mass covering</code> i.e. they overestimate variance but typically cover all modes (<code>fKL</code>, <code>IW</code>, <code>alpha</code> for alpha &lt; 1).</p> <code>'rKL'</code> <code>device</code> <code>str</code> <p>Training device, e.g., <code>cpu</code>, <code>cuda</code> or <code>cuda:0</code>. We will ensure that all other objects are also on this device.</p> <code>'cpu'</code> <code>x_shape</code> <code>Optional[Size]</code> <p>Deprecated, should not be passed.</p> <code>None</code> <code>parameters</code> <code>Iterable</code> <p>List of parameters of the variational posterior. This is only required for user-defined q i.e. if q does not have a <code>parameters</code> attribute.</p> <code>[]</code> <code>modules</code> <code>Iterable</code> <p>List of modules of the variational posterior. This is only required for user-defined q i.e. if q does not have a <code>modules</code> attribute.</p> <code>[]</code> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def __init__(\n    self,\n    potential_fn: Union[Callable, BasePotential],\n    prior: Optional[TorchDistribution] = None,\n    q: Union[str, PyroTransformedDistribution, \"VIPosterior\", Callable] = \"maf\",\n    theta_transform: Optional[TorchTransform] = None,\n    vi_method: str = \"rKL\",\n    device: str = \"cpu\",\n    x_shape: Optional[torch.Size] = None,\n    parameters: Iterable = [],\n    modules: Iterable = [],\n):\n    \"\"\"\n    Args:\n        potential_fn: The potential function from which to draw samples. Must be a\n            `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.\n        prior: This is the prior distribution. Note that this is only\n            used to check/construct the variational distribution or within some\n            quality metrics. Please make sure that this matches with the prior\n            within the potential_fn. If `None` is given, we will try to infer it\n            from potential_fn or q, if this fails we raise an Error.\n        q: Variational distribution, either string, `TransformedDistribution`, or a\n            `VIPosterior` object. This specifies a parametric class of distribution\n            over which the best possible posterior approximation is searched. For\n            string input, we currently support [nsf, scf, maf, mcf, gaussian,\n            gaussian_diag]. You can also specify your own variational family by\n            passing a pyro `TransformedDistribution`.\n            Additionally, we allow a `Callable`, which allows you the pass a\n            `builder` function, which if called returns a distribution. This may be\n            useful for setting the hyperparameters e.g. `num_transfroms` within the\n            `get_flow_builder` method specifying the number of transformations\n            within a normalizing flow. If q is already a `VIPosterior`, then the\n            arguments will be copied from it (relevant for multi-round training).\n        theta_transform: Maps form prior support to unconstrained space. The\n            inverse is used here to ensure that the posterior support is equal to\n            that of the prior.\n        vi_method: This specifies the variational methods which are used to fit q to\n            the posterior. We currently support [rKL, fKL, IW, alpha]. Note that\n            some of the divergences are `mode seeking` i.e. they underestimate\n            variance and collapse on multimodal targets (`rKL`, `alpha` for alpha &gt;\n            1) and some are `mass covering` i.e. they overestimate variance but\n            typically cover all modes (`fKL`, `IW`, `alpha` for alpha &lt; 1).\n        device: Training device, e.g., `cpu`, `cuda` or `cuda:0`. We will ensure\n            that all other objects are also on this device.\n        x_shape: Deprecated, should not be passed.\n        parameters: List of parameters of the variational posterior. This is only\n            required for user-defined q i.e. if q does not have a `parameters`\n            attribute.\n        modules: List of modules of the variational posterior. This is only\n            required for user-defined q i.e. if q does not have a `modules`\n            attribute.\n    \"\"\"\n    super().__init__(potential_fn, theta_transform, device, x_shape=x_shape)\n\n    # Especially the prior may be on another device -&gt; move it...\n    self._device = device\n    self.potential_fn.device = device\n    move_all_tensor_to_device(self.potential_fn, device)\n\n    # Get prior and previous builds\n    if prior is not None:\n        self._prior = prior\n    elif hasattr(self.potential_fn, \"prior\") and isinstance(\n        self.potential_fn.prior, Distribution\n    ):\n        self._prior = self.potential_fn.prior\n    elif isinstance(q, VIPosterior) and isinstance(q._prior, Distribution):\n        self._prior = q._prior\n    else:\n        raise ValueError(\n            \"We could not find a suitable prior distribution within `potential_fn` \"\n            \"or `q` (if a VIPosterior is given). Please explicitly specify a prior.\"\n        )\n    move_all_tensor_to_device(self._prior, device)\n    self._optimizer = None\n\n    # In contrast to MCMC we want to project into constrained space.\n    if theta_transform is None:\n        self.link_transform = mcmc_transform(self._prior).inv\n    else:\n        self.link_transform = theta_transform.inv\n\n    # This will set the variational distribution and VI method\n    self.set_q(q, parameters=parameters, modules=modules)\n    self.set_vi_method(vi_method)\n\n    self._purpose = (\n        \"It provides Variational inference to .sample() from the posterior and \"\n        \"can evaluate the _normalized_ posterior density with .log_prob().\"\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.__setstate__","title":"<code>__setstate__(state_dict)</code>","text":"<p>This method is called when unpickling the object.</p> <p>Especially, we need to restore the removed attributes and ensure that the object e.g. remains deep copy compatible.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict</code> <p>Given state dictionary, we will restore the object from it.</p> required Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def __setstate__(self, state_dict: Dict):\n    \"\"\"This method is called when unpickling the object.\n\n    Especially, we need to restore the removed attributes and ensure that the object\n    e.g. remains deep copy compatible.\n\n    Args:\n        state_dict: Given state dictionary, we will restore the object from it.\n    \"\"\"\n    self.__dict__ = state_dict\n    q = deepcopy(self._q)\n    # Restore removed attributes\n    self.set_q(*self._q_arg)\n    self._q = q\n    make_object_deepcopy_compatible(self)\n    make_object_deepcopy_compatible(self.q)\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.evaluate","title":"<code>evaluate(quality_control_metric='psis', N=int(50000.0))</code>","text":"<p>This function will evaluate the quality of the variational posterior distribution. We currently support two different metrics of type <code>psis</code>, which checks the quality based on the tails of importance weights (there should not be much with a large one), or <code>prop</code> which checks the proportionality between q and potential_fn.</p> <p>NOTE: In our experience <code>prop</code> is sensitive to distinguish <code>good</code> from <code>ok</code> whereas <code>psis</code> is more sensitive in distinguishing <code>very bad</code> from <code>ok</code>.</p> <p>Parameters:</p> Name Type Description Default <code>quality_control_metric</code> <code>str</code> <p>The metric of choice, we currently support [psis, prop, prop_prior].</p> <code>'psis'</code> <code>N</code> <code>int</code> <p>Number of samples which is used to evaluate the metric.</p> <code>int(50000.0)</code> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def evaluate(self, quality_control_metric: str = \"psis\", N: int = int(5e4)) -&gt; None:\n    \"\"\"This function will evaluate the quality of the variational posterior\n    distribution. We currently support two different metrics of type `psis`, which\n    checks the quality based on the tails of importance weights (there should not be\n    much with a large one), or `prop` which checks the proportionality between q\n    and potential_fn.\n\n    NOTE: In our experience `prop` is sensitive to distinguish ``good`` from ``ok``\n    whereas `psis` is more sensitive in distinguishing `very bad` from `ok`.\n\n    Args:\n        quality_control_metric: The metric of choice, we currently support [psis,\n            prop, prop_prior].\n        N: Number of samples which is used to evaluate the metric.\n    \"\"\"\n    quality_control_fn, quality_control_msg = get_quality_metric(\n        quality_control_metric\n    )\n    metric = round(float(quality_control_fn(self, N=N)), 3)\n    print(f\"Quality Score: {metric} \" + quality_control_msg)\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.log_prob","title":"<code>log_prob(theta, x=None, track_gradients=False)</code>","text":"<p>Returns the log-probability of theta under the variational posterior.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>Tensor</code> <p>Parameters</p> required <code>track_gradients</code> <code>bool</code> <p>Whether the returned tensor supports tracking gradients. This can be helpful for e.g. sensitivity analysis but increases memory consumption.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p><code>len($\\theta$)</code>-shaped log-probability.</p> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def log_prob(\n    self,\n    theta: Tensor,\n    x: Optional[Tensor] = None,\n    track_gradients: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Returns the log-probability of theta under the variational posterior.\n\n    Args:\n        theta: Parameters\n        track_gradients: Whether the returned tensor supports tracking gradients.\n            This can be helpful for e.g. sensitivity analysis but increases memory\n            consumption.\n\n    Returns:\n        `len($\\theta$)`-shaped log-probability.\n    \"\"\"\n    x = self._x_else_default_x(x)\n    if self._trained_on is None or (x != self._trained_on).all():\n        raise AttributeError(\n            f\"The variational posterior was not fit using observation {x}.\\\n                 Please train.\"\n        )\n    with torch.set_grad_enabled(track_gradients):\n        theta = ensure_theta_batched(torch.as_tensor(theta))\n        return self.q.log_prob(theta)\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.map","title":"<code>map(x=None, num_iter=1000, num_to_optimize=100, learning_rate=0.01, init_method='proposal', num_init_samples=10000, save_best_every=10, show_progress_bars=False, force_update=False)</code>","text":"<p>Returns the maximum-a-posteriori estimate (MAP).</p> <p>The method can be interrupted (Ctrl-C) when the user sees that the log-probability converges. The best estimate will be saved in <code>self._map</code> and can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient ascent from a given number of starting positions (samples from the posterior with the highest log-probability). After the optimization is done, we select the parameter set that has the highest log-probability after the optimization.</p> <p>Warning: The default values used by this function are not well-tested. They might require hand-tuning for the problem at hand.</p> <p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization in unbounded space and transform the result back into bounded space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[TorchTensor]</code> <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p> <code>None</code> <code>num_iter</code> <code>int</code> <p>Number of optimization steps that the algorithm takes to find the MAP.</p> <code>1000</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer.</p> <code>0.01</code> <code>init_method</code> <code>Union[str, TorchTensor]</code> <p>How to select the starting parameters for the optimization. If it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples the respective distribution <code>num_init_samples</code> times. If it is a tensor, the tensor will be used as init locations.</p> <code>'proposal'</code> <code>num_init_samples</code> <code>int</code> <p>Draw this number of samples from the posterior and evaluate the log-probability of all of them.</p> <code>10000</code> <code>num_to_optimize</code> <code>int</code> <p>From the drawn <code>num_init_samples</code>, use the <code>num_to_optimize</code> with highest log-probability as the initial points for the optimization.</p> <code>100</code> <code>save_best_every</code> <code>int</code> <p>The best log-probability is computed, saved in the <code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration. Computing the best log-probability creates a significant overhead (thus, the default is <code>10</code>.)</p> <code>10</code> <code>show_progress_bars</code> <code>bool</code> <p>Whether to show a progressbar during sampling from the posterior.</p> <code>False</code> <code>force_update</code> <code>bool</code> <p>Whether to re-calculate the MAP when x is unchanged and have a cached value.</p> <code>False</code> <code>log_prob_kwargs</code> <p>Will be empty for SNLE and SNRE. Will contain {\u2018norm_posterior\u2019: True} for SNPE.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MAP estimate.</p> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def map(\n    self,\n    x: Optional[TorchTensor] = None,\n    num_iter: int = 1_000,\n    num_to_optimize: int = 100,\n    learning_rate: float = 0.01,\n    init_method: Union[str, TorchTensor] = \"proposal\",\n    num_init_samples: int = 10_000,\n    save_best_every: int = 10,\n    show_progress_bars: bool = False,\n    force_update: bool = False,\n) -&gt; Tensor:\n    r\"\"\"Returns the maximum-a-posteriori estimate (MAP).\n\n    The method can be interrupted (Ctrl-C) when the user sees that the\n    log-probability converges. The best estimate will be saved in `self._map` and\n    can be accessed with `self.map()`. The MAP is obtained by running gradient\n    ascent from a given number of starting positions (samples from the posterior\n    with the highest log-probability). After the optimization is done, we select the\n    parameter set that has the highest log-probability after the optimization.\n\n    Warning: The default values used by this function are not well-tested. They\n    might require hand-tuning for the problem at hand.\n\n    For developers: if the prior is a `BoxUniform`, we carry out the optimization\n    in unbounded space and transform the result back into bounded space.\n\n    Args:\n        x: Deprecated - use `.set_default_x()` prior to `.map()`.\n        num_iter: Number of optimization steps that the algorithm takes\n            to find the MAP.\n        learning_rate: Learning rate of the optimizer.\n        init_method: How to select the starting parameters for the optimization. If\n            it is a string, it can be either [`posterior`, `prior`], which samples\n            the respective distribution `num_init_samples` times. If it is a\n            tensor, the tensor will be used as init locations.\n        num_init_samples: Draw this number of samples from the posterior and\n            evaluate the log-probability of all of them.\n        num_to_optimize: From the drawn `num_init_samples`, use the\n            `num_to_optimize` with highest log-probability as the initial points\n            for the optimization.\n        save_best_every: The best log-probability is computed, saved in the\n            `map`-attribute, and printed every `save_best_every`-th iteration.\n            Computing the best log-probability creates a significant overhead\n            (thus, the default is `10`.)\n        show_progress_bars: Whether to show a progressbar during sampling from\n            the posterior.\n        force_update: Whether to re-calculate the MAP when x is unchanged and\n            have a cached value.\n        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain\n            {'norm_posterior': True} for SNPE.\n\n    Returns:\n        The MAP estimate.\n    \"\"\"\n    self.proposal = self.q\n    return super().map(\n        x=x,\n        num_iter=num_iter,\n        num_to_optimize=num_to_optimize,\n        learning_rate=learning_rate,\n        init_method=init_method,\n        num_init_samples=num_init_samples,\n        save_best_every=save_best_every,\n        show_progress_bars=show_progress_bars,\n        force_update=force_update,\n    )\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.sample","title":"<code>sample(sample_shape=torch.Size(), x=None, **kwargs)</code>","text":"<p>Samples from the variational posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Shape</code> <p>Shape of samples</p> <code>Size()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Samples from posterior.</p> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def sample(\n    self,\n    sample_shape: Shape = torch.Size(),\n    x: Optional[Tensor] = None,\n    **kwargs,\n) -&gt; Tensor:\n    \"\"\"Samples from the variational posterior distribution.\n\n    Args:\n        sample_shape: Shape of samples\n\n    Returns:\n        Samples from posterior.\n    \"\"\"\n    x = self._x_else_default_x(x)\n    if self._trained_on is None or (x != self._trained_on).all():\n        raise AttributeError(\n            f\"The variational posterior was not fit on the specified `default_x` \"\n            f\"{x}. Please train using `posterior.train()`.\"\n        )\n    samples = self.q.sample(torch.Size(sample_shape))\n    return samples.reshape((*sample_shape, samples.shape[-1]))\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.set_q","title":"<code>set_q(q, parameters=[], modules=[])</code>","text":"<p>Defines the variational family.</p> <p>You can specify over which parameters/modules we optimize. This is required for custom distributions which e.g. do not inherit nn.Modules or has the function <code>parameters</code> or <code>modules</code> to give direct access to trainable parameters. Further, you can pass a function, which constructs a variational distribution if called.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Union[str, PyroTransformedDistribution, VIPosterior, Callable]</code> <p>Variational distribution, either string, distribution, or a VIPosterior object. This specifies a parametric class of distribution over which the best possible posterior approximation is searched. For string input, we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of course, you can also specify your own variational family by passing a <code>parameterized</code> distribution object i.e. a torch.distributions Distribution with methods <code>parameters</code> returning an iterable of all parameters (you can pass them within the paramters/modules attribute). Additionally, we allow a <code>Callable</code>, which allows you the pass a <code>builder</code> function, which if called returns an distribution. This may be useful for setting the hyperparameters e.g. <code>num_transfroms:int</code> by using the <code>get_flow_builder</code> method specifying the hyperparameters. If q is already a <code>VIPosterior</code>, then the arguments will be copied from it (relevant for multi-round training).</p> required <code>parameters</code> <code>Iterable</code> <p>List of parameters associated with the distribution object.</p> <code>[]</code> <code>modules</code> <code>Iterable</code> <p>List of modules associated with the distribution object.</p> <code>[]</code> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def set_q(\n    self,\n    q: Union[str, PyroTransformedDistribution, \"VIPosterior\", Callable],\n    parameters: Iterable = [],\n    modules: Iterable = [],\n) -&gt; None:\n    \"\"\"Defines the variational family.\n\n    You can specify over which parameters/modules we optimize. This is required for\n    custom distributions which e.g. do not inherit nn.Modules or has the function\n    `parameters` or `modules` to give direct access to trainable parameters.\n    Further, you can pass a function, which constructs a variational distribution\n    if called.\n\n    Args:\n        q: Variational distribution, either string, distribution, or a VIPosterior\n            object. This specifies a parametric class of distribution over which\n            the best possible posterior approximation is searched. For string input,\n            we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of\n            course, you can also specify your own variational family by passing a\n            `parameterized` distribution object i.e. a torch.distributions\n            Distribution with methods `parameters` returning an iterable of all\n            parameters (you can pass them within the paramters/modules attribute).\n            Additionally, we allow a `Callable`, which allows you the pass a\n            `builder` function, which if called returns an distribution. This may be\n            useful for setting the hyperparameters e.g. `num_transfroms:int` by\n            using the `get_flow_builder` method specifying the hyperparameters. If q\n            is already a `VIPosterior`, then the arguments will be copied from it\n            (relevant for multi-round training).\n        parameters: List of parameters associated with the distribution object.\n        modules: List of modules associated with the distribution object.\n\n    \"\"\"\n    self._q_arg = (q, parameters, modules)\n    if isinstance(q, Distribution):\n        q = adapt_variational_distribution(\n            q,\n            self._prior,\n            self.link_transform,\n            parameters=parameters,\n            modules=modules,\n        )\n        make_object_deepcopy_compatible(q)\n        self_custom_q_init_cache = deepcopy(q)\n        self._q_build_fn = lambda *args, **kwargs: self_custom_q_init_cache\n        self._trained_on = None\n    elif isinstance(q, (str, Callable)):\n        if isinstance(q, str):\n            self._q_build_fn = get_flow_builder(q)\n        else:\n            self._q_build_fn = q\n\n        q = self._q_build_fn(\n            self._prior.event_shape,\n            self.link_transform,\n            device=self._device,\n        )\n        make_object_deepcopy_compatible(q)\n        self._trained_on = None\n    elif isinstance(q, VIPosterior):\n        self._q_build_fn = q._q_build_fn\n        self._trained_on = q._trained_on\n        self.vi_method = q.vi_method  # type: ignore\n        self._device = q._device\n        self._prior = q._prior\n        self._x = q._x\n        self._q_arg = q._q_arg\n        make_object_deepcopy_compatible(q.q)\n        q = deepcopy(q.q)\n    move_all_tensor_to_device(q, self._device)\n    assert isinstance(\n        q, Distribution\n    ), \"\"\"Something went wrong when initializing the variational distribution.\n        Please create an issue on github https://github.com/mackelab/sbi/issues\"\"\"\n    check_variational_distribution(q, self._prior)\n    self._q = q\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.set_vi_method","title":"<code>set_vi_method(method)</code>","text":"<p>Sets variational inference method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>One of [rKL, fKL, IW, alpha].</p> required <p>Returns:</p> Type Description <code>VIPosterior</code> <p><code>VIPosterior</code> for chainable calls.</p> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def set_vi_method(self, method: str) -&gt; \"VIPosterior\":\n    \"\"\"Sets variational inference method.\n\n    Args:\n        method: One of [rKL, fKL, IW, alpha].\n\n    Returns:\n        `VIPosterior` for chainable calls.\n    \"\"\"\n    self._vi_method = method\n    self._optimizer_builder = get_VI_method(method)\n    return self\n</code></pre>"},{"location":"reference/posteriors/#sbi.inference.posteriors.vi_posterior.VIPosterior.train","title":"<code>train(x=None, n_particles=256, learning_rate=0.001, gamma=0.999, max_num_iters=2000, min_num_iters=10, clip_value=10.0, warm_up_rounds=100, retrain_from_scratch=False, reset_optimizer=False, show_progress_bar=True, check_for_convergence=True, quality_control=True, quality_control_metric='psis', **kwargs)</code>","text":"<p>This method trains the variational posterior.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[TorchTensor]</code> <p>The observation.</p> <code>None</code> <code>n_particles</code> <code>int</code> <p>Number of samples to approximate expectations within the variational bounds. The larger the more accurate are gradient estimates, but the computational cost per iteration increases.</p> <code>256</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of the optimizer.</p> <code>0.001</code> <code>gamma</code> <code>float</code> <p>Learning rate decay per iteration. We use an exponential decay scheduler.</p> <code>0.999</code> <code>max_num_iters</code> <code>int</code> <p>Maximum number of iterations.</p> <code>2000</code> <code>min_num_iters</code> <code>int</code> <p>Minimum number of iterations.</p> <code>10</code> <code>clip_value</code> <code>float</code> <p>Gradient clipping value, decreasing may help if you see invalid values.</p> <code>10.0</code> <code>warm_up_rounds</code> <code>int</code> <p>Initialize the posterior as the prior.</p> <code>100</code> <code>retrain_from_scratch</code> <code>bool</code> <p>Retrain the variational distributions from scratch.</p> <code>False</code> <code>reset_optimizer</code> <code>bool</code> <p>Reset the divergence optimizer</p> <code>False</code> <code>show_progress_bar</code> <code>bool</code> <p>If any progress report should be displayed.</p> <code>True</code> <code>quality_control</code> <code>bool</code> <p>If False quality control is skipped.</p> <code>True</code> <code>quality_control_metric</code> <code>str</code> <p>Which metric to use for evaluating the quality.</p> <code>'psis'</code> <code>kwargs</code> <p>Hyperparameters check corresponding <code>DivergenceOptimizer</code> for detail eps: Determines sensitivity of convergence check. retain_graph: Boolean which decides whether to retain the computation     graph. This may be required for some <code>exotic</code> user-specified q\u2019s. optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See     <code>DivergenceOptimizer</code> for details. scheduler: A PyTorch learning rate scheduler. See     <code>DivergenceOptimizer</code> for details. alpha: Only used if vi_method=<code>alpha</code>. Determines the alpha divergence. K: Only used if vi_method=<code>IW</code>. Determines the number of importance     weighted particles. stick_the_landing: If one should use the STL estimator (only for rKL,     IW, alpha). dreg: If one should use the DREG estimator (only for rKL, IW, alpha). weight_transform: Callable applied to importance weights (only for fKL)</p> <code>{}</code> <p>Returns:     VIPosterior: <code>VIPosterior</code> (can be used to chain calls).</p> Source code in <code>sbi/inference/posteriors/vi_posterior.py</code> <pre><code>def train(\n    self,\n    x: Optional[TorchTensor] = None,\n    n_particles: int = 256,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.999,\n    max_num_iters: int = 2000,\n    min_num_iters: int = 10,\n    clip_value: float = 10.0,\n    warm_up_rounds: int = 100,\n    retrain_from_scratch: bool = False,\n    reset_optimizer: bool = False,\n    show_progress_bar: bool = True,\n    check_for_convergence: bool = True,\n    quality_control: bool = True,\n    quality_control_metric: str = \"psis\",\n    **kwargs,\n) -&gt; \"VIPosterior\":\n    \"\"\"This method trains the variational posterior.\n\n    Args:\n        x: The observation.\n        n_particles: Number of samples to approximate expectations within the\n            variational bounds. The larger the more accurate are gradient\n            estimates, but the computational cost per iteration increases.\n        learning_rate: Learning rate of the optimizer.\n        gamma: Learning rate decay per iteration. We use an exponential decay\n            scheduler.\n        max_num_iters: Maximum number of iterations.\n        min_num_iters: Minimum number of iterations.\n        clip_value: Gradient clipping value, decreasing may help if you see invalid\n            values.\n        warm_up_rounds: Initialize the posterior as the prior.\n        retrain_from_scratch: Retrain the variational distributions from scratch.\n        reset_optimizer: Reset the divergence optimizer\n        show_progress_bar: If any progress report should be displayed.\n        quality_control: If False quality control is skipped.\n        quality_control_metric: Which metric to use for evaluating the quality.\n        kwargs: Hyperparameters check corresponding `DivergenceOptimizer` for detail\n            eps: Determines sensitivity of convergence check.\n            retain_graph: Boolean which decides whether to retain the computation\n                graph. This may be required for some `exotic` user-specified q's.\n            optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See\n                `DivergenceOptimizer` for details.\n            scheduler: A PyTorch learning rate scheduler. See\n                `DivergenceOptimizer` for details.\n            alpha: Only used if vi_method=`alpha`. Determines the alpha divergence.\n            K: Only used if vi_method=`IW`. Determines the number of importance\n                weighted particles.\n            stick_the_landing: If one should use the STL estimator (only for rKL,\n                IW, alpha).\n            dreg: If one should use the DREG estimator (only for rKL, IW, alpha).\n            weight_transform: Callable applied to importance weights (only for fKL)\n    Returns:\n        VIPosterior: `VIPosterior` (can be used to chain calls).\n    \"\"\"\n    # Update optimizer with current arguments.\n    if self._optimizer is not None:\n        self._optimizer.update({**locals(), **kwargs})\n\n    # Init q and the optimizer if necessary\n    if retrain_from_scratch:\n        self.q = self._q_build_fn()  # type: ignore\n        self._optimizer = self._optimizer_builder(\n            self.potential_fn,\n            self.q,\n            lr=learning_rate,\n            clip_value=clip_value,\n            gamma=gamma,\n            n_particles=n_particles,\n            prior=self._prior,\n            **kwargs,\n        )\n\n    if (\n        reset_optimizer\n        or self._optimizer is None\n        or not isinstance(self._optimizer, self._optimizer_builder)\n    ):\n        self._optimizer = self._optimizer_builder(\n            self.potential_fn,\n            self.q,\n            lr=learning_rate,\n            clip_value=clip_value,\n            gamma=gamma,\n            n_particles=n_particles,\n            prior=self._prior,\n            **kwargs,\n        )\n\n    # Check context\n    x = atleast_2d_float32_tensor(self._x_else_default_x(x)).to(  # type: ignore\n        self._device\n    )\n\n    already_trained = self._trained_on is not None and (x == self._trained_on).all()\n\n    # Optimize\n    optimizer = self._optimizer\n    optimizer.to(self._device)\n    optimizer.reset_loss_stats()\n\n    if show_progress_bar:\n        iters = tqdm(range(max_num_iters))\n    else:\n        iters = range(max_num_iters)\n\n    # Warmup before training\n    if reset_optimizer or (not optimizer.warm_up_was_done and not already_trained):\n        if show_progress_bar:\n            iters.set_description(  # type: ignore\n                \"Warmup phase, this may take a few seconds...\"\n            )\n        optimizer.warm_up(warm_up_rounds)\n\n    for i in iters:\n        optimizer.step(x)\n        mean_loss, std_loss = optimizer.get_loss_stats()\n        # Update progress bar\n        if show_progress_bar:\n            assert isinstance(iters, tqdm)\n            iters.set_description(  # type: ignore\n                f\"Loss: {np.round(float(mean_loss), 2)}\"\n                f\"Std: {np.round(float(std_loss), 2)}\"\n            )\n        # Check for convergence\n        if check_for_convergence and i &gt; min_num_iters and optimizer.converged():\n            if show_progress_bar:\n                print(f\"\\nConverged with loss: {np.round(float(mean_loss), 2)}\")\n            break\n    # Training finished:\n    self._trained_on = x\n\n    # Evaluate quality\n    if quality_control:\n        try:\n            self.evaluate(quality_control_metric=quality_control_metric)\n        except Exception as e:\n            print(\n                f\"Quality control showed a low quality of the variational \"\n                f\"posterior. We are automatically retraining the variational \"\n                f\"posterior from scratch with a smaller learning rate. \"\n                f\"Alternatively, if you want to skip quality control, please \"\n                f\"retrain with `VIPosterior.train(..., quality_control=False)`. \"\n                f\"\\nThe error that occured is: {e}\"\n            )\n            self.train(\n                learning_rate=learning_rate * 0.1,\n                retrain_from_scratch=True,\n                reset_optimizer=True,\n            )\n\n    return self\n</code></pre>"},{"location":"reference/potentials/","title":"Potentials","text":""},{"location":"reference/potentials/#sbi.inference.potentials.posterior_based_potential.posterior_estimator_based_potential","title":"<code>posterior_estimator_based_potential(posterior_estimator, prior, x_o, enable_transform=True)</code>","text":"<p>Returns the potential for posterior-based methods.</p> <p>It also returns a transformation that can be used to transform the potential into unconstrained space.</p> <p>The potential is the same as the log-probability of the <code>posterior_estimator</code>, but it is set to \\(-\\inf\\) outside of the prior bounds.</p> <p>Parameters:</p> Name Type Description Default <code>posterior_estimator</code> <code>ConditionalDensityEstimator</code> <p>The neural network modelling the posterior.</p> required <code>prior</code> <code>Distribution</code> <p>The prior distribution.</p> required <code>x_o</code> <code>Optional[Tensor]</code> <p>The observed data at which to evaluate the posterior.</p> required <code>enable_transform</code> <code>bool</code> <p>Whether to transform parameters to unconstrained space. When False, an identity transform will be returned for <code>theta_transform</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>PosteriorBasedPotential</code> <p>The potential function and a transformation that maps</p> <code>TorchTransform</code> <p>to unconstrained space.</p> Source code in <code>sbi/inference/potentials/posterior_based_potential.py</code> <pre><code>def posterior_estimator_based_potential(\n    posterior_estimator: ConditionalDensityEstimator,\n    prior: Distribution,\n    x_o: Optional[Tensor],\n    enable_transform: bool = True,\n) -&gt; Tuple[PosteriorBasedPotential, TorchTransform]:\n    r\"\"\"Returns the potential for posterior-based methods.\n\n    It also returns a transformation that can be used to transform the potential into\n    unconstrained space.\n\n    The potential is the same as the log-probability of the `posterior_estimator`, but\n    it is set to $-\\inf$ outside of the prior bounds.\n\n    Args:\n        posterior_estimator: The neural network modelling the posterior.\n        prior: The prior distribution.\n        x_o: The observed data at which to evaluate the posterior.\n        enable_transform: Whether to transform parameters to unconstrained space.\n            When False, an identity transform will be returned for `theta_transform`.\n\n    Returns:\n        The potential function and a transformation that maps\n        to unconstrained space.\n    \"\"\"\n\n    device = str(next(posterior_estimator.parameters()).device)\n\n    potential_fn = PosteriorBasedPotential(\n        posterior_estimator, prior, x_o, device=device\n    )\n\n    theta_transform = mcmc_transform(\n        prior, device=device, enable_transform=enable_transform\n    )\n\n    return potential_fn, theta_transform\n</code></pre>"},{"location":"reference/potentials/#sbi.inference.potentials.likelihood_based_potential.likelihood_estimator_based_potential","title":"<code>likelihood_estimator_based_potential(likelihood_estimator, prior, x_o, enable_transform=True)</code>","text":"<p>Returns potential \\(\\log(p(x_o|\\theta)p(\\theta))\\) for likelihood-based methods.</p> <p>It also returns a transformation that can be used to transform the potential into unconstrained space.</p> <p>Parameters:</p> Name Type Description Default <code>likelihood_estimator</code> <code>ConditionalDensityEstimator</code> <p>The density estimator modelling the likelihood.</p> required <code>prior</code> <code>Distribution</code> <p>The prior distribution.</p> required <code>x_o</code> <code>Optional[Tensor]</code> <p>The observed data at which to evaluate the likelihood.</p> required <code>enable_transform</code> <code>bool</code> <p>Whether to transform parameters to unconstrained space.  When False, an identity transform will be returned for <code>theta_transform</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The potential function \\(p(x_o|\\theta)p(\\theta)\\) and a transformation that maps</p> <code>TorchTransform</code> <p>to unconstrained space.</p> Source code in <code>sbi/inference/potentials/likelihood_based_potential.py</code> <pre><code>def likelihood_estimator_based_potential(\n    likelihood_estimator: ConditionalDensityEstimator,\n    prior: Distribution,\n    x_o: Optional[Tensor],\n    enable_transform: bool = True,\n) -&gt; Tuple[Callable, TorchTransform]:\n    r\"\"\"Returns potential $\\log(p(x_o|\\theta)p(\\theta))$ for likelihood-based methods.\n\n    It also returns a transformation that can be used to transform the potential into\n    unconstrained space.\n\n    Args:\n        likelihood_estimator: The density estimator modelling the likelihood.\n        prior: The prior distribution.\n        x_o: The observed data at which to evaluate the likelihood.\n        enable_transform: Whether to transform parameters to unconstrained space.\n             When False, an identity transform will be returned for `theta_transform`.\n\n    Returns:\n        The potential function $p(x_o|\\theta)p(\\theta)$ and a transformation that maps\n        to unconstrained space.\n    \"\"\"\n\n    device = str(next(likelihood_estimator.parameters()).device)\n\n    potential_fn = LikelihoodBasedPotential(\n        likelihood_estimator, prior, x_o, device=device\n    )\n    theta_transform = mcmc_transform(\n        prior, device=device, enable_transform=enable_transform\n    )\n\n    return potential_fn, theta_transform\n</code></pre>"},{"location":"reference/potentials/#sbi.inference.potentials.ratio_based_potential.ratio_estimator_based_potential","title":"<code>ratio_estimator_based_potential(ratio_estimator, prior, x_o, enable_transform=True)</code>","text":"<p>Returns the potential for ratio-based methods.</p> <p>It also returns a transformation that can be used to transform the potential into unconstrained space.</p> <p>Parameters:</p> Name Type Description Default <code>ratio_estimator</code> <code>Module</code> <p>The neural network modelling likelihood-to-evidence ratio.</p> required <code>prior</code> <code>Distribution</code> <p>The prior distribution.</p> required <code>x_o</code> <code>Optional[Tensor]</code> <p>The observed data at which to evaluate the likelihood-to-evidence ratio.</p> required <code>enable_transform</code> <code>bool</code> <p>Whether to transform parameters to unconstrained space. When False, an identity transform will be returned for <code>theta_transform</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The potential function and a transformation that maps</p> <code>TorchTransform</code> <p>to unconstrained space.</p> Source code in <code>sbi/inference/potentials/ratio_based_potential.py</code> <pre><code>def ratio_estimator_based_potential(\n    ratio_estimator: nn.Module,\n    prior: Distribution,\n    x_o: Optional[Tensor],\n    enable_transform: bool = True,\n) -&gt; Tuple[Callable, TorchTransform]:\n    r\"\"\"Returns the potential for ratio-based methods.\n\n    It also returns a transformation that can be used to transform the potential into\n    unconstrained space.\n\n    Args:\n        ratio_estimator: The neural network modelling likelihood-to-evidence ratio.\n        prior: The prior distribution.\n        x_o: The observed data at which to evaluate the likelihood-to-evidence ratio.\n        enable_transform: Whether to transform parameters to unconstrained space.\n            When False, an identity transform will be returned for `theta_transform`.\n\n    Returns:\n        The potential function and a transformation that maps\n        to unconstrained space.\n    \"\"\"\n\n    device = str(next(ratio_estimator.parameters()).device)\n\n    potential_fn = RatioBasedPotential(ratio_estimator, prior, x_o, device=device)\n    theta_transform = mcmc_transform(\n        prior, device=device, enable_transform=enable_transform\n    )\n\n    return potential_fn, theta_transform\n</code></pre>"},{"location":"tutorials/","title":"Tutorials for using the <code>sbi</code> toolbox","text":"<p>Before running the notebooks, follow our instructions to install sbi. Alternatively, you can also open a codespace on GitHub and work through the tutorials in the browser. The numbers of the notebooks are not informative of the order, please follow this structure depending on which group you identify with.</p> <p>Once you have familiarised yourself with the methods and identified how to apply SBI to your use case, ensure you work through the Diagnostics tutorials linked below, to identify failure cases and assess the quality of your inference.</p>"},{"location":"tutorials/#introduction","title":"Introduction","text":"<ul> <li>Getting started</li> <li>Amortized inference</li> <li>Implemented algorithms</li> </ul>"},{"location":"tutorials/#advanced","title":"Advanced","text":"<ul> <li>Multi-round inference</li> <li>Sampling algorithms in sbi</li> <li>Custom density estimators</li> <li>Embedding nets for observations</li> <li>SBI with trial-based data</li> <li>Handling invalid simulations</li> <li>Crafting summary statistics</li> <li>Importance sampling posteriors</li> </ul>"},{"location":"tutorials/#diagnostics","title":"Diagnostics","text":"<ul> <li>Posterior predictive checks</li> <li>Simulation-based calibration</li> <li>Density plots and MCMC diagnostics with ArviZ</li> <li>Local-C2ST coverage checks</li> </ul>"},{"location":"tutorials/#analysis","title":"Analysis","text":"<ul> <li>Conditional distributions</li> <li>Posterior sensitivity analysis</li> <li>Plotting functionality</li> </ul>"},{"location":"tutorials/#examples","title":"Examples","text":"<ul> <li>Hodgkin-Huxley model</li> <li>Decision-making model</li> </ul>"},{"location":"tutorials/00_getting_started_flexible/","title":"Getting started with <code>sbi</code>","text":"<p>Note, you can find the original version of this notebook at https://github.com/sbi-dev/sbi/blob/main/tutorials/00_getting_started_flexible.ipynb in the <code>sbi</code> repository.</p> <p><code>sbi</code> provides a simple interface to run state-of-the-art algorithms for simulation-based inference.</p> <p>The overall goal of simulation-based inference is to algorithmically identify model parameters which are consistent with data.</p> <p>In this tutorial we demonstrate how to get started with the <code>sbi</code> toolbox and how to perform parameter inference on a simple model.</p> <pre><code>import torch\n\nfrom sbi.analysis import pairplot\nfrom sbi.inference import SNPE, simulate_for_sbi\nfrom sbi.utils import BoxUniform\nfrom sbi.utils.user_input_checks import (\n    check_sbi_inputs,\n    process_prior,\n    process_simulator,\n)\n</code></pre>"},{"location":"tutorials/00_getting_started_flexible/#parameter-inference-in-a-linear-gaussian-example","title":"Parameter inference in a linear Gaussian example","text":"<p>Each of the implemented inference methods takes three inputs: 1. observational data (or summary statistics thereof) - the observations 2. a candidate (mechanistic) model - the simulator 3. prior knowledge or constraints on model parameters - the prior</p> <p>If you are new to simulation-based inference, please first read the information on the homepage of the website to familiarise with the motivation and relevant terms.</p> <p>For this illustrative example we consider a model simulator that takes in 3 parameters (\\(\\theta\\)). For simplicity, the simulator outputs simulations of the same dimensionality and adds 1.0 and some Gaussian noise to the parameter set. </p> <p>Note: This is where you instead would use your specific  simulator with its parameters.</p> <p>For the 3-dimensional parameter space we consider a uniform prior between [-2,2].</p> <p>Note: This is where you would incorporate prior knowlegde about the parameters you want to infer, e.g., ranges known from literature. </p> <pre><code>num_dim = 3\n\ndef simulator(theta):\n    # linear gaussian\n    return theta + 1.0 + torch.randn_like(theta) * 0.1\n\nprior = BoxUniform(low=-2 * torch.ones(num_dim), high=2 * torch.ones(num_dim))\n</code></pre> <p>We have to ensure that your simulator and prior adhere to the requirements of <code>sbi</code> such as returning <code>torch.Tensor</code>s in a standardised shape. </p> <p>You can do so with the <code>process_simulator()</code> and <code>process_prior()</code> functions, which prepare them appropriately. Finally, you can call <code>check_sbi_input()</code> to make sure they are consistent which each other.</p> <pre><code># Check prior, return PyTorch prior.\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\n\n# Check simulator, returns PyTorch simulator able to simulate batches.\nsimulator = process_simulator(simulator, prior, prior_returns_numpy)\n\n# Consistency check after making ready for sbi.\ncheck_sbi_inputs(simulator, prior)\n</code></pre> <p>Next, we instantiate the inference object. In this example, we will use neural perform posterior estimation (NPE):</p> <p>Note: In the <code>sbi</code> toolbox, NPE is run by using the <code>SNPE</code> (Sequential NPE) class for only one iteration of simulation and training. </p> <p>Note: This is where you could specify an alternative inference object such as (S)NRE for ratio estimation or (S)NLE for likelihood estimation. Here, you can see all implemented methods.</p> <pre><code>inference = SNPE(prior=prior)\n</code></pre> <p>Next, we need simulations, or more specifically, pairs of parameters \\(\\theta\\) which we sample from the prior and corresponding simulations \\(x = \\mathrm{simulator} (\\theta)\\). The <code>sbi</code> helper function called <code>simulate_for_sbi</code> allows to parallelize your code with <code>joblib</code>.</p> <p>Note: If you already have your own parameters, simulation pairs which were generated elsewhere (e.g., on a compute cluster), you would add them here.  </p> <pre><code>theta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=2000)\nprint(\"theta.shape\", theta.shape)\nprint(\"x.shape\", x.shape)\n</code></pre> <pre><code>Running 2000 simulations.:   0%|          | 0/2000 [00:00&lt;?, ?it/s]\n\n\ntheta.shape torch.Size([2000, 3])\nx.shape torch.Size([2000, 3])\n</code></pre> <p>We then pass the simulated data to the inference object. Both <code>theta</code> and <code>x</code> should be a <code>torch.Tensor</code> of type <code>float32</code>.</p> <pre><code>inference = inference.append_simulations(theta, x)\n</code></pre> <p>Next, we train the neural density estimator to learn the association between the simulated data (or data features) and the underlying parameters:</p> <pre><code>density_estimator = inference.train()\n</code></pre> <pre><code> Neural network successfully converged after 81 epochs.\n</code></pre> <p>Finally, we use this density estimator to build the posterior distribution \\(p(\\theta|x)\\), i.e., the distributions over paramters \\(\\theta\\) given observation \\(x\\). </p> <p>The <code>posterior</code> can then be used to (among other features which go beyond the scope of this introductory tutorial) sample parameters \\(\\theta\\) from the posterior via <code>.sample()</code>, i.e., parameters that are likely given the observation \\(x\\). </p> <p>We can also get log-probabilities under the posterior via <code>.log_prob()</code>, i.e., we can evaluate the likelihood of parameters \\(\\theta\\) given the observation \\(x\\). </p> <pre><code>posterior = inference.build_posterior(density_estimator)\n\nprint(posterior) # prints how the posterior was trained\n</code></pre> <pre><code>Posterior conditional density p(\u03b8|x) of type DirectPosterior. It samples the posterior network and rejects samples that\n            lie outside of the prior bounds.\n</code></pre>"},{"location":"tutorials/00_getting_started_flexible/#visualisations-of-the-inferred-posterior-for-a-new-observation","title":"Visualisations of the inferred posterior for a new observation","text":"<p>Let\u2019s say we have made some observation \\(x_{obs}\\) for which we now want to infer the posterior:</p> <p>Note: this is where your experimental observation would come in. For real observations, of course, you would not have access to the ground truth \\(\\theta\\). </p> <pre><code>theta_true = prior.sample((1,))\n# generate our observation\nx_obs = simulator(theta_true)\n</code></pre> <p>Given this observation, we can sample from the posterior \\(p(\\theta|x_{obs})\\) and visualise the univariate and pairwise marginals for the three parameters via <code>analysis.pairplot()</code>.</p> <pre><code>samples = posterior.sample((10000,), x=x_obs)\n_ = pairplot(samples, limits=[[-2, 2], [-2, 2], [-2, 2]], figsize=(6, 6),labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"])\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <p></p>"},{"location":"tutorials/00_getting_started_flexible/#assessing-the-posterior-for-the-known-theta-x-pair","title":"Assessing the posterior for the known \\(\\theta, x\\) - pair","text":"<p>For this special case, we have access to the ground-truth parameters that generated the observation. We can thus assess if the inferred distributions over the parameters match the parameters \\(\\theta_{true}\\) we used to generate our test observation \\(x_{obs}\\).</p> <pre><code>samples = posterior.sample((10000,), x=x_obs)\n_ = pairplot(samples, points=theta_true, limits=[[-2, 2], [-2, 2], [-2, 2]], figsize=(6, 6), labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"])\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <p>The log-probability should ideally indicate that the true parameters, given the corresponding observation, are more likely than a different set of randomly chosen parameters from the prior distribution. </p> <p>Relative to the obtained log-probabilities, we can investigate the range of log-probabilities of the parameters sampled from the posterior.</p> <pre><code># first sample an alternative parameter set from the prior\ntheta_diff = prior.sample((1,))\n</code></pre> <pre><code>log_probability_true_theta = posterior.log_prob(theta_true, x=x_obs)\nlog_probability_diff_theta = posterior.log_prob(theta_diff, x=x_obs)\nlog_probability_samples = posterior.log_prob(samples, x=x_obs)\n\nprint( r'high for true theta :', log_probability_true_theta)\nprint( r'low for different theta :', log_probability_diff_theta)\nprint( r'range of posterior samples: min:', torch.min(log_probability_samples),' max :', torch.max(log_probability_samples))\n</code></pre> <pre><code>high for true theta : tensor([3.7909])\nlow for different theta : tensor([-326.5020])\nrange of posterior samples: min: tensor(-8.1930)  max : tensor(4.0474)\n</code></pre>"},{"location":"tutorials/00_getting_started_flexible/#next-steps","title":"Next steps","text":"<p>To learn more about the capabilities of <code>sbi</code>, you can head over to the tutorial on inferring parameters for multiple observations  which introduces the concept of amortization. </p> <p>Alternatively, for an example with an actual simulator, you can read our example for a scientific simulator from neuroscience.</p>"},{"location":"tutorials/01_gaussian_amortized/","title":"Amortized posterior inference on Gaussian example","text":"<p>Note, you can find the original version of this notebook at https://github.com/sbi-dev/sbi/blob/main/tutorials/01_gaussian_amortized.ipynb in the <code>sbi</code> repository.</p> <p>In this tutorial, we introduce amortization that is the capability to evaluate the posterior for different observations without having to re-run inference.</p> <p>We will demonstrate how <code>sbi</code> can infer an amortized posterior for the illustrative linear Gaussian example introduced in Getting Started, that takes in 3 parameters (\\(\\theta\\)). </p> <pre><code>import torch\n\nfrom sbi import analysis as analysis\nfrom sbi import utils as utils\nfrom sbi.inference import SNPE, simulate_for_sbi\nfrom sbi.utils.user_input_checks import (\n    check_sbi_inputs,\n    process_prior,\n    process_simulator,\n)\n</code></pre>"},{"location":"tutorials/01_gaussian_amortized/#defining-simulator-prior-and-running-inference","title":"Defining simulator, prior, and running inference","text":"<p>Our  simulator (model) takes in 3 parameters (\\(\\theta\\)) and outputs simulations of the same dimensionality. It adds 1.0 and some Gaussian noise to the parameter set. For each dimension of \\(\\theta\\), we consider a uniform prior between [-2,2].</p> <pre><code>num_dim = 3\nprior = utils.BoxUniform(low=-2 * torch.ones(num_dim), high=2 * torch.ones(num_dim))\n\ndef simulator(theta):\n    # linear gaussian\n    return theta + 1.0 + torch.randn_like(theta) * 0.1\n\n# Check prior, simulator, consistency\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\nsimulator = process_simulator(simulator, prior, prior_returns_numpy)\ncheck_sbi_inputs(simulator, prior)\n</code></pre> <pre><code># Create inference object. Here, NPE is used.\ninference = SNPE(prior=prior)\n\n# generate simulations and pass to the inference object\ntheta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=2000)\ninference = inference.append_simulations(theta, x)\n\n# train the density estimator and build the posterior\ndensity_estimator = inference.train()\nposterior = inference.build_posterior(density_estimator)\n</code></pre> <pre><code>Running 2000 simulations.:   0%|          | 0/2000 [00:00&lt;?, ?it/s]\n\n\n Neural network successfully converged after 70 epochs.\n</code></pre>"},{"location":"tutorials/01_gaussian_amortized/#amortized-inference","title":"Amortized inference","text":"<p>Note that we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. An amortized posterior is one that is not focused on any particular observation. Naturally, if the diversity of observations is large, any of the inference methods will need to run a sufficient number of simulations for the resulting posterior to perform well across these diverse observations.</p> <p>Let\u2019s say we have not just one but two observations \\(x_{obs~1}\\) and \\(x_{obs~2}\\) for which we aim to do parameter inference. </p> <p>Note: For real observations, of course, you would not have access to the ground truth \\(\\theta\\).</p> <pre><code># generate the first observation\ntheta_1 = prior.sample((1,))\nx_obs_1 = simulator(theta_1)\n# now generate a second observation\ntheta_2 = prior.sample((1,))\nx_obs_2 = simulator(theta_2)\n</code></pre> <p>We can draw samples from the posterior given \\(x_{obs~1}\\) and then plot them:</p> <pre><code>posterior_samples_1 = posterior.sample((10000,), x=x_obs_1)\n\n# plot posterior samples\n_ = analysis.pairplot(\n    posterior_samples_1, limits=[[-2, 2], [-2, 2], [-2, 2]], figsize=(5, 5),\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n    points=theta_1 # add ground truth thetas\n)\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <p>The inferred distirbutions over the parameters given the first observation \\(x_{obs~1}\\) match the parameters \\(\\theta_{1}\\) (shown in orange), we used to generate our first observation \\(x_{obs~1}\\).</p> <p>Since the learned posterior is amortized, we can also draw samples from the posterior given the second observation \\(x_{obs~2}\\) without having to re-run inference:</p> <pre><code>posterior_samples_2 = posterior.sample((10000,), x=x_obs_2)\n\n# plot posterior samples\n_ = analysis.pairplot(\n    posterior_samples_2, limits=[[-2, 2], [-2, 2], [-2, 2]], figsize=(5, 5),\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n    points=theta_2 # add ground truth thetas\n)\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <p>The inferred distirbutions over the parameters given the second observation  \\(x_{obs~2}\\) also match the ground truth parameters \\(\\theta_{2}\\) we used to generate our second test observation  \\(x_{obs~2}\\).</p> <p>This in a nutshell demonstrates the benefit of amortized methods. </p>"},{"location":"tutorials/01_gaussian_amortized/#next-steps","title":"Next steps","text":"<p>Now that you got familiar with amortization and are probably good to go and have a first shot at applying <code>sbi</code> to your own inference problem. If you want to learn more, we recommend checking out our tutorial on multiround inference  which aims to make inference for a single observation more sampling efficient.</p>"},{"location":"tutorials/03_multiround_inference/","title":"Multi-round inference","text":"<p>In the previous tutorials, we have inferred the posterior using single-round inference. In single-round inference, we draw parameters from the prior, simulate the corresponding data, and then train a neural network to obtain the posterior. However, if one is interested in only one particular observation <code>x_o</code> sampling from the prior can be inefficient in the number of simulations because one is effectively learning a posterior estimate for all observations in the prior space. In this tutorial, we show how one can alleviate this issue by performing multi-round inference with <code>sbi</code>.</p> <p>Multi-round inference also starts by drawing parameters from the prior, simulating them, and training a neural network to estimate the posterior distribution. Afterwards, however, it continues inference in multiple rounds, focusing on a particular observation <code>x_o</code>. In each new round of inference, it draws samples from the obtained posterior distribution conditioned at <code>x_o</code> (instead of from the prior), simulates these, and trains the network again. This process can be repeated arbitrarily often to get increasingly good approximations to the true posterior distribution at <code>x_o</code>.</p> <p>Running multi-round inference can be more efficient in the number of simulations, but it will lead to the posterior no longer being amortized (i.e. it will be accurate only for a specific observation <code>x_o</code>, not for any <code>x</code>).</p> <p>Note, you can find the original version of this notebook at https://github.com/sbi-dev/sbi/blob/main/tutorials/03_multiround_inference.ipynb in the <code>sbi</code> repository.</p>"},{"location":"tutorials/03_multiround_inference/#main-syntax","title":"Main syntax","text":"<pre><code>import torch\n\nfrom sbi.analysis import pairplot\nfrom sbi.inference import SNPE, simulate_for_sbi\nfrom sbi.utils import BoxUniform\nfrom sbi.utils.user_input_checks import (\n    check_sbi_inputs,\n    process_prior,\n    process_simulator,\n)\n</code></pre> <pre><code># 2 rounds: first round simulates from the prior, second round simulates parameter set\n# that were sampled from the obtained posterior.\nnum_rounds = 2\nnum_dim = 3\n# The specific observation we want to focus the inference on.\nx_o = torch.zeros(num_dim,)\nprior = BoxUniform(low=-2 * torch.ones(num_dim), high=2 * torch.ones(num_dim))\nsimulator = lambda theta: theta + torch.randn_like(theta) * 0.1\n\n# Ensure compliance with sbi's requirements.\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\nsimulator = process_simulator(simulator, prior, prior_returns_numpy)\ncheck_sbi_inputs(simulator, prior)\n\ninference = SNPE(prior)\n\nposteriors = []\nproposal = prior\n\nfor _ in range(num_rounds):\n    theta, x = simulate_for_sbi(simulator, proposal, num_simulations=500)\n\n    # In `SNLE` and `SNRE`, you should not pass the `proposal` to\n    # `.append_simulations()`\n    density_estimator = inference.append_simulations(\n        theta, x, proposal=proposal\n    ).train()\n    posterior = inference.build_posterior(density_estimator)\n    posteriors.append(posterior)\n    proposal = posterior.set_default_x(x_o)\n</code></pre> <pre><code>Running 500 simulations.:   0%|          | 0/500 [00:00&lt;?, ?it/s]\n\n\n Neural network successfully converged after 105 epochs.\n\n\nDrawing 500 posterior samples:   0%|          | 0/500 [00:00&lt;?, ?it/s]\n\n\n\nRunning 500 simulations.:   0%|          | 0/500 [00:00&lt;?, ?it/s]\n\n\nUsing SNPE-C with atomic loss\n Neural network successfully converged after 36 epochs.\n</code></pre>"},{"location":"tutorials/03_multiround_inference/#linear-gaussian-example","title":"Linear Gaussian example","text":"<p>Below, we give a full example of inferring the posterior distribution over multiple rounds.</p> <p>First, we define a simple prior and simulator and ensure that they comply with <code>sbi</code> by using <code>process_simulator()</code>, <code>process_prior()</code> and <code>check_sbi_inputs()</code>:</p> <pre><code>def linear_gaussian(theta):\n    return theta + 1.0 + torch.randn_like(theta) * 0.1\n</code></pre> <pre><code># Check prior, return PyTorch prior.\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\n\n# Check simulator, returns PyTorch simulator able to simulate batches.\nsimulator = process_simulator(linear_gaussian, prior, prior_returns_numpy)\n\n# Consistency check after making ready for sbi.\ncheck_sbi_inputs(simulator, prior)\n</code></pre> <p>Then, we instantiate the inference object:</p> <pre><code>inference = SNPE(prior=prior)\n</code></pre> <p>And we can run inference. In this example, we will run inference over <code>2</code> rounds, potentially leading to a more focused posterior around the observation <code>x_o</code>.</p> <pre><code>num_rounds = 2\nx_o = torch.zeros(\n    3,\n)\n\nposteriors = []\nproposal = prior\n\nfor _ in range(num_rounds):\n    theta, x = simulate_for_sbi(simulator, proposal, num_simulations=500)\n    density_estimator = inference.append_simulations(\n        theta, x, proposal=proposal\n    ).train()\n    posterior = inference.build_posterior(density_estimator)\n    posteriors.append(posterior)\n    proposal = posterior.set_default_x(x_o)\n</code></pre> <pre><code>Running 500 simulations.:   0%|          | 0/500 [00:00&lt;?, ?it/s]\n\n\n Neural network successfully converged after 58 epochs.\n\n\nDrawing 500 posterior samples:   0%|          | 0/500 [00:00&lt;?, ?it/s]\n\n\n\nRunning 500 simulations.:   0%|          | 0/500 [00:00&lt;?, ?it/s]\n\n\nUsing SNPE-C with atomic loss\n Neural network successfully converged after 28 epochs.\n</code></pre> <p>Note that, for <code>num_rounds&gt;1</code>, the posterior is no longer amortized: it will give good results when sampled around <code>x=observation</code>, but possibly bad results for other <code>x</code>.</p> <p>Once we have obtained the posterior, we can <code>.sample()</code>, <code>.log_prob()</code>, or <code>.pairplot()</code> in the same way as for the simple interface.</p> <pre><code>posterior_samples = posterior.sample((10000,), x=x_o)\n\n# plot posterior samples\nfig, ax = pairplot(\n    posterior_samples, limits=[[-2, 2], [-2, 2], [-2, 2]], figsize=(5, 5)\n)\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <p></p>"},{"location":"tutorials/04_density_estimators/","title":"Customizing the density estimator","text":"<p><code>sbi</code> allows to specify a custom density estimator for each of the implemented methods. For all options, check the API reference here.</p>"},{"location":"tutorials/04_density_estimators/#changing-the-type-of-density-estimator","title":"Changing the type of density estimator","text":"<p>One option is to use one of set of preconfigured density estimators by passing a string in the <code>density_estimator</code> keyword argument to the inference object (<code>SNPE</code> or <code>SNLE</code>), e.g., \u201cmaf\u201d to use a Masked Autoregressive Flow, of \u201cnsf\u201d to use a Neural Spline Flow with default hyperparameters.</p> <pre><code>import torch\n\nfrom sbi.inference import SNPE, SNRE\nfrom sbi.utils import BoxUniform\n</code></pre> <pre><code>prior = BoxUniform(torch.zeros(2), torch.ones(2))\ninference = SNPE(prior=prior, density_estimator=\"maf\")\n</code></pre> <p>In the case of <code>SNRE</code>, the argument is called <code>classifier</code>:</p> <pre><code>inference = SNRE(prior=prior, classifier=\"resnet\")\n</code></pre>"},{"location":"tutorials/04_density_estimators/#changing-hyperparameters-of-density-estimators","title":"Changing hyperparameters of density estimators","text":"<p>Alternatively, you can use a set of utils functions to configure a density estimator yourself, e.g., use a MAF with hyperparameters chosen for your problem at hand.</p> <p>Here, because we want to use SN*P*E, we specifiy a neural network targeting the posterior (using the utils function <code>posterior_nn</code>). In this example, we will create a neural spline flow (<code>'nsf'</code>) with <code>60</code> hidden units and <code>3</code> transform layers:</p> <pre><code># For SNLE: likelihood_nn(). For SNRE: classifier_nn()\nfrom sbi.neural_nets import posterior_nn\n\ndensity_estimator_build_fun = posterior_nn(\n    model=\"nsf\", hidden_features=60, num_transforms=3\n)\ninference = SNPE(prior=prior, density_estimator=density_estimator_build_fun)\n</code></pre> <p>It is also possible to pass an <code>embedding_net</code> to <code>posterior_nn()</code> which learn summary statistics from high-dimensional simulation outputs. You can find a more detailed tutorial on this here.</p>"},{"location":"tutorials/04_density_estimators/#building-new-density-estimators-from-scratch","title":"Building new density estimators from scratch","text":"<p>Finally, it is also possible to implement your own density estimator from scratch, e.g., including embedding nets to preprocess data, or to a density estimator architecture of your choice.</p> <p>For this, the <code>density_estimator</code> argument needs to be a function that takes <code>theta</code> and <code>x</code> batches as arguments to then construct the density estimator after the first set of simulations was generated. Our factory functions in <code>sbi/neural_nets/factory.py</code> return such a function.</p> <p>The returned <code>density_estimator</code> object needs to be a subclass of <code>DensityEstimator</code>, which requires to implement three methods:</p> <ul> <li><code>log_prob(input, condition, **kwargs)</code>: Return the log probabilities of the inputs given a condition or multiple i.e. batched conditions.</li> <li><code>loss(input, condition, **kwargs)</code>: Return the loss for training the density estimator.</li> <li><code>sample(sample_shape, condition, **kwargs)</code>: Return samples from the density estimator.</li> </ul> <p>See more information on the Reference API page.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/05_embedding_net/","title":"Embedding nets for observations","text":"<p>Note</p> <p>You can find the original version of this notebook at tutorials/05_embedding_net.ipynb in the <code>sbi</code> repository.</p>"},{"location":"tutorials/05_embedding_net/#introduction","title":"Introduction","text":"<p>When engaging in simulation-based inference, the selection of appropriate summary statistics for observations holds significant importance. These statistics serve to succinctly describe the data generated by simulators, often leveraging domain-specific knowledge. However, in certain scenarios, particularly when dealing with highly complex raw data where domain knowledge may be limited or non-existent, it becomes necessary to learn directly from the data the appropriate summary statistics to employ. <code>sbi</code> offers functionality to learn summary statistics from simulation outputs with an embedding neural network, referred to as <code>embedding_net</code>. </p> <p>When an embedding network is used, the posterior approximation for a given observation \\(x_o\\) can be denoted as \\(q_\\phi\\big(\\theta \\mid f_\\lambda(x_o)\\big)\\) where \\(\\phi\\) are the parameters of the conditional density estimator and \\(\\lambda\\) the parameters of the embedding neural network. Note that the simulation outputs pass through the <code>embedding_net</code> before reaching the density estimator and that \\(\\phi\\) and \\(\\lambda\\) are jointly learned during training. <code>sbi</code> provides pre-configured embedding networks (MLP, CNN, and permutation-invariant networks) or allows to pass custom-written embedding networks.</p> <p>It is worth noting that only <code>SNPE</code> and <code>SNRE</code> methods can use an <code>embedding_net</code> to learn summary statistics from simulation outputs. <code>SNLE</code> does not offer such functionality because the simulation outputs are also the output of the neural density estimator. </p>"},{"location":"tutorials/05_embedding_net/#main-syntax","title":"Main syntax","text":"<pre><code># import required modules\nfrom sbi.neural_nets import posterior_nn\n\n# import the different choices of pre-configured embedding networks\nfrom sbi.neural_nets.embedding_nets import (\n    FCEmbedding,\n    CNNEmbedding,\n    PermutationInvariantEmbedding\n)\n\n# choose which type of pre-configured embedding net to use (e.g. CNN)\nembedding_net = CNNEmbedding(input_shape=(32, 32))\n\n# instantiate the conditional neural density estimator\nneural_posterior = posterior_nn(model=\"maf\", embedding_net=embedding_net)\n\n# setup the inference procedure with the SNPE-C procedure\ninferer = SNPE(prior=prior, density_estimator=neural_posterior)\n\n# train the density estimator\ndensity_estimator = inference.append_simulations(theta, x).train()\n\n# build the posterior\nposterior = inference.build_posterior(density_estimator)\n</code></pre>"},{"location":"tutorials/05_embedding_net/#inferring-parameters-from-images","title":"Inferring parameters from images","text":"<p>In the example that follows, we consider a simple setup where the data points generated by the simulator model are high-dimensional (32x32 grayscale images) and we use a convolutional neural network as summary statistics extractor.</p> <p>First of all, we import all the packages required for running the tutorial</p> <pre><code>import matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sbi import analysis, utils\nfrom sbi.inference import SNPE, simulate_for_sbi\nfrom sbi.utils.user_input_checks import (\n    check_sbi_inputs,\n    process_prior,\n    process_simulator,\n)\n\nseed = 0\ntorch.manual_seed(seed)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x13a41c190&gt;\n</code></pre>"},{"location":"tutorials/05_embedding_net/#the-simulator-model","title":"The simulator model","text":"<p>The simulator model that we consider has two parameters: \\(r\\) and \\(\\theta\\). On each run, it generates 100 two-dimensional points centered around \\((r \\cos(\\theta), r \\sin(\\theta))\\) and perturbed by a Gaussian noise with variance 0.01. Instead of simply outputting the \\((x,y)\\) coordinates of each data point, the model generates a grayscale image of the scattered points with dimensions 32 by 32. This image is further perturbed by an uniform noise with values betweeen 0 and 0.2. The code below defines such model.</p> <pre><code>def simulator_model(parameter, return_points=False):\n    \"\"\"Simulator model with two-dimensional input parameter and 1024-D output\n\n    This simulator serves as a basic example for using a neural net for learning\n    summary features. It has only two input parameters but generates\n    high-dimensional output vectors. The data is generated as follows:\n        (-) Input:  parameter = [r, phi] (1) Generate 100 two-dimensional\n        points centered around (r cos(phi),r sin(phi))\n            and perturbed by a Gaussian noise with variance 0.01\n        (2) Create a grayscale image I of the scattered points with dimensions\n            32 by 32\n        (3) Perturb I with an uniform noise with values betweeen 0 and 0.2\n        (-) Output: I\n\n    Parameters\n    ----------\n    parameter : array-like, shape (2)\n        The two input parameters of the model, ordered as [r, phi]\n    return_points : bool (default: False)\n        Whether the simulator should return the coordinates of the simulated\n        data points as well\n\n    Returns\n    -------\n    I: torch tensor, shape (1, 1024)\n        Output flattened image\n    (optional) points: array-like, shape (100, 2)\n        Coordinates of the 2D simulated data points\n\n    \"\"\"\n    r = parameter[0]\n    phi = parameter[1]\n\n    sigma_points = 0.10\n    npoints = 100\n    points = []\n    for _ in range(npoints):\n        x = r * torch.cos(phi) + sigma_points * torch.randn(1)\n        y = r * torch.sin(phi) + sigma_points * torch.randn(1)\n        points.append([x, y])\n    points = torch.as_tensor(points)\n\n    nx = 32\n    ny = 32\n    sigma_image = 0.20\n    im = torch.zeros(nx, ny)\n    for point in points:\n        pi = int((point[0] - (-1)) / ((+1) - (-1)) * nx)\n        pj = int((point[1] - (-1)) / ((+1) - (-1)) * ny)\n        if (pi &lt; nx) and (pj &lt; ny):\n            im[pi, pj] = 1\n    im = im + sigma_image * torch.rand(nx, ny)\n    im = im.T\n    im = im.reshape(1, -1)\n\n    if return_points:\n        return im, points\n    else:\n        return im\n</code></pre> <p>The figure below shows an example of the output of the simulator when \\(r = 0.70\\) and \\(\\theta = \\pi/4\\)</p> <pre><code># simulate samples\ntrue_parameter = torch.tensor([0.70, torch.pi / 4])\nx_observed, x_points = simulator_model(true_parameter, return_points=True)\n\n# plot the observation\nfig, ax = plt.subplots(\n    facecolor=\"white\", figsize=(11.15, 5.61), ncols=2, constrained_layout=True\n)\ncircle = plt.Circle((0, 0), 1.0, color=\"k\", ls=\"--\", lw=0.8, fill=False)\nax[0].add_artist(circle)\nax[0].scatter(x_points[:, 0], x_points[:, 1], s=20)\nax[0].set_xlabel(\"x\")\nax[0].set_ylabel(\"y\")\nax[0].set_xlim(-1, +1)\nax[0].set_xticks([-1, 0.0, +1.0])\nax[0].set_ylim(-1, +1)\nax[0].set_yticks([-1, 0.0, +1.0])\nax[0].set_title(r\"original simulated points with $r = 0.70$ and $\\phi = \\pi/4$\")\nax[1].imshow(x_observed.view(32, 32), origin=\"lower\", cmap=\"gray\")\nax[1].set_xticks([])\nax[1].set_yticks([])\nax[1].set_title(\"noisy observed data (gray image with 32 x 32 pixels)\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'noisy observed data (gray image with 32 x 32 pixels)')\n</code></pre> <p></p>"},{"location":"tutorials/05_embedding_net/#choosing-an-embedding_net","title":"Choosing an <code>embedding_net</code>","text":"<p>The outputs \\(x\\) from the simulator are defined in a 1024 dimensional space (32 x 32 = 1024). To avoid having to setup a conditional neural density estimator to work directly on such high-dimensional vectors, one could use an <code>embedding_net</code> that would take the images as input and encode them into smaller vectors.</p> <p><code>sbi</code> provides pre-configured embedding networks of the following types:</p> <ul> <li>Fully-connected multi-layer perceptron</li> <li>Convolutional neural network (1D and 2D convolutions)</li> <li>Permutation-invariant neural network (for trial-based data, see here)</li> </ul> <p>In the example considered here, the most appropriate <code>embedding_net</code> would be a CNN for two-dimensional images. We can setup it as per:</p> <pre><code>from sbi.neural_nets.embedding_nets import CNNEmbedding\n\nembedding_net = CNNEmbedding(\n    input_shape=(32, 32),\n    in_channels=1,\n    out_channels_per_layer=[6],\n    num_conv_layers=1,\n    num_linear_layers=1,\n    output_dim=8,\n    kernel_size=5,\n    pool_kernel_size=8\n)\n</code></pre> <p>Note</p> <p>See here for details on all hyperparametes for each available embedding net in <code>sbi</code></p>"},{"location":"tutorials/05_embedding_net/#the-inference-procedure","title":"The inference procedure","text":"<p>With the <code>embedding_net</code> defined and instantiated, we can follow the usual workflow of an inference procedure in <code>sbi</code>. The <code>embedding_net</code> object appears as an input argument when instantiating the neural density estimator with <code>sbi.neural_nets.posterior_nn</code>.</p> <pre><code># set prior distribution for the parameters\nprior = utils.BoxUniform(\n    low=torch.tensor([0.0, 0.0]), high=torch.tensor([1.0, 2 * torch.pi])\n)\n\n# make a SBI-wrapper on the simulator object for compatibility\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\nsimulator_wrapper = process_simulator(simulator_model, prior, prior_returns_numpy)\ncheck_sbi_inputs(simulator_wrapper, prior)\n</code></pre> <pre><code>from sbi.neural_nets import posterior_nn\n\n# instantiate the neural density estimator\nneural_posterior = posterior_nn(model=\"maf\", embedding_net=embedding_net)\n\n# setup the inference procedure with the SNPE-C procedure\ninferer = SNPE(prior=prior, density_estimator=neural_posterior)\n</code></pre> <pre><code># run the inference procedure on one round and 10000 simulated data points\ntheta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=10_000)\n</code></pre> <pre><code>Running 10000 simulations.:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>density_estimator = inferer.append_simulations(theta, x).train(training_batch_size=256)\nposterior = inferer.build_posterior(density_estimator)\n</code></pre> <pre><code> Neural network successfully converged after 198 epochs.\n</code></pre>"},{"location":"tutorials/05_embedding_net/#visualizing-the-results","title":"Visualizing the results","text":"<p>We now generate 50000 samples of the posterior distribution of \\(r\\) and \\(\\theta\\) when observing an input data point \\(x\\) generated from the <code>simulator model</code> with \\(r = 0.70\\) and \\(\\theta = \\pi/4\\).</p> <pre><code># generate posterior samples\ntrue_parameter = torch.tensor([0.50, torch.pi / 4])\nx_observed = simulator_model(true_parameter)\nsamples = posterior.set_default_x(x_observed).sample((50000,))\n</code></pre> <pre><code>Drawing 50000 posterior samples:   0%|          | 0/50000 [00:00&lt;?, ?it/s]\n</code></pre> <p>The figure below shows the statistics of the generated samples.</p> <pre><code># create the figure\nfig, ax = analysis.pairplot(\n    samples,\n    points=true_parameter,\n    labels=[\"r\", r\"$\\phi$\"],\n    limits=[[0, 1], [0, 2 * torch.pi]],\n    points_colors=\"r\",\n    points_offdiag={\"markersize\": 6},\n    figsize=(5, 5),\n)\n</code></pre> <p></p>"},{"location":"tutorials/05_embedding_net/#defining-custom-embedding-networks","title":"Defining custom embedding networks","text":"<p>It is also possible to define custom embedding networks and pass those to neural density estimator. For example, we could have defined our own architecture for the CNN as per</p> <pre><code>class SummaryNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2D convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2)\n        # Maxpool layer that reduces 32x32 image to 4x4\n        self.pool = nn.MaxPool2d(kernel_size=8, stride=8)\n        # Fully connected layer taking as input the 6 flattened output arrays\n        # from the maxpooling layer\n        self.fc = nn.Linear(in_features=6 * 4 * 4, out_features=8)\n\n    def forward(self, x):\n        x = x.view(-1, 1, 32, 32)\n        x = self.pool(F.relu(self.conv1(x)))\n        x = x.view(-1, 6 * 4 * 4)\n        x = F.relu(self.fc(x))\n        return x\n\n# instantiate the custom embedding_net\nembedding_net_custom = SummaryNet()\n</code></pre>"},{"location":"tutorials/07_conditional_distributions/","title":"Analysing variability and compensation mechansims with conditional distributions","text":"<p>A central advantage of <code>sbi</code> over parameter search methods such as genetic algorithms is that the posterior captures all models that can reproduce experimental data. This allows us to analyse whether parameters can be variable or have to be narrowly tuned, and to analyse compensation mechanisms between different parameters. See also Marder and Taylor, 2011 for further motivation to identify all models that capture experimental data.</p> <p>In this tutorial, we will show how one can use the posterior distribution to identify whether parameters can be variable or have to be finely tuned, and how we can use the posterior to find potential compensation mechanisms between model parameters. To investigate this, we will extract conditional distributions from the posterior inferred with <code>sbi</code>.</p> <p>Note, you can find the original version of this notebook at https://github.com/sbi-dev/sbi/blob/main/tutorials/07_conditional_distributions.ipynb in the <code>sbi</code> repository.</p>"},{"location":"tutorials/07_conditional_distributions/#main-syntax","title":"Main syntax","text":"<p>Asssuming you have already obtained <code>posterior</code>.</p> <pre><code>from sbi.analysis import conditional_corrcoeff, conditional_pairplot\n\n# Plot slices through posterior, i.e. conditionals.\n_ = conditional_pairplot(\n    density=posterior,\n    condition=posterior.sample((1,)),\n    limits=torch.tensor([[-2.0, 2.0], [-2.0, 2.0]]),\n)\n\n# Compute the matrix of correlation coefficients of the slices.\ncond_coeff_mat = conditional_corrcoeff(\n    density=posterior,\n    condition=posterior.sample((1,)),\n    limits=torch.tensor([[-2.0, 2.0], [-2.0, 2.0]]),\n)\nplt.imshow(cond_coeff_mat, clim=[-1, 1])\n</code></pre>"},{"location":"tutorials/07_conditional_distributions/#analysing-variability-and-compensation-mechanisms-in-a-toy-example","title":"Analysing variability and compensation mechanisms in a toy example","text":"<p>Below, we use a simple toy example to demonstrate the above described features. For an application of these features to a neuroscience problem, see figure 6 in Gon\u00e7alves, Lueckmann, Deistler et al., 2019.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom IPython.display import HTML\nfrom matplotlib import animation, rc\n\nfrom sbi.analysis import (\n    conditional_corrcoeff,\n    conditional_pairplot,\n    conditional_potential,\n    pairplot,\n)\n\n_ = torch.manual_seed(0)\n</code></pre> <p>Let\u2019s say we have used SNPE to obtain a posterior distribution over three parameters. In this tutorial, we just load the posterior from a file:</p> <pre><code>from toy_posterior_for_07_cc import ExamplePosterior\n\nposterior = ExamplePosterior()\n</code></pre> <p>First, we specify the experimental observation \\(x_o\\) at which we want to evaluate and sample the posterior \\(p(\\theta|x_o)\\):</p> <pre><code>x_o = torch.ones(1, 20)  # simulator output was 20-dimensional\nposterior.set_default_x(x_o)\n</code></pre> <p>As always, we can inspect the posterior marginals with the <code>pairplot()</code> function:</p> <pre><code>posterior_samples = posterior.sample((5000,))\n\nfig, ax = pairplot(\n    samples=posterior_samples,\n    limits=torch.tensor([[-2.0, 2.0]] * 3),\n    offdiag=[\"kde\"],\n    diag=[\"kde\"],\n    figsize=(5, 5),\n)\n</code></pre> <p></p> <p>The 1D and 2D marginals of the posterior fill almost the entire parameter space! Also, the Pearson correlation coefficient matrix of the marginal shows rather weak interactions (low correlations):</p> <pre><code>corr_matrix_marginal = np.corrcoef(posterior_samples.T)\nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\nim = plt.imshow(corr_matrix_marginal, clim=[-1, 1], cmap=\"PiYG\")\n_ = fig.colorbar(im)\n</code></pre> <p></p> <p>It might be tempting to conclude that the experimental data barely constrains our parameters and that almost all parameter combinations can reproduce the experimental data. As we will show below, this is not the case.</p> <p>Because our toy posterior has only three parameters, we can plot posterior samples in a 3D plot:</p> <pre><code>rc(\"animation\", html=\"html5\")\n\n# First set up the figure, the axis, and the plot element we want to animate\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\nax.set_xlim((-2, 2))\nax.set_ylim((-2, 2))\n\n\ndef init():\n    (line,) = ax.plot([], [], lw=2)\n    line.set_data([], [])\n    return (line,)\n\n\ndef animate(angle):\n    num_samples_vis = 1000\n    line = ax.scatter(\n        posterior_samples[:num_samples_vis, 0],\n        posterior_samples[:num_samples_vis, 1],\n        posterior_samples[:num_samples_vis, 2],\n        zdir=\"z\",\n        s=15,\n        c=\"#2171b5\",\n        depthshade=False,\n    )\n    ax.view_init(20, angle)\n    return (line,)\n\n\nanim = animation.FuncAnimation(\n    fig, animate, init_func=init, frames=range(0, 360, 5), interval=150, blit=True\n)\n\nplt.close()\n</code></pre> <pre><code>HTML(anim.to_html5_video())\n</code></pre>    Your browser does not support the video tag.  <p>Clearly, the range of admissible parameters is constrained to a narrow region in parameter space, which had not been evident from the marginals.</p> <p>If the posterior has more than three dimensions, inspecting all dimensions at once will not be possible anymore. One way to still reveal structures in high-dimensional posteriors is to inspect 2D-slices through the posterior. In <code>sbi</code>, this can be done with the <code>conditional_pairplot()</code> function, which computes the conditional distributions within the posterior. We can slice (i.e. condition) the posterior at any location, given by the <code>condition</code>. In the plot below, for all upper diagonal plots, we keep all but two parameters constant at values sampled from the posterior, and inspect what combinations of the remaining two parameters can reproduce experimental data. For the plots on the diagonal (the 1D conditionals), we keep all but one parameter constant.</p> <pre><code>condition = posterior.sample((1,))\n\n_ = conditional_pairplot(\n    density=posterior,\n    condition=condition,\n    limits=torch.tensor([[-2.0, 2.0]] * 3),\n    figsize=(5, 5),\n)\n</code></pre> <p></p> <p>This plot looks completely different from the marginals obtained with <code>pairplot()</code>. As it can be seen on the diagonal plots, if all parameters but one are kept constant, the remaining parameter has to be tuned to a narrow region in parameter space. In addition, the upper diagonal plots show strong correlations: deviations in one parameter can be compensated through changes in another parameter.</p> <p>We can summarize these correlations in a conditional correlation matrix, which computes the Pearson correlation coefficient of each of these pairwise plots. This matrix (below) shows strong correlations between many parameters, which can be interpreted as potential compensation mechansims:</p> <pre><code>cond_coeff_mat = conditional_corrcoeff(\n    density=posterior,\n    condition=condition,\n    limits=torch.tensor([[-2.0, 2.0]] * 3),\n)\nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\nim = plt.imshow(cond_coeff_mat, clim=[-1, 1], cmap=\"PiYG\")\n_ = fig.colorbar(im)\n</code></pre> <p></p> <p>So far, we have investigated the conditional distribution only at a specific <code>condition</code> sampled from the posterior. In many applications, it makes sense to repeat the above analyses with a different <code>condition</code> (another sample from the posterior), which can be interpreted as slicing the posterior at a different location. Note that <code>conditional_corrcoeff()</code> can directly compute the matrix for several <code>conditions</code> and then outputs the average over them. This can be done by passing a batch of \\(N\\) conditions as the <code>condition</code> argument.</p>"},{"location":"tutorials/07_conditional_distributions/#sampling-conditional-distributions","title":"Sampling conditional distributions","text":"<p>So far, we have demonstrated how one can plot 2D conditional distributions with <code>conditional_pairplot()</code> and how one can compute the pairwise conditional correlation coefficient with <code>conditional_corrcoeff()</code>. In some cases, it can be useful to keep a subset of parameters fixed and to vary more than two parameters. This can be done by sampling the conditonal posterior \\(p(\\theta_i | \\theta_{j \\neq i}, x_o)\\). As of <code>sbi</code> <code>v0.18.0</code>, this functionality requires using the sampler interface. In this tutorial, we demonstrate this functionality on a linear gaussian simulator with four parameters. We would like to fix the forth parameter to \\(\\theta_4=0.2\\) and sample the first three parameters given that value, i.e. we want to sample \\(p(\\theta_1, \\theta_2, \\theta_3 | \\theta_4 = 0.2, x_o)\\). For an application in neuroscience, see Deistler, Gon\u00e7alves, Macke, 2021.</p> <p>In this tutorial, we will use SNPE, but the same also works for SNLE and SNRE. First, we define the prior and the simulator and train the deep neural density estimator:</p> <pre><code>import torch\n\nfrom sbi.inference import (\n    SNPE,\n    MCMCPosterior,\n    posterior_estimator_based_potential,\n    simulate_for_sbi,\n)\nfrom sbi.utils import BoxUniform\nfrom sbi.utils.user_input_checks import (\n    check_sbi_inputs,\n    process_prior,\n    process_simulator,\n)\n\nnum_dim = 4\nprior = BoxUniform(low=-2 * torch.ones(num_dim), high=2 * torch.ones(num_dim))\n\n\ndef linear_gaussian(theta):\n    return theta + 1.0 + torch.randn_like(theta) * 0.1\n\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\nsimulator = process_simulator(linear_gaussian, prior, prior_returns_numpy)\ncheck_sbi_inputs(simulator, prior)\n\ninference = SNPE()\ntheta, x = simulate_for_sbi(simulator, prior, 1000)\nposterior_estimator = inference.append_simulations(theta, x).train()\n</code></pre> <pre><code>Running 1000 simulations.:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n Neural network successfully converged after 118 epochs.\n</code></pre> <p>Next, we follow the sampler interface and create a potential_function. The observation in this example is <code>x_o=[1, 1, 1, 1]</code>.</p> <pre><code>potential_fn, theta_transform = posterior_estimator_based_potential(\n    posterior_estimator, prior=prior, x_o=torch.ones(4)\n)\n</code></pre> <p>Now we want to build the conditional potential (please read throught the sampler interface tutorial for an explanation of potential functions). For this, we have to pass a <code>condition</code>. In our case, we want to condition the forth parameter on \\(\\theta_4=0.2\\). Regardless of how many parameters one wants to condition on, in <code>sbi</code>, one has to pass a <code>condition</code> value for all parameters. The first three values will simply be ignored. We can tell the algorithm which parameters should be kept fixed and which ones should be sampled with the argument <code>dims_to_sample</code>.</p> <pre><code>conditioned_potential_fn, restricted_tf, restricted_prior = conditional_potential(\n    potential_fn=potential_fn,\n    theta_transform=theta_transform,\n    prior=prior,\n    condition=torch.as_tensor(\n        [0.0, 0.0, 0.0, 0.2]\n    ),  # the first three values are arbitrary and are ignored internally\n    dims_to_sample=[0, 1, 2],\n)\n</code></pre> <p>Finally, we have to build a sampler for the <code>conditioned_potential_fn</code>. E.g., we can sample the conditional posterior with MCMC:</p> <pre><code>mcmc_posterior = MCMCPosterior(\n    potential_fn=conditioned_potential_fn,\n    theta_transform=restricted_tf,\n    proposal=restricted_prior,\n    method=\"slice_np_vectorized\",\n    num_chains=20,\n).set_default_x(x_o)\ncond_samples = mcmc_posterior.sample((100,))\n</code></pre> <pre><code>Running vectorized MCMC with 20 chains:   0%|          | 0/3000 [00:00&lt;?, ?it/s]\n</code></pre> <p>The resulting samples are 3-dimensional, corresponding to \\([\\theta_1, \\theta_2, \\theta_3]\\), sampled from \\(p(\\theta_1, \\theta_2, \\theta_3 | \\theta_4=0.2, x_o)\\).</p> <pre><code>print(cond_samples.shape)\n</code></pre> <pre><code>torch.Size([100, 3])\n</code></pre> <p>We can also plot them with <code>pairplot</code>:</p> <pre><code>pairplot(cond_samples, limits=[[-2, 2], [-2, 2], [-2, 2], [-2, 2]], figsize=(8, 8),\n         hist_diag=dict(bins=\"auto\"))\n</code></pre> <pre><code>(&lt;Figure size 800x800 with 9 Axes&gt;,\n array([[&lt;Axes: xlabel='dim 1'&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n        [&lt;Axes: &gt;, &lt;Axes: xlabel='dim 2'&gt;, &lt;Axes: &gt;],\n        [&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: xlabel='dim 3'&gt;]], dtype=object))\n</code></pre> <p></p>"},{"location":"tutorials/08_restriction_estimator/","title":"Efficient handling of invalid simulation outputs","text":"<p>For many simulators, the output of the simulator can be ill-defined or it can have non-sensical values. For example, in neuroscience models, if a specific parameter set does not produce a spike, features such as the spike shape can not be computed. When using <code>sbi</code>, such simulations that have <code>NaN</code> or <code>inf</code> in their output are discarded during neural network training. This can lead to inefficetive use of simulation budget: we carry out many simulations, but a potentially large fraction of them is discarded.</p> <p>In this tutorial, we show how we can use <code>sbi</code> to learn regions in parameter space that produce <code>valid</code> simulation outputs, and thereby improve the sampling efficiency. The key idea of the method is to use a classifier to distinguish parameters that lead to <code>valid</code> simulations from regions that lead to <code>invalid</code> simulations. After we have obtained the region in parameter space that produes <code>valid</code> simulation outputs, we train the deep neural density estimator used in <code>SNPE</code>. The method was originally proposed in Lueckmann, Goncalves et al. 2017 and later used in Deistler et al. 2021.</p>"},{"location":"tutorials/08_restriction_estimator/#main-syntax","title":"Main syntax","text":"<pre><code>from sbi.inference import SNPE\nfrom sbi.utils import RestrictionEstimator\n\nrestriction_estimator = RestrictionEstimator(prior=prior)\nproposals = [prior]\n\nfor r in range(num_rounds):\n    theta, x = simulate_for_sbi(simulator, proposals[-1], 1000)\n    restriction_estimator.append_simulations(theta, x)\n    if (\n        r &lt; num_rounds - 1\n    ):  # training not needed in last round because classifier will not be used anymore.\n        classifier = restriction_estimator.train()\n    proposals.append(restriction_estimator.restrict_prior())\n\nall_theta, all_x, _ = restriction_estimator.get_simulations()\n\ninference = SNPE(prior=prior)\ndensity_estimator = inference.append_simulations(all_theta, all_x).train()\nposterior = inference.build_posterior()\n</code></pre>"},{"location":"tutorials/08_restriction_estimator/#further-explanation-in-a-toy-example","title":"Further explanation in a toy example","text":"<pre><code>import torch\n\nfrom sbi.analysis import pairplot\nfrom sbi.inference import SNPE, simulate_for_sbi\nfrom sbi.utils import BoxUniform, RestrictionEstimator\n\n_ = torch.manual_seed(2)\n</code></pre> <p>We will define a simulator with two parameters and two simulation outputs. The simulator produces <code>NaN</code> whenever the first parameter is below <code>0.0</code>. If it is above <code>0.0</code> the simulator simply perturbs the parameter set with Gaussian noise:</p> <pre><code>def simulator(theta):\n    perturbed_theta = theta + 0.5 * torch.randn(2)\n    perturbed_theta[theta[:, 0] &lt; 0.0] = torch.as_tensor([float(\"nan\"), float(\"nan\")])\n    return perturbed_theta\n</code></pre> <p>The prior is a uniform distribution in [-2, 2]:</p> <pre><code>prior = BoxUniform(-2 * torch.ones(2), 2 * torch.ones(2))\n</code></pre> <p>We then begin by drawing samples from the prior and simulating them. Looking at the simulation outputs, half of them contain <code>NaN</code>:</p> <pre><code>theta, x = simulate_for_sbi(simulator, prior, 1000)\nprint(\"Simulation outputs: \", x)\n</code></pre> <pre><code>Running 1000 simulations.:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\nSimulation outputs:  tensor([[ 0.0538, -0.1295],\n        [ 0.7811, -0.1608],\n        [ 0.8663,  0.3622],\n        ...,\n        [    nan,     nan],\n        [    nan,     nan],\n        [ 1.7638,  0.1825]])\n</code></pre> <p>The simulations that contain <code>NaN</code> are wasted, and we want to learn to \u201crestrict\u201d the prior such that it produces only <code>valid</code> simulation outputs. To do so, we set up the <code>RestrictionEstimator</code>:</p> <pre><code>restriction_estimator = RestrictionEstimator(prior=prior)\n</code></pre> <p>The <code>RestrictionEstimator</code> trains a classifier to distinguish parameters that lead to <code>valid</code> simulation outputs from parameters that lead to <code>invalid</code> simulation outputs</p> <pre><code>restriction_estimator.append_simulations(theta, x)\nclassifier = restriction_estimator.train()\n</code></pre> <pre><code>Training neural network. Epochs trained:  46\n</code></pre> <p>We can inspect the <code>restricted_prior</code>, i.e. the parameters that the classifier believes will lead to <code>valid</code> simulation outputs, with:</p> <pre><code>restricted_prior = restriction_estimator.restrict_prior()\nsamples = restricted_prior.sample((10_000,))\n_ = pairplot(samples, limits=[[-2, 2], [-2, 2]], fig_size=(4, 4))\n</code></pre> <pre><code>The `RestrictedPrior` rejected 51.3%\n                of prior samples. You will get a speed-up of\n                105.2%.\n</code></pre> <p></p> <p>Indeed, parameter sets sampled from the <code>restricted_prior</code> always have a first parameter larger than <code>0.0</code>. These are the ones that produce <code>valid</code> simulation outputs (see our definition of the simulator above). We can then use the <code>restricted_prior</code> to generate more simulations. Almost all of them will have <code>valid</code> simulation outputs:</p> <pre><code>new_theta, new_x = simulate_for_sbi(simulator, restricted_prior, 1000)\nprint(\"Simulation outputs: \", new_x)\n</code></pre> <pre><code>The `RestrictedPrior` rejected 50.2%\n                of prior samples. You will get a speed-up of\n                100.8%.\n\n\n\nRunning 1000 simulations.:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\nSimulation outputs:  tensor([[ 1.0285,  1.2620],\n        [ 1.5197,  1.1854],\n        [ 0.6391,  2.6417],\n        ...,\n        [ 1.3871, -0.8298],\n        [ 0.9003, -1.7289],\n        [ 0.7951,  0.2624]])\n</code></pre> <p>We can now use all simulations and run <code>SNPE</code> as always:</p> <pre><code>restriction_estimator.append_simulations(\n    new_theta, new_x\n)  # Gather the new simulations in the `restriction_estimator`.\n(\n    all_theta,\n    all_x,\n    _,\n) = restriction_estimator.get_simulations()  # Get all simulations run so far.\n\ninference = SNPE(prior=prior)\ndensity_estimator = inference.append_simulations(all_theta, all_x).train()\nposterior = inference.build_posterior()\n\nposterior_samples = posterior.sample((10_000,), x=torch.ones(2))\n_ = pairplot(posterior_samples, limits=[[-2, 2], [-2, 2]], fig_size=(3, 3))\n</code></pre> <pre><code>WARNING:root:Found 524 NaN simulations and 0 Inf simulations. They will be excluded from training.\n\n\n Neural network successfully converged after 107 epochs.\n\n\nDrawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <p></p>"},{"location":"tutorials/08_restriction_estimator/#further-options-for-tuning-the-algorithm","title":"Further options for tuning the algorithm","text":"<ul> <li>the whole procedure can be repeated many times (see the loop shown in \u201cMain syntax\u201d in this tutorial)</li> <li>the classifier is trained to be relatively conservative, i.e. it will try to be very sure that a specific parameter set can indeed not produce <code>valid</code> simulation outputs. If you are ok with the restricted prior potentially ignoring a small fraction of parameter sets that might have produced <code>valid</code> data, you can use <code>restriction_estimator.restrict_prior(allowed_false_negatives=...)</code>. The argument <code>allowed_false_negatives</code> sets the fraction of potentially ignored parameter sets. A higher value will lead to more <code>valid</code> simulations.</li> <li>By default, the algorithm considers simulations that have at least one <code>NaN</code> of <code>inf</code> as <code>invalid</code>. You can specify custom criterions with <code>RestrictionEstimator(decision_criterion=...)</code></li> </ul>"},{"location":"tutorials/09_sensitivity_analysis/","title":"Active subspaces for sensitivity analysis","text":"<p>A standard method to analyse dynamical systems such as models of neural dynamics is to use a sensitivity analysis. We can use the posterior obtained with <code>sbi</code>, to perform such analyses.</p>"},{"location":"tutorials/09_sensitivity_analysis/#main-syntax","title":"Main syntax","text":"<pre><code>from sbi.analysis import ActiveSubspace\n\nsensitivity = ActiveSubspace(posterior.set_default_x(x_o))\ne_vals, e_vecs = sensitivity.find_directions(posterior_log_prob_as_property=True)\nprojected_data = sensitivity.project(theta_project, num_dimensions=1)\n</code></pre>"},{"location":"tutorials/09_sensitivity_analysis/#example-and-further-explanation","title":"Example and further explanation","text":"<pre><code>import torch\nfrom torch.distributions import MultivariateNormal\n\nfrom sbi.analysis import ActiveSubspace, pairplot\nfrom sbi.inference import infer, simulate_for_sbi\nfrom sbi.simulators import linear_gaussian\n\n_ = torch.manual_seed(0)\n</code></pre> <p>Let\u2019s define a simple Gaussian toy example:</p> <pre><code>prior = MultivariateNormal(0.0 * torch.ones(2), 2 * torch.eye(2))\n\n\ndef simulator(theta):\n    return linear_gaussian(\n        theta, -0.8 * torch.ones(2), torch.tensor([[1.0, 0.98], [0.98, 1.0]])\n    )\n\n\nposterior = infer(simulator, prior, num_simulations=2000, method=\"SNPE\").set_default_x(\n    torch.zeros(2)\n)\n</code></pre> <pre><code>Running 2000 simulations.:   0%|          | 0/2000 [00:00&lt;?, ?it/s]\n\n\n Neural network successfully converged after 86 epochs.\n</code></pre> <pre><code>posterior_samples = posterior.sample((2000,))\n</code></pre> <pre><code>Drawing 2000 posterior samples:   0%|          | 0/2000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>_ = pairplot(posterior_samples, limits=[[-3, 3], [-3, 3]], figsize=(4, 4))\n</code></pre> <p></p> <p>When performing a sensitivity analysis on this model, we would expect that there is one direction that is less sensitive (from bottom left to top right, along the vector [1, 1]) and one direction that is more sensitive (from top left to bottom right, along [1, -1]). We can recover these directions with the <code>ActiveSubspace</code> module in <code>sbi</code>.</p> <pre><code>sensitivity = ActiveSubspace(posterior)\ne_vals, e_vecs = sensitivity.find_directions(posterior_log_prob_as_property=True)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n</code></pre> <p>The method <code>.find_active()</code> returns eigenvalues and the corresponding eigenvectors. It does so by computing the matrix:</p> <p>\\(M = \\mathbb{E}_{p(\\theta|x_o)}[\\nabla_{\\theta}p(\\theta|x_o)^T \\nabla_{\\theta}p(\\theta|x_o)\\)]</p> <p>It then does an eigendecomposition: \\(M = Q \\Lambda Q^{-1}\\)</p> <p>A strong eigenvalue indicates that the gradient of the posterior density is large, i.e. the system output is sensitive to changes along the direction of the corresponding eigenvector (or <code>active</code>). The eigenvalue corresponding to the vector <code>[0.68, -0.73]</code> is much larger than the eigenvalue of <code>[0.73, 0.67]</code>. This matches the intuition we developed above.</p> <pre><code>print(\"Eigenvalues: \\n\", e_vals, \"\\n\")\nprint(\"Eigenvectors: \\n\", e_vecs)\n</code></pre> <pre><code>Eigenvalues: \n tensor([2.5892e-06, 9.2593e-05])\n\nEigenvectors: \n tensor([[-0.7012, -0.7130],\n        [-0.7130,  0.7012]])\n</code></pre> <p>Lastly, we can project the data into the active dimensions. In this case, we will just use one active dimension:</p> <pre><code>projected_data = sensitivity.project(posterior_samples, num_dimensions=1)\n</code></pre>"},{"location":"tutorials/09_sensitivity_analysis/#some-technical-details","title":"Some technical details","text":"<ul> <li>The gradients and thus the eigenvectors are computed in z-scored space. The mean and standard deviation are computed w.r.t. the prior distribution. Thus, the gradients (and thus the eigenvales) reflect changes on the scale of the prior.</li> <li>The expected value used to compute the matrix \\(M\\) is estimated using <code>1000</code> posterior samples. This value can be set with the <code>.find_active(num_monte_carlo_samples=...)</code> variable.</li> <li>How does this relate to Principal Component Analysis (PCA)? In the example above, the results of PCA would be very similar. However, there are two main differences to PCA: First, PCA ignores local changes in the posterior, whereas the active subspace can change a lot (since it computes the gradient, which is a local quantity). Second, active subspaces can be used characterize the sensitivity of any other quantity w.r.t. circuit parameters. This is outlined below:</li> </ul>"},{"location":"tutorials/09_sensitivity_analysis/#computing-the-sensitivity-of-a-specific-summary-statistic","title":"Computing the sensitivity of a specific summary statistic","text":"<p>Above, we have shown how to identify directions along which the posterior probability changes rapidly. Notably, the posterior probability reflects how consistent a specific parameter set is with all summary statistics, i.e. the entire \\(x_o\\). Sometimes, we might be interested in investigating how a specific features is influenced by the parameters. This feature could be one of the values of \\(x_o\\), but it could also be a different property.</p> <p>As a neuroscience example, in Deistler et al. 2021, we obtained the posterior distribution given burst durations and delays between bursts. After having obtained the posterior, we then wanted to analyse the sensitivity of metabolic cost w.r.t. circuit parameters. The framework we presented above can easily be extended to study such questions.</p> <pre><code>prior = MultivariateNormal(0.0 * torch.ones(2), 2 * torch.eye(2))\n\n\ndef simulator(theta):\n    return linear_gaussian(theta, -0.8 * torch.ones(2), torch.eye(2))\n\n\nposterior = infer(simulator, prior, num_simulations=2000, method=\"SNPE\").set_default_x(\n    torch.zeros(2)\n)\n</code></pre> <pre><code>Running 2000 simulations.:   0%|          | 0/2000 [00:00&lt;?, ?it/s]\n\n\n Neural network successfully converged after 74 epochs.\n</code></pre> <pre><code>_ = pairplot(posterior.sample((10_000,)), limits=[[-3, 3], [-3, 3]], figsize=(4, 4))\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <pre><code>sensitivity = ActiveSubspace(posterior)\n</code></pre> <p>This time, we begin by drawing samples from the posterior and then computing the desired property for each of the samples (i.e. you will probably have to run simulations for each theta and extract the property from the simulation output). As an example, we assume that the property is just the cube of the first dimension of the simulation output:</p> <pre><code>theta, x = simulate_for_sbi(simulator, posterior, 5000)\nproperty_ = x[:, :1] ** 3  # E.g. metabolic cost.\n</code></pre> <pre><code>Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nRunning 5000 simulations.:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n</code></pre> <p>To investigate the sensitivity of a given parameter, we train a neural network to predict the <code>property_</code> from the parameters and then analyse the neural network as above:</p> <p>\\(M = \\mathbb{E}_{p(\\theta|x_o)}[\\nabla_{\\theta}f(\\theta)^T \\nabla_{\\theta}f(\\theta)\\)]</p> <p>where \\(f(\\cdot)\\) is the trained neural network.</p> <pre><code>_ = sensitivity.add_property(theta, property_).train()\ne_vals, e_vecs = sensitivity.find_directions()\n</code></pre> <pre><code> Training neural network. Epochs trained:  67\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>print(\"Eigenvalues: \\n\", e_vals, \"\\n\")\nprint(\"Eigenvectors: \\n\", e_vecs)\n</code></pre> <pre><code>Eigenvalues: \n tensor([4.6135e-07, 3.3536e-05])\n\nEigenvectors: \n tensor([[ 0.0457,  0.9990],\n        [ 0.9990, -0.0457]])\n</code></pre> <p>As we can see, one of the eigenvalues is much smaller than the other one. The larger eigenvalue represents (approximately) the vector <code>[1.0, 0.0]</code>. This makes sense, because only the <code>property_</code> is influenced only by the first output which, in turn, is influenced only by the first parameter.</p>"},{"location":"tutorials/10_crafting_summary_statistics/","title":"Crafting summary statistics","text":"<p>Many simulators produce outputs that are high-dimesional. For example, a simulator might generate a time series or an image. In a previous tutorial, we discussed how a neural networks can be used to learn summary statistics from such data. In this notebook, we will instead focus on hand-crafting summary statistics. We demonstrate that the choice of summary statistics can be crucial for the performance of the inference algorithm.</p> <pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n# sbi\nimport sbi.utils as utils\nfrom sbi.analysis import pairplot\nfrom sbi.inference import SNPE\n</code></pre> <pre><code># remove top and right axis from plots\nmpl.rcParams[\"axes.spines.right\"] = False\nmpl.rcParams[\"axes.spines.top\"] = False\n</code></pre> <p>This notebook is not intended to provide a one-fits-all approach. In fact it argues against this: it argues for the user to carefully construct their summary statistics to (i) further help the user understand his observed data, (ii) help them understand exactly what they want the model to recover from the observation and (iii) help the inference framework itself.</p>"},{"location":"tutorials/10_crafting_summary_statistics/#example-1-the-quadratic-function","title":"Example 1: The quadratic function","text":"<p>Assume we have a simulator that is given by a quadratic function:</p> <p>\\(x(t) = a\\cdot t^2 + b\\cdot t + c + \\epsilon\\),</p> <p>where \\(\\epsilon\\) is Gaussian observation noise and \\(\\theta = \\{a, b, c\\}\\) are the parameters. Given an observed quadratic function \\(x_o\\), we would like to recover the posterior over parameters \\(a_o\\), \\(b_o\\) and \\(c_o\\).</p>"},{"location":"tutorials/10_crafting_summary_statistics/#11-prior-over-parameters","title":"1.1 Prior over parameters","text":"<p>First we define a prior distribution over parameters \\(a\\), \\(b\\) and \\(c\\). Here, we use a uniform prior for \\(a\\), \\(b\\) and \\(c\\) to go from \\(-1\\) to \\(1\\).</p> <pre><code>prior_min = [-1, -1, -1]\nprior_max = [1, 1, 1]\nprior = utils.torchutils.BoxUniform(\n    low=torch.as_tensor(prior_min), high=torch.as_tensor(prior_max)\n)\n</code></pre>"},{"location":"tutorials/10_crafting_summary_statistics/#12-simulator","title":"1.2 Simulator","text":"<p>Defining some helper functions first:</p> <pre><code>def create_t_x(theta, seed=None):\n    \"\"\"Return an t, x array for plotting based on params\"\"\"\n    if theta.ndim == 1:\n        theta = theta[np.newaxis, :]\n\n    rng = np.random.RandomState(seed) if seed is not None else np.random.RandomState()\n\n    t = np.linspace(-1, 1, 200)\n    ts = np.repeat(t[:, np.newaxis], theta.shape[0], axis=1)\n    x = (\n        theta[:, 0] * ts**2\n        + theta[:, 1] * ts\n        + theta[:, 2]\n        + 0.01 * rng.randn(ts.shape[0], theta.shape[0])\n    )\n    return t, x\n\n\ndef eval(theta, t, seed=None):\n    \"\"\"Evaluate the quadratic function at `t`\"\"\"\n\n    if theta.ndim == 1:\n        theta = theta[np.newaxis, :]\n\n    rng = np.random.RandomState(seed) if seed is not None else np.random.RandomState()\n\n    return theta[:, 0] * t**2 + theta[:, 1] * t + theta[:, 2] + 0.01 * rng.randn(1)\n</code></pre> <p>In this example, we generate the observation \\(x_o\\) from parameters \\(\\theta_o=(a_o, b_o, c_o)=(0.3, -0.2, -0.1)\\). The observation as follows.</p> <pre><code>theta_o = np.array([0.3, -0.2, -0.1])\nt, x = create_t_x(theta_o)\nplt.plot(t, x, \"k\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x31227cfd0&gt;]\n</code></pre> <p></p>"},{"location":"tutorials/10_crafting_summary_statistics/#13-summary-statistics","title":"1.3 Summary statistics","text":"<p>We will compare two methods for defining summary statistics. One method uses three summary statistics which are function evaluations at three points in time. The other method uses a single summary statistic: the mean squared error between the observed and the simulated trace. In the second case, one then tries to obtain the posterior \\(p(\\theta | 0)\\), i.e. the error being zero. These two methods are implemented below:  \\(\\textbf{get_3_values()}\\) returns 3 function evaluations at \\(x=-0.5, x=0\\) and \\(x=0.75\\).  \\(\\textbf{get_MSE()}\\) returns the mean squared error between true and a quadratic function corresponding to a prior distributions sample.</p> <pre><code>def get_3_values(theta, seed=None):\n    \"\"\"\n    Return 3 'x' values corresponding to t=-0.5,0,0.75 as summary statistic vector\n    \"\"\"\n    return np.array(\n        [\n            eval(theta, -0.5, seed=seed),\n            eval(theta, 0, seed=seed),\n            eval(theta, 0.75, seed=seed),\n        ]\n    ).T\n</code></pre> <pre><code>def get_MSE(theta, theta_o, seed=None):\n    \"\"\"\n    Return the mean-squared error (MSE) i.e. Euclidean distance from the\n    observation function\n    \"\"\"\n    _, x = create_t_x(theta_o, seed=seed)  # truth\n    _, x_ = create_t_x(theta, seed=seed)  # simulations\n    return np.mean(np.square(x_ - x), axis=0, keepdims=True).T  # MSE\n</code></pre> <p>Let\u2019s try a couple of samples from our prior and see their summary statistics. Notice that these indeed change in small amounts every time you rerun it due to the noise, except if you set the seed.</p>"},{"location":"tutorials/10_crafting_summary_statistics/#14-simulating-data","title":"1.4 Simulating data","text":"<p>Let us see various plots of prior samples and their summary statistics versus the truth, i.e. our artificial observation.</p> <pre><code>t, x_truth = create_t_x(theta_o)\nplt.plot(t, x_truth, \"k\", zorder=1, label=\"truth\")\nn_samples = 100\ntheta = prior.sample((n_samples,))\nt, x = create_t_x(theta.numpy())\nplt.plot(t, x, \"grey\", zorder=0)\nplt.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x315298390&gt;\n</code></pre> <p></p> <p>In summary, we defined reasonable summary statistics and, a priori, there might be an apparent reason why one method would be better than another. When we do inference, we\u2019d like our posterior to focus around parameter samples that have their simulated MSE very close to 0 (i.e. the truth MSE summary statistic) or their 3 extracted \\((t, x)\\) coordinates to be the truthful ones.</p>"},{"location":"tutorials/10_crafting_summary_statistics/#15-inference","title":"1.5 Inference","text":""},{"location":"tutorials/10_crafting_summary_statistics/#151-using-the-mse","title":"1.5.1 Using the MSE","text":"<p>Let\u2019s see if we can use the MSE to recover the true observation parameters \\(\\theta_o=(a_0,b_0,c_0)\\).</p> <pre><code>theta = prior.sample((1000,))\nx = get_MSE(theta.numpy(), theta_o)\n\ntheta = torch.as_tensor(theta, dtype=torch.float32)\nx = torch.as_tensor(x, dtype=torch.float32)\n</code></pre> <pre><code>inference = SNPE(prior)\n_ = inference.append_simulations(theta, x).train()\nposterior = inference.build_posterior()\n</code></pre> <pre><code> Neural network successfully converged after 113 epochs.\n</code></pre> <p>Now that we\u2019ve built the posterior as such, we can see how likely it finds certain parameters given that we tell it that we\u2019ve observed a certain summary statistic (in this case the MSE). We can then sample from it.</p> <pre><code>x_o = torch.as_tensor(\n    [\n        [\n            0.0,\n        ]\n    ]\n)\ntheta_p = posterior.sample((10000,), x=x_o)\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>fig, axes = pairplot(\n    theta_p,\n    limits=list(zip(prior_min, prior_max)),\n    ticks=list(zip(prior_min, prior_max)),\n    figsize=(7, 7),\n    labels=[\"a\", \"b\", \"c\"],\n    points_offdiag={\"markersize\": 6},\n    points_colors=\"r\",\n    points=theta_o,\n);\n</code></pre> <p></p> <p>The posterior seems to pretty broad: i.e. it is not so certain about the \u2018true\u2019 parameters (here showcased in red).</p> <pre><code>x_o_t, x_o_x = create_t_x(theta_o)\nplt.plot(x_o_t, x_o_x, \"k\", zorder=1, label=\"truth\")\n\ntheta_p = posterior.sample((10,), x=x_o)\nx_t, x_x = create_t_x(theta_p.numpy())\nplt.plot(x_t, x_x, \"grey\", zorder=0)\nplt.legend()\n</code></pre> <pre><code>Drawing 10 posterior samples:   0%|          | 0/10 [00:00&lt;?, ?it/s]\n\n\n\n\n\n&lt;matplotlib.legend.Legend at 0x3312e7d50&gt;\n</code></pre> <p></p> <p>The functions are a bit closer to the observation than prior samples, but many posterior samples generate activity that is very far off from the observation. We would expect <code>sbi</code> do better on such a simple example. So what\u2019s going on? Do we need more simulations? Feel free to try, but below we will show that one can use the same number of simulation samples with different summary statistics and do much better.</p>"},{"location":"tutorials/10_crafting_summary_statistics/#152-using-3-coordinates-as-summary-statistics","title":"1.5.2 Using 3 coordinates as summary statistics","text":"<pre><code>x = get_3_values(theta.numpy())\nx = torch.as_tensor(x, dtype=torch.float32)\n</code></pre> <pre><code>inference = SNPE(prior)\n\n_ = inference.append_simulations(theta, x).train()\nposterior = inference.build_posterior()\n</code></pre> <pre><code> Neural network successfully converged after 362 epochs.\n</code></pre> <p>The observation is now given by the values of the observed trace at three different coordinates:</p> <pre><code>x_o = torch.as_tensor(get_3_values(theta_o), dtype=float)\n</code></pre> <pre><code>theta_p = posterior.sample((10000,), x=x_o)\n\nfig, axes = pairplot(\n    theta_p,\n    limits=list(zip(prior_min, prior_max)),\n    ticks=list(zip(prior_min, prior_max)),\n    figsize=(7, 7),\n    labels=[\"a\", \"b\", \"c\"],\n    points_offdiag={\"markersize\": 6},\n    points_colors=\"r\",\n    points=theta_o,\n);\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <pre><code>x_o_t, x_o_x = create_t_x(theta_o)\nplt.plot(x_o_t, x_o_x, \"k\", zorder=1, label=\"truth\")\ntheta_p = posterior.sample((100,), x=x_o)\nind_10_highest = np.argsort(np.array(posterior.log_prob(theta=theta_p, x=x_o)))[-10:]\ntheta_p_considered = theta_p[ind_10_highest, :]\nx_t, x_x = create_t_x(theta_p_considered.numpy())\nplt.plot(x_t, x_x, \"grey\", zorder=0)\nplt.legend()\n</code></pre> <pre><code>Drawing 100 posterior samples:   0%|          | 0/100 [00:00&lt;?, ?it/s]\n\n\n\n\n\n&lt;matplotlib.legend.Legend at 0x334f25050&gt;\n</code></pre> <p></p> <p>Ok this definitely seems to work! The posterior correctly focuses on the true parameters with greater confidence. You can experiment yourself how this improves further with more training samples or you could try to see how many you\u2019d exactly need to keep having a satisfyingly looking posterior and high posterior sample simulations.</p> <p>So, what\u2019s up with the MSE? Why does it not seem so informative to constrain the posterior? In 1.6, we\u2019ll see both the power and pitfalls of summary statistics.</p>"},{"location":"tutorials/10_crafting_summary_statistics/#16-prior-simulations-summary-statistics-vs-observed-summary-statistics","title":"1.6 Prior simulations\u2019 summary statistics vs observed summary statistics","text":"<p>Let\u2019s try to understand this\u2026Let\u2019s look at a histogram of the four summary statistics we\u2019ve experimented with, and see how they compare to our observed truth summary statistic vector:</p> <pre><code>stats = np.concatenate(\n    (get_3_values(theta.numpy()), get_MSE(theta.numpy(), theta_o)), axis=1\n)\nx_o = np.concatenate((get_3_values(theta_o), np.asarray([[0.0]])), axis=1)\n\nfeatures = [\"x @ t=-0.5\", \"x @ t=0\", \"x @ t=0.7\", \"MSE\"]\nfig, axes = plt.subplots(1, 4, figsize=(10, 3))\nxlabelfontsize = 10\nfor i, ax in enumerate(axes.reshape(-1)):\n    ax.hist(\n        stats[:, i],\n        color=[\"grey\"],\n        alpha=0.5,\n        bins=30,\n        density=True,\n        histtype=\"stepfilled\",\n        label=[\"simulations\"],\n    )\n    ax.axvline(x_o[:, i], label=\"observation\", color='k')\n    ax.set_xlabel(features[i], fontsize=xlabelfontsize)\n    if i == 3:\n        ax.legend()\nplt.tight_layout()\n</code></pre> <p></p> <p>We see that for the coordinates (three plots on the left), simulations cover the observation. That is: it covers it from the left and right side in each case. For the MSE, simulations never truly reach the observation \\(0.0\\).</p> <p>For the trained neural network, it is strongly preferable if the simulations cover the observation. In that case, the neural network can interpolate between simulated data. Contrary to that, for the MSE, the neural network has to extrapolate: it never observes a simulation that is to the left of the observation and has to extrapolate to the region of MSE=\\(0.0\\). This seems like a technical point but, as we saw above, it makes a huge difference in performance.</p>"},{"location":"tutorials/10_crafting_summary_statistics/#17-explicit-recommendations","title":"1.7 Explicit recommendations","text":"<p>We give some explicit recommendation when using summary statistics</p> <ul> <li> <p>Visualize the histogram of each summary statistic and plot the value of the observation. If, for some summary statistics, the observation is not covered (or is at the very border, e.g. the MSE above), the trained neural network will struggle.</p> </li> <li> <p>Do not use an \u201cerror\u201d as summary statistic. This is common in optimization (e.g. genetic algorithms), but it often leads to trouble in <code>sbi</code> due to the reason above.</p> </li> <li> <p>Only use summary statistics that are necessary. The less summary statistics you use, the less can go wrong with them. Of course, you have to ensure that the summary statistics describe the raw data sufficiently well.</p> </li> </ul>"},{"location":"tutorials/11_sampler_interface/","title":"Sampling algorithms in <code>sbi</code>","text":"<p>Note: this tutorial requires that the user is already familiar with the flexible interface.</p> <p><code>sbi</code> implements three methods: SNPE, SNLE, and SNRE. When using SNPE, the trained neural network directly approximates the posterior. Thus, sampling from the posterior can be done by sampling from the trained neural network. The neural networks trained in SNLE and SNRE approximate the likelihood(-ratio). Thus, in order to draw samples from the posterior, one has to perform additional sampling steps, e.g. Markov-chain Monte-Carlo (MCMC). In <code>sbi</code>, the implemented samplers are:</p> <ul> <li> <p>Markov-chain Monte-Carlo (MCMC)</p> </li> <li> <p>Rejection sampling</p> </li> <li> <p>Variational inference (VI)</p> </li> </ul> <p>Below, we will demonstrate how these samplers can be used in <code>sbi</code>. First, we train the neural network as always:</p> <pre><code>import torch\n\nfrom sbi.inference import SNLE\n\n# dummy Gaussian simulator for demonstration\nnum_dim = 2\nprior = torch.distributions.MultivariateNormal(torch.zeros(num_dim), torch.eye(num_dim))\ntheta = prior.sample((1000,))\nx = theta + torch.randn((1000, num_dim))\nx_o = torch.randn((1, num_dim))\n\ninference = SNLE(prior=prior, show_progress_bars=False)\nlikelihood_estimator = inference.append_simulations(theta, x).train()\n</code></pre> <pre><code> Neural network successfully converged after 57 epochs.\n</code></pre> <p>And then we pass the options for which sampling method to use to the <code>build_posterior()</code> method:</p> <pre><code># Sampling with MCMC\nsampling_algorithm = \"mcmc\"\nmcmc_method = \"slice_np\"  # or nuts, or hmc\nposterior = inference.build_posterior(sample_with=sampling_algorithm,\n                                      mcmc_method=mcmc_method)\n\n# Sampling with variational inference\nsampling_algorithm = \"vi\"\nvi_method = \"rKL\"  # or fKL\nposterior = inference.build_posterior(sample_with=sampling_algorithm,\n                                      vi_method=vi_method)\n# Unlike other methods, vi needs a training step for every observation.\nposterior = posterior.set_default_x(x_o).train()\n\n# Sampling with rejection sampling\nsampling_algorithm = \"rejection\"\nposterior = inference.build_posterior(sample_with=sampling_algorithm)\n</code></pre> <pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]\n\n\n\nConverged with loss: 5.17\nQuality Score: -0.116    Good: Smaller than 0.5  Bad: Larger than 1.0            NOTE: Less sensitive to mode collapse.\n</code></pre>"},{"location":"tutorials/11_sampler_interface/#more-flexibility-in-adjusting-the-sampler","title":"More flexibility in adjusting the sampler","text":"<p>With the above syntax, you can easily try out different sampling algorithms. However, in many cases, you might want to customize your sampler. Below, we demonstrate how you can change hyperparameters of the samplers (e.g. number of warm-up steps of MCMC) or how you can write your own sampler from scratch.</p>"},{"location":"tutorials/11_sampler_interface/#main-syntax-for-snle-and-snre","title":"Main syntax (for SNLE and SNRE)","text":"<p>As above, we begin by training the neural network as always:</p> <p>Then, for full flexibility on using the sampler, we do not use the <code>.build_posterior()</code> method, but instead we explicitly define the potential function and the sampling algorithm (see below for explanation):</p> <pre><code>from sbi.inference import MCMCPosterior, likelihood_estimator_based_potential\n\npotential_fn, parameter_transform = likelihood_estimator_based_potential(\n    likelihood_estimator, prior, x_o\n)\nposterior = MCMCPosterior(\n    potential_fn, proposal=prior, theta_transform=parameter_transform, warmup_steps=10\n)\n</code></pre> <p>If you want to use variational inference or rejection sampling, you have to replace the last line with <code>VIPosterior</code> or <code>RejectionPosterior</code>:</p> <pre><code>from sbi.inference import RejectionPosterior, VIPosterior\n\n# For VI, we have to train.\nposterior = VIPosterior(\n    potential_fn, prior=prior, theta_transform=parameter_transform\n).train()\n\nposterior = RejectionPosterior(\n    potential_fn, proposal=prior, theta_transform=parameter_transform\n)\n</code></pre> <pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]\n\n\n\nConverged with loss: 5.18\nQuality Score: 0.104     Good: Smaller than 0.5  Bad: Larger than 1.0            NOTE: Less sensitive to mode collapse.\n</code></pre> <p>At this point, you could also plug the <code>potential_fn</code> into any sampler of your choice and not rely on any of the in-built <code>sbi</code>-samplers.</p>"},{"location":"tutorials/11_sampler_interface/#further-explanation","title":"Further explanation","text":"<p>The first lines are the same as for the flexible interface:</p> <pre><code>inference = SNLE()\nlikelihood_estimator = inference.append_simulations(theta, x).train()\n</code></pre> <pre><code> Neural network successfully converged after 59 epochs.\n</code></pre> <p>Next, we obtain the potential function. A potential function is a function of the parameter \\(f(\\theta)\\). The posterior is proportional to the product of likelihood and prior: \\(p(\\theta | x_o) \\propto p(x_o | \\theta)p(\\theta)\\). The potential function is the logarithm of the right-hand side of this equation: \\(f(\\theta) = \\log(p(x_o | \\theta)p(\\theta))\\)</p> <pre><code>potential_fn, parameter_transform = likelihood_estimator_based_potential(\n    likelihood_estimator, prior, x_o\n)\n</code></pre> <p>By calling the <code>potential_fn</code>, you can evaluate the potential:</p> <pre><code># Assuming that your parameters are 1D.\npotential = potential_fn(\n    torch.zeros(1, num_dim)\n)  # -&gt; returns f(0) = log( p(x_o|0) p(0) )\n</code></pre> <p>The other object that is returned by <code>likelihood_estimator_based_potential</code> is a <code>parameter_transform</code>. The <code>parameter_transform</code> is a pytorch transform. The <code>parameter_transform</code> is a fixed transform that is can be applied to parameter <code>theta</code>. It transforms the parameters into unconstrained space (if the prior is bounded, e.g. <code>BoxUniform</code>), and standardizes the parameters (i.e. zero mean, one std). Using <code>parameter_transform</code> during sampling is optional, but it usually improves the performance of MCMC.</p> <pre><code>theta_tf = parameter_transform(torch.zeros(1, num_dim))\ntheta_original = parameter_transform.inv(theta_tf)\nprint(theta_original)  # -&gt; tensor([[0.0]])\n</code></pre> <pre><code>tensor([[0., 0.]])\n</code></pre> <p>After having obtained the <code>potential_fn</code>, we can sample from the posterior with MCMC or rejection sampling:</p> <pre><code>posterior = MCMCPosterior(\n    potential_fn, proposal=prior, theta_transform=parameter_transform\n)\nposterior = RejectionPosterior(potential_fn, proposal=prior)\n</code></pre>"},{"location":"tutorials/11_sampler_interface/#main-syntax-for-snpe","title":"Main syntax for SNPE","text":"<p>SNPE usually does not require MCMC or rejection sampling (if you still need it, you can use the same syntax as above with the <code>posterior_estimator_based_potential</code> function). Instead, SNPE samples from the neural network. If the support of the prior is bounded, some samples can lie outside of the support of the prior. The <code>DirectPosterior</code> class automatically rejects these samples:</p> <pre><code>from sbi.inference import SNPE, DirectPosterior\n\ninference = SNPE()\nposterior_estimator = inference.append_simulations(theta, x).train()\n\nposterior = DirectPosterior(posterior_estimator, prior=prior)\n</code></pre> <pre><code> Neural network successfully converged after 76 epochs.\n</code></pre>"},{"location":"tutorials/12_diagnostics_posterior_predictive_check/","title":"Posterior Predictive Checks (PPC) in SBI","text":"<p>A common safety check performed as part of inference are Posterior Predictive Checks (PPC). A PPC compares data \\(x_{\\text{pp}}\\) generated using the parameters \\(\\theta_{\\text{posterior}}\\) sampled from the posterior with the observed data \\(x_o\\). The general concept is that -if the inference is correct- the generated data \\(x_{\\text{pp}}\\) should \u201clook similar\u201d to the oberved data \\(x_0\\). Said differently, \\(x_o\\) should be within the support of \\(x_{\\text{pp}}\\).</p> <p>A PPC usually should not be used as a validation metric. Nonetheless a PPC is a good start for an inference diagnosis and can provide an intuition about any bias introduced in inference: does \\(x_{\\text{pp}}\\) systematically differ from \\(x_o\\)?</p>"},{"location":"tutorials/12_diagnostics_posterior_predictive_check/#conceptual-code-for-ppc","title":"Conceptual Code for PPC","text":"<p>The following illustrates the main approach of PPCs. We have a trained neural posterior and want to check the correlation between the observation(s) \\(x_o\\) and the posterior sample(s) \\(x_{\\text{pp}}\\).</p> <pre><code>from sbi.analysis import pairplot\n\n# A PPC is performed after we trained a neural posterior `posterior`\nposterior.set_default_x(x_o) # x_o loaded from disk for example\n\n# We draw theta samples from the posterior. This part is not in the scope of SBI\nposterior_samples = posterior.sample((5_000,))\n\n# We use posterior theta samples to generate x data\nx_pp = simulator(posterior_samples)\n\n# We verify if the observed data falls within the support of the generated data\n_ = pairplot(\n    samples=x_pp,\n    points=x_o\n)\n</code></pre>"},{"location":"tutorials/12_diagnostics_posterior_predictive_check/#performing-a-ppc-of-a-toy-example","title":"Performing a PPC of a toy example","text":"<p>Below we provide an example Posterior Predictive Check (PPC) of some toy example:</p> <pre><code>import torch\n\nfrom sbi.analysis import pairplot\n\n_ = torch.manual_seed(0)\n</code></pre> <p>We work on an inference problem over three parameters using any of the techniques implemented in <code>sbi</code>. In this tutorial, we load the dummy posterior (from a python module <code>toy_posterior_for_07_cc</code> alongside this notebook):</p> <pre><code>from toy_posterior_for_07_cc import ExamplePosterior\n\nposterior = ExamplePosterior()\n</code></pre> <p>Let us say that we are observing the data point \\(x_o\\):</p> <pre><code>D = 5  # simulator output was 5-dimensional\nx_o = torch.ones(1, D)\nposterior.set_default_x(x_o)\n</code></pre> <p>The posterior can be used to draw \\(\\theta_{\\text{posterior}}\\) samples:</p> <pre><code>posterior_samples = posterior.sample((5_000,))\n\nfig, ax = pairplot(\n    samples=posterior_samples,\n    limits=torch.tensor([[-2.5, 2.5]] * 3),\n    offdiag=[\"kde\"],\n    diag=[\"kde\"],\n    figsize=(5, 5),\n    labels=[rf\"$\\theta_{d}$\" for d in range(3)],\n)\n</code></pre> <p></p> <p>Now we can use our simulator to generate some data \\(x_{\\text{PP}}\\). We will use the poterior samples \\(\\theta_{\\text{posterior}}\\) as input parameters. Note that the simulation part is not in the <code>sbi</code> scope, so any simulator -including a non-Python one- can be used at this stage. In our case we\u2019ll use a dummy simulator for the sake of demonstration:</p> <pre><code>def dummy_simulator(theta: torch.Tensor, *args, **kwargs) -&gt; torch.Tensor:\n    \"\"\" a function performing a simulation emulating a real simulator outside sbi\n\n    Args:\n        theta: parameters to control the simulation (in this tutorial,\n            these are the posterior_samples $\\theta_{\\text{posterior}}$ obtained\n            from the trained posterior.\n        args: parameters\n        kwargs: keyword arguments\n    \"\"\"\n\n    sample_size = theta.shape[0] # number of posterior_samples\n    scale = 1.0\n\n    shift = torch.distributions.Gumbel(loc=torch.zeros(D), scale=scale / 2).sample()\n    return torch.distributions.Gumbel(loc=x_o[0] + shift, scale=scale).sample(\n        (sample_size,)\n    )\n\n\nx_pp = dummy_simulator(posterior_samples)\n</code></pre> <p>Plotting \\(x_o\\) against the \\(x_{\\text{pp}}\\), we perform a PPC that represents a sanity check. In this case, the check indicates that \\(x_o\\) falls right within the support of \\(x_{\\text{pp}}\\), which should make the experimenter rather confident about the estimated <code>posterior</code>:</p> <pre><code>_ = pairplot(\n    samples=x_pp,\n    points=x_o[0],\n    limits=torch.tensor([[-2.0, 5.0]] * 5),\n    points_colors=\"red\",\n    figsize=(8, 8),\n    offdiag=\"scatter\",\n    scatter_offdiag=dict(marker=\".\", s=5),\n    points_offdiag=dict(marker=\"+\", markersize=20),\n    labels=[rf\"$x_{d}$\" for d in range(D)],\n)\n</code></pre> <p></p> <p>In contrast, \\(x_o\\) falling well outside the support of \\(x_{\\text{pp}}\\) is indicative of a failure to estimate the correct posterior. Here we simulate such a failure mode (by introducing a constant shift to the observations, which the neural estimator was not trained on):</p> <pre><code>error_shift = -2.0 * torch.ones(1, 5)\n\n_ = pairplot(\n    samples=x_pp,\n    points=x_o[0] + error_shift, # shift the observations\n    limits=torch.tensor([[-2.0, 5.0]] * 5),\n    points_colors=\"red\",\n    figsize=(8, 8),\n    offdiag=\"scatter\",\n    scatter_offdiag=dict(marker=\".\", s=5),\n    points_offdiag=dict(marker=\"+\", markersize=20),\n    labels=[rf\"$x_{d}$\" for d in range(D)],\n)\n</code></pre> <p></p> <p>A typical way to investigate this issue would be to run a prior* predictive check, applying the same plotting strategy, but drawing \\(\\theta\\) from the prior instead of the posterior. **The support for \\(x_{\\text{pp}}\\) should be larger and should contain \\(x_o\\)*. If this check is successful, the \u201cblame\u201d can then be shifted to the inference (method used, convergence of density estimators, number of sequential rounds, etc\u2026).</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/","title":"Simulation-based Calibration in SBI","text":"<p>After a density estimator has been trained with simulated data to obtain a posterior, the estimator should be made subject to several diagnostic tests. This needs to be performed before being used for inference given the actual observed data. Posterior Predictive Checks (see previous tutorial) provide one way to \u201ccritique\u201d a trained estimator based on its predictive performance. Another important approach to such diagnostics is simulation-based calibration as developed by Cook et al, 2006 and Talts et al, 2018. This tutorial will demonstrate and teach you this technique with sbi.</p> <p>Simulation-based calibration (SBC) provides a (qualitative) view and a quantitive measure to check, whether the variances of the posterior are balanced, i.e., neither over-confident nor under-confident. As such, SBC can be viewed as a necessary condition (but not sufficient) for a valid inference algorithm: If SBC checks fail, this tells you that your inference is invalid. If SBC checks pass, this is no guarantee that the posterior estimation is working.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#in-a-nutshell","title":"In a nutshell","text":"<p>To run SBC,</p> <ol> <li>we sample <code>theta_o_i</code> values from the prior of the problem at hand</li> <li>we simulate \u201cobservations\u201d from these parameters: <code>x_o_i = simulator(theta_o_i)</code></li> <li>we perform inference given each observation <code>x_o_i</code>.</li> </ol> <p>This produces a separate posterior \\(p_i(\\theta | x_{o,i})\\) for each of <code>x_o_i</code>. The key step for SBC is to generate a set of posterior samples \\(\\{\\theta\\}_i\\) from each posterior. We call this <code>theta_i_s</code>, referring to <code>s</code> samples from posterior \\(p_i(\\theta | x_{o,i})\\)). Next, we rank the corresponding <code>theta_o_i</code> under this set of samples. A rank is computed by counting how many samples <code>theta_i_s</code> fall below their corresponding <code>theta_o_i</code> value (see section 4.1 in Talts et al.). These ranks are then used to perform the SBC check itself.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#key-ideas-behind-sbc","title":"Key ideas behind SBC","text":"<p>The core idea behind SBC is two fold:</p> <ul> <li> <p>SBC ranks of ground truth parameters under the inferred posterior samples follow a uniform distribution.   (If the SBC ranks are not uniformly distributed, the posterior is not well calibrated.)</p> </li> <li> <p>samples from the data averaged posterior (ensemble of randomly chosen posterior samples given multiple distinct observations <code>x_o</code>) are distributed according to the prior</p> </li> </ul>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#what-can-sbc-diagnose","title":"What can SBC diagnose?","text":"<p>SBC can inform us whether we are not wrong. However, it cannot tell us whether we are right, i.e., SBC checks a necessary condition. For example, imagine you run SBC using the prior as a posterior. The ranks would be perfectly uniform. But the inference would be wrong as this scenario would only occur if the posterior is uninformative.</p> <p>The Posterior Predictive Checks (see tutorial 12) can be seen as the complementary sufficient check for the posterior (only as a methaphor, no theoretical guarantees here). Using the prior as a posterior and then doing predictive checks would clearly show that inference failed.</p> <p>To summarize, SBC can:</p> <ul> <li>tell us whether the SBI method applied to the problem at hand produces posteriors that have well-calibrated uncertainties,</li> <li>and if the posteriors have uncalibrated uncertainties, SBC surfaces what kind of systematic bias is present: negative or positive bias (shift in the mean of the predictions) or over- or underdispersion (too large or too small variance)</li> </ul>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#a-healthy-posterior","title":"A healthy posterior","text":"<p>Let\u2019s take the gaussian linear simulator from the previous tutorials and run inference with NPE on it.</p> <p>Note: SBC requires running inference several times. Using SBC with amortized methods like NPE is hence a justified endavour: repeated inference is cheap and SBC can be performed with little runtime penalty. This does not hold for sequential methods or anything relying on MCMC or VI. Should you require methods of MCMC or VI, consider exploiting parallelization and set <code>num_workers&gt;1</code> in the sbc functions.</p> <pre><code>import torch\nfrom torch import eye, ones\nfrom torch.distributions import MultivariateNormal\n\nfrom sbi.analysis.plot import sbc_rank_plot\nfrom sbi.diagnostics import check_sbc, check_tarp, run_sbc, run_tarp\nfrom sbi.inference import SNPE, simulate_for_sbi\n</code></pre> <pre><code>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n</code></pre> <pre><code>num_dim = 2\nnum_simulations = 10_000\n\nprior_mean = ones(num_dim)\nprior_cov = 2 * eye(num_dim)\nprior = MultivariateNormal(\n    loc=prior_mean, covariance_matrix=prior_cov, validate_args=False\n)\n</code></pre>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#an-ideal-case","title":"An ideal case","text":"<p>To explore SBC, we make our life easy and assume that we deal with a problem where the likelihood is modelled by an identity mapping and a bit of smear. But to start, we only use an almost vanishing smear of <code>0.01</code>.</p> <pre><code>default_likelihood_loc = 0.0  # let's start with 0 shift\ndefault_likelihood_scale = 0.01  # let's smear theta only by a little bit\n\n\ndef simulator(theta, loc=default_likelihood_loc, scale=default_likelihood_scale):\n    \"\"\"linear gaussian inspired by sbibm\n    https://github.com/sbi-benchmark/sbibm/blob/15f068a08a938383116ffd92b92de50c580810a3/sbibm/tasks/gaussian_linear/task.py#L74\n    \"\"\"\n    num_dim = theta.shape[-1]\n    cov_ = scale * eye(num_dim)  # always positively semi-definite\n\n    # using validate_args=False disables sanity checks on `covariance_matrix`\n    # for the sake of speed\n    value = MultivariateNormal(\n        loc=(theta + loc), covariance_matrix=cov_, validate_args=False\n    ).sample()\n    return value\n\n\n_ = torch.manual_seed(3)\ntheta = prior.sample((num_simulations,))\nx = simulator(theta)\n</code></pre> <pre><code>_ = torch.manual_seed(1)\n\n# let's sample an observation from the parameters we\n# just produced\ntheta_o = prior.sample((1,))\nx_o = simulator(theta_o)\nprint(\"theta:\", theta_o.numpy())\nprint(\"x    :\", x_o.numpy())\n</code></pre> <pre><code>theta: [[1.9352932 1.3774877]]\nx    : [[1.941461  1.4396194]]\n</code></pre> <pre><code>_ = torch.manual_seed(2)\n\n# we use a mdn model to have a fast turnaround with training the NPE\ninferer = SNPE(prior, density_estimator=\"nsf\")\n# append simulations and run training.\ninferer.append_simulations(theta, x).train(training_batch_size=200);\n</code></pre> <pre><code> Neural network successfully converged after 54 epochs.\n</code></pre> <pre><code>posterior = inferer.build_posterior()\nposterior_samples = posterior.sample((10_000,), x=x_o)\n# Generate predictive samples by simulating from posterior samples.\nposterior_predictive_samples = simulator(posterior_samples)\n</code></pre> <pre><code>Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n\n\n/Users/janteusen/miniconda3/envs/sbi-dev/lib/python3.12/site-packages/nflows/transforms/lu.py:80: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2190.)\n  outputs, _ = torch.triangular_solve(\n</code></pre> <pre><code># let's do some posterior predictive checks to see if the\n# posterior predictive samples cluster aournd the observation `x_o`.\nfrom sbi.analysis import pairplot\n\nfig, ax = pairplot(\n    samples=posterior_predictive_samples,\n    points=x_o,\n    limits=list(zip(x_o.flatten() - 1.0, x_o.flatten() + 1.0)),\n    upper=\"kde\",\n    diag=\"kde\",\n    figsize=(5, 5),\n    labels=[rf\"$x_{d}$\" for d in range(3)],\n)\n</code></pre> <p></p> <p>The observation <code>x_o</code> falls into the support of the predicted posterior samples, i.e. it is within <code>simulator(posterior_samples)</code>. Given the simulator, this is indicative that our posterior estimates the data well.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#running-sbc","title":"Running SBC","text":"<p>We have a working and trained posterior at this point! Hurray! Let\u2019s look at the SBC metrics now.</p> <pre><code>num_sbc_samples = 200  # choose a number of sbc runs, should be ~100s\n# generate ground truth parameters and corresponding simulated observations for SBC.\nthetas = prior.sample((num_sbc_samples,))\nxs = simulator(thetas)\n</code></pre> <p>SBC is implemented in <code>sbi</code> for your use on any <code>sbi</code> posterior. To run it, we only need to call <code>run_sbc</code> with appropriate parameters.</p> <p>Note: For amortized neural posteriors (like in this tutorial), execution of <code>sbc</code> is expected to be fast. For posteriors that conduct inference with MCMC and hence are slow, <code>run_sbc</code> exposes the use of multiple internal parallel workers to the user. To use this feature, add <code>num_workers = 2</code> to the parameters for use of two workers. See the API documentation for details.</p> <pre><code># run SBC: for each inference we draw 1000 posterior samples.\nnum_posterior_samples = 1_000\nnum_workers = 1\nranks, dap_samples = run_sbc(\n    thetas, xs, posterior, num_posterior_samples=num_posterior_samples, num_workers=num_workers\n)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nCalculating ranks for 200 sbc samples.:   0%|          | 0/200 [00:00&lt;?, ?it/s]\n</code></pre> <p><code>sbi</code> establishes two methods to do simulation-based calibration:</p> <ul> <li>metrics to compare the sbc ranks with a uniform distribution</li> <li>control plots for visual inspections like fig. 1 or 2 in Talts et al, 2018</li> </ul> <p>The <code>ranks</code> count is performed per dimension of <code>theta</code>, i.e. on the 1-D marginal posterior estimates. According to theory, the distribution of these ranks (per dimension of <code>theta</code>) should turn out to be uniformly distributed.</p> <p>The data average posterior <code>dap</code> (see equation 1 of Talts et al, 2018) is yet another metric of interest. It is built from singular random samples of the estimated posterior samples for each <code>xs</code> above. The <code>dap</code> is expected to match the prior distribution used (see equation 1 in Talts et al, 2018 too).</p> <pre><code>check_stats = check_sbc(\n    ranks, thetas, dap_samples, num_posterior_samples=num_posterior_samples\n)\n</code></pre> <p>The <code>check_stats</code> variable created contains a dictionary with 3 metrics that help to judge our posterior. The \u201cfirst\u201d two compare the ranks to a uniform distribution.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#ranks-versus-uniform-distribution","title":"Ranks versus Uniform distribution","text":"<pre><code>print(\n    f\"\"\"kolmogorov-smirnov p-values \\n\n    check_stats['ks_pvals'] = {check_stats['ks_pvals'].numpy()}\"\"\"\n)\n</code></pre> <pre><code>kolmogorov-smirnov p-values\n\n    check_stats['ks_pvals'] = [0.4934508 0.1883987]\n</code></pre> <p>The Kolmogorov-Smirnov (KS test, see also here) as used by <code>check_sbc</code> provides p-values <code>pvals</code> on the null hypothesis that the samples from <code>ranks</code> are drawn from a uniform distribution (in other words <code>H_0: PDF(ranks) == PDF(uniform)</code>). We are provided two values as our problem is two-dimensional - one p-value for each dimension.</p> <p>The null hypothesis (of both distributions being equal) is rejected if the p-values fall below a significance threshold (usually <code>&lt; 0.05</code>).  Therefore, the values we see here should ideally be much larger than <code>0.05</code> because the ranks should ideally be close to a uniform distribution. If they are below <code>0.05</code> then this indicates that the posterior is non well-calibrated. Note, however, that we can obtain arbitrarily small p-values if we use a large number of samples, (<code>num_sbc_samples</code>). Note also that this is only a neccessary check, i.e., we can only conclude that the posterior is not miscalibrated, not that it is accurate. </p> <pre><code>print(\n    f\"c2st accuracies \\ncheck_stats['c2st_ranks'] = {check_stats['c2st_ranks'].numpy()}\"\n)\n</code></pre> <pre><code>c2st accuracies \ncheck_stats['c2st_ranks'] = [0.55   0.4675]\n</code></pre> <p>The second tier of metrics comparing <code>ranks</code> with a uniform distributions is a <code>c2st</code> test (see here for details). This is a nonparametric two sample test based on training a classifier to differentiate two ensembles. Here, these two ensembles are the observed <code>ranks</code> and samples from a uniform distribution. The values reported are the accuracies from n-fold cross-validation. If you see values around <code>0.5</code>, the classifier was unable to differentiate both ensembles, i.e. <code>ranks</code> are very uniform. If the values are high towards <code>1</code>, this matches the case where <code>ranks</code> is very unlike a uniform distribution.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#data-averaged-posterior-dap-versus-prior","title":"Data averaged posterior (DAP) versus prior","text":"<pre><code>print(f\"- c2st accuracies check_stats['c2st_dap'] = {check_stats['c2st_dap'].numpy()}\")\n</code></pre> <pre><code>- c2st accuracies check_stats['c2st_dap'] = [0.46   0.5525]\n</code></pre> <p>The last metric reported is again based on <code>c2st</code> computed per dimension of <code>theta</code>. If you see values around <code>0.5</code>, the <code>c2st</code> classifier was unable to differentiate both ensembles for each dimension of <code>theta</code>, i.e. <code>dap</code> are much like (if not identical to) the prior. If the values are very high towards <code>1</code>, this represents the case where <code>dap</code> is very unlike the prior distribution.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#visual-inspection","title":"Visual Inspection","text":"<pre><code>f, ax = sbc_rank_plot(\n    ranks=ranks,\n    num_posterior_samples=num_posterior_samples,\n    plot_type=\"hist\",\n    num_bins=None,  # by passing None we use a heuristic for the number of bins.\n)\n</code></pre> <p>The two plots visualize the distribution of <code>ranks</code> (here depicted in red) in each dimension. Highlighted in grey, you see the 99% confidence interval of a uniform distribution given the number of samples provided. In plain english: for a uniform distribution, we would expect 1 out of 100 (red) bars to lie outside the grey area.</p> <p>We also observe, that the entries fluctuate to some degree. This can be considered a hint that <code>sbc</code> should be conducted with a lot more samples than <code>1000</code>. A good rule of thumb is that given the number of bins <code>B</code> and the number of SBC samples <code>N</code> (chosed to be <code>1_000</code> here) should amount to <code>N / B ~ 20</code>.</p> <pre><code>f, ax = sbc_rank_plot(ranks, 1_000, plot_type=\"cdf\")\n</code></pre> <p></p> <p>The above provides a visual representation of the cumulative density function (CDF) of <code>ranks</code> (blue and orange for each dimension of <code>theta</code>) with respect to the 95% confidence interval of a uniform distribution (grey).</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#multi-dimensional-sbc","title":"multi dimensional SBC","text":"<p>So far, we have performed the SBC checks for each dimension of our parameters \\(\\theta\\) separately. SBI offers a way to perform this check for all dimensions at once.</p> <pre><code># use reduce_fns in order to map the test statistic to only one dimension\nranks, dap_samples = run_sbc(\n    thetas,\n    xs,\n    posterior,\n    num_posterior_samples=num_posterior_samples,\n    reduce_fns=posterior.log_prob,\n)\ncheck_stats = check_sbc(ranks, thetas, dap_samples, 1_000)\nprint(check_stats)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nCalculating ranks for 200 sbc samples.:   0%|          | 0/200 [00:00&lt;?, ?it/s]\n\n\n{'ks_pvals': tensor([0.2002]), 'c2st_ranks': tensor([0.5075], dtype=torch.float64), 'c2st_dap': tensor([0.5400, 0.4500], dtype=torch.float64)}\n</code></pre> <p>In the code above, we depart from the default behavior of <code>run_sbc</code>. The standard behavior of <code>run_sbc</code> is to calculate the SBC ranks of parameters \\(\\theta\\) by comparing the marginal values of the predicted parameter \\(\\theta_i\\) to the reference value \\(\\theta_o\\), i.e. we are ranking each parameter only within its marginal dimension <code>idx</code> which evaluates <code>theta_i[idx] &lt; theta_o[idx]</code> to perform sbc ranking.  </p> <p>The <code>reduce_fns</code> parameter of <code>run_sbc</code> allows users to specify a <code>Callable</code> which is invoked on the tuple <code>(theta,x)</code> before the sbc ranking is performed. As an example, we specify <code>reduce_fns=posterior.log_prob</code> here to rank according to the log probability of theta (given x) under the posterior. This results in the ranking being performed across all dimensions of <code>theta</code> at once (instead of each marginal dimension separately) by reducing <code>theta</code> to the log probabilities under the posterior. Internally, <code>run_sbc</code> would then evaluate <code>posterior.log_prob(theta_i, x_o) &lt; posterior.log_prob(theta_o, x_o)</code> to perform the ranking.</p> <p>Note: The results of <code>check_sbc</code> using ranking <code>posterior.log_prob</code> values from above produce the same conclusion as those using the marginals from further above. <code>ks_pvals</code> is above <code>0.05</code> again and <code>c2st_ranks</code> is about <code>0.5</code> again.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#when-things-go-haywire","title":"When things go haywire","text":"<p>Next, we would like to explore some pathologies visible in sbc plots which can hint at our estimated posterior being somewhat wrong or completely off.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#a-shifted-posterior-mean","title":"A shifted posterior mean","text":"<p>In this scenario, we emulate the situation that our posterior estimates incorrectly with a constant shift. We reuse our trained NPE posterior from above and wrap it so that all samples returned expose a constant shift by <code>+0.1</code>.</p> <pre><code>from utils_13_diagnosis_sbc import BiasedPosterior\n\n# this posterior shifts the expected value of the prior by .1\nposterior_ = BiasedPosterior(posterior, shift=0.1)\n</code></pre> <pre><code>ranks, dap_samples = run_sbc(thetas, xs, posterior_)\ncheck_stats = check_sbc(ranks, thetas, dap_samples, 1_000)\nprint(check_stats)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nCalculating ranks for 200 sbc samples.:   0%|          | 0/200 [00:00&lt;?, ?it/s]\n\n\n{'ks_pvals': tensor([1.8095e-26, 9.1055e-35]), 'c2st_ranks': tensor([0.5800, 0.6600], dtype=torch.float64), 'c2st_dap': tensor([0.4650, 0.4725], dtype=torch.float64)}\n</code></pre> <p>We can see that the Kolmogorov-Smirnov p-values vanish (<code>'ks_pvals': tensor([0., 0.])</code>). Thus, we can reject the hypothesis that the <code>ranks</code> PDF is a uniform PDF. The <code>c2st</code> accuracies show values higher than <code>0.5</code>. This is supports as well that the <code>ranks</code> distribution is not a uniform PDF.</p> <pre><code>f, ax = sbc_rank_plot(ranks, 1_000, plot_type=\"hist\", num_bins=30)\n</code></pre> <p></p> <p>Inspecting the histograms for both dimenions, the rank distribution is clearly tilted to low rank values for both dimensions. Because we have shifted the expected value of the posterior to higher values (by <code>0.1</code>), we see more entries at low rank values.</p> <p>Let\u2019s try to shift all posterior samples in the opposite direction. We shift the expectation value by <code>-0.1</code>:</p> <pre><code>posterior_ = BiasedPosterior(posterior, shift=-0.1)\n</code></pre> <pre><code>ranks, dap_samples = run_sbc(thetas, xs, posterior_)\ncheck_stats = check_sbc(ranks, thetas, dap_samples, 1_000)\nprint(check_stats)\nf, ax = sbc_rank_plot(ranks, 1_000, plot_type=\"hist\", num_bins=30)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nCalculating ranks for 200 sbc samples.:   0%|          | 0/200 [00:00&lt;?, ?it/s]\n\n\n{'ks_pvals': tensor([2.6559e-30, 7.4908e-24]), 'c2st_ranks': tensor([0.6300, 0.5325], dtype=torch.float64), 'c2st_dap': tensor([0.4475, 0.4775], dtype=torch.float64)}\n</code></pre> <p></p> <p>A similar behavior is observed, but this time we see an upshot of ranks to higher values of posterior rank. Because we have shifted the expected value of the posterior to smaller values, we see an upshot in high rank counts.</p> <p>The historgams above provide convincing evidence that this is not a uniform distribution.</p> <p>To conlude at this point, the rank distribution is capable of identifying pathologies of the estimated posterior:</p> <ul> <li>a left-skewed rank distribution shows a systematic underestimation of the posterior mean   (we shifted the posterior by <code>0.1</code>)</li> <li>a rank-skewed rank distribution shows a systematic overestimation of the posterior mean   (we shifted the posterior by <code>-0.1</code>)</li> </ul>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#a-dispersed-posterior","title":"A dispersed posterior","text":"<p>In this scenario we emulate the situation if our posterior estimates incorrectly with a dispersion, i.e. the posterior is too wide or too thin. We reuse our trained NPE posterior from above and wrap it so that all samples return a dispersion by 100% more wide (<code>dispersion=2.0</code>), i.e. the variance is overestimated by a factor of 2.</p> <pre><code>from utils_13_diagnosis_sbc import DispersedPosterior\n\n# this posterior which disperses the expected posterior value of the prior by 2.\nposterior_ = DispersedPosterior(posterior, dispersion=2.0)\n</code></pre> <pre><code>ranks, dap_samples = run_sbc(thetas, xs, posterior_)\ncheck_stats = check_sbc(ranks, thetas, dap_samples, 1_000)\nprint(check_stats)\nf, ax = sbc_rank_plot(ranks, 1_000, plot_type=\"hist\", num_bins=30)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nCalculating ranks for 200 sbc samples.:   0%|          | 0/200 [00:00&lt;?, ?it/s]\n\n\n{'ks_pvals': tensor([0.0433, 0.1049]), 'c2st_ranks': tensor([0.5175, 0.5125], dtype=torch.float64), 'c2st_dap': tensor([0.4750, 0.4700], dtype=torch.float64)}\n</code></pre> <p></p> <p>The rank histograms now look more like a very wide gaussian distribution centered in the middle. The KS p-values again vanish unsurprisingly (we must reject the hypothesis that both distributions are from the same uniform PDF) and the c2st_ranks indicate that the rank histogram is not uniform too. As our posterior samples are distributed too broad now, we obtain more \u201cmedium\u201d range ranks and hence produce the peak of ranks in the center of the histogram.</p> <p>We can repeat this exercise by making our posterior too thin, i.e. the variance of the posterior is too small. Let\u2019s cut it by half (<code>dispersion=0.5</code>).</p> <pre><code>posterior_ = DispersedPosterior(posterior, dispersion=0.5)\n</code></pre> <pre><code>ranks, dap_samples = run_sbc(thetas, xs, posterior_)\ncheck_stats = check_sbc(ranks, thetas, dap_samples, 1_000)\nprint(check_stats)\nf, ax = sbc_rank_plot(ranks, 1_000, plot_type=\"hist\", num_bins=30)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nCalculating ranks for 200 sbc samples.:   0%|          | 0/200 [00:00&lt;?, ?it/s]\n\n\n{'ks_pvals': tensor([0.0146, 0.0021]), 'c2st_ranks': tensor([0.5250, 0.5875], dtype=torch.float64), 'c2st_dap': tensor([0.4450, 0.4725], dtype=torch.float64)}\n</code></pre> <p></p> <p>The histogram of ranks now shoots above the allowed (greyed) area for a uniform distributed around the extrema. We made the posterior samples too thin, so we received more extreme counts of ranks. The KS p-values vanish again and the <code>c2st</code> metric of the ranks is also larger than <code>0.5</code> which underlines that our rank distribution is not uniformly distributed.</p> <p>We again see, the rank distribution is capable of identifying pathologies of the estimated posterior:</p> <ul> <li>a centrally peaked rank distribution shows a systematic over-estimation of the posterior variance   (we dispersed the variance of the posterior by a factor of <code>2</code>)</li> <li>a U shaped rank distribution shows a systematic under-estimation of the posterior variance   (we dispersed the variance of the posterior by a factor of <code>.5</code>)</li> </ul> <p>Simulation-based calibration offers a direct handle on which pathology an estimated posterior examines. Outside of this tutorial, you may very well encounter situations with mixtures of effects (a shifted mean and over-estimated variance). Moreover, uncovering a malignant posterior is only the first step to fix your analysis.</p>"},{"location":"tutorials/13_diagnostics_simulation_based_calibration/#posterior-calibration-with-tarp-lemos-et-al-2023","title":"Posterior calibration with TARP (Lemos et al. 2023)","text":"<p>TARP is an alternative calibration check proposed recently in https://arxiv.org/abs/2302.03026.  It is implemented in the <code>sbi</code> package as well, following a very similar API than above.</p> <p>Given a test set \\((\\theta^*, x^*)\\) and a set of reference points \\(\\theta_r\\), TARP calculates  statistics for posterior calibration by  - drawing posterior samples \\(\\theta\\) given each \\(x_*\\) - calculating the distance \\(r\\) between \\(\\theta_*\\) and \\(\\theta_r\\) - counting for how many of the posterior samples their distance to \\(\\theta_r\\) is smaller than \\(r\\)</p> <p>See https://arxiv.org/abs/2302.03026, Figure 2, for an illustration.</p> <p>For each given coverage level \\(\\alpha\\), one can then calculate the corresponding average counts and check, whether they correspond to the given \\(\\alpha\\). </p> <p>The visualization and interpretation of TARP is therefore similar to that of SBC.</p> <pre><code>num_tarp_samples = 200  # choose a number of sbc runs, should be ~100s\n# generate ground truth parameters and corresponding simulated observations for SBC.\nthetas = prior.sample((num_tarp_samples,))\nxs = simulator(thetas)\n</code></pre> <pre><code># the tarp method returns the ECP values for a given set of alpha coverage levels.\necp, alpha = run_tarp(\n    thetas,\n    xs,\n    posterior,\n    references=None,  # will be calculated automatically.\n    num_posterior_samples=1000,\n)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code># Similar to SBC, we can check then check whether the distribution of ecp is close to\n# that of alpha.\natc, ks_pval = check_tarp(ecp, alpha)\nprint(atc, \"Should be close to 0\")\nprint(ks_pval, \"Should be larger than 0.05\")\n</code></pre> <pre><code>0.16200074553489685 Should be close to 0\n0.9999999953860058 Should be larger than 0.05\n</code></pre> <pre><code># Or, we can perform a visual check.\nfrom sbi.analysis.plot import plot_tarp\nplot_tarp(ecp, alpha);\n</code></pre> <p></p> <p>In contrast to SBC (Talts et al.) and coverage based highest posterior density regions (Deistler et al.,), TARP provides a necessary and sufficient condition for posterior accuracy, i.e., it can also detect inaccurate posterior estimators. </p> <p>Note, however, that this property depends on the choice of reference point distribution: to obtain the full diagnostic power of TARP, one would need to sample reference points from a distribution that depends on \\(x\\). Thus, in general, we recommend using and interpreting TARP like SBC and complementing coverage checks with posterior predictive checks.</p>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/","title":"SBI with iid data and permutation-invariant embeddings","text":"<p>There are scenarios in which we observe multiple data points per experiment and we can assume that they are independent and identically distributed (iid, i.e., they are assumed to have the same underlying model parameters). For example, in decision-making experiments, the experiment is often repeated in trials with the same experimental settings and conditions. The corresponding set of trials is then assumed to be \u201ciid\u201d given a single parameter set. In such a scenario, we may want to obtain the posterior given a set of observation \\(p(\\theta | X=\\{x_i\\}_i^N)\\).</p>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#amortization-of-neural-network-training-iid-inference-with-nle-nre","title":"Amortization of neural network training: iid-inference with NLE / NRE","text":"<p>For some SBI variants the iid assumption can be exploited: when using a likelihood-based SBI method (<code>SNLE</code>, <code>SNRE</code>) one can train the density or ratio estimator on single-trial data, and then perform inference with <code>MCMC</code> or variational inference (<code>VI</code>). Crucially, because the data is iid and the estimator is trained on single-trial data, one can repeat the inference with a different <code>x_o</code> (a different set of trials, or different number of trials) without having to retrain the density estimator. One can interpet this as amortization of the SBI training: we can obtain a neural likelihood, or likelihood-ratio estimate for new <code>x_o</code>s without retraining, but we still have to run <code>MCMC</code> or <code>VI</code> to do inference.</p> <p>In addition, one cannot only change the number of trials of a new <code>x_o</code>, but also the entire inference setting. For example, one can apply hierarchical inference with changing hierarchical denpendencies between the model parameters\u2013all without having to retrain the density estimator because it estimates single-trail likelihoods.</p>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#full-amortization-iid-inference-with-npe-and-permutation-invariant-embedding-nets","title":"Full amortization: iid-inference with NPE and permutation-invariant embedding nets","text":"<p>When performing neural posterior estimation (<code>SNPE</code>) we cannot exploit the iid assumption directly. Thus, the underlying neural network takes <code>x</code> as input and predicts the parameters of the density estimator. As a consequence, if <code>x</code> is a set of iid observations \\(X=\\{x_i\\}_i^N\\) then the neural network has to be invariant to permutations of this set, i.e., it has to be permutation invariant. In addition, the neural network has to be able to consume a varying number of iid datapoints in order to be amortized over the number of trials. Therefore, in order to use <code>SNPE</code> for inference on iid data, we need to provide a corresponding embedding network that handles the iid-data. This will likely require some hyperparameter tuning and more training data for inference to work accurately. But once we have this, inference is fully amortized, i.e., we can get new posterior samples almost instantly without retraining and without running <code>MCMC</code> or <code>VI</code>.</p>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#sbi-with-trial-based-data","title":"SBI with trial-based data","text":"<p>For illustration, we use a simple linear Gaussian simulator, as in previous tutorials. The simulator takes a single parameter (vector) which is the mean of a Gaussian. The simulator then adds noise with a fixed variance (set to one). We define a Gaussian prior over the mean and perform inference.</p> <p>The observed data is also sampled from a Gaussian with some fixed \u201cground-truth\u201d parameter \\(\\theta_o\\). Crucially, the observed data <code>x_o</code> can consist of multiple samples given the same ground-truth parameters and these samples are iid given \\(\\theta\\):</p> \\[ \\theta \\sim \\mathcal{N}(\\mu_0,\\; \\Sigma_0) \\\\ x | \\theta \\sim \\mathcal{N}(\\theta,\\; \\Sigma=I) \\\\ \\mathbf{x_o} = \\{x_o^i\\}_{i=1}^N \\sim  \\mathcal{N}(\\theta_o,\\; \\Sigma=I) \\] <p>For this toy problem, the ground-truth posterior is well defined, it is again a Gaussian, centered on the mean of \\(\\mathbf{x_o}\\) and with variance scaled by the number of trials \\(N\\), i.e., the more trials we observe, the more information about the underlying \\(\\theta_o\\) we have and the more concentrated the posteriors becomes.</p> <p>We will illustrate this below:</p> <pre><code>import matplotlib.pyplot as plt\nimport torch\nfrom torch import eye, zeros\nfrom torch.distributions import MultivariateNormal\n\nfrom sbi.analysis import pairplot\nfrom sbi.inference import SNLE, SNPE, simulate_for_sbi\nfrom sbi.simulators.linear_gaussian import (\n    linear_gaussian,\n    true_posterior_linear_gaussian_mvn_prior,\n)\nfrom sbi.utils.metrics import c2st\nfrom sbi.utils.user_input_checks import (\n    check_sbi_inputs,\n    process_prior,\n    process_simulator,\n)\n\n# Seeding\ntorch.manual_seed(1);\n</code></pre> <pre><code># Gaussian simulator\ntheta_dim = 2\nx_dim = theta_dim\n\n# likelihood_mean will be likelihood_shift+theta\nlikelihood_shift = -1.0 * zeros(x_dim)\nlikelihood_cov = 0.3 * eye(x_dim)\n\nprior_mean = zeros(theta_dim)\nprior_cov = eye(theta_dim)\nprior = MultivariateNormal(loc=prior_mean, covariance_matrix=prior_cov)\n\n# Define Gaussian simulator\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\nsimulator = process_simulator(\n    lambda theta: linear_gaussian(theta, likelihood_shift, likelihood_cov),\n    prior,\n    prior_returns_numpy,\n)\ncheck_sbi_inputs(simulator, prior)\n\n\n# Use built-in function to obtain ground-truth posterior given x_o\ndef get_true_posterior_samples(x_o, num_samples=1):\n    return true_posterior_linear_gaussian_mvn_prior(\n        x_o, likelihood_shift, likelihood_cov, prior_mean, prior_cov\n    ).sample((num_samples,))\n</code></pre>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#the-analytical-posterior-concentrates-around-true-parameters-with-increasing-number-of-iid-trials","title":"The analytical posterior concentrates around true parameters with increasing number of IID trials","text":"<pre><code>num_trials = [1, 5, 15, 20]\ntheta_o = zeros(1, theta_dim)\n\n# Generate multiple x_os with increasing number of trials.\nxos = [theta_o.repeat(nt, 1) for nt in num_trials]\n\n# Obtain analytical posterior samples for each of them.\ntrue_samples = [get_true_posterior_samples(xo, 1000) for xo in xos]\n</code></pre> <pre><code># Plot them in one pairplot as contours (obtained via KDE on the samples).\nfig, ax = pairplot(\n    true_samples,\n    points=theta_o,\n    diag=\"kde\",\n    upper=\"contour\",\n    kde_offdiag=dict(bins=50),\n    kde_diag=dict(bins=100),\n    contour_offdiag=dict(levels=[0.95]),\n    points_colors=[\"k\"],\n    points_offdiag=dict(marker=\"*\", markersize=10),\n)\nplt.sca(ax[1, 1])\nplt.legend(\n    [f\"{nt} trials\" if nt &gt; 1 else f\"{nt} trial\" for nt in num_trials]\n    + [r\"$\\theta_o$\"],\n    frameon=False,\n    fontsize=12,\n);\n</code></pre> <pre><code>WARNING:root:upper is deprecated, use offdiag instead.\n</code></pre> <p>Indeed, with increasing number of trials the posterior density concentrates around the true underlying parameter.</p>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#iid-inference-with-nle","title":"IID inference with NLE","text":"<p>(S)NLE and (S)NRE can perform inference given multiple IID obserations by using only single-trial training data (i.e., for training, we run the simulator only once per parameter set). Once the likelihood is learned on single trials (i.e., a neural network that predicts the likelihood of a single observation given a parameter set), one can sample the posterior for any number of trials. This works because, given a single-trial neural likelihood from (S)NLE or (S)NRE, we can calculate the joint likelihoods of all trials by multiplying them together (or adding them in log-space). The joint likelihood can then be plugged into <code>MCMC</code> or <code>VI</code>. <code>sbi</code> takes care of all of these steps, so you do not have to implement anything yourself:</p> <pre><code># Train SNLE.\ninferer = SNLE(prior, show_progress_bars=True, density_estimator=\"mdn\")\ntheta, x = simulate_for_sbi(simulator, prior, 10000, simulation_batch_size=1000)\ninferer.append_simulations(theta, x).train(training_batch_size=1000);\n</code></pre> <pre><code>Running 10000 simulations.:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n\n\n Neural network successfully converged after 40 epochs.\n</code></pre> <pre><code># Obtain posterior samples for different number of iid xos.\nnle_samples = []\nnum_samples = 1000\n\nmcmc_parameters = dict(\n    num_chains=50,\n    thin=5,\n    warmup_steps=30,\n    init_strategy=\"proposal\",\n)\nmcmc_method = \"slice_np_vectorized\"\n\nposterior = inferer.build_posterior(\n    mcmc_method=mcmc_method,\n    mcmc_parameters=mcmc_parameters,\n)\n\n# Generate samples with MCMC given the same set of x_os as above.\nfor xo in xos:\n    nle_samples.append(posterior.sample(sample_shape=(num_samples,), x=xo))\n</code></pre> <pre><code>Running vectorized MCMC with 50 chains:   0%|          | 0/12500 [00:00&lt;?, ?it/s]\n\n\nC:\\Users\\zina\\Desktop\\Documents\\repos\\sbi\\sbi\\utils\\user_input_checks.py:600: UserWarning: An x with a batch size of 5 was passed. It will be interpreted as a batch of independent and identically\n            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n            same underlying (unknown) parameter. The resulting posterior will be with\n            respect to entire batch, i.e,. p(theta | X).\n  warn_on_iid_x(num_trials=input_x_shape[0])\n\n\n\nRunning vectorized MCMC with 50 chains:   0%|          | 0/12500 [00:00&lt;?, ?it/s]\n\n\nC:\\Users\\zina\\Desktop\\Documents\\repos\\sbi\\sbi\\utils\\user_input_checks.py:600: UserWarning: An x with a batch size of 15 was passed. It will be interpreted as a batch of independent and identically\n            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n            same underlying (unknown) parameter. The resulting posterior will be with\n            respect to entire batch, i.e,. p(theta | X).\n  warn_on_iid_x(num_trials=input_x_shape[0])\n\n\n\nRunning vectorized MCMC with 50 chains:   0%|          | 0/12500 [00:00&lt;?, ?it/s]\n\n\nC:\\Users\\zina\\Desktop\\Documents\\repos\\sbi\\sbi\\utils\\user_input_checks.py:600: UserWarning: An x with a batch size of 20 was passed. It will be interpreted as a batch of independent and identically\n            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n            same underlying (unknown) parameter. The resulting posterior will be with\n            respect to entire batch, i.e,. p(theta | X).\n  warn_on_iid_x(num_trials=input_x_shape[0])\n\n\n\nRunning vectorized MCMC with 50 chains:   0%|          | 0/12500 [00:00&lt;?, ?it/s]\n</code></pre> <p>Note that <code>sbi</code> warns about <code>iid-x</code> with increasing number of trial here. We ignore the warning because that\u2019s exactly what we want to do.</p> <pre><code># Plot them in one pairplot as contours (obtained via KDE on the samples).\nfig, ax = pairplot(\n    nle_samples,\n    points=theta_o,\n    diag=\"kde\",\n    upper=\"contour\",\n    kde_offdiag=dict(bins=50),\n    kde_diag=dict(bins=100),\n    contour_offdiag=dict(levels=[0.95]),\n    points_colors=[\"k\"],\n    points_offdiag=dict(marker=\"*\", markersize=10),\n)\nplt.sca(ax[1, 1])\nplt.legend(\n    [f\"{nt} trials\" if nt &gt; 1 else f\"{nt} trial\" for nt in num_trials]\n    + [r\"$\\theta_o$\"],\n    frameon=False,\n    fontsize=12,\n);\n</code></pre> <pre><code>WARNING:root:upper is deprecated, use offdiag instead.\n</code></pre> <p></p> <p>The pairplot above already indicates that (S)NLE is well able to obtain accurate posterior samples also for increasing number of trials (note that we trained the single-round version of SNLE so that we did not have to re-train it for new \\(x_o\\)).</p> <p>Quantitatively we can measure the accuracy of SNLE by calculating the <code>c2st</code> score between SNLE and the true posterior samples, where the best accuracy is perfect for <code>0.5</code>:</p> <pre><code>cs = [\n    c2st(torch.from_numpy(s1), torch.from_numpy(s2))\n    for s1, s2 in zip(true_samples, nle_samples)\n]\n\nfor _ in range(len(num_trials)):\n    print(f\"c2st score for num_trials={num_trials[_]}: {cs[_].item():.2f}\")\n</code></pre> <pre><code>c2st score for num_trials=1: 0.52\nc2st score for num_trials=5: 0.48\nc2st score for num_trials=15: 0.49\nc2st score for num_trials=20: 0.51\n</code></pre>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#iid-inference-with-npe-using-permutation-invariant-embedding-nets","title":"IID inference with NPE using permutation-invariant embedding nets","text":"<p>For NPE we need to define an embedding net that handles the set-like structure of iid-data, i.e., that it permutation invariant and can handle different number of trials.</p> <p>We implemented several embedding net classes that allow to construct such a permutation- and number-of-trials invariant embedding net.</p> <p>To become permutation invariant, the neural net first learns embeddings for single trials and then performs a permutation invariant operation on those embeddings, e.g., by taking the sum or the mean (Chen et al. 2018, Radev et al. 2021).</p> <p>To become invariant w.r.t. the number-of-trials, we train the net with varying number of trials for each parameter setting. This means that, unlike for (S)NLE and (S)NRE, (S)NPE requires to run the simulator multiple times for individual parameter sets to generate the training data.</p> <p>In order to implement this in <code>sbi</code>, \u201cunobserved\u201d trials in the training dataset have to be masked by NaNs (and ignore the resulting SBI warning about NaNs in the training data).</p>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#construct-training-data-set","title":"Construct training data set.","text":"<pre><code># we need to fix the maximum number of trials.\nmax_num_trials = 20\n\n# construct training data set: we want to cover the full range of possible number of\n# trials\nnum_training_samples = 1000\ntheta = prior.sample((num_training_samples,))\n\n# there are certainly smarter ways to construct the training data set, but we go with a\n# for loop here for illustration purposes.\nx = torch.ones(num_training_samples * max_num_trials, max_num_trials, x_dim) * float(\n    \"nan\"\n)\nfor i in range(num_training_samples):\n    xi = simulator(theta[i].repeat(max_num_trials, 1))\n    for j in range(max_num_trials):\n        x[i * max_num_trials + j, : j + 1, :] = xi[: j + 1, :]\n\ntheta = theta.repeat_interleave(max_num_trials, dim=0)\n</code></pre>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#build-embedding-net","title":"Build embedding net","text":"<pre><code>from sbi.neural_nets.embedding_nets import FCEmbedding, PermutationInvariantEmbedding\nfrom sbi.neural_nets import posterior_nn\n\n# embedding\nlatent_dim = 10\nsingle_trial_net = FCEmbedding(\n    input_dim=theta_dim,\n    num_hiddens=40,\n    num_layers=2,\n    output_dim=latent_dim,\n)\nembedding_net = PermutationInvariantEmbedding(\n    single_trial_net,\n    trial_net_output_dim=latent_dim,\n    # NOTE: post-embedding is not needed really.\n    num_layers=1,\n    num_hiddens=10,\n    output_dim=10,\n)\n\n# we choose a simple MDN as the density estimator.\n# NOTE: we turn off z-scoring of the data, as we used NaNs for the missing trials.\ndensity_estimator = posterior_nn(\"mdn\", embedding_net=embedding_net, z_score_x=\"none\")\n</code></pre>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#run-training","title":"Run training","text":"<pre><code>inference = SNPE(prior, density_estimator=density_estimator)\n# NOTE: we don't exclude invalid x because we used NaNs for the missing trials.\ninference.append_simulations(\n    theta,\n    x,\n    exclude_invalid_x=False,\n).train(training_batch_size=1000)\nposterior = inference.build_posterior()\n</code></pre> <pre><code>WARNING:root:Found 19000 NaN simulations and 0 Inf simulations. They are not excluded from training due to `exclude_invalid_x=False`.Training will likely fail, we strongly recommend `exclude_invalid_x=True` for Single-round NPE.\n\n\n Neural network successfully converged after 244 epochs.\n</code></pre>"},{"location":"tutorials/14_iid_data_and_permutation_invariant_embeddings/#amortized-inference","title":"Amortized inference","text":"<p>Comparing runtimes, we see that the NPE training takes a bit longer than the training on single trials for <code>NLE</code> above.</p> <p>However, we trained the density estimator such that it can handle multiple and changing number of iid trials (up to 20).</p> <p>Thus, we can obtain posterior samples for different <code>x_o</code> with just a single forward pass instead of having to run <code>MCMC</code> for each new observation.</p> <p>As you can see below, the c2st score for increasing number of observed trials remains close to the ideal <code>0.5</code>.</p> <pre><code>npe_samples = []\nfor xo in xos:\n    # we need to pad the x_os with NaNs to match the shape of the training data.\n    xoi = torch.ones(1, max_num_trials, x_dim) * float(\"nan\")\n    xoi[0, : len(xo), :] = xo\n    npe_samples.append(posterior.sample(sample_shape=(num_samples,), x=xoi))\n\ncs = [c2st(torch.from_numpy(s1), s2) for s1, s2 in zip(true_samples, npe_samples)]\n\nfor _ in range(len(num_trials)):\n    print(f\"c2st score for num_trials={num_trials[_]}: {cs[_].item():.2f}\")\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\nc2st score for num_trials=1: 0.52\nc2st score for num_trials=5: 0.54\nc2st score for num_trials=15: 0.53\nc2st score for num_trials=20: 0.54\n</code></pre> <pre><code>num_trials = [1, 5, 15, 20]\nxos = [theta_o.repeat(nt, 1) for nt in num_trials]\n\nnpe_samples = []\nfor xo in xos:\n    # we need to pad the x_os with NaNs to match the shape of the training data.\n    xoi = torch.ones(1, max_num_trials, x_dim) * float(\"nan\")\n    xoi[0, : len(xo), :] = xo\n    npe_samples.append(posterior.sample(sample_shape=(num_samples,), x=xoi))\n\n\n# Plot them in one pairplot as contours (obtained via KDE on the samples).\nfig, ax = pairplot(\n    npe_samples,\n    points=theta_o,\n    diag=\"kde\",\n    upper=\"contour\",\n    kde_offdiag=dict(bins=50),\n    kde_diag=dict(bins=100),\n    contour_offdiag=dict(levels=[0.95]),\n    points_colors=[\"k\"],\n    points_offdiag=dict(marker=\"*\", markersize=10),\n)\nplt.sca(ax[1, 1])\nplt.legend(\n    [f\"{nt} trials\" if nt &gt; 1 else f\"{nt} trial\" for nt in num_trials]\n    + [r\"$\\theta_o$\"],\n    frameon=False,\n    fontsize=12,\n);\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\nWARNING:root:upper is deprecated, use offdiag instead.\n</code></pre> <p></p> <pre><code># We can easily obtain posteriors for many different x_os, instantly, because\n# NPE is fully amortized:\nnum_trials = [2, 4, 6, 8, 12, 14, 18]\nnpe_samples = []\nfor xo in xos:\n    # we need to pad the x_os with NaNs to match the shape of the training data.\n    xoi = torch.ones(1, max_num_trials, x_dim) * float(\"nan\")\n    xoi[0, : len(xo), :] = xo\n    npe_samples.append(posterior.sample(sample_shape=(num_samples,), x=xoi))\n\n\n# Plot them in one pairplot as contours (obtained via KDE on the samples).\nfig, ax = pairplot(\n    npe_samples,\n    points=theta_o,\n    diag=\"kde\",\n    upper=\"contour\",\n    kde_offdiag=dict(bins=50),\n    kde_diag=dict(bins=100),\n    contour_offdiag=dict(levels=[0.95]),\n    points_colors=[\"k\"],\n    points_offdiag=dict(marker=\"*\", markersize=10),\n)\nplt.sca(ax[1, 1])\nplt.legend(\n    [f\"{nt} trials\" if nt &gt; 1 else f\"{nt} trial\" for nt in num_trials]\n    + [r\"$\\theta_o$\"],\n    frameon=False,\n    fontsize=12,\n);\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\nWARNING:root:upper is deprecated, use offdiag instead.\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"tutorials/15_mcmc_diagnostics_with_arviz/","title":"MCMC diagnostics with Arviz","text":"<p>This tutorial shows how to evaluate the quality of MCMC samples generated via <code>sbi</code> using the <code>arviz</code> package.</p> <p>Outline:</p> <ol> <li>Train MNLE to approximate the likelihood underlying the simulator</li> <li>Run MCMC using <code>pyro</code> MCMC samplers via <code>sbi</code> interface</li> <li>Use <code>arviz</code> to visualize the posterior, predictive distributions and MCMC diagnostics.</li> </ol> <pre><code>import arviz as az\nimport torch\nfrom pyro.distributions import InverseGamma\nfrom torch.distributions import Beta, Binomial, Gamma\n\nfrom sbi.inference import MNLE, MCMCPosterior, likelihood_estimator_based_potential\nfrom sbi.utils import MultipleIndependent\n\n# Seeding\ntorch.manual_seed(1);\n</code></pre> <pre><code># Toy simulator for mixed data\ndef mixed_simulator(theta):\n    beta, ps = theta[:, :1], theta[:, 1:]\n\n    choices = Binomial(probs=ps).sample()\n    rts = InverseGamma(concentration=2 * torch.ones_like(beta), rate=beta).sample()\n\n    return torch.cat((rts, choices), dim=1)\n\n\n# Define independent priors for each dimension.\nprior = MultipleIndependent(\n    [\n        Gamma(torch.tensor([1.0]), torch.tensor([0.5])),\n        Beta(torch.tensor([2.0]), torch.tensor([2.0])),\n    ],\n    validate_args=False,\n)\n</code></pre>"},{"location":"tutorials/15_mcmc_diagnostics_with_arviz/#train-mnle-to-approximate-the-likelihood","title":"Train MNLE to approximate the likelihood","text":"<p>For this tutorial, we will use a simple simulator with two parameters. For details see the example on the decision making model.</p> <p>Here, we pass <code>mcmc_method=\"nuts\"</code> in order to use the underlying <code>pyro</code> No-U-turn sampler, but it would work as well with other samplers (e.g. \u201cslice_np_vectorized\u201d, \u201chmc\u201d).</p> <p>Additionally, when calling <code>posterior.sample(...)</code> we pass <code>return_arviz=True</code> so that the <code>Arviz InferenceData</code> object is returned. This object gives us access to the wealth of MCMC diagnostics tool provided by <code>arviz</code>.</p> <pre><code># Generate training data and train MNLE.\nnum_simulations = 10000\ntheta = prior.sample((num_simulations,))\nx = mixed_simulator(theta)\n\ntrainer = MNLE(prior)\nlikelihood_estimator = trainer.append_simulations(theta, x).train()\n</code></pre> <pre><code>/Users/janteusen/qode/sbi/sbi/neural_nets/mnle.py:60: UserWarning: The mixed neural likelihood estimator assumes that x contains\n        continuous data in the first n-1 columns (e.g., reaction times) and\n        categorical data in the last column (e.g., corresponding choices). If\n        this is not the case for the passed `x` do not use this function.\n  warnings.warn(\n\n\n Neural network successfully converged after 65 epochs.\n</code></pre>"},{"location":"tutorials/15_mcmc_diagnostics_with_arviz/#run-pyro-nuts-mcmc-and-obtain-arviz-inferencedata-object","title":"Run Pyro NUTS MCMC and obtain <code>arviz InferenceData</code> object","text":"<pre><code># Simulate \"observed\" data x_o\ntorch.manual_seed(42)\nnum_trials = 100\ntheta_o = prior.sample((1,))\nx_o = mixed_simulator(theta_o.repeat(num_trials, 1))\n\n# Set MCMC parameters and run Pyro NUTS.\nmcmc_parameters = dict(\n    num_chains=4,\n    thin=5,\n    warmup_steps=50,\n    init_strategy=\"proposal\",\n    method=\"nuts\",\n)\nnum_samples = 1000\n\n# get the potential function and parameter transform for constructing the posterior\npotential_fn, parameter_transform = likelihood_estimator_based_potential(\n    likelihood_estimator, prior, x_o\n)\nmnle_posterior = MCMCPosterior(\n    potential_fn, proposal=prior, theta_transform=parameter_transform, **mcmc_parameters\n)\n\nmnle_samples = mnle_posterior.sample(\n    (num_samples,), x=x_o, show_progress_bars=False\n)\n# get arviz InferenceData object from posterior\ninference_data = mnle_posterior.get_arviz_inference_data()\n</code></pre> <pre><code>/Users/janteusen/qode/sbi/sbi/utils/sbiutils.py:341: UserWarning: An x with a batch size of 100 was passed. It will be interpreted as a batch of independent and identically\n            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n            same underlying (unknown) parameter. The resulting posterior will be with\n            respect to entire batch, i.e,. p(theta | X).\n  warnings.warn(\n/Users/janteusen/miniconda3/envs/sbi/lib/python3.8/site-packages/torch/__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n  _C._set_default_tensor_type(t)\n/Users/janteusen/miniconda3/envs/sbi/lib/python3.8/site-packages/torch/__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n  _C._set_default_tensor_type(t)\n/Users/janteusen/miniconda3/envs/sbi/lib/python3.8/site-packages/torch/__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n  _C._set_default_tensor_type(t)\n/Users/janteusen/miniconda3/envs/sbi/lib/python3.8/site-packages/torch/__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n  _C._set_default_tensor_type(t)\n</code></pre>"},{"location":"tutorials/15_mcmc_diagnostics_with_arviz/#generate-arviz-plots","title":"Generate <code>arviz</code> plots","text":"<p>The resulting <code>InferenceData</code> object can be passed to most <code>arviz</code> plotting functions, and there are plenty see here for an overview.</p> <p>To get a better understanding of the <code>InferenceData</code> object see here.</p> <p>Below and overview of common MCMC diagnostics plot, see the corresponding <code>arviz</code> documentation for interpretation of the plots.</p> <p>We will a full use-case using the SBI-MCMC-arviz workflow soon.</p> <pre><code>print(inference_data.posterior)\n</code></pre> <pre><code>&lt;xarray.Dataset&gt;\nDimensions:      (chain: 4, draw: 1254, theta_dim_0: 2)\nCoordinates:\n  * chain        (chain) int64 0 1 2 3\n  * draw         (draw) int64 0 1 2 3 4 5 6 ... 1248 1249 1250 1251 1252 1253\n  * theta_dim_0  (theta_dim_0) int64 0 1\nData variables:\n    theta        (chain, draw, theta_dim_0) float32 2.195 0.8838 ... 2.593 0.806\nAttributes:\n    created_at:     2024-03-06T13:33:00.426835\n    arviz_version:  0.15.1\n</code></pre>"},{"location":"tutorials/15_mcmc_diagnostics_with_arviz/#diagnostic-plots","title":"Diagnostic plots","text":"<pre><code>az.style.use(\"arviz-darkgrid\")\naz.plot_rank(inference_data)\n</code></pre> <pre><code>array([&lt;Axes: title={'center': 'theta\\n0'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;,\n       &lt;Axes: title={'center': 'theta\\n1'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;],\n      dtype=object)\n</code></pre> <pre><code>az.plot_autocorr(inference_data);\n</code></pre> <pre><code>az.plot_trace(inference_data, compact=False);\n</code></pre> <pre><code>az.plot_ess(inference_data, kind=\"evolution\");\n</code></pre>"},{"location":"tutorials/15_mcmc_diagnostics_with_arviz/#posterior-density-plots","title":"Posterior density plots","text":"<pre><code>az.plot_posterior(inference_data)\n</code></pre> <pre><code>array([&lt;Axes: title={'center': 'theta\\n0'}&gt;,\n       &lt;Axes: title={'center': 'theta\\n1'}&gt;], dtype=object)\n</code></pre> <pre><code>print(\n    f\"\"\"Given the {num_trials} we observed, the posterior is centered around\n    true underlying parameters theta_o: {theta_o}\"\"\"\n)\n</code></pre> <pre><code>Given the 100 we observed, the posterior is centered around\n    true underlying parameters theta_o: tensor([[1.9622, 0.7550]])\n</code></pre> <pre><code>az.plot_pair(inference_data)\n</code></pre> <pre><code>&lt;Axes: xlabel='theta\\n0', ylabel='theta\\n1'&gt;\n</code></pre> <pre><code>az.plot_pair(\n    inference_data,\n    var_names=[\"theta\"],\n    kind=\"hexbin\",\n    marginals=True,\n    figsize=(10, 10),\n)\n</code></pre> <pre><code>array([[&lt;Axes: &gt;, None],\n       [&lt;Axes: xlabel='theta\\n0', ylabel='theta\\n1'&gt;, &lt;Axes: &gt;]],\n      dtype=object)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/16_implemented_methods/","title":"API of implemented methods","text":"<p>This notebook spells out the API for all algorithms implemented in the <code>sbi</code> toolbox:</p> <ul> <li> <p>Posterior estimation (SNPE)</p> </li> <li> <p>Likelihood estimation (SNLE)</p> </li> <li> <p>Likelihood-ratio estimation (SNRE)</p> </li> <li> <p>Utilities</p> </li> </ul>"},{"location":"tutorials/16_implemented_methods/#posterior-estimation-snpe","title":"Posterior estimation (SNPE)","text":"<p>Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios &amp; Murray (NeurIPS 2016) [PDF] [BibTeX]</p> <pre><code># Example setup\nimport torch\n\nfrom sbi.utils import BoxUniform\n\n# Define the prior\nnum_dims = 2\nnum_sims = 1000\nnum_rounds = 2\nprior = BoxUniform(low=torch.zeros(num_dims), high=torch.ones(num_dims))\nsimulator = lambda theta: theta + torch.randn_like(theta) * 0.1\nx_o = torch.tensor([0.5, 0.5])\n</code></pre> <pre><code>from sbi.inference import SNPE_A\n\ninference = SNPE_A(prior)\nproposal = prior\nfor _ in range(num_rounds):\n    theta = proposal.sample((num_sims,))\n    x = simulator(theta)\n    _ = inference.append_simulations(theta, x, proposal=proposal).train()\n    posterior = inference.build_posterior().set_default_x(x_o)\n    proposal = posterior\n</code></pre> <p>Automatic posterior transformation for likelihood-free inferenceby Greenberg, Nonnenmacher &amp; Macke (ICML 2019) [PDF]</p> <pre><code>from sbi.inference import SNPE\n\ninference = SNPE(prior)\nproposal = prior\nfor _ in range(num_rounds):\n    theta = proposal.sample((num_sims,))\n    x = simulator(theta)\n    _ = inference.append_simulations(theta, x, proposal=proposal).train()\n    posterior = inference.build_posterior().set_default_x(x_o)\n    proposal = posterior\n</code></pre> <p>BayesFlow: Learning complex stochastic models with invertible neural networks by Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., &amp; K\u00f6the, U. (2020) (IEEE transactions on neural networks and learning systems 2020) Paper</p> <p>The density estimation part of BayesFlow is equivalent to single-round NPE. The additional contribution of the paper are several embedding networks for high-dimensional data including permutation invariant embeddings. Similar embeddings networks are implemented in <code>sbi</code> as well, under <code>sbi.neural_nets.embedding_nets.py</code>.</p> <pre><code># Posterior estimation with BayesFlow is equivalent to single-round SNPE.\nfrom sbi.inference import SNPE\n\ninference = SNPE(prior)\ntheta = prior.sample((num_sims,))\nx = simulator(theta)\ninference.append_simulations(theta, x).train()\nposterior = inference.build_posterior()\nsamples = posterior.sample((1000,), x=x_o)\n</code></pre> <p>Truncated proposals for scalable and hassle-free simulation-based inference  by Deistler, Goncalves &amp; Macke (NeurIPS 2022) [Paper]</p> <pre><code>from sbi.inference import SNPE\nfrom sbi.utils import RestrictedPrior, get_density_thresholder\n\ninference = SNPE(prior)\nproposal = prior\nfor _ in range(num_rounds):\n    theta = proposal.sample((num_sims,))\n    x = simulator(theta)\n    _ = inference.append_simulations(theta, x).train(force_first_round_loss=True)\n    posterior = inference.build_posterior().set_default_x(x_o)\n\n    accept_reject_fn = get_density_thresholder(posterior, quantile=1e-4)\n    proposal = RestrictedPrior(prior, accept_reject_fn, sample_with=\"rejection\")\n</code></pre>"},{"location":"tutorials/16_implemented_methods/#likelihood-estimation-snle","title":"Likelihood estimation (SNLE)","text":"<p>Sequential neural likelihood: Fast likelihood-free inference with autoregressive flowsby Papamakarios, Sterratt &amp; Murray (AISTATS 2019) [PDF] [BibTeX]</p> <pre><code>from sbi.inference import SNLE\n\ninference = SNLE(prior)\nproposal = prior\nfor _ in range(num_rounds):\n    theta = proposal.sample((num_sims,))\n    x = simulator(theta)\n    _ = inference.append_simulations(theta, x).train()\n    posterior = inference.build_posterior(mcmc_method=\"slice_np_vectorized\",\n                                          mcmc_parameters={\"num_chains\": 20,\n                                                           \"thin\": 5})\n    proposal = posterior.set_default_x(x_o)\n</code></pre> <p>Variational methods for simulation-based inference  by Gl\u00f6ckler, Deistler, Macke (ICLR 2022) [Paper]</p> <pre><code>from sbi.inference import SNLE\n\ninference = SNLE(prior)\nproposal = prior\nfor _ in range(num_rounds):\n    theta = proposal.sample((num_sims,))\n    x = simulator(theta)\n    _ = inference.append_simulations(theta, x).train()\n    posterior = inference.build_posterior(sample_with=\"vi\",\n                                          vi_method=\"fKL\").set_default_x(x_o)\n    proposal = posterior.train()  # Train VI posterior on given x_o.\n</code></pre> <p>Flexible and efficient simulation-based inference for models of decision-making  by Boelts, Lueckmann, Gao, Macke (Elife 2022) [Paper]</p> <pre><code>from sbi.inference import MNLE\n\ninference = MNLE(prior)\ntheta = prior.sample((num_sims,))\nx = simulator(theta)\n_ = inference.append_simulations(theta, x).train()\nposterior = inference.build_posterior().set_default_x(x_o)\n</code></pre>"},{"location":"tutorials/16_implemented_methods/#likelihood-ratio-estimation-snre","title":"Likelihood-ratio estimation (SNRE)","text":"<p>Likelihood-free MCMC with Amortized Approximate Likelihood Ratiosby Hermans, Begy &amp; Louppe (ICML 2020) [PDF]</p> <pre><code>from sbi.inference import SNRE_A\n\ninference = SNRE_A(prior)\ntheta = prior.sample((num_sims,))\nx = simulator(theta)\n_ = inference.append_simulations(theta, x).train()\nposterior = inference.build_posterior().set_default_x(x_o)\n</code></pre> <p>On Contrastive Learning for Likelihood-free InferenceDurkan, Murray &amp; Papamakarios (ICML 2020) [PDF].</p> <pre><code>from sbi.inference import SNRE\n\ninference = SNRE(prior)\nproposal = prior\nfor _ in range(num_rounds):\n    theta = proposal.sample((num_sims,))\n    x = simulator(theta)\n    _ = inference.append_simulations(theta, x).train()\n    posterior = inference.build_posterior(mcmc_method=\"slice_np_vectorized\",\n                                          mcmc_parameters={\"num_chains\": 20,\n                                                           \"thin\": 5})\n    proposal = posterior.set_default_x(x_o)\n</code></pre> <p>Towards Reliable Simulation-Based Inference with Balanced Neural Ratio Estimationby Delaunoy, Hermans, Rozet, Wehenkel &amp; Louppe (NeurIPS 2022) [PDF]</p> <pre><code>from sbi.inference import BNRE\n\ninference = BNRE(prior)\ntheta = prior.sample((num_sims,))\nx = simulator(theta)\n_ = inference.append_simulations(theta, x).train(regularization_strength=100.)\nposterior = inference.build_posterior().set_default_x(x_o)\n</code></pre> <p>Contrastive Neural Ratio EstimationBenjamin Kurt Miller, Christoph Weniger, Patrick Forr\u00e9 (NeurIPS 2022) [PDF]</p> <pre><code># The main feature of NRE-C is producing an exact ratio of densities at optimum,\n# even when using multiple contrastive pairs (classes).\n\nfrom sbi.inference import SNRE_C\n\ninference = SNRE_C(prior)\nproposal = prior\ntheta = proposal.sample((num_sims,))\nx = simulator(theta)\n_ = inference.append_simulations(theta, x).train(\n    num_classes=5,  # sees `2 * num_classes - 1` marginally drawn contrastive pairs.\n    gamma=1.0,  # controls the weight between terms in its loss function.\n)\nposterior = inference.build_posterior().set_default_x(x_o)\n</code></pre>"},{"location":"tutorials/16_implemented_methods/#flow-matching-posterior-estimation","title":"Flow Matching Posterior Estimation","text":"<p>Flow Matching for Scalable Simulation-Based Inference  by Dax, Wildberger, Buchholz, Green, Macke, Sch\u00f6lkopf (NeurIPS 2023)  [Paper]</p> <pre><code>from sbi.inference import FMPE\n\ninference = FMPE(prior)\n# FMPE does support multiple rounds of inference\ntheta = prior.sample((num_sims,))\nx = simulator(theta)\ninference.append_simulations(theta, x).train()\nposterior = inference.build_posterior().set_default_x(x_o)\n</code></pre>"},{"location":"tutorials/16_implemented_methods/#diagnostics-and-utilities","title":"Diagnostics and utilities","text":"<p>Simulation-based calibrationby Talts, Betancourt, Simpson, Vehtari, Gelman (arxiv 2018) [Paper])</p> <pre><code>from sbi.diagnostics import run_sbc\nfrom sbi.analysis import sbc_rank_plot\n\nthetas = prior.sample((1000,))\nxs = simulator(thetas)\n\n# SBC is fast for fully amortized NPE.\ninference = SNPE(prior)\ntheta = prior.sample((num_sims,))\nx = simulator(theta)\ninference.append_simulations(theta, x).train()\nposterior = inference.build_posterior()\n\nranks, dap_samples = run_sbc(\n    thetas, xs, posterior, num_posterior_samples=1_000\n)\n\nfig, axes = sbc_rank_plot(\n    ranks=ranks,\n    num_posterior_samples=1000,\n    plot_type=\"hist\",\n    num_bins=None,\n)\n</code></pre> <p>Expected coverage (sample-based)as computed in Deistler, Goncalves, Macke (Neurips 2022) [Paper] and in Rozet, Louppe (2021) [Paper]</p> <pre><code>thetas = prior.sample((1_000,))\nxs = simulator(thetas)\n\nranks, dap_samples = run_sbc(\n    thetas,\n    xs,\n    posterior,\n    num_posterior_samples=1_000,\n    reduce_fns=posterior.log_prob  # Difference to SBC.\n)\n\n# NOTE: Here we obtain a single rank plot because ranks are calculated\n# for the entire posterior and not for each marginal like in SBC.\nfig, axes = sbc_rank_plot(\n    ranks=ranks,\n    num_posterior_samples=1000,\n    plot_type=\"hist\",\n    num_bins=None,\n)\n</code></pre> <p>TARP: Sampling-Based Accuracy Testing of Posterior Estimators for General Inference</p> <p>Lemos, Coogan, Hezaveh &amp; Perreault-Levasseur (ICML 2023)[Paper]</p> <pre><code>from sbi.diagnostics.tarp import run_tarp, plot_tarp\n\nthetas = prior.sample((1_000,))\nxs = simulator(thetas)\n\nexpected_coverage, ideal_coverage = run_tarp(\n    thetas,\n    xs,\n    posterior,\n    references=None,  # optional, defaults to uniform samples across parameter space.\n    num_posterior_samples=1_000,\n)\n\nfix, axes = plot_tarp(expected_coverage, ideal_coverage)\n</code></pre> <p>Restriction estimatorby Deistler, Macke &amp; Goncalves (PNAS 2022) [Paper]</p> <pre><code>from sbi.inference import SNPE\nfrom sbi.utils import RestrictionEstimator\n\nrestriction_estimator = RestrictionEstimator(prior=prior)\nproposal = prior\n\nfor _ in range(num_rounds):\n    theta = proposal.sample((num_sims,))\n    x = simulator(theta)\n    restriction_estimator.append_simulations(theta, x)\n    classifier = restriction_estimator.train()\n    proposal = restriction_estimator.restrict_prior()\n\nall_theta, all_x, _ = restriction_estimator.get_simulations()\n\ninference = SNPE(prior)\ndensity_estimator = inference.append_simulations(all_theta, all_x).train()\nposterior = inference.build_posterior()\n</code></pre>"},{"location":"tutorials/17_importance_sampled_posteriors/","title":"17 importance sampled posteriors","text":"<pre><code>### TLDR:\n</code></pre>"},{"location":"tutorials/17_importance_sampled_posteriors/#theory","title":"Theory","text":"<p>SBI estimates the posterior \\(p(\\theta|x) = {p(\\theta)p(x|\\theta)}/{p(x)}\\) based on samples from the prior \\(\\theta\\sim p(\\theta)\\) and the likelihood \\(x\\sim p(x|\\theta)\\). Sometimes, we can do both, sample and evaluate the prior and likelihood. In this case, we can combine the simulation-based estimate \\(q(\\theta|x)\\) with likelihood-based importance sampling, and thereby generate an asymptotically exact estimate for \\(p(\\theta|x)\\).</p>"},{"location":"tutorials/17_importance_sampled_posteriors/#importance-weights","title":"Importance weights","text":"<p>The main idea is to interpret \\(q(\\theta|x)\\) as a proposal distribution and generate proposal samples \\(\\theta_i\\sim q(\\theta|x)\\), and then augment each sample with an importance weight \\(w_i = p(\\theta_i|x) / q(\\theta_i|x)\\). The definition of the importance weights is motivated from Monte Carlo estimates for the random variable \\(f(\\theta)\\), </p> \\[  \\mathbb{E}_{\\theta\\sim p(\\theta|x)}\\left[f(\\theta)\\right]  =\\int p(\\theta|x) f(\\theta)\\,\\text{d}\\theta \\approx \\sum_{\\theta_i\\sim p(\\theta_i|x)} f(\\theta_i). \\] <p>We can rewrite this expression as </p> \\[  \\mathbb{E}_{\\theta\\sim p(\\theta|x)}\\left[f(\\theta)\\right]  =\\int p(\\theta|x) f(\\theta)\\,\\text{d}\\theta =\\int q(\\theta|x) \\frac{p(\\theta|x)}{q(\\theta|x)}f(\\theta)\\,\\text{d}\\theta \\approx \\sum_{\\theta_i\\sim q(\\theta_i|x)} \\frac{p(\\theta_i|x)}{q(\\theta_i|x)}f(\\theta_i) \\approx \\sum_{\\theta_i\\sim q(\\theta_i|x)} w_i\\cdot f(\\theta_i). \\] <p>Instead of sampling \\(\\theta_i\\sim p(\\theta_i|x)\\), we can thus sample \\(\\theta_i\\sim q(\\theta_i|x)\\) and attach a corresponding importance weight \\(w_i\\) to each sample. Intuitively, the importance weights downweight samples where \\(q(\\theta|x)\\) overestimates \\(p(\\theta|x)\\) and upweight samples where \\(p(\\theta|x)\\) underestimates \\(q(\\theta|x)\\).</p>"},{"location":"tutorials/17_importance_sampled_posteriors/#effective-sample-size-n_texteff-and-sample-efficiency-epsilon","title":"Effective sample size \\(n_\\text{eff}\\) and sample efficiency \\(\\epsilon\\)","text":"<p>If inference were perfect, we would have \\(w_i = p(\\theta_i|x) / q(\\theta_i|x) = 1~\\forall i\\). In practice however, SBI does not provide exact inference results \\(q(\\theta_i|x)\\), and the weights will have a finite variance \\(\\text{Var}(w) &gt; 0\\). Performing the Monte Carlo estimate above with \\(n\\) samples from \\(q(\\theta|x)\\) thus results in reduced precision compared to doing the same with \\(n\\) samples from \\(p(\\theta|x)\\). This is formalized by the notion of the effective sample size (see e.g. here)</p> \\[ n_\\text{eff} = \\frac{n}{1 + \\text{Var}(w)} = \\frac{\\left(\\sum_i w_i\\right)^2}{\\sum_i \\left(w_i^2\\right)}. \\] <p>Loosely speaking, using \\(n\\) samples \\(\\theta_i\\sim q(\\theta_i|x)\\) is equivalent to using \\(n_\\text{eff}\\) samples from the true posterior \\(p(\\theta|x)\\). The sample efficiency</p> \\[ \\epsilon = \\frac{n_\\text{eff}}{n} \\in (0, 1] \\] <p>is an indirect measure of the quality of the proposal \\(q(\\theta|x)\\).</p>"},{"location":"tutorials/17_importance_sampled_posteriors/#mass-coverage","title":"Mass coverage","text":"<p>Importance sampling requires \\(p(\\theta|x) \\subseteq q(\\theta|x)\\). When using NPE, this should naturally be ensured, as NPE is trained with the mass-covering forward KL divergence, such that \\(p(\\theta|x) \\not\\subseteq q(\\theta|x)\\) for in-distribution data would imply a diverging validation loss. </p> <p>When \\(q(\\theta|x)\\) is a light-tailed estimate of \\(p(\\theta|x)\\), the variance of the importance weights is unbounded and we may encounter a small sample efficiency \\(\\epsilon\\).</p>"},{"location":"tutorials/17_importance_sampled_posteriors/#self-normalized-importance-sampling-and-the-bayesian-evidence","title":"Self-normalized importance sampling and the Bayesian evidence","text":"<p>In practice, we don\u2019t have access to the normalized posterior, but only to \\(p(\\theta|x) \\cdot p(x) = p(\\theta)p(x|\\theta)\\). We thus have to use self-normalized importance sampling. In this case, an unbiased estimate of the Bayesian evidence \\(p(x)\\) can be computed from the normalization of the importance weights (see e.g. here)</p> \\[ p(x) = \\frac{\\sum_i w_i}{n} \\] <p>with a statistical uncertainty scaling with \\(1/\\sqrt{n}\\),</p> \\[ \\sigma_{p(x)} = p(x)\\cdot \\sqrt{\\frac{1-\\epsilon}{n\\cdot \\epsilon}}. \\]"},{"location":"tutorials/17_importance_sampled_posteriors/#implementation","title":"Implementation","text":"<pre><code>from torch import ones, eye\nimport torch\nfrom torch.distributions import MultivariateNormal\nimport matplotlib.pyplot as plt\n\nfrom sbi.inference import SNPE, ImportanceSamplingPosterior\nfrom sbi.utils import BoxUniform\nfrom sbi.inference.potentials.base_potential import BasePotential\nfrom sbi.analysis import pairplot, marginal_plot\n</code></pre> <p>We first define a simulator and a prior which both have functions for sampling (as required for SBI) and log_prob evaluations (as required for importance sampling).</p> <p>Next we train an NPE model for inference.</p> <p>Now we perfrom inference with the model.</p> <pre><code># define prior and simulator\nclass Simulator:\n    def __init__(self):\n        pass\n\n    def log_likelihood(self, theta, x):\n        return MultivariateNormal(theta, eye(2)).log_prob(x)\n\n    def sample(self, theta):\n        return theta + torch.randn((theta.shape))\n\nprior = BoxUniform(-5 * ones((2,)), 5 * ones((2,)))\nsim = Simulator()\nlog_prob_fn = lambda theta, x_o: sim.log_likelihood(theta, x_o) + prior.log_prob(theta)\n\n# generate train data\n_ = torch.manual_seed(3)\ntheta = prior.sample((10,))\nx = sim.sample(theta)\n\n# train NPE model\n_ = torch.manual_seed(4)\ninference = SNPE(prior=prior)\n_ = inference.append_simulations(theta, x).train()\nposterior = inference.build_posterior()\n\n# generate a synthetic observation\n_ = torch.manual_seed(2)\ntheta_gt = prior.sample((1,))\nobservation = sim.sample(theta_gt)[0]\nposterior = posterior.set_default_x(observation)\nprint(\"observations.shape\", observation.shape)\n\n# sample from posterior\ntheta_inferred = posterior.sample((10_000,))\n</code></pre> <pre><code> Neural network successfully converged after 70 epochs.observations.shape torch.Size([2])\n\n\n\nDrawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n</code></pre>"},{"location":"tutorials/17_importance_sampled_posteriors/#manual-importance-sampling","title":"Manual importance sampling","text":"<p>For the inferred samples, we can evaluate the proposal density (i.e., the density under our inference model) and the ground truth density defined by the prior and the simulator likelihood (unnormalized).</p> <pre><code>log_probs_inferred = posterior.log_prob(theta_inferred)  # log probs of proposal\nlog_probs_gt = log_prob_fn(theta_inferred, observation)  # gt log probs (unnormalized)\n\nplt.plot(log_probs_inferred, log_probs_gt, '.')\nplt.xlabel(\"proposal log prob\")\nplt.ylabel(\"ground truth log prob (unnormalized)\")\nplt.show()\n</code></pre> <p></p> <p>Based on these densities, we can now compute the importance weights.</p> <pre><code>w = torch.exp(log_probs_gt - log_probs_inferred)  # importance weights\nw = w / torch.mean(w)  # self-normalized importance sampling: normalize weights to mean 1\nESS = torch.sum(w)**2 / torch.sum(w**2)\nsample_efficiency = ESS / len(w)\n\nprint(f\"Effective sample size: {ESS:.0f}\")\nprint(f\"Sample efficiency: {100 * sample_efficiency:.1f}%\")\n\nplt.hist(w, density=True, bins=50)\nplt.show()\n</code></pre> <pre><code>Effective sample size: 1466\nSample efficiency: 14.7%\n</code></pre> <p></p> <p>With these importance weights, we can correct the inferred samples.</p> <pre><code># get weighted samples\ntheta_inferred_is = theta_inferred[torch.where(w &gt; torch.rand(len(w)) * torch.max(w))]\n# *Note*: we here perform rejection sampling, as the plotting function\n# used below does not support weighted samples. In general, with rejection\n# sampling the number of samples will be smaller than the effective sample\n# size unless we allow for duplicate samples.\n\n# gt samples\ngt_samples = MultivariateNormal(observation, eye(2)).sample((len(theta_inferred) * 5,))\ngt_samples = gt_samples[prior.support.check(gt_samples)][:len(theta_inferred)]\n\n# plot\nfig, ax = marginal_plot(\n    [theta_inferred, theta_inferred_is, gt_samples],\n    limits=[[-5, 5], [-5, 5]],\n    figsize=(5, 1.5),\n    diag=\"kde\",  # smooth histogram\n)\nax[0][1].legend([\"NPE\", \"NPE-IS\", \"Groud Truth\"], loc=\"upper right\", bbox_to_anchor=[1.8, 1.0, 0.0, 0.0])\nplt.show()\n</code></pre> <p></p> <p>Indeed, the importance-sampled posterior matches the ground truth well, despite significant deviations of the initial NPE estimate.</p>"},{"location":"tutorials/17_importance_sampled_posteriors/#importance-sampling-with-the-sbi-toolbox","title":"Importance sampling with the SBI toolbox","text":"<p>With the SBI toolbox, importance sampling is a one-liner. SBI supports two methods for importance sampling: - <code>\"importance\"</code>: returns <code>n_samples</code> weighted samples (as above) corresponding to <code>n_samples * sample_efficiency</code> samples from the posterior. This results in unbiased samples, but the number of effective samples may be small when the SBI estimate is inaccurate. - <code>\"sir\"</code> (sampling-importance-resampling): performs rejection sampling on a batched basis with batch size <code>oversampling_factor</code>.  This is a guaranteed way to obtain <code>N / oversampling_factor</code> samples, but these may be biased as the weight normalization is not performed across the entire set of samples.</p> <pre><code>posterior_sir = ImportanceSamplingPosterior(\n    potential_fn=log_prob_fn,\n    proposal=posterior.set_default_x(observation),\n    method=\"sir\",\n)\ntheta_inferred_sir_2 = posterior_sir.sample((len(theta_inferred),), x=observation[None, :], oversampling_factor=1)\ntheta_inferred_sir_32 = posterior_sir.sample((len(theta_inferred),), x=observation[None, :], oversampling_factor=32)\n</code></pre> <pre><code>Posterior: oversampling factor 1\nNum candidate samples: 1\n\n\n\nDrawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n\n\nbatch_size 10000\nweights tensor([[1.],\n        [1.],\n        [1.],\n        ...,\n        [1.],\n        [1.],\n        [1.]]) torch.Size([10000, 1])\nuniform_decision tensor([[0.2584],\n        [0.8255],\n        [0.5367],\n        ...,\n        [0.1285],\n        [0.6491],\n        [0.4784]]) torch.Size([10000, 1])\nmask tensor([[True],\n        [True],\n        [True],\n        ...,\n        [True],\n        [True],\n        [True]]) torch.Size([10000, 1])\nthetas tensor([[-0.6660, -0.0286],\n        [ 2.9079,  2.3848],\n        [ 4.9626,  3.5430],\n        ...,\n        [-0.5071,  1.7816],\n        [ 2.1563,  1.5912],\n        [ 0.3401,  0.3996]]) torch.Size([10000, 2])\nsamples tensor([[-0.6660, -0.0286],\n        [ 2.9079,  2.3848],\n        [ 4.9626,  3.5430],\n        ...,\n        [-0.5071,  1.7816],\n        [ 2.1563,  1.5912],\n        [ 0.3401,  0.3996]]) torch.Size([10000, 2])\nPosterior: oversampling factor 32\nNum candidate samples: 32\n\n\n\nDrawing 10000 posterior samples:   0%|          | 0/10000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[3.3657e-03, 9.2805e-02, 9.6103e-02,  ..., 9.7422e-01, 9.9998e-01,\n         1.0000e+00],\n        [1.8362e-02, 4.8308e-02, 6.8778e-02,  ..., 9.9464e-01, 9.9464e-01,\n         1.0000e+00],\n        [2.9750e-02, 2.8256e-01, 2.8398e-01,  ..., 9.8681e-01, 9.8682e-01,\n         1.0000e+00],\n        ...,\n        [1.3679e-04, 1.3680e-04, 8.5970e-03,  ..., 9.9854e-01, 9.9863e-01,\n         1.0000e+00],\n        [5.8125e-05, 4.3372e-02, 9.2864e-02,  ..., 9.1176e-01, 9.9218e-01,\n         1.0000e+00],\n        [9.8639e-02, 1.1216e-01, 1.1227e-01,  ..., 9.9939e-01, 9.9955e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.5609],\n        [0.1802],\n        [0.7231],\n        [0.2811],\n        [0.9153],\n        [0.8000],\n        [0.3323],\n        [0.8574],\n        [0.7233],\n        [0.7077],\n        [0.3043],\n        [0.7238],\n        [0.0384],\n        [0.5480],\n        [0.8734],\n        [0.0630],\n        [0.8672],\n        [0.8422],\n        [0.5536],\n        [0.0682],\n        [0.2335],\n        [0.6293],\n        [0.9219],\n        [0.2054],\n        [0.8724],\n        [0.4763],\n        [0.2748],\n        [0.8620],\n        [0.1215],\n        [0.6521],\n        [0.4422],\n        [0.1607],\n        [0.6894],\n        [0.3839],\n        [0.2287],\n        [0.4263],\n        [0.4987],\n        [0.5518],\n        [0.5254],\n        [0.2184],\n        [0.0103],\n        [0.6425],\n        [0.7706],\n        [0.4420],\n        [0.5052],\n        [0.9328],\n        [0.0958],\n        [0.6404],\n        [0.9120],\n        [0.6256],\n        [0.1449],\n        [0.1109],\n        [0.9517],\n        [0.7369],\n        [0.7208],\n        [0.0287],\n        [0.5828],\n        [0.4510],\n        [0.1043],\n        [0.1766],\n        [0.3053],\n        [0.8417],\n        [0.9702],\n        [0.0579],\n        [0.0975],\n        [0.6361],\n        [0.0086],\n        [0.8690],\n        [0.7788],\n        [0.5924],\n        [0.4042],\n        [0.5683],\n        [0.9278],\n        [0.9242],\n        [0.0113],\n        [0.9374],\n        [0.5024],\n        [0.4333],\n        [0.6370],\n        [0.5163],\n        [0.4462],\n        [0.6453],\n        [0.5009],\n        [0.4439],\n        [0.9271],\n        [0.2878],\n        [0.9775],\n        [0.2036],\n        [0.0303],\n        [0.4066],\n        [0.8465],\n        [0.5130],\n        [0.8401],\n        [0.7749],\n        [0.4766],\n        [0.3304],\n        [0.4431],\n        [0.7184],\n        [0.1640],\n        [0.4833],\n        [0.6065],\n        [0.1900],\n        [0.1621],\n        [0.7186],\n        [0.5576],\n        [0.7064],\n        [0.9667],\n        [0.8879],\n        [0.5902],\n        [0.1974],\n        [0.0630],\n        [0.9251],\n        [0.1121],\n        [0.4407],\n        [0.4111],\n        [0.2963],\n        [0.7778],\n        [0.0479],\n        [0.2150],\n        [0.1796],\n        [0.1219],\n        [0.0688],\n        [0.0302],\n        [0.5158],\n        [0.3120],\n        [0.9318],\n        [0.9023],\n        [0.8749],\n        [0.4669],\n        [0.7286],\n        [0.9173],\n        [0.6609],\n        [0.2603],\n        [0.0082],\n        [0.3112],\n        [0.1892],\n        [0.0540],\n        [0.4101],\n        [0.3867],\n        [0.4954],\n        [0.4728],\n        [0.8358],\n        [0.2954],\n        [0.0694],\n        [0.6179],\n        [0.0370],\n        [0.1232],\n        [0.7175],\n        [0.8207],\n        [0.6838],\n        [0.2718],\n        [0.0692],\n        [0.5984],\n        [0.1902],\n        [0.9762],\n        [0.5874],\n        [0.4535],\n        [0.1849],\n        [0.4605],\n        [0.4567],\n        [0.8305],\n        [0.0683],\n        [0.1479],\n        [0.0413],\n        [0.4149],\n        [0.3259],\n        [0.7442],\n        [0.6243],\n        [0.6045],\n        [0.9940],\n        [0.1933],\n        [0.3464],\n        [0.2005],\n        [0.4178],\n        [0.6302],\n        [0.9007],\n        [0.5954],\n        [0.6486],\n        [0.9823],\n        [0.8163],\n        [0.6731],\n        [0.3539],\n        [0.4640],\n        [0.6019],\n        [0.8181],\n        [0.6761],\n        [0.7376],\n        [0.3226],\n        [0.5677],\n        [0.1445],\n        [0.0055],\n        [0.4362],\n        [0.2881],\n        [0.3128],\n        [0.2238],\n        [0.2772],\n        [0.8488],\n        [0.1720],\n        [0.0930],\n        [0.0302],\n        [0.6756],\n        [0.3056],\n        [0.0593],\n        [0.8843],\n        [0.2660],\n        [0.1958],\n        [0.8084],\n        [0.3739],\n        [0.7914],\n        [0.6191],\n        [0.5149],\n        [0.3978],\n        [0.7901],\n        [0.5726],\n        [0.0672],\n        [0.2448],\n        [0.8224],\n        [0.3410],\n        [0.0982],\n        [0.9255],\n        [0.9197],\n        [0.8272],\n        [0.2612],\n        [0.3210],\n        [0.2199],\n        [0.3715],\n        [0.8633],\n        [0.4053],\n        [0.1952],\n        [0.9599],\n        [0.0494],\n        [0.6150],\n        [0.3925],\n        [0.1228],\n        [0.2823],\n        [0.1211],\n        [0.8708],\n        [0.2565],\n        [0.6887],\n        [0.4460],\n        [0.0465],\n        [0.4974],\n        [0.3677],\n        [0.3318],\n        [0.4483],\n        [0.9363],\n        [0.4510],\n        [0.8819],\n        [0.1970],\n        [0.1233],\n        [0.8601],\n        [0.6889],\n        [0.4228],\n        [0.1048],\n        [0.7264],\n        [0.1140],\n        [0.7646],\n        [0.8379],\n        [0.0531],\n        [0.7497],\n        [0.5102],\n        [0.2252],\n        [0.5736],\n        [0.5161],\n        [0.4190],\n        [0.2871],\n        [0.6234],\n        [0.0184],\n        [0.8827],\n        [0.8717],\n        [0.9982],\n        [0.8366],\n        [0.6113],\n        [0.0491],\n        [0.9632],\n        [0.3444],\n        [0.0099],\n        [0.2690],\n        [0.1148],\n        [0.7341],\n        [0.8503],\n        [0.5112],\n        [0.2581],\n        [0.4834],\n        [0.2352],\n        [0.1443],\n        [0.6261],\n        [0.8550],\n        [0.8140],\n        [0.2807],\n        [0.7274],\n        [0.4870],\n        [0.4802],\n        [0.0626],\n        [0.2117],\n        [0.1284],\n        [0.4525],\n        [0.4933],\n        [0.2016],\n        [0.7381],\n        [0.1386],\n        [0.2107],\n        [0.2620],\n        [0.3730],\n        [0.3606],\n        [0.1613],\n        [0.4335],\n        [0.3323],\n        [0.9146],\n        [0.3787],\n        [0.9761],\n        [0.4871]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False,  True, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 0.8433,  1.9718],\n        [ 1.3871, -0.1943],\n        [ 0.9396,  2.2405],\n        ...,\n        [-0.2881,  1.4732],\n        [-0.9959,  0.2876],\n        [-0.2192,  1.6795]]) torch.Size([9984, 2])\nsamples tensor([[ 3.7983e+00, -9.5277e-01],\n        [ 2.4002e+00,  6.5210e-01],\n        [ 3.1402e+00, -4.9353e-01],\n        [ 2.2563e+00,  3.4680e-01],\n        [ 3.1766e+00,  1.3099e-01],\n        [ 2.1083e+00, -7.2484e-01],\n        [ 1.6346e+00, -1.7790e+00],\n        [ 2.3102e+00, -2.3281e-01],\n        [ 3.3878e+00, -6.4433e-01],\n        [ 2.7905e+00,  2.1508e-01],\n        [ 1.4078e+00, -1.0705e+00],\n        [ 3.2774e+00,  5.2925e-01],\n        [ 2.2223e+00, -1.3279e+00],\n        [ 3.3039e+00, -1.4722e+00],\n        [ 1.9496e+00, -4.4928e-01],\n        [ 4.4393e-01, -8.4045e-02],\n        [ 1.5696e+00,  1.0164e+00],\n        [ 2.1504e+00, -4.8981e-01],\n        [ 2.4236e+00, -1.5703e+00],\n        [ 1.4733e+00, -1.4278e+00],\n        [ 1.2428e+00,  6.7372e-01],\n        [ 8.9325e-01, -1.2726e+00],\n        [ 2.1457e+00,  4.0246e-01],\n        [ 3.9577e+00, -3.6130e-01],\n        [ 1.7966e+00, -3.6581e-01],\n        [ 1.9434e+00, -2.8449e+00],\n        [ 2.0145e+00, -3.3267e-01],\n        [ 1.9601e+00, -1.2445e+00],\n        [ 2.0064e+00,  2.4686e+00],\n        [ 2.1769e+00, -6.5537e-01],\n        [ 3.0098e+00, -6.0037e-01],\n        [-5.4974e-01, -2.6071e-01],\n        [ 2.8795e+00,  2.2497e-01],\n        [ 1.6841e+00, -1.2873e+00],\n        [ 2.0572e+00,  1.0230e+00],\n        [ 1.5382e+00, -1.5489e+00],\n        [ 2.0763e+00, -1.2098e+00],\n        [ 3.1763e+00, -1.8304e+00],\n        [ 1.2967e+00, -6.6523e-01],\n        [ 2.5349e+00,  5.2330e-01],\n        [ 3.9458e+00,  1.9862e+00],\n        [ 2.8098e+00, -1.3060e+00],\n        [ 4.3993e-01, -3.0887e-01],\n        [ 1.5327e+00, -4.5640e-01],\n        [ 1.0073e+00, -6.6029e-01],\n        [ 2.3466e+00, -9.3973e-02],\n        [ 3.1582e+00,  8.9177e-02],\n        [ 3.4102e+00, -6.4670e-01],\n        [ 3.8619e+00,  7.0547e-01],\n        [ 1.3932e+00,  6.5893e-01],\n        [ 1.7876e+00, -1.6402e+00],\n        [ 2.8164e+00, -1.4278e+00],\n        [ 1.9493e+00,  1.3156e+00],\n        [ 3.2439e+00, -8.0610e-01],\n        [ 4.0511e+00,  6.6065e-01],\n        [ 1.9407e+00,  7.0360e-01],\n        [ 3.2989e+00, -1.3813e+00],\n        [ 2.0591e+00, -1.2774e+00],\n        [ 7.8391e-01, -9.4996e-01],\n        [ 3.1565e+00,  4.8285e-01],\n        [ 3.5133e+00, -3.4145e-01],\n        [ 2.0846e+00,  3.8069e-02],\n        [ 2.1852e+00,  1.3435e+00],\n        [ 3.3503e-02, -1.6286e+00],\n        [ 1.6089e+00, -1.2668e+00],\n        [ 2.4408e+00, -1.0148e+00],\n        [ 3.1669e-01, -4.1545e-01],\n        [ 4.0254e+00, -8.7693e-01],\n        [ 3.8696e+00,  1.2203e+00],\n        [ 4.3109e+00, -8.6762e-01],\n        [ 3.6158e+00,  9.4057e-01],\n        [ 2.8432e+00, -9.4343e-02],\n        [ 2.2427e+00, -5.7202e-01],\n        [ 2.0642e+00,  1.3923e+00],\n        [ 1.8554e+00,  1.8467e+00],\n        [ 2.9234e+00,  2.4788e+00],\n        [ 4.3535e+00, -1.9480e-01],\n        [ 4.4564e+00,  3.7741e-02],\n        [ 3.2895e+00,  5.8227e-01],\n        [ 3.2214e+00,  3.1523e-01],\n        [ 4.2248e+00, -8.4053e-01],\n        [ 1.4050e+00, -7.2848e-01],\n        [ 1.5643e+00, -1.0109e+00],\n        [ 4.8989e-01,  6.1437e-01],\n        [ 1.5063e+00,  4.9172e-01],\n        [ 3.0549e-01, -5.3478e-03],\n        [ 1.1059e+00,  7.6544e-01],\n        [ 1.0731e+00, -2.8631e-01],\n        [ 1.8460e+00,  7.2345e-01],\n        [ 2.2318e+00, -3.3119e-02],\n        [ 2.3323e+00, -3.2224e-01],\n        [ 3.2991e+00, -1.2857e+00],\n        [ 2.4766e+00,  6.3352e-01],\n        [ 2.9113e+00,  2.5213e-01],\n        [ 1.4102e+00,  6.3385e-01],\n        [ 1.1767e+00, -8.0411e-01],\n        [ 4.3097e+00, -2.2955e-01],\n        [ 1.6657e+00,  6.8432e-01],\n        [ 2.4005e+00,  1.8280e-01],\n        [ 3.7550e+00, -2.2970e-01],\n        [ 9.3162e-01,  5.0684e-01],\n        [ 4.8047e-01, -1.4644e-01],\n        [ 6.7121e-03, -3.7322e-01],\n        [ 2.2019e+00, -6.4783e-01],\n        [ 4.4300e+00, -1.9488e+00],\n        [ 1.9269e+00,  5.9016e-01],\n        [-2.9857e-01, -1.2069e+00],\n        [ 1.7388e+00,  8.1079e-01],\n        [ 2.7056e+00,  1.3022e+00],\n        [-5.2956e-01, -4.9895e-01],\n        [ 1.6626e+00,  3.2019e-01],\n        [ 2.5406e+00,  1.3105e-01],\n        [ 2.2977e+00,  7.3025e-02],\n        [ 4.6541e+00, -1.6362e-01],\n        [ 4.7530e+00, -7.5240e-01],\n        [ 2.0598e+00,  7.7153e-02],\n        [ 2.2679e+00,  2.3658e+00],\n        [ 5.6738e-01, -9.9163e-01],\n        [ 3.7123e+00, -5.6098e-01],\n        [ 2.8703e+00,  1.3631e+00],\n        [ 2.3020e+00,  1.6844e+00],\n        [ 2.4091e+00, -2.9735e-01],\n        [ 1.0008e+00,  1.2947e-03],\n        [ 2.0032e+00, -2.1697e+00],\n        [ 1.5862e+00,  9.3714e-01],\n        [ 3.2001e+00,  8.6221e-01],\n        [ 1.2566e+00, -6.4470e-01],\n        [ 2.6587e+00, -1.4783e-01],\n        [-2.7778e-01,  7.3893e-01],\n        [ 2.1558e+00, -1.4962e+00],\n        [ 2.4752e+00, -9.8409e-02],\n        [ 2.1549e+00, -3.4141e-01],\n        [ 2.4425e+00, -2.8502e-01],\n        [ 1.1164e+00,  1.5390e+00],\n        [ 2.0054e+00,  8.1152e-02],\n        [ 3.7117e+00, -8.3607e-01],\n        [ 2.2339e+00,  2.7737e-01],\n        [ 1.0191e+00,  8.5823e-02],\n        [ 1.4512e+00, -7.1243e-02],\n        [ 1.3787e+00,  1.6523e-01],\n        [ 1.6046e+00, -1.4023e+00],\n        [-1.6447e-01,  1.0489e+00],\n        [ 4.1686e+00, -5.9055e-01],\n        [ 2.5249e+00,  8.4978e-01],\n        [ 2.4523e+00,  2.5104e-01],\n        [ 2.6398e+00,  1.2358e+00],\n        [ 6.1637e-01, -8.4783e-01],\n        [ 3.9443e+00, -1.1663e+00],\n        [ 3.0445e+00,  6.1312e-01],\n        [ 2.0608e+00, -9.5405e-02],\n        [ 1.4325e+00, -5.9439e-01],\n        [ 2.0060e+00,  6.0488e-01],\n        [ 4.1712e+00,  1.0511e+00],\n        [ 2.5063e+00, -5.9858e-01],\n        [ 2.1273e+00,  5.5034e-02],\n        [ 2.2900e+00, -2.2832e+00],\n        [ 2.5081e+00,  1.6178e+00],\n        [ 3.4493e+00,  9.8032e-01],\n        [ 2.0718e+00, -7.8538e-02],\n        [ 1.0518e+00,  6.9901e-01],\n        [ 1.7676e-01, -1.1576e+00],\n        [ 2.2866e+00,  2.1740e-01],\n        [ 1.6043e+00, -1.2261e+00],\n        [ 2.3357e+00, -1.7587e-01],\n        [ 2.0536e+00, -5.7002e-01],\n        [ 2.4997e+00,  1.6891e-01],\n        [ 3.3021e+00,  3.7247e-01],\n        [ 3.6295e+00,  1.1077e-01],\n        [ 6.0719e-01, -9.0288e-01],\n        [ 2.4757e+00,  9.7253e-01],\n        [ 8.3692e-01, -6.9187e-01],\n        [ 3.7373e+00,  2.9465e-01],\n        [ 1.2235e+00, -1.4487e+00],\n        [ 3.0314e+00, -1.1525e+00],\n        [ 2.4343e+00, -1.9783e-01],\n        [ 3.3991e+00,  2.5957e-02],\n        [ 7.5365e-01, -5.6195e-01],\n        [ 1.6370e+00,  1.1503e+00],\n        [ 3.0400e+00, -1.7989e+00],\n        [ 1.8449e+00, -1.1344e+00],\n        [ 4.4212e+00,  4.9499e-01],\n        [ 1.4890e+00, -9.5507e-01],\n        [ 3.3824e-01, -9.3983e-01],\n        [ 4.0452e+00,  7.4246e-01],\n        [ 2.6677e+00, -4.0791e-01],\n        [ 7.1996e-01, -1.1502e+00],\n        [ 1.8901e+00, -9.9159e-01],\n        [ 2.4602e+00, -2.1743e-01],\n        [ 2.1541e+00,  1.1640e-01],\n        [ 1.2773e+00,  8.3463e-02],\n        [ 2.0549e+00,  4.7520e-01],\n        [ 2.5565e+00, -1.2946e+00],\n        [ 3.6072e+00, -1.0358e-01],\n        [ 1.6421e+00, -1.6996e-02],\n        [ 1.8249e+00, -5.8015e-01],\n        [ 3.0471e+00,  3.5202e-01],\n        [ 3.4445e+00,  8.1781e-01],\n        [ 4.2016e+00, -2.7191e-01],\n        [ 2.4785e+00, -1.4414e+00],\n        [ 1.0011e+00, -7.7364e-01],\n        [ 4.0414e+00, -7.2181e-01],\n        [ 3.6486e+00,  6.8027e-01],\n        [ 2.6788e+00,  1.0019e+00],\n        [ 1.5905e-01,  7.6390e-01],\n        [ 3.0073e+00, -8.1662e-01],\n        [ 1.6606e+00, -1.4729e+00],\n        [ 3.8198e+00,  1.7631e-01],\n        [ 3.2103e+00, -6.1321e-01],\n        [ 3.7624e+00,  6.3539e-02],\n        [ 1.6196e+00,  3.3790e-01],\n        [ 2.3316e+00, -2.6333e-01],\n        [ 1.8108e+00, -5.4011e-01],\n        [ 3.9371e+00, -1.4623e-01],\n        [ 1.3494e+00,  1.9220e-01],\n        [ 1.7386e+00,  5.7527e-01],\n        [ 1.5992e+00, -7.6538e-01],\n        [ 3.1849e+00, -1.9510e+00],\n        [ 2.1099e+00, -3.1680e-01],\n        [ 1.0614e+00,  5.7811e-01],\n        [ 3.1728e+00, -1.3652e+00],\n        [ 2.5590e+00, -2.8911e-01],\n        [ 3.5773e+00, -1.5837e+00],\n        [ 1.2180e+00, -9.6802e-01],\n        [ 4.6189e+00,  1.5654e-01],\n        [ 2.3208e+00,  9.6199e-01],\n        [ 1.3187e+00,  1.2538e+00],\n        [ 2.4013e+00, -1.0061e+00],\n        [ 4.2516e+00, -4.8570e-01],\n        [ 9.3050e-01,  1.8211e-01],\n        [ 1.5551e+00,  1.7529e+00],\n        [ 3.9270e+00, -1.2346e+00],\n        [ 2.4576e+00,  1.9438e-01],\n        [ 1.8812e+00, -7.9041e-03],\n        [ 4.2549e+00, -1.1162e+00],\n        [ 1.0708e+00, -7.9334e-01],\n        [ 2.4405e+00,  1.2201e-01],\n        [ 3.0748e+00,  9.1456e-01],\n        [ 1.5960e+00, -5.9089e-01],\n        [ 4.4295e+00, -1.5030e+00],\n        [ 2.4463e+00,  6.1737e-01],\n        [ 3.8516e+00,  6.8419e-01],\n        [ 4.2196e+00, -2.0696e+00],\n        [ 1.4543e+00, -3.5198e+00],\n        [ 3.0740e+00,  1.0472e+00],\n        [ 2.8106e+00,  1.2350e+00],\n        [ 2.7629e+00,  1.5657e+00],\n        [ 2.3611e+00, -2.0997e-01],\n        [ 1.4600e+00, -1.7766e+00],\n        [ 2.6444e+00,  1.4622e+00],\n        [ 1.5539e+00,  9.1215e-03],\n        [ 2.2077e+00, -1.2364e+00],\n        [ 3.3571e+00, -1.9238e+00],\n        [ 2.5387e+00, -1.8228e+00],\n        [ 3.2229e+00, -6.5571e-01],\n        [ 1.5865e+00, -6.9649e-01],\n        [ 3.0733e+00,  1.4725e-01],\n        [ 2.6344e+00,  5.9856e-01],\n        [ 2.0615e+00,  1.5584e+00],\n        [ 1.4886e+00, -2.4713e-01],\n        [ 3.5266e+00, -1.0504e+00],\n        [ 2.6565e+00, -1.4062e-01],\n        [ 2.7700e+00, -1.0328e-02],\n        [ 2.7264e+00, -1.2401e+00],\n        [ 3.1060e+00, -6.6170e-01],\n        [ 2.8169e+00, -9.6543e-01],\n        [ 2.0481e+00, -1.7716e+00],\n        [ 2.0045e+00, -6.0776e-01],\n        [ 1.1749e+00,  8.0867e-01],\n        [ 1.4048e+00,  3.8333e-01],\n        [ 1.7773e+00,  8.0196e-01],\n        [ 3.0387e+00,  2.2004e+00],\n        [ 2.3810e+00, -2.2900e-01],\n        [ 3.3341e+00, -1.4585e+00],\n        [ 1.9658e+00, -2.2663e-01],\n        [ 1.6753e+00,  3.2544e-01],\n        [ 1.6836e+00,  5.6933e-01],\n        [ 2.0034e+00,  1.5343e+00],\n        [ 2.8837e+00, -5.7627e-01],\n        [ 2.6466e+00,  1.0002e-01],\n        [ 3.3988e+00, -2.8329e-01],\n        [ 2.1983e+00,  5.5921e-01],\n        [ 1.7686e+00, -6.4981e-01],\n        [ 3.0100e+00, -7.1992e-01],\n        [ 1.3642e+00, -4.1955e-01],\n        [ 1.9538e+00,  9.4807e-01],\n        [ 3.2115e+00, -8.4148e-01],\n        [ 1.5068e+00,  3.4619e-01],\n        [ 3.1295e+00,  3.3034e-02],\n        [ 1.9385e+00, -3.6905e-01],\n        [ 3.2890e+00, -1.5475e+00],\n        [ 2.9379e+00, -1.7714e-01],\n        [ 2.8923e+00, -1.9061e-02],\n        [ 1.1472e+00,  9.7185e-01],\n        [ 3.0245e+00, -1.0569e-01],\n        [ 9.2289e-01,  9.6421e-02],\n        [ 3.2172e+00, -5.3941e-01],\n        [ 1.5665e+00, -9.9056e-01],\n        [ 2.1412e+00, -1.1210e+00],\n        [ 1.8904e+00, -1.6612e+00],\n        [ 2.0903e+00, -4.0059e-01],\n        [ 3.3491e+00, -1.0291e+00],\n        [ 1.5224e+00,  2.5912e-01],\n        [ 1.5243e+00, -6.2866e-01],\n        [ 9.2886e-01,  3.4410e-01],\n        [ 2.3223e+00,  2.0268e+00],\n        [ 1.0284e+00, -1.8811e-01],\n        [ 3.3947e+00, -3.4225e-01],\n        [ 2.0881e+00,  7.3325e-01],\n        [ 1.9337e+00,  6.8761e-01],\n        [ 2.4650e+00,  1.3310e+00],\n        [ 4.7288e+00,  8.3481e-01],\n        [ 3.6763e+00,  2.2086e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[6.2398e-03, 9.2738e-03, 2.1753e-01,  ..., 9.9973e-01, 9.9973e-01,\n         1.0000e+00],\n        [8.0091e-03, 1.0854e-01, 1.0870e-01,  ..., 9.8941e-01, 9.8941e-01,\n         1.0000e+00],\n        [4.0719e-04, 5.0207e-03, 5.0209e-03,  ..., 9.8403e-01, 9.8423e-01,\n         1.0000e+00],\n        ...,\n        [2.4598e-08, 6.5316e-05, 3.3890e-03,  ..., 9.8542e-01, 9.8542e-01,\n         1.0000e+00],\n        [6.3988e-09, 1.3645e-03, 2.4852e-02,  ..., 7.6731e-01, 9.9847e-01,\n         1.0000e+00],\n        [3.0970e-02, 4.6140e-02, 4.6140e-02,  ..., 9.9728e-01, 9.9728e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.2560],\n        [0.7925],\n        [0.2993],\n        [0.4847],\n        [0.7226],\n        [0.9429],\n        [0.8611],\n        [0.2856],\n        [0.3295],\n        [0.1790],\n        [0.2382],\n        [0.7403],\n        [0.5663],\n        [0.5192],\n        [0.0788],\n        [0.7923],\n        [0.3727],\n        [0.0698],\n        [0.8765],\n        [0.8109],\n        [0.5033],\n        [0.3537],\n        [0.0077],\n        [0.3766],\n        [0.2331],\n        [0.8579],\n        [0.6924],\n        [0.7780],\n        [0.7414],\n        [0.6080],\n        [0.0854],\n        [0.4152],\n        [0.7289],\n        [0.1530],\n        [0.2957],\n        [0.6249],\n        [0.2416],\n        [0.4875],\n        [0.8688],\n        [0.0085],\n        [0.0213],\n        [0.0275],\n        [0.9762],\n        [0.3832],\n        [0.2991],\n        [0.6320],\n        [0.8687],\n        [0.5092],\n        [0.9463],\n        [0.2066],\n        [0.0484],\n        [0.4383],\n        [0.9531],\n        [0.2773],\n        [0.8798],\n        [0.8848],\n        [0.7497],\n        [0.2819],\n        [0.9913],\n        [0.5278],\n        [0.8286],\n        [0.0922],\n        [0.8336],\n        [0.7050],\n        [0.7471],\n        [0.6862],\n        [0.2535],\n        [0.9456],\n        [0.7878],\n        [0.1912],\n        [0.1993],\n        [0.1554],\n        [0.9289],\n        [0.0765],\n        [0.0997],\n        [0.8280],\n        [0.5044],\n        [0.7917],\n        [0.8309],\n        [0.7914],\n        [0.7287],\n        [0.9350],\n        [0.4203],\n        [0.3648],\n        [0.4457],\n        [0.9357],\n        [0.3221],\n        [0.5701],\n        [0.2321],\n        [0.3586],\n        [0.2059],\n        [0.9101],\n        [0.3132],\n        [0.9742],\n        [0.2794],\n        [0.7454],\n        [0.1621],\n        [0.8004],\n        [0.3226],\n        [0.9474],\n        [0.5478],\n        [0.6200],\n        [0.9250],\n        [0.9034],\n        [0.8492],\n        [0.4774],\n        [0.5914],\n        [0.1281],\n        [0.3642],\n        [0.3897],\n        [0.3208],\n        [0.2808],\n        [0.5784],\n        [0.9130],\n        [0.1946],\n        [0.4574],\n        [0.0092],\n        [0.1275],\n        [0.9770],\n        [0.1740],\n        [0.9113],\n        [0.6728],\n        [0.8551],\n        [0.8739],\n        [0.0240],\n        [0.1684],\n        [0.5899],\n        [0.9765],\n        [0.3800],\n        [0.2374],\n        [0.0248],\n        [0.7682],\n        [0.0738],\n        [0.5186],\n        [0.7463],\n        [0.9094],\n        [0.2842],\n        [0.2935],\n        [0.9949],\n        [0.6000],\n        [0.6879],\n        [0.0565],\n        [0.9761],\n        [0.7692],\n        [0.3668],\n        [0.8701],\n        [0.3804],\n        [0.0884],\n        [0.4573],\n        [0.6420],\n        [0.7238],\n        [0.3659],\n        [0.5424],\n        [0.3261],\n        [0.6093],\n        [0.6833],\n        [0.6782],\n        [0.4591],\n        [0.8069],\n        [0.6771],\n        [0.6356],\n        [0.1197],\n        [0.3697],\n        [0.2450],\n        [0.9702],\n        [0.6633],\n        [0.1493],\n        [0.9264],\n        [0.8780],\n        [0.8943],\n        [0.6970],\n        [0.7766],\n        [0.6645],\n        [0.1188],\n        [0.1572],\n        [0.1046],\n        [0.3123],\n        [0.3480],\n        [0.5230],\n        [0.6607],\n        [0.7290],\n        [0.0232],\n        [0.2994],\n        [0.9106],\n        [0.3128],\n        [0.4601],\n        [0.4950],\n        [0.2400],\n        [0.4331],\n        [0.7203],\n        [0.9882],\n        [0.8751],\n        [0.8372],\n        [0.8006],\n        [0.8519],\n        [0.5122],\n        [0.2215],\n        [0.5260],\n        [0.8086],\n        [0.4610],\n        [0.7727],\n        [0.2656],\n        [0.9680],\n        [0.1008],\n        [0.0049],\n        [0.4811],\n        [0.2013],\n        [0.6808],\n        [0.1726],\n        [0.7172],\n        [0.8282],\n        [0.6390],\n        [0.3210],\n        [0.0898],\n        [0.4058],\n        [0.0670],\n        [0.3134],\n        [0.5519],\n        [0.0033],\n        [0.2042],\n        [0.1602],\n        [0.6171],\n        [0.4396],\n        [0.4507],\n        [0.9845],\n        [0.4236],\n        [0.8076],\n        [0.0671],\n        [0.0766],\n        [0.3597],\n        [0.2583],\n        [0.3981],\n        [0.1960],\n        [0.2846],\n        [0.6579],\n        [0.5590],\n        [0.1592],\n        [0.6033],\n        [0.3030],\n        [0.4321],\n        [0.1504],\n        [0.0863],\n        [0.2352],\n        [0.4203],\n        [0.3468],\n        [0.7733],\n        [0.9803],\n        [0.5568],\n        [0.9751],\n        [0.4518],\n        [0.1096],\n        [0.2438],\n        [0.9409],\n        [0.8682],\n        [0.8449],\n        [0.3121],\n        [0.1573],\n        [0.4192],\n        [0.4152],\n        [0.2340],\n        [0.6331],\n        [0.7351],\n        [0.9250],\n        [0.3486],\n        [0.7726],\n        [0.7533],\n        [0.3045],\n        [0.0333],\n        [0.4418],\n        [0.5557],\n        [0.8306],\n        [0.7703],\n        [0.8633],\n        [0.3770],\n        [0.8774],\n        [0.1393],\n        [0.0043],\n        [0.1794],\n        [0.1018],\n        [0.1126],\n        [0.2447],\n        [0.9087],\n        [0.3991],\n        [0.8870],\n        [0.7749],\n        [0.4215],\n        [0.4131],\n        [0.2127],\n        [0.5598],\n        [0.5789],\n        [0.9568],\n        [0.0039],\n        [0.8914],\n        [0.8192],\n        [0.1380],\n        [0.2042],\n        [0.1583],\n        [0.1879],\n        [0.8169],\n        [0.8909],\n        [0.3464],\n        [0.5024],\n        [0.9358],\n        [0.2248],\n        [0.7168],\n        [0.3037],\n        [0.0482],\n        [0.7970],\n        [0.5611],\n        [0.1072],\n        [0.9239],\n        [0.6759]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False,  True, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 0.2920,  0.0682],\n        [ 0.7337,  1.4474],\n        [ 3.5707, -0.0824],\n        ...,\n        [-1.9299,  1.1226],\n        [-3.8134, -1.2183],\n        [ 0.1067,  0.9197]]) torch.Size([9984, 2])\nsamples tensor([[ 2.7071e+00,  6.9457e-01],\n        [ 1.0092e+00, -1.3004e+00],\n        [ 1.5173e+00, -1.1472e+00],\n        [ 1.8908e+00, -5.4203e-01],\n        [ 4.1989e+00,  7.0473e-01],\n        [-3.5896e-01,  2.9994e-01],\n        [ 2.2502e+00, -1.8622e+00],\n        [ 2.7638e+00, -2.4133e+00],\n        [ 4.5395e+00,  8.1171e-01],\n        [ 2.1407e+00,  3.6853e-01],\n        [ 2.7260e+00, -1.9128e+00],\n        [ 2.9426e+00, -5.0815e-01],\n        [ 2.5577e+00, -1.9825e+00],\n        [ 4.4048e+00,  2.8199e-01],\n        [ 3.6191e+00,  6.8608e-01],\n        [-6.2237e-02, -1.0250e+00],\n        [ 4.3257e+00, -2.9630e+00],\n        [ 3.2196e+00,  3.3040e-01],\n        [ 1.7248e+00,  7.5856e-01],\n        [ 2.9527e+00,  1.5323e+00],\n        [ 2.9377e+00, -1.6184e+00],\n        [ 2.5379e+00, -5.5600e-01],\n        [ 2.1903e+00,  1.9454e+00],\n        [ 3.6080e+00, -7.6772e-02],\n        [ 1.2345e+00,  7.5257e-01],\n        [ 4.6578e+00, -9.8970e-01],\n        [ 1.6401e+00,  8.9483e-01],\n        [ 1.8908e+00, -2.2203e+00],\n        [ 1.9619e+00, -1.8329e+00],\n        [ 1.8283e+00,  1.7634e+00],\n        [ 2.9472e+00,  4.2269e-01],\n        [ 1.6309e+00,  8.5541e-01],\n        [ 2.4667e+00,  6.3859e-01],\n        [ 2.8617e+00, -5.5973e-01],\n        [ 1.8152e+00, -7.8208e-01],\n        [ 1.3490e+00,  1.1928e+00],\n        [ 2.8111e+00,  8.7813e-01],\n        [ 3.5550e+00,  6.3002e-01],\n        [ 1.7689e+00, -7.5105e-02],\n        [ 1.6709e+00,  2.6404e-01],\n        [ 1.5974e-01,  1.8706e-01],\n        [ 1.1608e+00,  8.0139e-01],\n        [ 2.5080e+00,  1.5566e+00],\n        [ 2.3530e+00, -8.2125e-01],\n        [ 1.8930e+00, -1.4909e+00],\n        [ 1.1379e+00, -1.0628e+00],\n        [ 8.7954e-01,  5.9697e-01],\n        [ 3.4618e+00,  7.7575e-01],\n        [ 1.0434e+00,  2.5684e-01],\n        [ 6.2713e-01, -1.4934e+00],\n        [ 3.8264e+00, -8.6602e-02],\n        [ 3.1188e+00, -8.3132e-01],\n        [ 1.4871e+00, -6.4350e-01],\n        [ 3.7657e+00,  1.1247e+00],\n        [ 1.8944e+00,  6.8105e-01],\n        [ 2.1937e+00,  6.5030e-01],\n        [ 2.9569e+00,  9.5132e-01],\n        [ 3.0537e+00, -1.2179e+00],\n        [ 3.6177e+00,  1.4716e+00],\n        [ 2.5196e+00,  2.3030e-01],\n        [ 1.3977e+00, -9.4914e-01],\n        [ 2.3105e+00,  8.7829e-01],\n        [ 2.0459e+00,  3.8111e-01],\n        [ 1.8935e+00, -2.2412e-01],\n        [ 1.8396e+00,  7.9413e-01],\n        [ 1.6191e+00, -2.4596e-01],\n        [ 2.2300e+00,  7.6460e-01],\n        [ 1.4558e+00,  1.3999e-01],\n        [ 2.3122e+00, -1.2291e+00],\n        [ 2.1900e+00,  1.0298e+00],\n        [ 1.7363e+00, -2.1785e-01],\n        [ 2.3241e+00, -3.2953e-01],\n        [ 2.3491e+00,  6.8270e-01],\n        [ 2.1451e+00, -3.1757e-01],\n        [ 3.5975e+00,  9.0552e-01],\n        [ 2.3777e+00, -3.7477e-01],\n        [ 2.5427e+00, -8.4497e-01],\n        [ 1.9164e+00,  2.4206e-01],\n        [ 2.3329e+00,  6.9644e-01],\n        [ 3.4578e+00, -5.7645e-01],\n        [ 2.9863e+00,  2.4101e-02],\n        [ 2.0199e+00,  2.3594e+00],\n        [ 2.3902e+00,  1.6138e+00],\n        [ 5.6204e-01, -5.1995e-01],\n        [ 7.4729e-01, -7.9245e-01],\n        [ 1.8202e+00, -1.1973e+00],\n        [ 1.8182e+00, -1.5041e+00],\n        [ 4.4979e+00,  7.1886e-01],\n        [ 1.1714e+00,  2.2460e-01],\n        [ 1.5699e+00,  1.2156e+00],\n        [ 3.2066e+00, -1.4063e+00],\n        [ 2.1199e+00, -1.0456e+00],\n        [ 4.0588e+00, -8.7980e-01],\n        [ 2.0461e+00,  9.7621e-01],\n        [ 3.4609e+00, -1.1032e+00],\n        [ 1.8440e+00,  9.8863e-01],\n        [ 3.3262e+00, -2.4148e-01],\n        [-1.4272e-01, -2.1192e+00],\n        [ 3.8822e+00, -1.0421e+00],\n        [ 2.3056e+00, -7.8776e-02],\n        [ 3.0862e+00, -5.5915e-01],\n        [ 1.0496e+00, -4.4142e-02],\n        [ 1.9092e+00,  2.3114e-01],\n        [ 1.6809e+00, -1.8540e+00],\n        [ 3.7326e+00,  4.2484e-04],\n        [ 2.6762e+00, -7.0295e-01],\n        [ 3.1001e+00, -1.8774e+00],\n        [ 6.9074e-01, -1.0811e+00],\n        [ 1.4787e+00,  7.0380e-01],\n        [ 3.4210e+00,  2.9791e-01],\n        [ 1.4831e+00,  1.1902e+00],\n        [ 3.6643e+00, -6.7498e-01],\n        [ 2.7041e+00, -3.3148e-01],\n        [ 1.7836e+00,  9.4583e-01],\n        [ 2.9468e+00, -7.0626e-01],\n        [ 2.6041e+00,  2.3771e-01],\n        [ 2.3540e+00, -9.3334e-01],\n        [ 3.0877e+00, -1.0170e+00],\n        [ 2.3907e+00, -7.1131e-01],\n        [ 3.8192e+00,  3.6719e-01],\n        [ 3.2634e+00, -1.1991e+00],\n        [ 1.7749e+00,  1.0107e-02],\n        [ 1.8454e+00, -9.6291e-01],\n        [ 1.4572e+00, -8.1822e-01],\n        [ 2.0157e+00,  2.2562e+00],\n        [ 1.8113e+00,  7.5037e-01],\n        [ 3.2362e+00,  1.1377e+00],\n        [ 3.3865e+00, -1.2948e+00],\n        [ 9.9888e-01, -1.1378e+00],\n        [ 1.7586e+00, -1.5091e+00],\n        [ 2.8511e+00,  6.9916e-01],\n        [ 3.2805e+00, -2.3702e-01],\n        [ 1.2892e+00,  3.2415e-01],\n        [ 2.0112e+00,  5.1955e-01],\n        [ 2.1215e+00,  2.6353e-01],\n        [ 2.0479e+00, -1.3728e+00],\n        [ 1.0737e+00, -2.2315e+00],\n        [ 1.7165e+00, -1.9811e+00],\n        [ 5.2757e-01, -1.0101e+00],\n        [ 2.5951e+00,  4.4232e-01],\n        [ 2.2638e+00, -1.1847e+00],\n        [ 2.9473e+00,  9.4301e-01],\n        [ 1.9964e+00, -3.5490e-01],\n        [ 1.7646e+00, -1.0003e+00],\n        [ 2.5326e+00, -2.3648e+00],\n        [ 2.4303e+00,  5.4415e-01],\n        [ 2.8585e+00,  4.5626e-01],\n        [ 3.9163e+00,  6.6740e-01],\n        [ 2.9568e+00, -3.5690e-01],\n        [ 2.1638e+00, -1.0075e+00],\n        [ 3.4691e+00, -6.8483e-01],\n        [ 1.0103e+00,  5.0561e-01],\n        [ 8.0475e-01, -2.0307e-02],\n        [ 3.2437e+00, -1.9878e+00],\n        [ 2.4461e+00, -8.4143e-01],\n        [ 4.3017e+00, -1.3531e+00],\n        [ 1.6917e+00, -1.6968e-01],\n        [ 3.7974e+00, -4.0212e-01],\n        [ 2.9584e+00, -9.3366e-03],\n        [ 2.8775e+00,  1.3183e-01],\n        [ 4.0837e+00, -1.2031e+00],\n        [ 2.9350e+00,  4.3231e-01],\n        [ 2.3956e+00, -1.8635e-01],\n        [ 2.1778e+00, -8.2174e-01],\n        [ 3.7092e+00, -6.0826e-02],\n        [ 1.2363e+00,  4.2003e-01],\n        [ 2.4492e+00,  1.7312e+00],\n        [ 1.2864e+00, -7.0073e-02],\n        [ 4.0220e+00, -1.6718e+00],\n        [ 1.6555e+00, -2.1304e-01],\n        [-3.8797e-01, -1.1021e+00],\n        [ 2.6501e+00, -1.6100e+00],\n        [ 2.7467e+00,  3.2343e-01],\n        [ 4.9969e-01, -2.4207e-01],\n        [ 1.3535e+00, -1.7134e-01],\n        [ 1.7409e+00,  1.5805e-01],\n        [ 2.1468e+00, -2.5997e+00],\n        [ 7.5982e-01,  1.2230e-01],\n        [ 1.4599e+00,  9.5430e-01],\n        [ 2.3947e+00,  9.9643e-01],\n        [ 3.8404e+00,  1.7022e+00],\n        [ 1.5994e+00, -2.9554e-01],\n        [ 2.4003e+00, -1.3487e+00],\n        [ 1.4568e-01, -1.3714e-01],\n        [ 2.8087e+00, -6.4062e-01],\n        [ 3.3635e+00, -1.0392e+00],\n        [ 2.0032e+00, -8.5905e-01],\n        [ 1.8777e+00, -1.7190e+00],\n        [ 1.4998e+00, -7.2023e-01],\n        [ 1.5301e+00,  1.8143e+00],\n        [ 2.8681e+00,  1.1507e+00],\n        [ 9.3683e-01,  2.0155e+00],\n        [ 1.3762e+00,  1.1047e-01],\n        [ 3.5379e+00,  4.2880e-01],\n        [ 2.9569e+00, -1.8776e+00],\n        [ 2.9175e+00,  5.8475e-01],\n        [ 3.1205e+00, -8.3904e-01],\n        [ 7.3747e-01,  7.3285e-01],\n        [ 1.8715e+00,  4.0018e-01],\n        [ 4.1206e+00, -3.7558e-01],\n        [ 3.6312e+00, -2.3141e+00],\n        [ 2.7489e+00,  4.3075e-01],\n        [ 3.1712e+00,  1.0820e+00],\n        [ 1.4530e+00, -3.4160e-01],\n        [ 3.8884e+00, -2.1161e+00],\n        [ 2.2119e+00, -4.6963e-01],\n        [ 1.3435e+00, -8.3001e-01],\n        [ 1.6902e+00, -1.5431e-01],\n        [ 1.8088e+00, -2.4353e-01],\n        [ 8.3334e-01, -1.4906e+00],\n        [ 2.4322e+00, -1.7689e+00],\n        [ 2.9927e+00, -8.3781e-01],\n        [ 1.9025e+00,  2.4226e-01],\n        [ 1.2903e+00,  6.9466e-01],\n        [ 1.9940e+00,  1.8096e-01],\n        [ 2.8688e+00,  7.0916e-01],\n        [ 2.4400e+00, -2.1985e+00],\n        [ 6.9615e-01,  5.9637e-02],\n        [ 2.5632e+00,  3.6060e-01],\n        [ 2.6632e+00, -8.9592e-02],\n        [ 1.7350e+00,  7.0229e-01],\n        [ 2.8006e+00, -1.2100e+00],\n        [ 3.7213e+00,  2.3871e-01],\n        [ 2.8952e+00, -1.6402e+00],\n        [ 2.6600e+00,  5.0881e-01],\n        [ 4.6817e+00,  9.1427e-01],\n        [ 3.2891e+00,  2.3933e-01],\n        [ 1.7885e+00,  4.4187e-02],\n        [ 4.6976e+00,  1.8305e-01],\n        [ 2.5037e+00,  1.9691e+00],\n        [ 1.4856e+00, -4.2494e-01],\n        [ 2.4278e+00, -9.2035e-01],\n        [ 1.9592e+00, -6.7515e-02],\n        [ 3.3632e+00, -2.2450e+00],\n        [ 2.4521e-01,  1.5956e+00],\n        [ 1.6938e+00, -1.7257e+00],\n        [ 1.7368e+00, -2.1904e-01],\n        [ 2.5856e+00, -2.1636e-01],\n        [ 3.2149e+00, -1.4153e+00],\n        [ 3.1281e+00,  1.4368e+00],\n        [ 1.6353e+00, -1.4636e-01],\n        [ 3.1546e-01, -6.6804e-01],\n        [ 3.0988e+00, -5.8149e-01],\n        [ 1.9457e+00,  1.0536e+00],\n        [ 1.5653e+00,  1.0035e-01],\n        [ 3.0710e+00,  1.5257e-01],\n        [ 1.3443e+00,  9.2217e-01],\n        [ 1.1468e+00, -1.6257e+00],\n        [ 2.4843e+00,  7.3122e-01],\n        [ 1.8878e+00,  3.5638e-01],\n        [ 3.4906e+00, -5.5896e-01],\n        [ 1.6854e+00,  1.0148e+00],\n        [ 2.9475e+00, -7.8805e-01],\n        [ 1.7473e+00, -9.7766e-01],\n        [ 3.1159e+00, -1.1123e+00],\n        [ 2.8695e+00, -7.0422e-01],\n        [ 1.5784e+00, -9.6912e-01],\n        [ 2.4525e+00, -1.9845e-01],\n        [ 2.1427e+00,  3.6348e-01],\n        [-1.8671e-01,  2.8422e-01],\n        [ 1.2668e+00, -2.8126e-01],\n        [ 2.9750e+00,  9.8529e-01],\n        [ 2.9838e+00,  1.0754e+00],\n        [ 3.4163e+00,  2.4502e+00],\n        [ 2.3697e+00, -1.0390e+00],\n        [ 2.6261e+00, -1.1176e+00],\n        [ 1.2615e+00, -1.1320e-01],\n        [ 3.0492e+00,  8.9739e-01],\n        [ 1.6898e+00,  7.5717e-01],\n        [ 1.3703e+00,  3.7512e-01],\n        [ 1.6169e-03, -4.4538e-01],\n        [ 2.4913e+00,  1.0059e+00],\n        [ 1.8225e+00, -1.5882e+00],\n        [ 1.9670e+00, -5.0956e-01],\n        [ 1.6048e+00, -2.1720e-01],\n        [ 1.3671e+00,  8.7040e-01],\n        [ 1.7855e+00,  2.1773e+00],\n        [ 2.9156e+00,  6.7569e-01],\n        [ 1.4042e+00,  1.1169e+00],\n        [ 2.9632e+00,  5.7582e-01],\n        [ 2.9645e+00,  8.5207e-01],\n        [ 2.7831e+00,  5.6442e-02],\n        [ 3.1349e+00, -8.7431e-01],\n        [ 1.8396e+00,  3.0701e-01],\n        [ 3.3299e+00, -3.3251e-01],\n        [ 3.5359e+00, -1.0241e+00],\n        [ 4.1639e+00, -1.2957e+00],\n        [ 3.8491e+00, -6.1818e-01],\n        [ 3.1847e+00, -1.4861e+00],\n        [ 1.5483e+00,  1.2715e+00],\n        [ 3.7812e+00,  8.0985e-01],\n        [ 1.2416e+00,  1.2407e-01],\n        [ 6.7867e-01, -9.3393e-01],\n        [ 2.2598e+00, -5.6145e-02],\n        [ 1.6331e+00,  4.4621e-02],\n        [ 3.6659e+00, -4.3895e-01],\n        [ 4.0194e+00, -6.3254e-01],\n        [ 3.4200e-01,  1.5390e+00],\n        [ 2.0801e+00, -2.4542e+00],\n        [ 2.1878e+00, -3.0880e+00],\n        [ 3.5210e+00, -1.2321e+00],\n        [ 2.3945e+00, -4.5160e-01],\n        [ 1.1140e+00, -5.4413e-01],\n        [ 1.3425e+00,  1.4111e+00],\n        [ 7.9628e-01, -1.2602e+00],\n        [ 2.9178e+00, -2.3472e-01],\n        [ 1.5648e+00,  7.1612e-01],\n        [ 2.4639e+00,  5.0586e-01],\n        [ 4.0690e+00,  5.6994e-01],\n        [ 2.1965e+00,  8.8718e-01],\n        [ 1.8677e+00, -1.0360e+00],\n        [ 2.4120e+00, -2.5143e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.4709e-02, 1.5426e-02, 3.9493e-02,  ..., 9.9185e-01, 9.9195e-01,\n         1.0000e+00],\n        [1.1468e-01, 1.3385e-01, 1.3474e-01,  ..., 9.9982e-01, 9.9982e-01,\n         1.0000e+00],\n        [8.9613e-07, 2.5286e-03, 1.0665e-02,  ..., 8.9513e-01, 8.9518e-01,\n         1.0000e+00],\n        ...,\n        [2.3893e-02, 2.3893e-02, 9.5164e-02,  ..., 9.9995e-01, 9.9995e-01,\n         1.0000e+00],\n        [1.7123e-03, 2.4250e-03, 1.4646e-02,  ..., 6.0634e-01, 6.0635e-01,\n         1.0000e+00],\n        [1.0549e-02, 1.0552e-02, 6.9013e-02,  ..., 6.5248e-01, 9.2858e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[5.2506e-01],\n        [9.7691e-01],\n        [9.5048e-01],\n        [9.6085e-01],\n        [5.4514e-03],\n        [4.6091e-01],\n        [9.7526e-01],\n        [4.2565e-01],\n        [7.7305e-01],\n        [9.7982e-01],\n        [2.2154e-01],\n        [7.8294e-01],\n        [6.7628e-01],\n        [4.0828e-02],\n        [8.2773e-01],\n        [7.4014e-01],\n        [4.7713e-01],\n        [8.1016e-01],\n        [9.3583e-01],\n        [4.0816e-01],\n        [1.5991e-01],\n        [8.6017e-01],\n        [5.7492e-01],\n        [8.6257e-01],\n        [5.1009e-02],\n        [2.6023e-01],\n        [4.8749e-01],\n        [4.5875e-01],\n        [6.2711e-01],\n        [9.1686e-01],\n        [7.6142e-01],\n        [8.3126e-01],\n        [5.8841e-01],\n        [5.4371e-02],\n        [7.7157e-01],\n        [5.7222e-01],\n        [7.0108e-01],\n        [3.2055e-01],\n        [8.7727e-01],\n        [9.8308e-01],\n        [3.6764e-01],\n        [3.4296e-01],\n        [8.4379e-01],\n        [3.3148e-01],\n        [4.1582e-01],\n        [1.9998e-01],\n        [9.5964e-01],\n        [9.1412e-01],\n        [3.1939e-01],\n        [6.7028e-01],\n        [5.5981e-01],\n        [7.2075e-01],\n        [5.7485e-01],\n        [6.1995e-02],\n        [3.1495e-01],\n        [1.2097e-01],\n        [5.4263e-02],\n        [2.8722e-01],\n        [6.0499e-01],\n        [4.0053e-01],\n        [1.0163e-01],\n        [5.5139e-01],\n        [9.0612e-01],\n        [9.3569e-01],\n        [5.3694e-01],\n        [4.4507e-01],\n        [3.9033e-02],\n        [2.1281e-01],\n        [4.1735e-01],\n        [3.0515e-01],\n        [4.5058e-01],\n        [7.6226e-02],\n        [3.9951e-02],\n        [2.6025e-02],\n        [3.7806e-01],\n        [3.1973e-01],\n        [5.7551e-01],\n        [8.7680e-01],\n        [8.1483e-01],\n        [3.2412e-01],\n        [2.9835e-02],\n        [7.6681e-01],\n        [3.5176e-01],\n        [9.2374e-01],\n        [3.6468e-01],\n        [2.2612e-02],\n        [7.2683e-01],\n        [8.6797e-01],\n        [6.4888e-01],\n        [3.2222e-01],\n        [5.3347e-01],\n        [1.0790e-01],\n        [8.6951e-01],\n        [8.8162e-01],\n        [7.3105e-03],\n        [1.1966e-01],\n        [4.4549e-01],\n        [7.8345e-01],\n        [1.3205e-01],\n        [7.8814e-01],\n        [5.5539e-02],\n        [4.6227e-01],\n        [3.2035e-01],\n        [2.0572e-01],\n        [1.9883e-01],\n        [4.0867e-01],\n        [6.8357e-01],\n        [2.8452e-01],\n        [3.4987e-01],\n        [5.6335e-01],\n        [8.6535e-02],\n        [9.8789e-01],\n        [7.1327e-01],\n        [1.8570e-01],\n        [8.2205e-01],\n        [5.4303e-01],\n        [6.0472e-02],\n        [2.6594e-01],\n        [5.6519e-01],\n        [7.1653e-01],\n        [7.5448e-01],\n        [7.1069e-01],\n        [1.0826e-01],\n        [7.1797e-01],\n        [7.4533e-02],\n        [3.9320e-01],\n        [2.9348e-01],\n        [8.8088e-01],\n        [8.1461e-01],\n        [4.4161e-01],\n        [9.1066e-01],\n        [8.4285e-01],\n        [3.4269e-01],\n        [6.6002e-01],\n        [1.9180e-01],\n        [8.1155e-01],\n        [5.0205e-01],\n        [3.5244e-01],\n        [9.4100e-01],\n        [9.1684e-01],\n        [8.4867e-01],\n        [6.1658e-01],\n        [8.5004e-01],\n        [9.9966e-01],\n        [7.3799e-01],\n        [7.8466e-01],\n        [6.1812e-01],\n        [9.7768e-01],\n        [6.8510e-01],\n        [1.2127e-01],\n        [9.0878e-01],\n        [1.9635e-01],\n        [1.6868e-01],\n        [4.4179e-01],\n        [2.0453e-01],\n        [6.7242e-01],\n        [1.5856e-01],\n        [3.1493e-01],\n        [1.4669e-01],\n        [8.1997e-01],\n        [1.5708e-01],\n        [3.8113e-01],\n        [1.6404e-01],\n        [4.1317e-01],\n        [6.7534e-01],\n        [7.4636e-01],\n        [1.9015e-01],\n        [6.6988e-02],\n        [6.2986e-02],\n        [9.2460e-01],\n        [4.6638e-01],\n        [7.9950e-01],\n        [3.0663e-01],\n        [5.5350e-01],\n        [4.3338e-02],\n        [6.2915e-01],\n        [5.0118e-01],\n        [6.4646e-01],\n        [6.1925e-01],\n        [9.4420e-01],\n        [7.4936e-01],\n        [5.3338e-01],\n        [4.2851e-02],\n        [7.6773e-02],\n        [6.2094e-01],\n        [5.3123e-01],\n        [9.7888e-01],\n        [6.6067e-01],\n        [6.2631e-01],\n        [8.9787e-01],\n        [5.3753e-01],\n        [4.2584e-01],\n        [1.8270e-01],\n        [4.2607e-01],\n        [8.6629e-01],\n        [4.2215e-01],\n        [9.1079e-01],\n        [7.4714e-01],\n        [7.2904e-01],\n        [3.5357e-04],\n        [9.3258e-01],\n        [9.0728e-01],\n        [5.6357e-01],\n        [2.0556e-01],\n        [7.7383e-01],\n        [4.6628e-01],\n        [2.7015e-01],\n        [2.4009e-01],\n        [3.9793e-02],\n        [8.1542e-01],\n        [1.9310e-02],\n        [3.2079e-01],\n        [4.2148e-01],\n        [8.0445e-02],\n        [8.5937e-02],\n        [9.3669e-02],\n        [1.8645e-01],\n        [9.8083e-01],\n        [4.3263e-01],\n        [1.7070e-01],\n        [1.0533e-01],\n        [3.5415e-01],\n        [6.0060e-01],\n        [7.7559e-01],\n        [5.1420e-01],\n        [9.0432e-01],\n        [3.9692e-01],\n        [6.6026e-01],\n        [6.1780e-02],\n        [1.1240e-01],\n        [2.4442e-01],\n        [1.4858e-01],\n        [9.3005e-01],\n        [6.5793e-01],\n        [8.2776e-01],\n        [7.3702e-01],\n        [4.5390e-01],\n        [4.2654e-01],\n        [7.6769e-01],\n        [7.6479e-01],\n        [1.5651e-01],\n        [4.0410e-01],\n        [7.2284e-02],\n        [6.3024e-02],\n        [2.5806e-01],\n        [1.5592e-01],\n        [5.5513e-01],\n        [5.3050e-01],\n        [9.3949e-01],\n        [7.2322e-01],\n        [9.2010e-01],\n        [5.3690e-01],\n        [4.6464e-01],\n        [5.6838e-01],\n        [2.5354e-01],\n        [5.5642e-01],\n        [6.5851e-02],\n        [1.9760e-01],\n        [5.8078e-01],\n        [3.8148e-01],\n        [1.6590e-01],\n        [2.5032e-01],\n        [1.1477e-01],\n        [5.9273e-01],\n        [4.2225e-01],\n        [6.0284e-01],\n        [7.3756e-02],\n        [6.7438e-01],\n        [1.1236e-01],\n        [7.0039e-01],\n        [3.3120e-01],\n        [6.4324e-01],\n        [1.2605e-01],\n        [8.1833e-01],\n        [8.1225e-01],\n        [8.5613e-01],\n        [5.7160e-01],\n        [8.3530e-02],\n        [9.1431e-01],\n        [3.6423e-01],\n        [1.2398e-01],\n        [4.2834e-01],\n        [4.3880e-01],\n        [3.0699e-01],\n        [5.1058e-01],\n        [7.7657e-01],\n        [2.4941e-01],\n        [2.2120e-01],\n        [4.5667e-01],\n        [3.9957e-01],\n        [8.0880e-01],\n        [7.1123e-01],\n        [5.8210e-01],\n        [6.6115e-01],\n        [8.0549e-02],\n        [6.2569e-01],\n        [6.0800e-01],\n        [4.5505e-01],\n        [9.1097e-01],\n        [7.1683e-01],\n        [8.6330e-01],\n        [2.8315e-01],\n        [5.9156e-01],\n        [9.8282e-02],\n        [1.4577e-01],\n        [5.6502e-01],\n        [5.6335e-01],\n        [4.9166e-01],\n        [7.9069e-01],\n        [9.5383e-01],\n        [1.3854e-01],\n        [1.7082e-01]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False,  True],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 0.9303,  0.7979],\n        [-0.0770,  2.1590],\n        [ 1.3662,  0.9718],\n        ...,\n        [-2.2755,  0.9524],\n        [ 3.1071, -1.8171],\n        [ 2.4258,  0.7896]]) torch.Size([9984, 2])\nsamples tensor([[ 2.3457, -0.8343],\n        [ 1.0262,  0.6289],\n        [ 1.3049, -0.3708],\n        [ 4.9774, -0.0567],\n        [ 1.4525,  0.9430],\n        [ 1.6871, -1.3678],\n        [ 2.2811, -0.1807],\n        [ 3.4394, -0.7998],\n        [ 2.3080, -0.2831],\n        [ 1.4879,  1.7569],\n        [ 2.2915,  2.2162],\n        [ 3.6119,  0.1065],\n        [ 2.0363, -1.4478],\n        [ 1.1556,  1.1752],\n        [ 2.4251, -2.3452],\n        [-0.0636,  0.8658],\n        [ 0.2300, -0.8970],\n        [ 1.6111, -0.4922],\n        [ 3.6374,  1.2071],\n        [ 2.2254, -0.9649],\n        [ 1.0003, -0.6511],\n        [ 2.6872, -0.1290],\n        [ 3.3785,  1.1370],\n        [ 4.2680,  0.5710],\n        [ 0.7815,  0.3296],\n        [ 1.2567, -0.7687],\n        [ 2.5790, -1.6790],\n        [ 1.4138, -0.3044],\n        [ 1.4004,  1.7109],\n        [ 0.8326,  1.3832],\n        [ 4.3238, -0.3455],\n        [ 2.2222, -0.8979],\n        [ 1.5849,  0.1711],\n        [ 1.6235,  0.4108],\n        [ 3.3719, -1.0107],\n        [ 0.9424,  0.5413],\n        [ 1.1135, -0.3616],\n        [ 2.3732, -0.2094],\n        [ 0.6087, -0.5875],\n        [ 1.9397,  1.7225],\n        [ 1.6128,  0.1554],\n        [ 2.7268, -0.3394],\n        [ 3.3822, -0.3687],\n        [ 1.6698,  1.5893],\n        [ 2.9417,  0.5396],\n        [ 3.5912, -0.1703],\n        [ 1.0382, -0.8067],\n        [-1.1611, -0.4964],\n        [ 3.2822, -0.5234],\n        [ 4.3676, -2.0459],\n        [-0.7063, -0.0058],\n        [ 4.4659, -1.8122],\n        [ 0.9190,  0.1471],\n        [-0.3106, -1.5198],\n        [ 1.9215, -2.0563],\n        [ 3.9567, -0.4427],\n        [ 4.0112,  1.5687],\n        [ 4.2383, -1.5866],\n        [ 4.2867, -0.3755],\n        [ 2.5268, -0.3809],\n        [ 0.4991,  0.7938],\n        [ 4.7167,  1.7311],\n        [ 1.6038, -0.1364],\n        [ 3.7047, -1.9313],\n        [ 3.5033,  1.1635],\n        [ 2.2690, -0.2005],\n        [ 1.1836,  1.5756],\n        [ 1.2049, -0.3614],\n        [ 1.9835, -0.7566],\n        [ 1.8936, -1.2783],\n        [ 2.9745,  0.3463],\n        [ 2.9892,  1.7311],\n        [ 1.7378,  1.3411],\n        [-0.0822,  0.0763],\n        [ 2.9958,  0.2452],\n        [ 1.9906,  0.0944],\n        [ 1.2312,  0.4802],\n        [ 2.0539,  0.0050],\n        [ 2.6997, -0.1954],\n        [ 2.6936, -0.0734],\n        [ 1.5671,  0.4338],\n        [ 3.6470,  0.7231],\n        [ 3.5103,  0.1922],\n        [ 2.7410, -1.7333],\n        [ 2.0415, -0.7258],\n        [ 2.0168,  1.4122],\n        [ 2.2608,  0.9721],\n        [ 1.3366, -1.4304],\n        [ 1.7465, -0.4648],\n        [ 1.2032, -0.1526],\n        [ 3.4193, -0.7933],\n        [ 1.8681,  2.0617],\n        [ 2.3486,  0.1427],\n        [ 3.5778, -0.3105],\n        [ 1.9966,  0.9065],\n        [ 3.7107,  0.5576],\n        [ 2.3938,  1.3781],\n        [ 2.3335, -1.1695],\n        [ 2.8722, -0.7832],\n        [ 1.6022, -1.2782],\n        [ 2.1074,  1.5017],\n        [ 4.6388,  0.1096],\n        [ 1.1790,  0.1798],\n        [ 2.4030, -0.4633],\n        [ 2.6338,  0.1707],\n        [ 1.4026, -1.6054],\n        [ 3.8105, -0.1716],\n        [ 1.4600, -0.5683],\n        [ 2.7006,  1.1532],\n        [ 1.9279,  1.5400],\n        [ 1.1697, -0.3885],\n        [ 1.8320,  1.0095],\n        [ 2.1817,  1.0249],\n        [ 3.7171,  0.1429],\n        [ 1.3266,  1.0044],\n        [ 4.4805,  0.3908],\n        [ 1.7160, -1.4352],\n        [ 3.3371, -1.4333],\n        [ 2.5133, -1.3366],\n        [ 2.2456, -0.5313],\n        [ 3.7250, -0.6644],\n        [ 3.1178, -0.2853],\n        [ 3.4224, -0.6717],\n        [ 3.2173, -0.4696],\n        [ 1.4027,  0.4755],\n        [ 3.3354, -0.0253],\n        [ 1.4566,  0.7736],\n        [ 2.9095,  0.4208],\n        [ 2.2789,  1.4262],\n        [ 2.4672, -0.6897],\n        [ 1.6105,  0.4794],\n        [ 2.7485, -0.7318],\n        [ 3.2133, -0.6305],\n        [ 3.2707, -0.4538],\n        [ 4.3452,  0.7518],\n        [ 2.5872, -0.7759],\n        [ 3.3184, -0.5040],\n        [ 0.5434,  1.0246],\n        [ 1.1616, -0.4061],\n        [ 1.4767, -0.7822],\n        [ 3.8924, -1.1413],\n        [ 2.3995, -1.3147],\n        [ 0.1032, -0.7095],\n        [ 4.0366, -3.9489],\n        [ 3.5104,  1.5794],\n        [ 1.9652,  0.7101],\n        [ 2.2159, -0.4974],\n        [ 1.3154,  1.6906],\n        [ 2.4193,  0.7448],\n        [ 3.1700,  0.6798],\n        [ 4.2739,  0.0708],\n        [ 3.9036, -1.1386],\n        [ 2.2852, -0.2976],\n        [ 2.8306, -1.2885],\n        [ 2.7309,  0.4983],\n        [ 3.4583, -0.6322],\n        [ 3.1807,  0.1521],\n        [ 2.2547, -3.1438],\n        [ 2.1547,  1.0484],\n        [ 2.4811,  1.4540],\n        [ 1.1618, -1.9679],\n        [ 2.8473, -1.5171],\n        [ 3.6083, -0.6252],\n        [ 1.0108,  0.7887],\n        [ 3.5275,  1.1408],\n        [ 2.6574, -1.8183],\n        [ 3.4650,  2.2539],\n        [ 1.8399,  1.0889],\n        [ 2.4046,  1.0473],\n        [ 1.0857,  0.1136],\n        [ 2.7575,  1.1005],\n        [ 2.0920,  0.5088],\n        [ 3.7886, -0.7629],\n        [ 1.4322, -1.8309],\n        [ 0.9895, -0.4767],\n        [ 2.1359, -0.7295],\n        [ 2.2518, -1.2528],\n        [ 3.1722, -0.0629],\n        [ 2.0695, -0.6366],\n        [ 4.5688,  1.1063],\n        [ 2.5370, -1.7618],\n        [ 3.1228, -0.9317],\n        [ 1.8676,  0.5724],\n        [ 3.5526,  1.1557],\n        [ 3.4022, -3.6508],\n        [ 1.0071, -0.9635],\n        [ 1.1653, -0.4330],\n        [ 0.7136, -1.2722],\n        [ 3.9266, -1.1182],\n        [ 2.6705, -2.6509],\n        [ 0.8136, -2.2884],\n        [ 2.0903, -1.9606],\n        [ 3.2331, -3.0426],\n        [ 2.1272, -0.5005],\n        [ 1.7247, -0.6217],\n        [ 0.8664,  0.2268],\n        [ 2.2236,  0.8457],\n        [ 2.4633,  1.0288],\n        [ 1.5562,  0.6984],\n        [ 2.2342, -0.9467],\n        [ 2.8065, -0.1132],\n        [ 2.0671,  0.2796],\n        [ 1.3695,  0.2581],\n        [ 2.3462, -0.6366],\n        [ 1.6090, -1.4771],\n        [ 1.9151, -0.6460],\n        [ 1.4078, -1.4257],\n        [ 0.4631, -0.3953],\n        [ 1.4836, -0.3444],\n        [ 2.1620, -0.3480],\n        [ 0.5723,  1.5419],\n        [ 4.0884, -0.7600],\n        [ 0.5757,  0.1967],\n        [ 1.5177,  0.4948],\n        [ 1.1754, -0.1856],\n        [ 2.0358, -0.0666],\n        [ 3.0967,  0.2160],\n        [ 4.9889,  0.7010],\n        [ 2.9835,  0.1238],\n        [ 2.7618, -0.8615],\n        [ 3.1440,  0.7897],\n        [ 1.9549, -0.8055],\n        [ 1.8997, -0.4715],\n        [ 2.5426, -0.6874],\n        [ 2.4106,  1.5313],\n        [ 1.1374,  0.3602],\n        [ 1.6856, -0.2316],\n        [ 2.4479, -0.5656],\n        [ 1.6236,  1.0762],\n        [ 4.1139, -0.3494],\n        [ 3.8973, -0.3777],\n        [ 3.9068, -0.7614],\n        [ 3.6247,  0.0335],\n        [ 4.9091,  0.4348],\n        [ 4.5004,  0.0286],\n        [ 2.4822,  0.6448],\n        [ 2.5098, -0.3802],\n        [ 1.4052, -0.8661],\n        [ 2.3125,  0.0139],\n        [ 4.0272,  0.3865],\n        [ 1.2567,  2.7807],\n        [ 1.2609,  0.1185],\n        [ 3.1524,  0.2359],\n        [ 3.1990, -0.8110],\n        [ 3.0000, -0.1676],\n        [ 1.3552, -0.1595],\n        [ 3.2301, -0.3856],\n        [ 1.2554,  0.5860],\n        [ 2.5150,  1.7473],\n        [ 2.6539, -2.5281],\n        [ 2.0110, -0.9258],\n        [ 3.4101, -3.0627],\n        [ 2.0512,  1.9925],\n        [ 0.9381, -1.2794],\n        [ 1.8110,  0.6225],\n        [ 3.9626, -0.0609],\n        [ 1.7930,  1.5181],\n        [ 2.6744, -0.1783],\n        [ 3.4566, -0.6334],\n        [ 3.3113, -2.6501],\n        [ 2.1487,  0.2918],\n        [ 1.8940, -0.2569],\n        [ 1.0519, -0.9995],\n        [ 1.1471, -0.5777],\n        [ 3.9452,  0.7069],\n        [ 2.4202, -0.3590],\n        [ 2.8190,  0.2988],\n        [ 1.7910,  0.0521],\n        [ 3.7899, -0.8641],\n        [ 1.6574, -0.3977],\n        [ 2.6365, -1.0022],\n        [ 3.6528, -0.8160],\n        [ 3.8605, -0.1074],\n        [ 1.5757, -2.1420],\n        [ 0.9897, -0.1928],\n        [ 4.8392,  0.1518],\n        [ 2.2276, -0.7557],\n        [ 3.6888,  0.2933],\n        [ 3.2383,  0.1311],\n        [ 2.8987, -1.6291],\n        [ 1.7249, -0.9256],\n        [ 2.4321,  0.3025],\n        [ 3.0736,  1.4414],\n        [ 3.2322, -0.7036],\n        [ 1.5121,  0.5651],\n        [ 2.2122,  0.6929],\n        [ 2.2206,  0.4395],\n        [ 1.3385,  0.8792],\n        [ 1.5592,  0.3054],\n        [ 3.6447, -0.6356],\n        [ 2.0611,  1.4160],\n        [ 3.6631,  1.3462],\n        [ 3.2628,  1.4179],\n        [ 2.5578,  0.0170],\n        [ 2.6071,  1.9462],\n        [ 4.2352, -1.9760],\n        [ 3.2748, -1.5610],\n        [ 2.6056,  1.0544],\n        [ 2.4220, -0.4502],\n        [ 3.5368,  0.6422],\n        [ 2.8400, -2.1863],\n        [ 1.6469, -2.0553],\n        [ 2.7737, -1.1386],\n        [ 4.9488,  0.1838],\n        [ 3.2816,  0.8292],\n        [ 3.5953, -1.1235],\n        [ 2.0789, -2.3060],\n        [ 3.2081,  1.1989],\n        [ 0.6151, -1.6353],\n        [ 1.6518, -0.9949],\n        [ 1.8086, -0.2008],\n        [ 1.7282,  0.0073]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[2.7364e-06, 3.2918e-06, 1.3887e-04,  ..., 9.2989e-01, 9.6429e-01,\n         1.0000e+00],\n        [4.7252e-02, 4.7252e-02, 1.1774e-01,  ..., 9.8684e-01, 9.8684e-01,\n         1.0000e+00],\n        [7.1423e-02, 7.1452e-02, 7.1524e-02,  ..., 9.7792e-01, 9.9079e-01,\n         1.0000e+00],\n        ...,\n        [1.0652e-02, 1.0421e-01, 1.0477e-01,  ..., 9.9358e-01, 9.9421e-01,\n         1.0000e+00],\n        [8.6970e-07, 3.4477e-01, 3.4750e-01,  ..., 9.8260e-01, 9.9960e-01,\n         1.0000e+00],\n        [7.3169e-07, 1.0781e-02, 8.3317e-02,  ..., 9.9009e-01, 1.0000e+00,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[7.2410e-01],\n        [7.3140e-01],\n        [3.0839e-01],\n        [6.3693e-01],\n        [2.4748e-01],\n        [4.7156e-01],\n        [2.8385e-01],\n        [9.0308e-01],\n        [1.9355e-03],\n        [9.8726e-01],\n        [6.4838e-01],\n        [3.8005e-01],\n        [4.1623e-01],\n        [3.4537e-01],\n        [3.7655e-01],\n        [4.1583e-01],\n        [3.0857e-01],\n        [7.3752e-01],\n        [5.7410e-02],\n        [8.8475e-01],\n        [6.4767e-01],\n        [7.0290e-01],\n        [6.0624e-01],\n        [3.9668e-02],\n        [7.9491e-01],\n        [3.6950e-01],\n        [3.9633e-01],\n        [7.9018e-01],\n        [4.1755e-01],\n        [6.3118e-01],\n        [6.3712e-01],\n        [9.6587e-01],\n        [5.4270e-01],\n        [5.7773e-01],\n        [2.6888e-02],\n        [2.3635e-01],\n        [7.5510e-01],\n        [3.4481e-01],\n        [5.0092e-03],\n        [4.9879e-01],\n        [7.6212e-02],\n        [1.1098e-01],\n        [6.3234e-01],\n        [6.7083e-01],\n        [1.4824e-01],\n        [7.3943e-01],\n        [3.4263e-01],\n        [5.7433e-01],\n        [2.8761e-01],\n        [5.6730e-01],\n        [9.4041e-02],\n        [6.4244e-01],\n        [5.2330e-01],\n        [9.5706e-02],\n        [7.2351e-01],\n        [4.3631e-01],\n        [1.6683e-01],\n        [3.8986e-01],\n        [3.5924e-01],\n        [2.8140e-02],\n        [5.3944e-01],\n        [9.8688e-01],\n        [1.4887e-01],\n        [5.5890e-01],\n        [8.1442e-01],\n        [2.8027e-01],\n        [7.8937e-03],\n        [4.3768e-01],\n        [4.6871e-01],\n        [5.5616e-01],\n        [1.5822e-01],\n        [9.8843e-01],\n        [7.1579e-01],\n        [2.0453e-01],\n        [3.5735e-01],\n        [3.0425e-01],\n        [9.1826e-01],\n        [7.2403e-01],\n        [8.8980e-01],\n        [6.5273e-01],\n        [6.2962e-01],\n        [8.4859e-01],\n        [4.6554e-01],\n        [3.5309e-01],\n        [8.5385e-01],\n        [4.5564e-02],\n        [9.6205e-01],\n        [5.3957e-01],\n        [5.8793e-01],\n        [6.1591e-01],\n        [2.8928e-02],\n        [6.2062e-01],\n        [1.5421e-01],\n        [6.0873e-01],\n        [5.6428e-01],\n        [3.4976e-01],\n        [5.9171e-01],\n        [8.0753e-01],\n        [7.3113e-01],\n        [2.6317e-01],\n        [4.9392e-01],\n        [7.9036e-01],\n        [3.5089e-01],\n        [6.5996e-01],\n        [5.1934e-01],\n        [9.6382e-03],\n        [8.4940e-01],\n        [7.6270e-01],\n        [1.9398e-01],\n        [2.0297e-01],\n        [5.3737e-01],\n        [8.5077e-01],\n        [9.2426e-01],\n        [5.7255e-01],\n        [7.0507e-01],\n        [8.1141e-01],\n        [4.9238e-01],\n        [4.7668e-01],\n        [1.5045e-01],\n        [6.5500e-01],\n        [5.8245e-01],\n        [1.6561e-01],\n        [7.4098e-01],\n        [1.2756e-01],\n        [1.3657e-01],\n        [7.3694e-01],\n        [8.2506e-01],\n        [1.2410e-01],\n        [5.7952e-01],\n        [5.9498e-01],\n        [9.3138e-01],\n        [1.1895e-01],\n        [3.6058e-01],\n        [5.0956e-01],\n        [1.1398e-01],\n        [8.7724e-01],\n        [6.6923e-02],\n        [5.6221e-01],\n        [4.3433e-01],\n        [6.5741e-01],\n        [5.6521e-01],\n        [5.1663e-01],\n        [5.8323e-01],\n        [6.7124e-01],\n        [5.5396e-02],\n        [8.6117e-01],\n        [8.5096e-01],\n        [1.5706e-01],\n        [3.9863e-01],\n        [4.0497e-01],\n        [1.3265e-01],\n        [7.8560e-01],\n        [8.7103e-02],\n        [6.3039e-01],\n        [1.2734e-01],\n        [4.9616e-01],\n        [3.4135e-01],\n        [4.2412e-01],\n        [2.6773e-01],\n        [5.4293e-01],\n        [6.4039e-02],\n        [8.0594e-01],\n        [2.2945e-01],\n        [9.0374e-01],\n        [3.9321e-01],\n        [5.0867e-01],\n        [6.4143e-01],\n        [4.1177e-01],\n        [2.9432e-02],\n        [9.2663e-02],\n        [1.7127e-01],\n        [8.9677e-02],\n        [6.0988e-01],\n        [7.7746e-01],\n        [7.5642e-01],\n        [5.9786e-01],\n        [8.5551e-01],\n        [8.8502e-01],\n        [4.8192e-01],\n        [8.6457e-01],\n        [2.1176e-01],\n        [2.0058e-01],\n        [5.0197e-01],\n        [8.7698e-01],\n        [4.1152e-01],\n        [4.3931e-01],\n        [1.4247e-01],\n        [9.0939e-01],\n        [9.1107e-01],\n        [3.6306e-01],\n        [5.7478e-01],\n        [6.6120e-01],\n        [5.2104e-01],\n        [2.9795e-01],\n        [5.7628e-01],\n        [1.5151e-01],\n        [5.4756e-01],\n        [4.2449e-01],\n        [7.9860e-01],\n        [8.8196e-01],\n        [8.8557e-01],\n        [5.4377e-01],\n        [5.6654e-01],\n        [4.1858e-02],\n        [4.6154e-01],\n        [5.6387e-01],\n        [8.1393e-01],\n        [5.5202e-01],\n        [1.9778e-01],\n        [4.7218e-02],\n        [9.1986e-01],\n        [1.8983e-01],\n        [1.8929e-01],\n        [6.7415e-01],\n        [9.3171e-01],\n        [3.1257e-01],\n        [1.6888e-01],\n        [6.3862e-01],\n        [4.6881e-01],\n        [8.1923e-01],\n        [1.3225e-02],\n        [7.8015e-01],\n        [1.1721e-01],\n        [6.0018e-01],\n        [2.7089e-01],\n        [3.9319e-01],\n        [8.3839e-01],\n        [5.5523e-01],\n        [4.8753e-01],\n        [4.3553e-01],\n        [3.5350e-01],\n        [3.0368e-01],\n        [2.5468e-01],\n        [7.6817e-01],\n        [6.6179e-02],\n        [6.9423e-01],\n        [9.0479e-01],\n        [8.5239e-01],\n        [2.4682e-01],\n        [8.2064e-01],\n        [1.9972e-03],\n        [4.6821e-01],\n        [4.8636e-01],\n        [8.4696e-01],\n        [4.4743e-01],\n        [6.4257e-01],\n        [6.0901e-01],\n        [2.2033e-01],\n        [6.4793e-01],\n        [7.6111e-01],\n        [8.3951e-01],\n        [6.4989e-02],\n        [4.6715e-02],\n        [5.3125e-01],\n        [8.1563e-01],\n        [7.9609e-01],\n        [2.3318e-01],\n        [6.0671e-01],\n        [4.0770e-05],\n        [1.3643e-01],\n        [9.8774e-01],\n        [2.7342e-01],\n        [6.5860e-01],\n        [4.6580e-01],\n        [6.2642e-01],\n        [4.9026e-01],\n        [1.2639e-01],\n        [5.1742e-01],\n        [1.3747e-01],\n        [4.3061e-01],\n        [2.5987e-01],\n        [3.4893e-01],\n        [8.8125e-01],\n        [8.5189e-02],\n        [3.2402e-01],\n        [1.0781e-01],\n        [3.3275e-01],\n        [8.3320e-01],\n        [9.3263e-01],\n        [6.4145e-01],\n        [1.9598e-02],\n        [6.3697e-01],\n        [2.3139e-01],\n        [9.9086e-01],\n        [8.1893e-02],\n        [9.5421e-01],\n        [9.5129e-02],\n        [2.4622e-01],\n        [7.9369e-01],\n        [3.6806e-01],\n        [2.7474e-01],\n        [2.4580e-01],\n        [5.1981e-01],\n        [8.3429e-01],\n        [7.5507e-01],\n        [7.5237e-01],\n        [4.9439e-01],\n        [6.5312e-01],\n        [6.4704e-01],\n        [5.9037e-01],\n        [8.0689e-01],\n        [7.5269e-01],\n        [5.2062e-02],\n        [6.3599e-01],\n        [1.9239e-01],\n        [3.9173e-01],\n        [9.2849e-01],\n        [8.8729e-01],\n        [3.0711e-01],\n        [6.6435e-01],\n        [5.0910e-01],\n        [6.3608e-01]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-1.7626,  2.5913],\n        [-2.4331,  0.7854],\n        [-0.8169,  1.0625],\n        ...,\n        [ 1.8874,  1.1056],\n        [ 1.0799,  0.9373],\n        [-2.4379,  0.6437]]) torch.Size([9984, 2])\nsamples tensor([[ 4.2837e+00, -8.8502e-01],\n        [ 3.7088e+00, -8.6289e-01],\n        [ 2.4411e+00,  3.4825e-01],\n        [ 2.4020e+00, -4.6282e-01],\n        [ 3.5088e+00,  1.7409e+00],\n        [ 2.7144e+00, -7.3335e-01],\n        [ 3.1912e+00, -1.4227e+00],\n        [ 3.2789e+00, -7.6721e-01],\n        [ 3.0690e+00,  1.1631e+00],\n        [ 1.3750e+00,  7.0951e-01],\n        [ 3.2314e+00, -1.0913e+00],\n        [ 1.6146e+00,  2.8618e-01],\n        [ 4.5419e+00, -1.4912e+00],\n        [ 1.2676e+00,  1.4454e+00],\n        [ 8.8501e-01, -8.4649e-01],\n        [ 1.5234e+00,  1.5235e+00],\n        [ 3.7592e+00, -1.1930e+00],\n        [ 7.6763e-01, -6.7597e-01],\n        [ 1.9242e+00,  6.1347e-01],\n        [ 3.8475e+00, -4.0391e-01],\n        [ 2.1327e+00, -7.8380e-01],\n        [ 3.5244e+00,  8.0775e-01],\n        [ 2.8758e+00,  6.6896e-01],\n        [ 1.6114e+00,  4.5490e-01],\n        [ 8.3655e-01, -2.9318e-01],\n        [ 2.1287e+00,  4.4238e-01],\n        [ 9.6490e-01, -1.8424e+00],\n        [ 2.1408e+00,  5.1620e-01],\n        [ 2.7425e+00,  6.9456e-01],\n        [ 3.3574e+00,  8.6056e-02],\n        [ 4.1667e+00, -4.8885e-01],\n        [ 1.5175e+00,  6.1336e-01],\n        [ 2.0298e+00,  6.7576e-01],\n        [ 1.5945e+00,  7.0133e-01],\n        [ 2.2574e+00,  1.7794e+00],\n        [ 2.8596e+00, -6.3812e-01],\n        [ 2.8689e+00, -5.3144e-01],\n        [ 4.6656e+00,  3.4421e-01],\n        [ 4.4290e+00, -2.2159e+00],\n        [ 3.1167e+00, -2.6769e-01],\n        [ 2.0716e+00,  7.6597e-01],\n        [ 2.7007e+00,  8.2478e-02],\n        [ 9.5696e-01,  1.0060e+00],\n        [ 1.4845e+00, -1.7371e-01],\n        [ 3.1526e+00,  6.7023e-02],\n        [ 1.7216e+00,  1.4060e+00],\n        [ 2.1001e+00,  1.3621e+00],\n        [ 3.2781e+00,  2.5549e-01],\n        [ 3.1105e+00, -3.7236e-01],\n        [ 3.0872e+00, -2.0572e+00],\n        [ 2.4851e+00, -2.3790e-01],\n        [ 2.2349e+00, -6.1396e-01],\n        [ 3.2090e+00, -4.8643e-01],\n        [ 1.5603e+00,  6.4005e-01],\n        [ 1.4733e+00,  1.0092e-01],\n        [ 3.5731e+00, -2.4068e+00],\n        [ 2.2541e+00, -2.0405e-01],\n        [ 2.9580e+00, -1.2405e+00],\n        [ 4.7938e+00, -1.1439e+00],\n        [ 2.6669e+00,  3.5621e-01],\n        [ 1.5787e+00,  8.7527e-01],\n        [ 2.0938e+00,  7.1867e-01],\n        [ 2.6316e+00,  4.7429e-01],\n        [ 2.8005e+00, -1.7547e+00],\n        [ 1.1884e+00, -1.9101e-01],\n        [ 1.8379e+00, -1.1497e+00],\n        [ 3.0411e+00,  1.8639e+00],\n        [ 3.8670e-01, -1.4580e+00],\n        [ 3.1984e+00, -4.1440e-01],\n        [ 2.2266e+00,  1.2930e-01],\n        [ 1.7687e+00,  3.2187e-01],\n        [ 1.2318e-01, -4.8196e-01],\n        [ 1.6309e+00,  3.5821e-01],\n        [ 1.9196e+00, -9.4750e-01],\n        [ 1.0343e+00, -2.4621e+00],\n        [ 4.3197e+00, -9.3106e-02],\n        [ 4.1939e+00,  1.0713e+00],\n        [ 2.0828e+00, -5.0339e-01],\n        [ 3.0094e+00,  7.5717e-01],\n        [ 1.7112e+00, -2.8433e+00],\n        [ 2.7360e+00,  8.7167e-01],\n        [ 4.3462e+00, -2.0282e-01],\n        [ 2.0931e+00, -8.8640e-01],\n        [ 3.5893e+00,  8.9228e-01],\n        [ 1.7009e-01,  3.1368e-01],\n        [ 1.8177e+00, -5.1895e-01],\n        [ 1.8927e+00,  1.2925e+00],\n        [ 3.4733e+00, -2.2168e+00],\n        [ 3.5399e+00,  8.6467e-01],\n        [ 3.2381e+00,  1.2728e+00],\n        [ 1.6885e+00,  6.3899e-01],\n        [ 2.2693e+00, -4.7994e-01],\n        [ 2.7684e+00, -3.4902e-01],\n        [ 4.2448e+00,  1.9161e-01],\n        [ 2.3816e+00,  6.4727e-02],\n        [ 7.7277e-01, -1.4287e+00],\n        [ 1.9095e+00,  2.4817e-01],\n        [ 2.4539e+00,  1.0643e+00],\n        [ 1.9040e+00, -7.2195e-01],\n        [ 2.9784e+00, -5.1013e-01],\n        [ 2.8879e+00,  8.0244e-01],\n        [ 3.0335e+00, -2.3109e-01],\n        [ 4.1723e+00, -1.2860e+00],\n        [ 2.1918e+00,  2.6160e-01],\n        [ 1.7095e+00, -8.6072e-01],\n        [ 2.5130e+00,  1.0296e+00],\n        [ 2.4211e+00,  1.6811e+00],\n        [ 1.1142e+00,  9.9503e-01],\n        [ 4.4661e-01,  4.5632e-01],\n        [ 4.1465e+00,  7.2573e-01],\n        [ 3.3500e+00, -1.4737e+00],\n        [ 3.0711e+00, -8.0185e-01],\n        [ 2.5703e+00,  1.3293e-01],\n        [ 2.6645e+00, -7.5605e-01],\n        [ 3.1756e+00, -6.8346e-01],\n        [ 3.1965e+00, -3.2811e-02],\n        [ 2.4093e+00,  1.1926e+00],\n        [ 2.9130e+00, -1.0338e+00],\n        [ 4.1858e+00,  7.1652e-02],\n        [ 2.2339e+00, -4.8469e-01],\n        [ 2.5875e+00,  1.7717e+00],\n        [ 2.7105e+00, -3.1119e-01],\n        [ 2.3057e+00,  1.1052e+00],\n        [ 2.2752e+00, -1.5585e+00],\n        [ 2.9285e+00, -1.5220e+00],\n        [ 1.8782e+00,  7.1074e-01],\n        [ 4.7479e+00, -1.5346e+00],\n        [ 1.9519e+00, -3.3263e-01],\n        [ 3.7588e+00,  3.1470e-01],\n        [ 2.6543e+00,  4.4119e-01],\n        [ 1.8751e+00,  5.0484e-01],\n        [ 2.4796e+00,  1.0541e+00],\n        [ 2.5095e+00,  7.2458e-01],\n        [ 3.0558e+00, -8.2290e-01],\n        [ 2.4479e+00,  3.7471e-01],\n        [ 1.9585e+00,  2.6109e-01],\n        [ 2.0489e+00, -3.1786e-03],\n        [ 1.1457e+00, -2.3354e+00],\n        [ 1.1121e+00, -1.0315e+00],\n        [ 2.5100e+00, -1.3110e+00],\n        [ 4.0882e+00, -8.0192e-01],\n        [ 3.3090e+00,  3.0827e-01],\n        [ 3.0967e+00, -1.0194e+00],\n        [ 4.0827e+00,  1.6958e-01],\n        [ 1.8633e+00,  2.9419e-01],\n        [ 2.1451e+00, -1.2528e+00],\n        [ 2.5496e+00,  1.7810e-02],\n        [ 2.6286e+00, -6.2313e-02],\n        [ 2.5214e+00, -7.0783e-01],\n        [ 4.2636e+00, -3.7459e+00],\n        [ 3.6124e+00, -6.7981e-01],\n        [ 3.8471e+00, -2.3199e+00],\n        [ 1.9158e+00,  8.9943e-01],\n        [ 3.3920e+00, -2.2415e+00],\n        [ 2.7890e+00, -4.2217e-02],\n        [ 1.5315e+00, -5.6023e-01],\n        [ 1.6304e+00,  1.1108e+00],\n        [ 2.4760e+00, -1.0713e+00],\n        [ 3.7715e-01,  6.5614e-01],\n        [ 1.0620e+00,  7.3818e-02],\n        [ 1.9144e+00,  2.3884e-01],\n        [ 1.6763e+00, -5.3032e-01],\n        [ 2.5037e+00, -1.6187e-01],\n        [ 1.1676e+00, -8.9948e-01],\n        [ 4.0058e+00, -1.5817e+00],\n        [ 1.3580e+00, -1.2308e+00],\n        [ 1.3672e+00,  1.0705e-04],\n        [ 2.5982e+00, -1.6250e+00],\n        [ 1.8626e+00,  5.1887e-01],\n        [ 2.5271e+00, -3.0413e-01],\n        [ 3.3622e+00,  1.9105e-01],\n        [ 2.7455e+00,  8.0055e-01],\n        [ 2.8038e+00,  2.3278e-01],\n        [ 1.9289e+00, -1.0400e+00],\n        [ 2.1095e+00, -5.3196e-01],\n        [ 3.9527e+00,  1.9739e-01],\n        [ 1.9774e+00,  1.7029e+00],\n        [ 2.1653e+00, -3.5604e-01],\n        [ 2.0543e+00,  6.1577e-01],\n        [ 3.3565e+00, -4.7824e-01],\n        [ 2.0107e+00,  8.8817e-02],\n        [ 2.3473e+00,  1.3219e+00],\n        [ 2.6123e+00, -2.0907e+00],\n        [ 2.7751e+00, -2.1964e-01],\n        [ 2.1053e+00,  1.9737e+00],\n        [ 3.5921e+00,  2.5457e+00],\n        [ 2.7095e+00,  8.8301e-02],\n        [ 7.5951e-01, -8.7484e-01],\n        [ 2.4944e+00,  2.3068e-01],\n        [ 8.6160e-01, -6.7937e-01],\n        [ 2.3678e+00, -1.7733e+00],\n        [ 4.1572e+00, -2.5294e+00],\n        [ 4.1899e+00, -8.9525e-01],\n        [ 2.7033e+00, -5.8220e-01],\n        [ 4.0357e+00, -2.1616e+00],\n        [ 2.1100e+00,  5.1614e-01],\n        [ 4.3236e+00,  1.4941e+00],\n        [ 2.8040e+00, -5.7775e-01],\n        [ 3.5768e+00, -1.8889e+00],\n        [ 1.4704e+00,  3.8972e-01],\n        [ 2.6057e+00, -5.2659e-01],\n        [ 2.3468e+00, -1.1980e+00],\n        [-1.1353e+00, -1.1574e+00],\n        [ 1.3367e+00,  9.1802e-01],\n        [ 3.0952e+00,  1.4732e+00],\n        [ 2.4468e+00, -1.5934e-01],\n        [ 2.9537e+00, -9.3210e-01],\n        [ 2.1922e+00,  3.3052e-01],\n        [ 2.4856e+00, -1.5788e+00],\n        [ 1.7853e+00,  4.7995e-01],\n        [ 2.9445e+00, -2.4234e-01],\n        [ 2.5997e+00, -1.0246e+00],\n        [ 2.8505e+00, -3.1168e-01],\n        [ 4.2228e+00,  1.6677e+00],\n        [ 2.3577e+00,  3.8835e-01],\n        [ 1.1403e+00, -1.4540e+00],\n        [ 2.2134e+00, -1.0792e+00],\n        [ 3.0626e+00, -8.9367e-01],\n        [ 2.2456e+00, -8.2852e-01],\n        [ 3.1942e+00,  7.4540e-01],\n        [ 9.2470e-01,  1.4513e-01],\n        [ 3.4068e+00, -9.4572e-01],\n        [ 3.2136e+00, -2.5677e+00],\n        [ 3.2041e+00,  7.8558e-01],\n        [ 3.3867e+00,  4.0044e-01],\n        [ 5.2993e-01, -1.1072e-01],\n        [ 1.4150e+00,  1.4917e-01],\n        [ 1.8814e+00, -6.2556e-02],\n        [ 2.1710e+00,  1.0758e+00],\n        [ 2.0361e+00, -1.4289e+00],\n        [ 2.3804e+00, -1.5252e+00],\n        [ 1.0970e+00,  6.0492e-01],\n        [ 3.5952e+00,  1.6568e+00],\n        [ 3.1585e+00, -1.5528e+00],\n        [ 1.6696e+00,  4.9989e-01],\n        [ 2.4857e+00,  5.4938e-01],\n        [ 2.0556e+00, -1.8397e-02],\n        [ 3.2569e+00, -1.2997e+00],\n        [ 2.7988e+00, -8.1886e-01],\n        [ 2.9186e+00, -1.8050e+00],\n        [ 3.5625e-01,  3.8358e-02],\n        [ 2.1766e+00, -7.8004e-01],\n        [ 3.4470e+00, -4.3517e-01],\n        [ 3.6683e+00, -2.7028e-01],\n        [ 2.2347e+00, -2.1667e-01],\n        [ 1.9039e+00,  1.6152e+00],\n        [ 2.2902e+00,  5.7558e-01],\n        [ 2.0358e+00,  1.0975e+00],\n        [ 1.5349e+00, -1.5416e+00],\n        [ 3.3978e+00, -5.1615e-01],\n        [ 2.6682e-01, -6.5158e-01],\n        [ 1.7460e+00,  6.4705e-01],\n        [ 1.5187e+00,  4.8934e-01],\n        [ 1.6644e+00, -2.3149e+00],\n        [ 3.8759e+00,  6.2774e-01],\n        [ 1.7444e+00,  6.9348e-02],\n        [ 1.1096e+00,  4.0783e-01],\n        [ 1.2084e+00, -5.4748e-01],\n        [ 4.4783e+00, -9.8398e-01],\n        [ 3.8662e+00,  9.9942e-01],\n        [ 3.3556e+00, -2.5671e+00],\n        [ 2.6341e+00,  2.2223e-01],\n        [ 1.2991e+00, -1.1036e+00],\n        [ 2.7941e+00, -1.0916e+00],\n        [ 3.8872e+00, -1.7363e-01],\n        [ 6.4468e-01, -1.7852e+00],\n        [ 2.9923e+00,  3.9525e-01],\n        [ 2.2383e+00, -2.0806e-01],\n        [ 2.1136e+00,  3.6637e-01],\n        [ 1.1202e+00, -1.7134e+00],\n        [ 4.2076e+00, -1.6829e+00],\n        [ 1.8854e+00,  1.0684e-01],\n        [ 3.2409e+00,  5.8522e-01],\n        [ 1.9270e+00, -1.7537e+00],\n        [ 3.1608e+00, -1.3821e+00],\n        [ 1.4867e+00, -9.7866e-01],\n        [ 3.0482e+00,  3.4801e-01],\n        [ 3.6542e+00, -6.6914e-01],\n        [ 2.8808e+00,  8.2327e-01],\n        [ 1.3074e+00,  8.3312e-01],\n        [ 1.8057e+00,  5.8454e-01],\n        [ 2.7097e+00,  2.5801e-01],\n        [ 1.5205e+00,  6.4901e-01],\n        [ 6.7009e-01, -1.1546e+00],\n        [ 1.4302e+00, -1.1266e-01],\n        [ 2.9934e+00,  2.1386e+00],\n        [ 1.5739e+00, -8.4010e-01],\n        [ 2.9594e+00, -7.3117e-03],\n        [ 2.6211e+00,  2.0658e-02],\n        [ 1.5543e+00, -1.7219e+00],\n        [ 1.5260e+00,  9.1423e-01],\n        [ 3.1493e+00, -1.7991e+00],\n        [ 2.7053e+00, -1.0017e+00],\n        [ 1.9916e+00,  2.0684e-01],\n        [ 2.8538e+00, -5.1779e-01],\n        [ 1.3785e+00, -3.9736e-01],\n        [ 3.5648e+00,  1.2273e+00],\n        [ 2.4587e+00, -4.0786e-01],\n        [ 1.0408e+00,  9.1333e-01],\n        [ 1.6787e+00, -3.4521e-01],\n        [ 2.2373e+00, -1.7860e+00],\n        [ 1.2590e+00, -2.5248e+00],\n        [ 2.3394e+00,  8.6646e-01],\n        [ 2.7888e+00,  2.4309e+00],\n        [ 2.3874e+00, -6.3496e-01],\n        [ 1.9386e+00, -1.3461e+00],\n        [ 3.7233e+00, -1.6176e+00],\n        [ 2.9241e+00,  8.8637e-01],\n        [ 3.7087e+00,  6.8921e-01],\n        [ 1.6466e+00,  3.4867e-02],\n        [ 2.3546e+00, -1.7662e-01],\n        [ 6.2629e-01, -1.3923e+00]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[6.0472e-03, 6.0472e-03, 1.8282e-02,  ..., 9.7753e-01, 1.0000e+00,\n         1.0000e+00],\n        [2.5264e-02, 2.5264e-02, 3.0328e-02,  ..., 9.9719e-01, 9.9726e-01,\n         1.0000e+00],\n        [3.1696e-02, 3.1697e-02, 3.6318e-02,  ..., 8.4158e-01, 8.4255e-01,\n         1.0000e+00],\n        ...,\n        [2.6932e-05, 1.5258e-03, 1.5271e-03,  ..., 9.4119e-01, 9.4399e-01,\n         1.0000e+00],\n        [8.4984e-05, 8.4986e-03, 8.5645e-03,  ..., 9.6005e-01, 9.6698e-01,\n         1.0000e+00],\n        [2.4355e-03, 2.4365e-03, 4.8751e-02,  ..., 9.8897e-01, 1.0000e+00,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.1788],\n        [0.0315],\n        [0.5880],\n        [0.5029],\n        [0.6750],\n        [0.7497],\n        [0.5103],\n        [0.3571],\n        [0.5345],\n        [0.5693],\n        [0.8721],\n        [0.3841],\n        [0.3620],\n        [0.7376],\n        [0.7406],\n        [0.3944],\n        [0.6119],\n        [0.9727],\n        [0.8705],\n        [0.1956],\n        [0.9594],\n        [0.5227],\n        [0.3722],\n        [0.1022],\n        [0.8363],\n        [0.6765],\n        [0.9596],\n        [0.4272],\n        [0.8407],\n        [0.4164],\n        [0.4758],\n        [0.2756],\n        [0.4041],\n        [0.4850],\n        [0.8235],\n        [0.0908],\n        [0.9137],\n        [0.4827],\n        [0.8238],\n        [0.2219],\n        [0.7573],\n        [0.0437],\n        [0.9286],\n        [0.2685],\n        [0.5080],\n        [0.8729],\n        [0.7007],\n        [0.8828],\n        [0.8770],\n        [0.5064],\n        [0.0536],\n        [0.0479],\n        [0.7126],\n        [0.9087],\n        [0.8490],\n        [0.8567],\n        [0.9528],\n        [0.5650],\n        [0.9146],\n        [0.5973],\n        [0.6539],\n        [0.6711],\n        [0.8310],\n        [0.1965],\n        [0.6942],\n        [0.1559],\n        [0.2091],\n        [0.3347],\n        [0.5906],\n        [0.2044],\n        [0.5440],\n        [0.6776],\n        [0.5546],\n        [0.7445],\n        [0.6625],\n        [0.7974],\n        [0.2382],\n        [0.9677],\n        [0.5016],\n        [0.8997],\n        [0.7730],\n        [0.0026],\n        [0.6197],\n        [0.4840],\n        [0.3939],\n        [0.5931],\n        [0.6334],\n        [0.0941],\n        [0.5145],\n        [0.0306],\n        [0.7151],\n        [0.1629],\n        [0.3080],\n        [0.7008],\n        [0.3336],\n        [0.5269],\n        [0.1787],\n        [0.9140],\n        [0.0189],\n        [0.6462],\n        [0.7889],\n        [0.9286],\n        [0.2977],\n        [0.7437],\n        [0.1227],\n        [0.9533],\n        [0.0939],\n        [0.9482],\n        [0.3862],\n        [0.7337],\n        [0.3446],\n        [0.7313],\n        [0.7032],\n        [0.4136],\n        [0.4556],\n        [0.7928],\n        [0.6209],\n        [0.6573],\n        [0.5652],\n        [0.0437],\n        [0.0868],\n        [0.2978],\n        [0.1979],\n        [0.2048],\n        [0.7278],\n        [0.8711],\n        [0.2948],\n        [0.4573],\n        [0.5702],\n        [0.3052],\n        [0.2268],\n        [0.5144],\n        [0.7723],\n        [0.8219],\n        [0.9420],\n        [0.1257],\n        [0.4758],\n        [0.0643],\n        [0.6357],\n        [0.0337],\n        [0.5106],\n        [0.6359],\n        [0.3556],\n        [0.4065],\n        [0.8610],\n        [0.3671],\n        [0.6117],\n        [0.2895],\n        [0.8074],\n        [0.7910],\n        [0.1084],\n        [0.8244],\n        [0.7620],\n        [0.6178],\n        [0.5819],\n        [0.9979],\n        [0.8774],\n        [0.8153],\n        [0.0426],\n        [0.5110],\n        [0.7137],\n        [0.8256],\n        [0.5113],\n        [0.5126],\n        [0.7109],\n        [0.9386],\n        [0.4800],\n        [0.7514],\n        [0.0537],\n        [0.2397],\n        [0.3374],\n        [0.0326],\n        [0.0287],\n        [0.0132],\n        [0.4765],\n        [0.9095],\n        [0.3943],\n        [0.1644],\n        [0.4595],\n        [0.8864],\n        [0.4582],\n        [0.9269],\n        [0.6153],\n        [0.0822],\n        [0.8623],\n        [0.3076],\n        [0.9393],\n        [0.1863],\n        [0.1785],\n        [0.7381],\n        [0.5603],\n        [0.4952],\n        [0.3130],\n        [0.1637],\n        [0.7120],\n        [0.0825],\n        [0.4540],\n        [0.7800],\n        [0.2613],\n        [0.8586],\n        [0.0820],\n        [0.3502],\n        [0.1896],\n        [0.6427],\n        [0.4649],\n        [0.3638],\n        [0.0298],\n        [0.6969],\n        [0.6966],\n        [0.6685],\n        [0.0657],\n        [0.6748],\n        [0.1131],\n        [0.4724],\n        [0.4490],\n        [0.3666],\n        [0.3894],\n        [0.6276],\n        [0.2083],\n        [0.4938],\n        [0.2654],\n        [0.0448],\n        [0.2057],\n        [0.4299],\n        [0.3505],\n        [0.7821],\n        [0.5172],\n        [0.7954],\n        [0.0278],\n        [0.2304],\n        [0.2633],\n        [0.0389],\n        [0.3786],\n        [0.6766],\n        [0.3567],\n        [0.2972],\n        [0.3587],\n        [0.2001],\n        [0.0532],\n        [0.1688],\n        [0.3419],\n        [0.0303],\n        [0.1128],\n        [0.1886],\n        [0.8435],\n        [0.1118],\n        [0.5980],\n        [0.0497],\n        [0.2646],\n        [0.9386],\n        [0.4188],\n        [0.1610],\n        [0.4818],\n        [0.9443],\n        [0.2982],\n        [0.1699],\n        [0.3009],\n        [0.2174],\n        [0.1679],\n        [0.6350],\n        [0.5269],\n        [0.8346],\n        [0.3356],\n        [0.4555],\n        [0.5120],\n        [0.2818],\n        [0.7863],\n        [0.4923],\n        [0.2148],\n        [0.4060],\n        [0.6244],\n        [0.6231],\n        [0.6727],\n        [0.5720],\n        [0.4778],\n        [0.0472],\n        [0.3759],\n        [0.5495],\n        [0.0670],\n        [0.7439],\n        [0.4656],\n        [0.7313],\n        [0.0758],\n        [0.2775],\n        [0.4855],\n        [0.7933],\n        [0.1067],\n        [0.8393],\n        [0.7457],\n        [0.5475],\n        [0.2653],\n        [0.9322],\n        [0.2550],\n        [0.7143],\n        [0.6503],\n        [0.5842],\n        [0.4031],\n        [0.3067],\n        [0.3200],\n        [0.9231],\n        [0.0517],\n        [0.4561],\n        [0.8432],\n        [0.4019],\n        [0.8237],\n        [0.4550],\n        [0.7170],\n        [0.8336],\n        [0.7921],\n        [0.5518],\n        [0.1360],\n        [0.3981]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 1.7216,  2.4647],\n        [-3.9924,  0.6029],\n        [ 0.6497,  0.3499],\n        ...,\n        [-1.9344,  1.8395],\n        [ 2.1071,  1.8656],\n        [-3.1773,  2.1948]]) torch.Size([9984, 2])\nsamples tensor([[ 1.6437e+00, -7.4844e-01],\n        [ 7.7750e-01,  1.9505e-01],\n        [ 2.4083e+00,  4.2907e-01],\n        [ 1.8980e+00,  8.1617e-01],\n        [ 2.2277e+00,  1.2194e+00],\n        [ 2.2840e+00, -3.8158e-01],\n        [ 1.8336e+00, -4.4810e-01],\n        [ 2.8606e+00,  1.2067e+00],\n        [ 3.8168e+00, -1.7247e+00],\n        [ 2.7537e+00, -8.2532e-01],\n        [ 1.5042e+00,  2.2845e+00],\n        [ 2.5564e+00, -4.8896e-01],\n        [ 3.8644e+00,  1.5093e+00],\n        [ 1.2489e+00,  7.5349e-01],\n        [ 6.0983e-01,  4.8856e-01],\n        [ 3.0382e+00, -2.8588e-01],\n        [ 2.9154e+00, -1.8896e+00],\n        [ 3.3728e+00, -1.6329e+00],\n        [ 3.8120e+00,  4.2314e-01],\n        [ 3.3034e+00, -8.9001e-01],\n        [ 3.8010e-01,  4.3964e-01],\n        [ 1.2680e+00,  2.6291e-01],\n        [ 3.5559e+00,  4.1745e-01],\n        [ 3.0805e+00, -6.2923e-02],\n        [ 2.0241e+00, -6.8315e-01],\n        [ 1.9131e+00, -1.5707e+00],\n        [ 5.7578e-01,  4.6773e-01],\n        [ 2.3190e+00, -1.1517e+00],\n        [ 1.3415e+00,  7.2033e-01],\n        [ 2.5161e+00, -8.7687e-01],\n        [ 2.1139e-01,  8.1594e-01],\n        [ 3.3201e+00, -1.1507e-01],\n        [ 3.7660e+00, -2.7799e-01],\n        [ 2.5221e+00,  1.1301e+00],\n        [ 9.2511e-01,  1.3128e+00],\n        [ 2.5288e+00, -5.6627e-01],\n        [ 4.0670e+00, -1.2270e-01],\n        [ 3.4439e+00, -1.9035e+00],\n        [ 2.8549e+00,  1.1961e+00],\n        [ 1.4883e+00,  5.1012e-01],\n        [ 3.4393e+00, -7.9757e-02],\n        [ 5.0440e-01, -3.4878e-01],\n        [ 2.3954e+00,  9.0329e-01],\n        [ 2.8726e+00, -1.5621e+00],\n        [ 1.8247e+00, -1.3021e+00],\n        [ 3.0519e+00,  1.9824e+00],\n        [ 2.2273e+00,  1.0109e-01],\n        [ 2.9438e+00,  4.9102e-01],\n        [ 3.4565e+00, -3.2245e-01],\n        [ 2.1923e+00, -4.3758e-01],\n        [ 1.0316e+00,  6.8893e-01],\n        [ 1.2575e+00, -1.7783e-01],\n        [ 1.8382e+00,  3.2897e-01],\n        [ 2.0152e+00, -1.0906e-01],\n        [ 2.8011e+00, -1.2350e+00],\n        [ 9.1763e-01,  2.3025e-02],\n        [ 2.3840e+00,  1.5419e+00],\n        [ 3.0841e+00,  8.0527e-01],\n        [ 2.1941e+00,  6.1635e-01],\n        [ 3.7742e+00, -5.8988e-02],\n        [ 3.1509e+00,  2.2053e-01],\n        [ 1.7170e+00, -4.2134e-01],\n        [ 4.8933e+00, -9.4426e-01],\n        [ 1.8445e+00, -1.0404e+00],\n        [ 1.5697e+00, -3.5241e-01],\n        [ 1.2414e+00,  2.8389e-01],\n        [ 2.4232e+00,  8.3230e-01],\n        [ 2.3596e+00,  1.3646e-01],\n        [ 2.0809e+00, -8.1779e-01],\n        [ 1.2002e+00,  1.6548e-01],\n        [ 2.1831e+00, -1.4239e+00],\n        [ 3.9798e+00, -6.7868e-01],\n        [ 8.5204e-01, -1.8139e+00],\n        [ 8.7579e-01,  7.3337e-01],\n        [ 4.3356e+00, -6.5386e-01],\n        [ 3.0279e+00,  1.3220e-01],\n        [ 3.2236e+00,  8.6904e-01],\n        [ 4.2454e+00,  1.5770e-01],\n        [ 7.5806e-01, -1.3676e+00],\n        [ 2.8381e+00,  7.8230e-01],\n        [ 2.6310e+00, -7.6313e-01],\n        [ 1.1282e+00,  1.0501e+00],\n        [ 4.6889e+00,  2.8726e-01],\n        [ 1.6515e-01,  4.9863e-01],\n        [ 2.1642e+00, -2.9246e-01],\n        [ 2.0228e+00,  4.2385e-01],\n        [ 2.9804e+00, -1.2911e-01],\n        [ 1.1813e+00,  1.5653e-01],\n        [ 2.2883e+00, -6.9952e-01],\n        [-3.5305e-01,  5.0417e-01],\n        [ 2.0247e+00, -2.9619e-01],\n        [ 1.8288e+00, -2.6634e-02],\n        [ 1.1263e+00, -1.3439e+00],\n        [ 3.0255e+00, -1.2209e+00],\n        [ 2.7358e+00,  3.7883e-01],\n        [ 2.7303e+00,  3.0181e-01],\n        [ 2.2188e+00, -3.8960e-01],\n        [ 1.5069e+00, -1.1648e+00],\n        [ 1.8951e+00,  1.6951e+00],\n        [ 3.6373e+00,  5.8200e-01],\n        [ 2.0365e+00, -3.6653e-01],\n        [ 3.3840e+00,  3.3859e-01],\n        [ 2.2767e+00, -2.7094e+00],\n        [ 3.8348e+00, -1.1632e+00],\n        [ 2.6706e+00, -8.5457e-02],\n        [ 9.7629e-01, -6.1804e-01],\n        [ 3.8246e+00, -2.6070e-01],\n        [ 3.1157e+00,  1.1327e+00],\n        [ 2.7359e+00,  1.2829e+00],\n        [ 1.4105e+00,  8.0641e-01],\n        [ 1.9642e+00, -1.6932e+00],\n        [ 4.5627e+00, -5.1475e-01],\n        [ 3.2860e+00,  3.4386e-01],\n        [ 4.4158e+00,  1.1984e+00],\n        [ 1.5600e+00,  1.0283e+00],\n        [ 2.2352e+00, -2.0752e+00],\n        [ 1.6618e+00, -1.0231e+00],\n        [ 1.3492e+00,  6.3629e-01],\n        [ 2.0871e+00, -8.7292e-01],\n        [ 3.8643e+00,  1.7497e+00],\n        [ 1.5292e+00, -3.1165e-01],\n        [ 2.0478e+00, -1.1692e+00],\n        [ 3.1307e+00, -4.8718e-02],\n        [ 2.5946e+00, -1.2774e+00],\n        [ 1.7985e+00, -2.3544e-01],\n        [ 1.3625e+00,  3.1613e-01],\n        [ 2.4219e+00, -6.3253e-01],\n        [ 1.3118e+00, -7.3776e-01],\n        [ 3.5898e+00, -1.9010e+00],\n        [ 4.0840e+00, -1.9450e-01],\n        [ 2.8415e+00,  1.2238e+00],\n        [ 2.4627e+00, -5.5118e-01],\n        [ 1.9770e+00,  4.3574e-01],\n        [ 1.6805e+00, -9.9546e-01],\n        [ 1.2663e+00, -7.4480e-01],\n        [ 1.9412e+00,  4.3630e-01],\n        [ 1.4192e+00, -1.7485e+00],\n        [ 3.0892e+00,  1.2906e+00],\n        [ 1.5571e+00, -5.7388e-01],\n        [ 9.1817e-01,  2.8559e-01],\n        [ 1.6927e+00, -1.8258e+00],\n        [ 3.0860e+00, -1.4765e+00],\n        [ 1.5803e+00, -1.4105e-02],\n        [ 2.8034e+00, -7.0490e-01],\n        [ 8.2564e-01, -3.0131e-01],\n        [ 3.3134e+00,  7.8028e-01],\n        [ 3.4927e+00, -1.3787e+00],\n        [ 2.6722e+00, -2.0291e+00],\n        [ 1.3258e+00, -4.1716e-01],\n        [ 3.2507e+00, -5.9782e-02],\n        [ 1.7739e+00, -1.9490e-01],\n        [ 2.5942e+00,  2.2725e-01],\n        [ 2.3826e+00, -2.0215e-02],\n        [ 1.8143e+00,  1.0633e+00],\n        [ 4.4219e+00, -9.9515e-01],\n        [ 1.6195e+00,  9.8776e-02],\n        [ 2.7949e+00,  8.6179e-01],\n        [ 3.0790e+00, -1.2457e+00],\n        [ 1.8067e+00,  6.4060e-01],\n        [ 3.0672e+00, -3.6129e-01],\n        [ 1.6318e+00,  6.1121e-01],\n        [ 1.3657e+00, -2.8513e-02],\n        [ 3.1174e+00, -1.5957e+00],\n        [ 2.5562e+00,  6.5907e-01],\n        [ 4.0404e+00, -1.5461e+00],\n        [ 4.3936e+00, -8.8490e-01],\n        [ 2.3866e+00, -9.8948e-02],\n        [ 3.8538e+00,  3.6338e-01],\n        [ 3.1625e+00,  1.5186e+00],\n        [ 1.6886e+00, -7.0487e-01],\n        [ 2.0441e+00, -1.6745e-01],\n        [ 4.0776e+00, -3.1471e-01],\n        [ 3.2059e+00, -8.8394e-01],\n        [ 1.8375e+00,  3.2230e-01],\n        [ 2.4960e+00,  4.6508e-01],\n        [ 2.6857e+00, -1.4376e+00],\n        [ 2.9917e+00,  2.1193e+00],\n        [ 2.1420e+00,  2.9615e-01],\n        [ 9.6490e-01, -9.8345e-01],\n        [ 2.6779e+00, -2.3251e-01],\n        [ 2.0082e+00, -5.9715e-01],\n        [ 3.3140e+00,  2.9383e-01],\n        [ 2.4565e+00, -2.0462e+00],\n        [ 2.1203e+00,  2.7738e-01],\n        [ 3.4424e+00, -4.4378e-01],\n        [ 1.4600e+00, -1.4632e+00],\n        [ 2.3316e+00,  5.5212e-01],\n        [ 2.0336e+00, -1.5060e-01],\n        [ 1.9142e+00, -4.9896e-01],\n        [ 4.9282e+00, -8.4021e-01],\n        [ 1.7295e+00,  2.3404e-01],\n        [ 3.4586e+00,  3.3770e-01],\n        [ 2.2184e+00,  4.8176e-01],\n        [ 3.5994e+00, -4.5049e-01],\n        [ 3.8096e+00, -5.6496e-01],\n        [ 1.6036e+00,  8.5717e-02],\n        [ 1.3633e+00, -1.9531e+00],\n        [ 2.2022e+00, -1.1086e-01],\n        [ 4.2789e+00,  8.7351e-02],\n        [ 2.0651e+00, -5.0064e-01],\n        [ 2.5365e+00, -1.3938e+00],\n        [ 2.2453e+00, -6.1086e-01],\n        [ 3.5108e+00, -1.7948e+00],\n        [ 2.8420e+00,  2.6300e+00],\n        [ 1.3966e+00,  8.5309e-01],\n        [ 2.1099e+00,  1.7433e+00],\n        [ 2.9270e+00, -1.6875e-01],\n        [ 4.4542e-01, -2.0865e+00],\n        [ 1.3065e+00,  9.2941e-02],\n        [ 2.9367e+00,  1.6111e+00],\n        [ 2.3105e+00,  3.4311e-01],\n        [ 6.9174e-01,  1.3168e-02],\n        [ 3.0718e+00,  1.2363e+00],\n        [ 5.3286e-01,  5.6079e-01],\n        [ 3.8179e+00,  4.5692e-01],\n        [ 2.2750e+00, -7.2733e-01],\n        [ 2.2142e+00, -8.3803e-01],\n        [ 1.1800e+00, -4.1098e-01],\n        [ 3.7552e+00,  4.4848e-01],\n        [ 1.5942e+00, -1.4880e+00],\n        [ 2.7175e+00, -3.2825e-01],\n        [ 2.3663e+00,  9.6473e-01],\n        [ 3.6263e+00,  1.5845e+00],\n        [ 3.4163e+00, -2.0853e-01],\n        [ 1.4394e+00,  7.5855e-01],\n        [ 1.9666e+00, -3.6441e-01],\n        [ 3.7503e+00, -1.6680e-01],\n        [ 2.3445e+00, -9.6749e-01],\n        [ 3.2293e+00, -7.4986e-01],\n        [ 2.6143e+00,  1.2320e+00],\n        [ 1.7346e+00, -9.4639e-01],\n        [ 2.5089e+00, -6.6315e-02],\n        [ 2.0065e+00, -4.3757e-01],\n        [ 2.7247e+00,  3.6275e-01],\n        [ 4.2996e+00, -8.5383e-01],\n        [ 2.8253e+00, -3.7046e-01],\n        [ 2.6905e+00,  5.5180e-01],\n        [ 2.7842e+00, -3.9170e-01],\n        [ 2.1701e+00,  4.4063e-01],\n        [ 2.7426e+00, -7.8742e-02],\n        [ 1.8263e+00,  9.3675e-01],\n        [ 1.4849e+00, -1.3374e-01],\n        [ 3.4245e+00, -2.0739e+00],\n        [ 4.5409e+00,  1.5119e-02],\n        [ 2.2470e+00, -2.8184e-01],\n        [ 2.2247e+00, -1.7873e-01],\n        [ 3.3136e+00,  1.0288e+00],\n        [ 1.6108e+00,  4.4682e-01],\n        [ 3.9134e+00, -1.1257e+00],\n        [ 2.9378e+00,  8.8820e-01],\n        [ 2.5173e+00,  1.6977e-02],\n        [ 2.5058e+00,  1.4311e+00],\n        [ 2.3559e+00,  1.6702e-01],\n        [ 3.1017e+00, -8.6207e-01],\n        [ 2.9962e+00, -1.4620e-01],\n        [ 1.8722e+00, -5.1925e-01],\n        [ 2.8121e+00, -2.2185e+00],\n        [ 1.5823e+00, -1.2643e-01],\n        [ 1.7340e+00, -2.6859e-01],\n        [ 3.7766e+00, -1.3757e+00],\n        [ 8.1859e-01,  4.3183e-01],\n        [ 2.1376e+00, -2.3597e-01],\n        [ 3.7650e+00, -1.1217e+00],\n        [ 2.9830e+00, -2.4547e+00],\n        [ 1.1424e+00, -1.2870e+00],\n        [ 2.7281e+00,  9.9171e-01],\n        [ 4.3198e+00,  8.7906e-01],\n        [ 2.5075e+00, -1.7945e+00],\n        [ 3.7235e+00, -1.7012e+00],\n        [ 7.3998e-01, -6.9610e-01],\n        [ 2.1621e+00, -8.2707e-01],\n        [ 3.5142e+00, -1.1992e+00],\n        [ 2.1270e-01,  1.2673e+00],\n        [ 1.5767e+00,  5.8122e-01],\n        [ 4.6978e+00,  1.3795e-01],\n        [ 7.9195e-01,  1.4828e-01],\n        [ 1.4151e+00,  6.9674e-01],\n        [ 1.2125e+00, -1.0282e+00],\n        [ 1.2214e+00,  1.8473e-03],\n        [ 3.0564e+00, -8.8628e-01],\n        [ 1.4226e+00,  1.4861e-01],\n        [ 1.9247e+00, -1.7999e-02],\n        [ 1.3163e+00,  1.0148e+00],\n        [ 2.3334e+00,  6.3973e-02],\n        [ 1.4075e+00, -1.4631e+00],\n        [ 1.5970e+00,  1.4726e-02],\n        [ 1.4320e+00,  7.9244e-01],\n        [ 3.4438e+00,  1.4105e-01],\n        [ 1.6273e+00,  1.4173e+00],\n        [ 3.3910e+00, -6.8120e-02],\n        [ 3.1721e+00, -1.0727e+00],\n        [ 2.0858e+00, -1.3484e+00],\n        [ 2.6880e+00,  1.2567e+00],\n        [ 3.7773e+00,  1.0677e+00],\n        [ 3.0011e+00, -4.3459e-02],\n        [ 3.2131e+00,  8.0539e-01],\n        [ 3.2038e+00, -1.0171e+00],\n        [ 1.8982e+00,  2.1511e-01],\n        [ 3.1325e+00, -8.2946e-01],\n        [ 1.2064e+00, -6.7851e-02],\n        [ 3.7280e+00,  9.0026e-01],\n        [ 2.4264e+00,  7.1333e-01],\n        [ 3.9168e+00, -1.1046e+00],\n        [ 3.3685e+00, -1.8537e-01],\n        [ 3.0096e+00, -1.5759e+00],\n        [ 2.8632e+00, -9.9409e-01],\n        [ 2.1603e+00,  6.3715e-01],\n        [ 2.5074e+00,  1.1188e+00],\n        [ 2.4751e+00,  2.4049e-01],\n        [ 2.4080e+00, -5.2561e-01],\n        [ 2.8650e+00,  7.4574e-01],\n        [ 2.3519e+00, -8.0385e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[6.1636e-03, 1.6784e-02, 1.6850e-02,  ..., 9.8934e-01, 9.9642e-01,\n         1.0000e+00],\n        [6.5194e-04, 3.5668e-03, 3.6117e-03,  ..., 9.7318e-01, 9.9998e-01,\n         1.0000e+00],\n        [6.9815e-02, 7.2701e-02, 3.0898e-01,  ..., 9.8129e-01, 9.9892e-01,\n         1.0000e+00],\n        ...,\n        [2.6592e-06, 5.6185e-02, 1.0586e-01,  ..., 8.5868e-01, 8.5868e-01,\n         1.0000e+00],\n        [4.4585e-02, 4.4585e-02, 1.9517e-01,  ..., 9.9787e-01, 9.9914e-01,\n         1.0000e+00],\n        [7.5458e-02, 1.6915e-01, 2.1447e-01,  ..., 7.9696e-01, 9.9751e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.8435],\n        [0.4485],\n        [0.8962],\n        [0.4267],\n        [0.4878],\n        [0.2262],\n        [0.0619],\n        [0.9569],\n        [0.3586],\n        [0.3820],\n        [0.8918],\n        [0.8782],\n        [0.3504],\n        [0.1298],\n        [0.7344],\n        [0.1334],\n        [0.4679],\n        [0.8437],\n        [0.9366],\n        [0.8777],\n        [0.6276],\n        [0.1286],\n        [0.4840],\n        [0.3697],\n        [0.9808],\n        [0.9024],\n        [0.8468],\n        [0.2197],\n        [0.0633],\n        [0.3518],\n        [0.2854],\n        [0.7017],\n        [0.2884],\n        [0.2103],\n        [0.5870],\n        [0.6304],\n        [0.2251],\n        [0.0774],\n        [0.2870],\n        [0.6932],\n        [0.3193],\n        [0.0402],\n        [0.9686],\n        [0.4886],\n        [0.5439],\n        [0.0409],\n        [0.7419],\n        [0.2753],\n        [0.9132],\n        [0.7007],\n        [0.3024],\n        [0.8726],\n        [0.6682],\n        [0.9887],\n        [0.5707],\n        [0.2724],\n        [0.9606],\n        [0.6425],\n        [0.5627],\n        [0.4195],\n        [0.5446],\n        [0.7668],\n        [0.5673],\n        [0.0399],\n        [0.4014],\n        [0.8697],\n        [0.0657],\n        [0.9340],\n        [0.0859],\n        [0.5482],\n        [0.0751],\n        [0.6760],\n        [0.1341],\n        [0.7190],\n        [0.0584],\n        [0.9557],\n        [0.0651],\n        [0.3217],\n        [0.2046],\n        [0.1919],\n        [0.7569],\n        [0.2527],\n        [0.1728],\n        [0.7594],\n        [0.9525],\n        [0.7791],\n        [0.4512],\n        [0.8157],\n        [0.3979],\n        [0.3988],\n        [0.7562],\n        [0.4522],\n        [0.1109],\n        [0.1841],\n        [0.7176],\n        [0.3975],\n        [0.8529],\n        [0.4454],\n        [0.1256],\n        [0.6627],\n        [0.8022],\n        [0.8365],\n        [0.7997],\n        [0.6245],\n        [0.9230],\n        [0.2552],\n        [0.4531],\n        [0.7769],\n        [0.5309],\n        [0.0470],\n        [0.5554],\n        [0.8117],\n        [0.9564],\n        [0.3774],\n        [0.5231],\n        [0.6251],\n        [0.8625],\n        [0.2458],\n        [0.9808],\n        [0.2850],\n        [0.9560],\n        [0.6140],\n        [0.6345],\n        [0.6957],\n        [0.9393],\n        [0.5791],\n        [0.9448],\n        [0.2374],\n        [0.0365],\n        [0.8105],\n        [0.9158],\n        [0.1933],\n        [0.1134],\n        [0.0793],\n        [0.2920],\n        [0.0632],\n        [0.0056],\n        [0.8578],\n        [0.3031],\n        [0.3843],\n        [0.7664],\n        [0.7579],\n        [0.6757],\n        [0.3574],\n        [0.3375],\n        [0.8272],\n        [0.2720],\n        [0.6348],\n        [0.2616],\n        [0.2452],\n        [0.6913],\n        [0.1239],\n        [0.8491],\n        [0.0445],\n        [0.6109],\n        [0.5810],\n        [0.6908],\n        [0.7668],\n        [0.4827],\n        [0.5119],\n        [0.7805],\n        [0.7910],\n        [0.7156],\n        [0.9705],\n        [0.6035],\n        [0.3595],\n        [0.4488],\n        [0.7831],\n        [0.3677],\n        [0.0535],\n        [0.6372],\n        [0.2900],\n        [0.3048],\n        [0.2933],\n        [0.7434],\n        [0.2356],\n        [0.9283],\n        [0.1911],\n        [0.7423],\n        [0.6344],\n        [0.0143],\n        [0.3658],\n        [0.2819],\n        [0.6924],\n        [0.8601],\n        [0.3807],\n        [0.9741],\n        [0.1993],\n        [0.1641],\n        [0.4688],\n        [0.3770],\n        [0.3167],\n        [0.3112],\n        [0.0142],\n        [0.9725],\n        [0.4501],\n        [0.4075],\n        [0.1331],\n        [0.6897],\n        [0.1561],\n        [0.6601],\n        [0.2602],\n        [0.9405],\n        [0.0692],\n        [0.9502],\n        [0.8044],\n        [0.2823],\n        [0.8450],\n        [0.3080],\n        [0.2028],\n        [0.3314],\n        [0.8544],\n        [0.7840],\n        [0.6097],\n        [0.7162],\n        [0.1354],\n        [0.6373],\n        [0.5098],\n        [0.4347],\n        [0.6930],\n        [0.5354],\n        [0.5293],\n        [0.4095],\n        [0.4049],\n        [0.5996],\n        [0.1079],\n        [0.1195],\n        [0.3514],\n        [0.9569],\n        [0.5639],\n        [0.0424],\n        [0.1096],\n        [0.2538],\n        [0.8935],\n        [0.6541],\n        [0.0048],\n        [0.3147],\n        [0.5516],\n        [0.2429],\n        [0.2892],\n        [0.6549],\n        [0.6725],\n        [0.5476],\n        [0.5760],\n        [0.6758],\n        [0.6368],\n        [0.1396],\n        [0.9985],\n        [0.1789],\n        [0.1620],\n        [0.0844],\n        [0.8396],\n        [0.4153],\n        [0.5326],\n        [0.9896],\n        [0.2971],\n        [0.6903],\n        [0.7774],\n        [0.2419],\n        [0.5934],\n        [0.6998],\n        [0.2347],\n        [0.7215],\n        [0.9314],\n        [0.5709],\n        [0.6539],\n        [0.9555],\n        [0.7955],\n        [0.1825],\n        [0.9211],\n        [0.9816],\n        [0.0500],\n        [0.7112],\n        [0.7878],\n        [0.8319],\n        [0.8756],\n        [0.7133],\n        [0.9518],\n        [0.6066],\n        [0.5452],\n        [0.1872],\n        [0.8590],\n        [0.2236],\n        [0.6182],\n        [0.7396],\n        [0.5262],\n        [0.7879],\n        [0.9506],\n        [0.0798],\n        [0.6786],\n        [0.9899],\n        [0.5460],\n        [0.8091],\n        [0.9249],\n        [0.1499],\n        [0.1756],\n        [0.7952],\n        [0.7772],\n        [0.5291],\n        [0.8490],\n        [0.1693],\n        [0.8970],\n        [0.2496],\n        [0.5876],\n        [0.8337],\n        [0.2728],\n        [0.0012],\n        [0.3817],\n        [0.8781],\n        [0.8716],\n        [0.2743],\n        [0.0592]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False,  True],\n        [False, False, False,  ..., False, False, False],\n        [ True, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 0.1099,  0.1492],\n        [ 1.6421,  2.0162],\n        [-1.0607,  1.5947],\n        ...,\n        [-4.0304,  0.5826],\n        [ 2.2170, -0.5443],\n        [ 0.1219,  0.4578]]) torch.Size([9984, 2])\nsamples tensor([[ 2.9657e+00, -2.2872e+00],\n        [ 2.3530e+00,  4.3963e-01],\n        [ 1.1388e+00, -2.3566e-01],\n        [ 3.1339e+00, -1.0403e+00],\n        [ 3.0376e+00,  7.9671e-01],\n        [ 3.5525e+00, -1.4021e+00],\n        [ 4.3002e-01,  1.7392e+00],\n        [ 2.3270e+00, -1.9986e+00],\n        [ 2.1370e+00, -8.1543e-01],\n        [ 1.7168e+00,  5.7614e-01],\n        [ 3.2795e+00,  7.5839e-02],\n        [ 3.4566e+00,  8.1547e-01],\n        [ 1.5234e+00, -1.3324e-01],\n        [ 4.0582e+00, -1.4643e+00],\n        [ 3.1957e+00,  1.8051e+00],\n        [ 2.4840e+00,  1.4400e+00],\n        [ 1.8162e+00,  1.4347e+00],\n        [ 4.1521e+00,  2.1002e-01],\n        [ 3.4108e+00, -4.5181e-03],\n        [ 5.1155e-01, -8.4804e-01],\n        [ 2.1484e+00, -2.2702e+00],\n        [ 1.4928e+00, -3.7655e-01],\n        [ 3.1368e+00, -2.3925e-01],\n        [ 3.2901e+00, -7.0252e-01],\n        [ 7.6993e-01, -7.5179e-01],\n        [ 1.9462e+00, -2.4434e-01],\n        [ 3.0506e+00,  8.3386e-01],\n        [ 2.9336e+00, -1.4991e+00],\n        [ 3.4457e+00,  6.7513e-02],\n        [ 2.0453e+00,  2.9507e-01],\n        [ 3.6458e+00, -7.4538e-01],\n        [ 2.2230e+00,  8.7649e-01],\n        [ 1.1389e+00, -3.4560e-01],\n        [ 7.8356e-01,  1.9940e+00],\n        [ 2.2975e+00, -7.7888e-01],\n        [ 3.1538e+00, -1.0969e+00],\n        [ 2.8097e+00,  1.1846e+00],\n        [ 1.5646e+00,  2.5253e+00],\n        [ 2.7589e+00, -2.7790e-01],\n        [ 2.3286e+00, -3.8714e-01],\n        [ 3.5047e+00, -2.5401e-01],\n        [ 1.7483e+00,  1.1951e+00],\n        [ 9.4268e-01,  7.6729e-02],\n        [ 1.2216e+00,  1.5396e+00],\n        [ 2.0324e+00, -1.0091e+00],\n        [ 2.0018e+00,  4.5439e-01],\n        [ 2.7107e+00, -1.7071e+00],\n        [ 3.8258e+00, -1.9209e+00],\n        [ 2.5283e+00,  1.3017e+00],\n        [ 3.8680e+00, -6.8939e-01],\n        [ 4.2915e+00, -1.3337e+00],\n        [ 1.0993e+00,  8.5924e-01],\n        [ 3.1021e+00, -7.2094e-01],\n        [ 2.3822e+00,  1.8721e+00],\n        [ 4.4876e+00,  1.2056e+00],\n        [ 4.1170e+00, -1.1943e+00],\n        [ 1.8479e+00,  4.7472e-01],\n        [ 1.6388e+00,  1.2324e+00],\n        [ 2.2607e+00,  6.5706e-01],\n        [ 2.5680e+00, -2.4062e-01],\n        [ 4.5750e+00, -1.6723e+00],\n        [ 2.6623e+00, -1.8610e+00],\n        [ 1.0512e+00,  1.2274e+00],\n        [ 2.5989e+00,  5.5973e-01],\n        [ 2.1530e+00, -4.5815e-01],\n        [ 2.7300e+00,  6.9093e-01],\n        [ 6.4152e-01,  9.2202e-02],\n        [ 3.0435e+00,  3.9221e-01],\n        [ 3.1845e+00,  3.2683e-01],\n        [ 2.2725e+00, -6.2572e-01],\n        [ 3.1611e+00, -1.7398e+00],\n        [ 1.1645e+00,  6.2799e-02],\n        [ 1.9492e+00, -2.9392e-01],\n        [ 4.3089e+00, -2.5495e+00],\n        [ 8.0646e-01,  1.5030e+00],\n        [ 1.9281e+00, -7.7953e-01],\n        [ 2.2634e+00,  1.4472e+00],\n        [ 2.9961e+00,  1.3259e+00],\n        [ 3.1255e+00, -7.8176e-01],\n        [ 4.1665e+00, -7.9732e-01],\n        [ 2.5931e+00,  3.1515e-02],\n        [ 2.7059e+00, -5.7742e-01],\n        [ 1.9162e+00, -1.0909e+00],\n        [ 2.7963e+00, -3.5605e-01],\n        [ 1.5620e+00,  2.0002e-03],\n        [ 4.0743e+00, -3.6243e-01],\n        [ 2.5128e+00, -4.5114e-01],\n        [ 3.3915e+00,  1.6647e-01],\n        [ 2.7303e+00,  7.6949e-01],\n        [ 3.9574e+00,  6.8896e-01],\n        [ 2.9409e+00, -3.5249e-01],\n        [ 2.9053e+00, -9.1766e-01],\n        [ 7.3981e-01, -7.2854e-01],\n        [ 1.4433e+00,  7.6577e-01],\n        [ 1.7906e+00, -1.6196e+00],\n        [ 8.5764e-01, -1.2297e+00],\n        [ 2.1775e+00,  1.7914e-01],\n        [ 1.7538e+00, -1.5727e+00],\n        [ 2.1101e+00, -5.7969e-01],\n        [ 1.8247e+00, -1.8827e+00],\n        [ 2.2812e+00, -1.6893e-01],\n        [ 3.5136e+00,  3.0335e-01],\n        [ 2.0143e+00,  9.5310e-01],\n        [ 2.4745e+00, -3.9636e-01],\n        [ 2.4381e+00,  4.8259e-01],\n        [ 4.3644e+00, -5.8027e-01],\n        [ 3.3497e+00, -7.7300e-01],\n        [ 2.9026e+00,  3.7432e-01],\n        [ 2.6809e+00, -1.3215e+00],\n        [ 2.8784e+00, -4.9218e-01],\n        [ 1.7801e+00,  6.2658e-01],\n        [ 3.0323e-01, -5.1247e-01],\n        [ 1.8107e+00, -6.0530e-01],\n        [ 3.0382e+00,  1.4276e+00],\n        [ 2.5316e+00, -4.2038e-01],\n        [ 2.2184e+00, -2.0604e-02],\n        [ 2.7242e+00, -3.8620e-01],\n        [ 3.6129e+00, -1.0842e+00],\n        [ 2.5676e+00,  2.5641e-01],\n        [ 2.1661e+00, -1.1357e+00],\n        [ 4.3436e-01, -5.4120e-01],\n        [ 2.7900e+00, -2.2682e+00],\n        [ 1.5614e+00, -1.5451e+00],\n        [ 2.3727e+00, -9.9162e-01],\n        [ 9.5625e-01, -7.2706e-01],\n        [ 8.0146e-01, -1.2978e-01],\n        [ 1.0081e+00,  8.1571e-01],\n        [ 2.6416e+00, -9.8118e-01],\n        [ 2.3898e+00, -3.5043e-01],\n        [ 1.8701e+00,  4.7798e-01],\n        [ 1.3639e+00, -1.2627e+00],\n        [ 9.5572e-01, -9.3707e-01],\n        [ 2.3749e+00,  1.3755e+00],\n        [ 2.1659e+00,  1.1081e-01],\n        [ 2.0824e+00,  5.3115e-01],\n        [ 2.5198e+00, -1.1751e+00],\n        [ 3.5575e+00, -1.3541e+00],\n        [ 2.9779e+00, -2.3551e-02],\n        [ 1.6525e+00, -1.7198e-01],\n        [ 2.9999e+00, -2.0764e-01],\n        [ 2.7795e+00, -1.2982e+00],\n        [ 1.9418e+00,  1.0371e+00],\n        [ 2.7592e+00, -5.0463e-01],\n        [ 3.0367e+00, -3.6818e-01],\n        [ 1.1250e+00, -5.9455e-01],\n        [ 2.0576e-01, -1.2643e+00],\n        [ 3.3428e+00,  7.0896e-01],\n        [ 1.2861e+00,  6.7099e-01],\n        [ 3.4451e-02, -1.0264e+00],\n        [ 3.7692e+00, -4.8635e-01],\n        [ 9.9138e-01, -4.6335e-01],\n        [ 3.0677e+00, -3.7799e-01],\n        [ 2.6798e+00,  1.0502e+00],\n        [ 9.6844e-01,  8.9024e-02],\n        [ 4.3714e+00, -2.4618e-01],\n        [ 3.4464e+00, -1.1598e+00],\n        [ 2.7309e+00, -7.3896e-01],\n        [ 2.1785e+00, -1.2509e+00],\n        [ 1.1845e+00,  3.8009e-01],\n        [ 3.1762e-01, -1.2373e+00],\n        [ 4.3364e+00, -5.2662e-01],\n        [ 6.7095e-01,  1.2239e-01],\n        [ 1.6576e+00, -1.5527e-01],\n        [ 3.3566e+00,  1.9983e+00],\n        [ 1.9011e+00, -2.9758e-01],\n        [ 9.4850e-01, -3.2394e-01],\n        [ 3.2564e+00,  6.5815e-01],\n        [ 2.0119e+00,  5.2770e-02],\n        [ 2.1160e+00,  2.9776e-01],\n        [ 3.9874e+00, -5.5869e-01],\n        [ 4.2195e+00, -3.1988e-01],\n        [ 3.2535e+00, -1.1819e+00],\n        [ 3.0359e+00,  5.6345e-01],\n        [ 2.6826e+00,  1.5262e+00],\n        [ 2.0739e+00, -1.6738e+00],\n        [ 1.3986e+00,  7.3159e-03],\n        [ 2.4231e+00, -4.0420e-01],\n        [ 2.6612e+00,  1.6407e-01],\n        [ 4.3172e+00, -2.3403e-01],\n        [ 3.7476e+00, -7.2244e-02],\n        [ 1.1408e+00,  1.4352e+00],\n        [ 2.1293e+00, -2.5382e+00],\n        [ 1.9435e+00,  3.0485e-01],\n        [ 1.1548e+00, -6.2491e-01],\n        [ 1.1492e+00, -1.9763e+00],\n        [ 2.0250e+00, -7.4576e-01],\n        [ 2.9400e+00,  1.5066e+00],\n        [ 2.8737e+00,  2.0071e+00],\n        [ 2.5289e+00, -1.9478e-01],\n        [ 3.1963e+00,  1.2596e-01],\n        [ 2.4323e+00, -2.5952e-01],\n        [ 3.4132e+00,  2.4380e+00],\n        [ 2.9337e+00, -1.2908e+00],\n        [ 2.9322e+00,  4.2660e-01],\n        [ 1.8456e+00, -5.5211e-01],\n        [ 9.9292e-01, -8.7653e-01],\n        [ 3.1459e+00,  3.3836e-01],\n        [ 2.3248e+00, -1.2616e+00],\n        [ 5.0722e-01,  1.9755e+00],\n        [ 3.2719e+00, -4.2506e-01],\n        [ 3.4641e+00, -2.7503e-01],\n        [ 3.3118e+00, -3.2873e-01],\n        [ 2.0246e+00, -5.1943e-01],\n        [ 3.5263e+00,  1.5689e+00],\n        [ 3.4541e+00, -2.2962e+00],\n        [ 1.6579e+00, -1.5403e+00],\n        [ 1.9991e+00, -1.8926e+00],\n        [ 1.4147e+00,  2.1552e+00],\n        [ 3.2007e+00, -9.7527e-01],\n        [ 3.9824e+00,  7.5457e-01],\n        [ 2.3416e+00, -1.2115e+00],\n        [ 2.1285e+00,  1.3661e+00],\n        [ 6.0898e-01, -6.0028e-01],\n        [ 1.8496e+00, -1.4520e+00],\n        [ 2.4312e+00,  8.4721e-01],\n        [ 3.9684e+00,  8.5949e-01],\n        [ 2.6525e+00,  9.7864e-01],\n        [ 1.3807e+00, -6.9088e-01],\n        [ 2.7525e+00, -6.8729e-02],\n        [ 2.8847e+00, -1.7152e-01],\n        [ 2.6045e+00, -9.0113e-01],\n        [ 3.5573e+00, -4.6582e-01],\n        [ 3.2486e+00, -9.2187e-01],\n        [ 2.3346e+00, -8.5906e-01],\n        [ 4.5876e+00,  3.0823e-01],\n        [ 2.6428e+00,  4.3773e-01],\n        [ 2.5436e+00, -4.8757e-01],\n        [ 3.5289e+00,  1.4123e-01],\n        [ 1.7028e+00,  1.6054e+00],\n        [ 1.9901e+00, -6.3124e-01],\n        [ 3.3373e+00,  2.9931e+00],\n        [ 2.7666e+00,  1.0056e+00],\n        [ 2.5438e+00,  1.1695e+00],\n        [ 4.1372e+00, -1.1980e-01],\n        [ 3.4739e+00,  6.7403e-01],\n        [ 1.6990e+00,  9.7458e-01],\n        [ 1.5680e+00,  8.9833e-01],\n        [ 7.6078e-01, -1.6647e+00],\n        [ 2.1727e+00,  9.4705e-01],\n        [ 1.2996e+00, -1.3542e-01],\n        [ 3.9253e+00, -9.4386e-01],\n        [ 2.9312e+00, -1.0675e+00],\n        [ 3.5424e+00,  1.0659e+00],\n        [ 2.9808e+00, -1.4539e+00],\n        [ 1.4555e+00, -1.1875e+00],\n        [ 2.8250e+00, -2.3087e-01],\n        [ 3.1814e+00,  1.3798e+00],\n        [ 1.3185e+00,  1.9447e+00],\n        [ 2.3657e+00, -1.0358e+00],\n        [ 2.7741e+00, -8.9056e-02],\n        [ 2.9895e+00,  7.6625e-01],\n        [ 2.0813e+00,  1.1837e+00],\n        [ 2.9717e+00,  2.3711e-01],\n        [ 1.2834e+00, -1.4328e+00],\n        [ 2.0853e+00,  1.3163e+00],\n        [ 1.1174e+00, -5.5195e-01],\n        [ 3.0021e+00, -1.0072e+00],\n        [ 3.5776e+00, -2.1068e+00],\n        [ 4.4485e+00, -1.4867e+00],\n        [ 1.2077e+00,  1.0401e-01],\n        [ 2.4157e+00,  4.0483e-01],\n        [ 1.9433e+00,  6.7962e-01],\n        [ 2.5700e+00, -2.1094e+00],\n        [ 1.4092e+00, -6.3303e-01],\n        [ 8.3063e-01, -6.3792e-01],\n        [ 3.9916e+00,  1.4767e+00],\n        [ 9.6432e-01,  7.5697e-01],\n        [ 3.1185e+00, -2.1934e+00],\n        [ 2.8763e+00,  1.5925e-01],\n        [ 1.8170e+00,  6.1499e-01],\n        [ 9.4739e-02,  5.0273e-01],\n        [ 1.7620e+00, -3.2654e-01],\n        [ 1.3427e+00,  2.1228e-01],\n        [ 2.9656e+00, -1.3997e+00],\n        [ 2.0383e+00, -6.5744e-02],\n        [ 2.9594e+00, -7.6985e-01],\n        [ 3.0409e+00, -1.7213e+00],\n        [ 2.8874e+00,  1.5842e+00],\n        [ 2.0407e+00, -9.3427e-01],\n        [-1.3326e-01,  1.6925e+00],\n        [ 2.8247e+00, -1.0042e+00],\n        [ 2.7533e+00, -1.3468e+00],\n        [ 1.6370e+00, -1.0413e-01],\n        [ 3.1834e+00,  3.9136e-01],\n        [ 1.8812e+00, -2.5528e-01],\n        [ 2.1982e+00, -8.4741e-01],\n        [ 2.8393e+00, -7.1381e-02],\n        [ 2.1204e+00,  4.5792e-02],\n        [ 9.2738e-01, -6.6420e-01],\n        [ 3.3926e+00, -6.1388e-02],\n        [ 1.2266e+00,  5.2923e-01],\n        [ 1.6717e+00, -2.8657e-01],\n        [ 3.0695e+00, -4.3460e-02],\n        [ 3.1287e+00, -9.6450e-03],\n        [ 4.4655e+00,  5.7535e-01],\n        [ 3.6313e+00, -3.7899e-02],\n        [ 7.5281e-01, -7.1040e-01],\n        [ 1.8608e+00,  6.9243e-01],\n        [ 2.2796e+00,  2.4851e+00],\n        [ 2.7397e+00,  1.2802e-01],\n        [ 1.8498e+00,  1.6494e+00],\n        [ 3.7722e+00,  1.7106e-01],\n        [ 1.9090e+00, -4.4204e-01],\n        [ 3.3886e+00,  2.3962e-02],\n        [ 3.6682e+00,  4.0805e-01],\n        [ 2.1415e+00,  1.3123e+00],\n        [ 2.0704e+00,  9.7459e-01],\n        [ 1.0864e+00, -6.2759e-01],\n        [ 3.3033e+00,  6.2499e-01],\n        [ 2.5065e+00, -5.3080e-01],\n        [ 2.3758e+00,  3.3807e-01],\n        [ 1.4961e+00, -1.9197e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.0552e-01, 1.2652e-01, 1.2671e-01,  ..., 9.9570e-01, 1.0000e+00,\n         1.0000e+00],\n        [5.3999e-02, 5.4026e-02, 5.4026e-02,  ..., 9.2778e-01, 9.2778e-01,\n         1.0000e+00],\n        [7.7411e-07, 4.9654e-02, 4.9654e-02,  ..., 9.5128e-01, 9.5128e-01,\n         1.0000e+00],\n        ...,\n        [1.8276e-02, 3.0427e-02, 3.0503e-02,  ..., 9.6812e-01, 9.8823e-01,\n         1.0000e+00],\n        [2.5354e-01, 6.1418e-01, 6.2429e-01,  ..., 9.8513e-01, 9.9962e-01,\n         1.0000e+00],\n        [1.3276e-02, 1.6500e-02, 1.6500e-02,  ..., 9.4120e-01, 9.7638e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.1125],\n        [0.8988],\n        [0.0641],\n        [0.8922],\n        [0.6369],\n        [0.0160],\n        [0.6496],\n        [0.9575],\n        [0.5712],\n        [0.6179],\n        [0.1671],\n        [0.9701],\n        [0.2936],\n        [0.6792],\n        [0.5107],\n        [0.6065],\n        [0.4229],\n        [0.8082],\n        [0.6681],\n        [0.2062],\n        [0.7477],\n        [0.5120],\n        [0.9033],\n        [0.6881],\n        [0.7898],\n        [0.3288],\n        [0.8786],\n        [0.1466],\n        [0.7308],\n        [0.8924],\n        [0.6720],\n        [0.6511],\n        [0.8237],\n        [0.1804],\n        [0.7133],\n        [0.2724],\n        [0.2279],\n        [0.7538],\n        [0.6608],\n        [0.7998],\n        [0.2509],\n        [0.2469],\n        [0.1740],\n        [0.8356],\n        [0.2006],\n        [0.7280],\n        [0.0971],\n        [0.4101],\n        [0.5764],\n        [0.9370],\n        [0.4190],\n        [0.4312],\n        [0.7422],\n        [0.3258],\n        [0.5785],\n        [0.8581],\n        [0.9248],\n        [0.1705],\n        [0.8992],\n        [0.9922],\n        [0.1140],\n        [0.0446],\n        [0.3710],\n        [0.1557],\n        [0.0567],\n        [0.1739],\n        [0.9689],\n        [0.9389],\n        [0.4609],\n        [0.6797],\n        [0.0703],\n        [0.3225],\n        [0.6535],\n        [0.8343],\n        [0.8395],\n        [0.2795],\n        [0.7023],\n        [0.2837],\n        [0.8030],\n        [0.8491],\n        [0.1165],\n        [0.3822],\n        [0.3087],\n        [0.6745],\n        [0.7919],\n        [0.7112],\n        [0.0111],\n        [0.1866],\n        [0.5621],\n        [0.7099],\n        [0.3575],\n        [0.4920],\n        [0.4846],\n        [0.1188],\n        [0.5066],\n        [0.2353],\n        [0.2812],\n        [0.0507],\n        [0.6346],\n        [0.9382],\n        [0.7837],\n        [0.5637],\n        [0.6067],\n        [0.9793],\n        [0.7047],\n        [0.7520],\n        [0.1162],\n        [0.6711],\n        [0.9877],\n        [0.3096],\n        [0.3340],\n        [0.0361],\n        [0.2920],\n        [0.3884],\n        [0.6124],\n        [0.0383],\n        [0.1149],\n        [0.4916],\n        [0.0592],\n        [0.8013],\n        [0.8377],\n        [0.1302],\n        [0.4447],\n        [0.5925],\n        [0.0694],\n        [0.3008],\n        [0.4262],\n        [0.7356],\n        [0.1409],\n        [0.1127],\n        [0.0875],\n        [0.0452],\n        [0.9952],\n        [0.2432],\n        [0.4918],\n        [0.3332],\n        [0.6087],\n        [0.5151],\n        [0.4689],\n        [0.0674],\n        [0.1224],\n        [0.2686],\n        [0.7732],\n        [0.0312],\n        [0.7441],\n        [0.4427],\n        [0.1710],\n        [0.6243],\n        [0.1213],\n        [0.5081],\n        [0.8795],\n        [0.4691],\n        [0.4158],\n        [0.8953],\n        [0.7609],\n        [0.6195],\n        [0.4867],\n        [0.9467],\n        [0.3175],\n        [0.6760],\n        [0.2210],\n        [0.7146],\n        [0.1188],\n        [0.2135],\n        [0.2545],\n        [0.5339],\n        [0.5606],\n        [0.7681],\n        [0.1149],\n        [0.2043],\n        [0.7243],\n        [0.6404],\n        [0.3913],\n        [0.9802],\n        [0.7672],\n        [0.3694],\n        [0.5979],\n        [0.1777],\n        [0.3086],\n        [0.7300],\n        [0.0792],\n        [0.9131],\n        [0.2686],\n        [0.5125],\n        [0.2540],\n        [0.7999],\n        [0.5820],\n        [0.1591],\n        [0.9962],\n        [0.3506],\n        [0.3125],\n        [0.5298],\n        [0.1862],\n        [0.2147],\n        [0.3374],\n        [0.0932],\n        [0.3956],\n        [0.2911],\n        [0.0442],\n        [0.7658],\n        [0.0501],\n        [0.6184],\n        [0.8278],\n        [0.1905],\n        [0.5220],\n        [0.4267],\n        [0.6764],\n        [0.9387],\n        [0.8691],\n        [0.0539],\n        [0.1616],\n        [0.1701],\n        [0.4167],\n        [0.4565],\n        [0.1300],\n        [0.5684],\n        [0.0633],\n        [0.5804],\n        [0.5855],\n        [0.9938],\n        [0.5638],\n        [0.6598],\n        [0.6675],\n        [0.1273],\n        [0.6023],\n        [0.9159],\n        [0.1705],\n        [0.5693],\n        [0.4900],\n        [0.1976],\n        [0.7779],\n        [0.9812],\n        [0.0315],\n        [0.4459],\n        [0.1086],\n        [0.7369],\n        [0.4787],\n        [0.6493],\n        [0.4575],\n        [0.6569],\n        [0.3884],\n        [0.1782],\n        [0.8821],\n        [0.6643],\n        [0.9410],\n        [0.9685],\n        [0.8136],\n        [0.0358],\n        [0.3139],\n        [0.6382],\n        [0.5413],\n        [0.3979],\n        [0.1204],\n        [0.1072],\n        [0.0693],\n        [0.8096],\n        [0.3023],\n        [0.6192],\n        [0.0294],\n        [0.3354],\n        [0.4144],\n        [0.4387],\n        [0.3803],\n        [0.7301],\n        [0.0602],\n        [0.4525],\n        [0.2205],\n        [0.7501],\n        [0.3368],\n        [0.4773],\n        [0.1195],\n        [0.6422],\n        [0.2867],\n        [0.3390],\n        [0.5343],\n        [0.3218],\n        [0.6394],\n        [0.8000],\n        [0.0776],\n        [0.5664],\n        [0.6044],\n        [0.0992],\n        [0.0074],\n        [0.9989],\n        [0.8587],\n        [0.0790],\n        [0.6304],\n        [0.8713],\n        [0.5655],\n        [0.2660],\n        [0.4989],\n        [0.4503],\n        [0.9125],\n        [0.9619],\n        [0.8825],\n        [0.2695],\n        [0.1579],\n        [0.7867],\n        [0.4123],\n        [0.4739],\n        [0.9112],\n        [0.7234],\n        [0.5596],\n        [0.0062],\n        [0.0710],\n        [0.6461],\n        [0.3476],\n        [0.5028],\n        [0.6287],\n        [0.3559],\n        [0.5593],\n        [0.3043]]) torch.Size([312, 1])\nmask tensor([[False,  True, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False,  True, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 1.9902,  0.2575],\n        [ 0.4965, -0.1970],\n        [-0.8788,  0.6341],\n        ...,\n        [ 0.8854, -0.2468],\n        [ 0.8544,  0.0981],\n        [ 4.8383,  1.5133]]) torch.Size([9984, 2])\nsamples tensor([[ 4.9654e-01, -1.9702e-01],\n        [ 2.6673e+00,  7.4623e-01],\n        [ 6.8400e-01, -3.7153e-01],\n        [ 2.7622e+00,  2.2776e-01],\n        [ 3.4825e+00,  3.2074e-01],\n        [ 4.5861e+00,  1.3170e+00],\n        [ 2.2975e+00, -1.4211e+00],\n        [ 2.0168e+00,  1.2184e+00],\n        [ 1.5898e+00,  7.1762e-01],\n        [ 3.1529e+00, -8.9702e-01],\n        [ 3.1953e+00,  2.4822e+00],\n        [ 2.0056e+00,  9.5282e-01],\n        [ 6.9424e-01, -1.8124e+00],\n        [ 2.4043e+00, -7.5692e-01],\n        [ 1.9949e+00, -5.3584e-01],\n        [ 1.6248e+00, -1.4770e+00],\n        [ 3.2743e+00,  7.4022e-01],\n        [ 1.8165e+00, -4.0394e-01],\n        [ 3.5427e+00, -8.6947e-01],\n        [ 2.0457e+00, -1.4224e+00],\n        [ 1.2374e+00, -9.9061e-01],\n        [ 3.4599e+00, -1.0417e+00],\n        [ 1.2198e+00, -2.8369e-01],\n        [ 3.4766e+00,  6.3923e-01],\n        [ 1.3954e+00, -7.2690e-01],\n        [ 3.3330e+00, -1.7335e+00],\n        [ 4.0543e+00,  7.4271e-01],\n        [ 4.3271e+00, -1.5756e+00],\n        [ 4.3188e+00,  1.1342e+00],\n        [ 9.8683e-01, -1.2212e+00],\n        [ 1.9415e+00, -1.4586e+00],\n        [ 3.1242e+00, -1.5702e+00],\n        [ 2.0196e+00,  1.3151e-01],\n        [ 2.1918e+00,  8.9454e-01],\n        [ 2.1423e+00,  1.4781e-01],\n        [ 1.5997e+00, -9.4240e-01],\n        [ 2.4163e+00,  1.2930e+00],\n        [ 3.9379e+00,  2.5614e-01],\n        [ 6.3305e-01, -1.8688e+00],\n        [ 3.8023e+00, -2.0168e+00],\n        [ 2.9694e+00,  1.0280e+00],\n        [ 3.0111e+00, -5.1519e-01],\n        [ 2.7112e+00, -4.9070e-01],\n        [ 2.6831e+00, -1.7767e+00],\n        [ 2.6058e+00,  1.2614e+00],\n        [ 3.4712e+00, -1.0907e+00],\n        [ 2.5415e+00, -8.1490e-01],\n        [ 4.0595e+00,  3.6419e-01],\n        [ 3.4123e+00, -6.8384e-01],\n        [ 3.9031e+00, -1.0606e-01],\n        [ 7.6360e-01,  2.3862e-01],\n        [ 2.6917e+00, -2.3534e+00],\n        [ 1.6933e+00, -2.6118e-01],\n        [ 1.9458e+00,  9.2672e-01],\n        [ 2.9013e+00,  3.7131e-01],\n        [ 1.8196e+00,  3.2028e-01],\n        [ 3.3359e+00, -2.5478e+00],\n        [ 3.0237e+00,  3.7865e-01],\n        [ 3.8524e-01,  4.7566e-01],\n        [ 3.1456e+00,  4.8345e-01],\n        [ 3.3835e+00, -8.6039e-01],\n        [ 1.3186e+00, -9.1735e-01],\n        [ 2.1144e+00,  5.7335e-01],\n        [ 3.1902e+00, -4.0049e+00],\n        [ 1.7011e+00, -1.0008e+00],\n        [ 1.9057e+00, -1.1573e+00],\n        [ 2.5390e+00, -1.7687e-02],\n        [ 2.8074e+00, -1.1030e+00],\n        [ 1.9801e+00,  2.6442e-01],\n        [ 2.6212e+00,  5.0771e-01],\n        [ 1.9866e+00,  1.5112e+00],\n        [ 2.1378e+00, -7.8147e-01],\n        [-2.0265e-02, -6.3579e-01],\n        [ 3.5994e+00, -1.3515e+00],\n        [ 1.9020e+00,  1.2432e-01],\n        [ 2.4121e+00,  7.8750e-03],\n        [ 2.3665e+00, -2.9335e+00],\n        [ 3.7855e-01,  8.6429e-01],\n        [ 2.5035e+00,  2.5819e-01],\n        [ 1.4028e+00, -7.4659e-01],\n        [ 1.6725e+00,  4.5093e-01],\n        [ 3.6412e+00, -1.0188e+00],\n        [ 1.7736e+00,  1.0934e+00],\n        [ 3.1573e+00, -1.6438e+00],\n        [ 2.6123e+00,  9.2290e-01],\n        [ 2.3847e+00,  1.6111e-01],\n        [ 1.6588e+00,  6.2662e-01],\n        [ 3.0679e+00,  1.6340e+00],\n        [ 2.8932e+00, -5.3648e-01],\n        [ 2.5231e+00,  2.4630e-03],\n        [ 5.5865e-01, -8.3290e-01],\n        [ 2.9262e+00, -4.0825e-01],\n        [ 3.4097e+00, -7.5496e-01],\n        [ 3.1374e+00,  8.8319e-02],\n        [ 2.6285e+00, -7.1583e-01],\n        [ 1.9456e+00,  1.0117e+00],\n        [ 2.8965e+00, -1.8875e+00],\n        [ 2.5530e+00,  8.2268e-01],\n        [ 3.2064e+00, -1.9025e-01],\n        [ 2.3959e+00, -5.0412e-02],\n        [ 2.8147e+00,  6.9314e-01],\n        [ 2.2059e+00,  1.6913e-02],\n        [ 3.5692e+00,  3.6714e-01],\n        [ 2.1456e+00, -2.1722e-02],\n        [ 1.3923e+00, -1.0417e+00],\n        [ 2.3890e+00, -2.9156e-01],\n        [ 4.5723e+00, -7.3750e-02],\n        [ 4.2779e+00, -1.1435e+00],\n        [ 9.6068e-02, -8.6624e-01],\n        [ 2.5147e+00, -1.6484e+00],\n        [ 3.9199e+00, -1.7574e+00],\n        [ 1.5666e+00,  4.2622e-01],\n        [ 1.8638e+00, -4.1939e-01],\n        [ 3.3967e+00,  1.2599e+00],\n        [ 1.2842e+00, -4.2579e-01],\n        [ 1.3386e+00,  1.0444e+00],\n        [ 1.7404e+00,  5.5149e-01],\n        [ 2.8011e+00, -2.0462e+00],\n        [ 3.1753e+00,  1.6265e-01],\n        [ 5.4251e-01,  4.7261e-01],\n        [ 7.9057e-01,  7.3752e-01],\n        [ 2.1105e+00,  6.4739e-01],\n        [ 1.7754e+00, -2.1142e+00],\n        [ 3.6394e+00,  3.1429e-01],\n        [ 1.9607e+00,  5.3502e-01],\n        [ 4.9713e+00, -8.7853e-01],\n        [ 4.0548e+00, -2.3470e+00],\n        [ 3.5695e+00, -2.7595e-01],\n        [ 1.7115e+00, -1.3397e+00],\n        [ 4.1620e+00,  6.5348e-01],\n        [-5.1460e-01,  3.4327e-01],\n        [ 9.3894e-02,  2.0467e-01],\n        [ 2.7688e+00,  8.0378e-01],\n        [ 2.5718e+00,  1.0324e-01],\n        [ 2.6741e+00,  4.5355e-01],\n        [ 2.2068e+00, -1.1324e+00],\n        [ 3.8165e+00, -1.2208e+00],\n        [ 1.8124e+00,  7.8222e-01],\n        [ 1.6442e+00, -1.1857e+00],\n        [ 1.9452e+00, -4.6321e-01],\n        [ 1.5131e+00, -1.3933e+00],\n        [ 7.2433e-01, -1.2261e+00],\n        [ 4.2867e+00,  1.6959e-01],\n        [ 3.7881e+00,  7.7483e-01],\n        [ 3.2253e+00,  1.2021e-01],\n        [ 2.1800e+00,  6.7049e-01],\n        [ 2.1660e+00,  1.8152e+00],\n        [ 3.2286e+00,  2.3269e-01],\n        [ 2.2540e+00, -1.1817e+00],\n        [ 2.1430e+00, -3.3501e-02],\n        [ 2.7116e+00, -1.4358e+00],\n        [ 4.3270e+00,  1.4934e+00],\n        [ 2.8182e+00,  1.5823e-01],\n        [ 2.3078e+00, -4.2261e-01],\n        [ 2.7674e+00, -1.2112e+00],\n        [ 3.4348e+00,  1.3274e-01],\n        [ 9.9928e-01, -2.5908e-01],\n        [ 3.1551e+00,  7.5563e-03],\n        [ 1.3151e+00,  1.3834e-01],\n        [ 3.0624e+00, -3.5574e-01],\n        [ 4.5316e-01,  9.7019e-02],\n        [ 3.1173e+00,  1.1262e+00],\n        [ 6.5210e-01, -2.9295e-01],\n        [ 2.9865e+00,  4.8254e-01],\n        [ 3.6554e+00,  3.1929e-01],\n        [ 2.4845e+00, -1.3669e-01],\n        [ 1.9505e+00, -2.6949e+00],\n        [ 3.0004e+00, -1.3608e-01],\n        [ 9.4948e-01, -9.3284e-01],\n        [ 2.5946e+00,  2.0194e-01],\n        [ 2.6567e+00, -1.1034e+00],\n        [ 2.3896e+00, -6.5041e-01],\n        [ 3.5944e+00,  2.1711e-01],\n        [ 1.7366e+00,  3.1546e-01],\n        [ 3.3669e+00,  8.1736e-01],\n        [ 2.1663e+00, -3.4544e-01],\n        [ 1.2019e+00, -1.5180e+00],\n        [ 3.9139e+00,  4.4212e-02],\n        [ 1.2218e+00, -4.9754e-01],\n        [ 1.8440e+00, -6.3070e-01],\n        [ 9.9737e-01, -4.0178e-01],\n        [ 2.4091e+00, -5.2592e-01],\n        [ 3.9178e+00, -1.6497e+00],\n        [ 1.9287e+00,  3.5642e-01],\n        [ 1.0123e+00, -4.6850e-01],\n        [ 3.0487e+00, -3.7758e-01],\n        [ 4.4998e+00, -4.7381e-01],\n        [-1.1697e+00,  1.5663e-01],\n        [ 1.1941e+00,  1.5805e+00],\n        [ 3.1698e+00, -2.8283e-01],\n        [ 1.9299e+00, -5.6455e-01],\n        [ 1.4224e+00,  3.2209e-01],\n        [ 9.3844e-01, -1.4300e-01],\n        [ 1.1953e+00,  5.2479e-01],\n        [ 3.1426e+00, -2.7835e-01],\n        [ 7.6982e-01,  6.9494e-01],\n        [ 2.1209e+00,  3.8925e-01],\n        [ 3.8624e+00,  1.1000e-01],\n        [ 2.7818e+00,  8.2527e-01],\n        [ 2.1266e+00,  1.8707e+00],\n        [ 1.4643e+00, -1.0112e+00],\n        [ 2.8275e+00,  7.8550e-01],\n        [ 2.9548e+00,  3.6114e-01],\n        [ 2.0100e+00, -8.2690e-01],\n        [ 3.6454e+00, -2.5933e-01],\n        [ 1.9396e+00, -1.4811e-01],\n        [ 4.7469e+00,  9.4971e-01],\n        [ 2.3904e+00, -6.8059e-01],\n        [ 4.5866e+00, -3.0081e-01],\n        [ 1.9797e+00,  8.3684e-01],\n        [ 2.1561e+00,  2.2719e+00],\n        [ 1.3222e+00, -1.3607e+00],\n        [ 4.2623e+00,  1.1892e+00],\n        [ 3.2377e+00,  2.7784e-01],\n        [ 3.1731e-01,  1.2933e-02],\n        [ 3.4998e+00,  4.3428e-01],\n        [ 2.1577e+00,  3.0244e-01],\n        [ 4.0632e+00, -8.6530e-01],\n        [ 1.0030e+00, -2.9199e-01],\n        [ 2.9077e+00,  1.5502e-03],\n        [ 6.8857e-01, -1.6368e+00],\n        [ 3.2865e+00, -1.9061e+00],\n        [ 2.3967e+00,  4.2014e-02],\n        [ 1.1772e+00, -1.2222e-01],\n        [ 3.1677e+00,  6.5371e-01],\n        [ 2.8696e+00, -7.0473e-01],\n        [ 3.7392e+00, -1.7419e+00],\n        [ 2.0727e+00,  5.4913e-01],\n        [ 2.5441e+00, -8.3165e-01],\n        [ 1.9064e+00, -7.0854e-01],\n        [ 1.9896e+00, -1.0756e+00],\n        [ 1.2783e-01, -2.8169e-01],\n        [ 3.4464e+00,  9.3078e-01],\n        [ 1.8894e+00, -9.2216e-01],\n        [ 3.0782e+00, -1.0067e+00],\n        [ 2.0754e+00,  5.2953e-01],\n        [ 2.8030e+00,  9.8116e-01],\n        [ 2.7712e+00, -2.9538e-01],\n        [ 2.0594e+00, -1.3823e+00],\n        [ 3.5097e+00, -5.6231e-02],\n        [ 3.2894e+00,  9.8677e-01],\n        [ 4.9048e+00, -4.0197e-01],\n        [ 3.3190e+00, -9.7977e-01],\n        [ 3.6615e+00, -1.5448e-01],\n        [ 2.8383e+00,  1.0162e-01],\n        [ 2.8883e+00,  3.7198e-01],\n        [ 3.4482e+00, -9.6283e-01],\n        [ 3.2339e+00, -3.0207e-01],\n        [ 9.2425e-01, -1.6301e+00],\n        [ 2.7571e+00,  1.1026e-01],\n        [ 1.7498e+00, -1.1635e+00],\n        [ 3.2491e+00,  3.4144e-01],\n        [ 2.5182e+00, -7.9460e-01],\n        [ 3.3258e+00,  3.6808e-01],\n        [ 2.2754e+00,  6.5024e-01],\n        [ 2.2749e+00,  1.0154e-01],\n        [ 1.0400e+00,  1.3930e+00],\n        [ 2.7492e+00, -4.8264e-01],\n        [ 3.3246e+00,  1.3992e+00],\n        [ 1.7606e+00, -1.8385e+00],\n        [ 1.1852e+00, -3.3301e-01],\n        [ 2.7679e+00, -3.1409e-01],\n        [ 3.6315e+00, -8.0895e-02],\n        [ 3.6538e+00, -6.1554e-01],\n        [ 1.2058e+00,  1.0887e+00],\n        [ 2.6159e+00, -1.0199e+00],\n        [ 1.9385e+00, -7.8014e-01],\n        [ 5.4272e-01,  1.5334e+00],\n        [ 3.1592e+00, -1.3876e+00],\n        [ 3.1392e+00, -1.0036e+00],\n        [ 2.2305e+00, -1.5007e+00],\n        [ 3.7161e+00, -1.7968e-01],\n        [ 2.0418e+00,  2.2796e-01],\n        [ 4.3008e+00, -2.7451e-01],\n        [ 3.1464e+00, -1.6793e-01],\n        [ 3.3730e+00, -4.5294e-01],\n        [ 3.8503e+00,  1.3407e+00],\n        [ 3.5458e+00,  1.8180e+00],\n        [ 2.4730e+00, -1.1465e+00],\n        [ 1.3054e+00, -5.7421e-01],\n        [ 4.0239e+00, -1.3977e+00],\n        [ 2.1184e+00,  1.0144e+00],\n        [ 3.8625e+00,  1.0415e+00],\n        [ 2.8061e+00,  1.0023e+00],\n        [ 3.7003e+00, -1.1987e+00],\n        [ 1.5156e+00, -4.7096e-01],\n        [ 1.9272e+00, -3.3029e-02],\n        [ 2.9438e+00, -5.3766e-01],\n        [ 3.4507e+00, -3.4172e-01],\n        [ 2.3724e+00, -7.3532e-01],\n        [ 2.3807e+00, -4.0331e-01],\n        [ 2.8734e+00, -9.2605e-01],\n        [ 2.3928e+00, -4.6690e-01],\n        [ 2.6395e+00, -2.7299e+00],\n        [ 2.8487e+00,  7.6402e-01],\n        [ 3.4645e+00, -4.0915e-01],\n        [ 1.9548e+00, -3.9780e-01],\n        [ 3.6911e+00, -3.1119e-02],\n        [ 2.6071e+00, -1.2465e+00],\n        [ 2.8214e+00, -6.4985e-01],\n        [ 1.5930e+00,  2.9120e+00],\n        [ 1.4605e+00,  7.2332e-01],\n        [ 1.7990e+00,  1.4443e+00],\n        [ 2.9508e+00,  2.7149e+00],\n        [ 2.8324e+00,  9.6945e-01],\n        [ 2.7792e+00, -8.7107e-01],\n        [ 2.8536e+00, -6.4021e-01],\n        [ 1.9764e+00, -7.1593e-01],\n        [ 2.8083e+00, -2.2212e+00],\n        [ 4.5912e+00,  1.0887e+00],\n        [ 1.7598e+00, -1.4101e+00],\n        [ 3.6951e+00, -1.2749e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[5.3881e-03, 9.5553e-03, 1.1751e-02,  ..., 9.2907e-01, 9.2945e-01,\n         1.0000e+00],\n        [1.1783e-04, 3.6637e-02, 3.9891e-02,  ..., 9.9944e-01, 1.0000e+00,\n         1.0000e+00],\n        [3.6439e-06, 1.1590e-03, 4.1941e-03,  ..., 9.8275e-01, 9.8281e-01,\n         1.0000e+00],\n        ...,\n        [7.1364e-03, 7.8323e-03, 9.1220e-03,  ..., 9.9685e-01, 9.9993e-01,\n         1.0000e+00],\n        [8.0499e-02, 8.1992e-02, 8.1992e-02,  ..., 9.8542e-01, 9.8635e-01,\n         1.0000e+00],\n        [1.2392e-02, 1.8121e-02, 4.5993e-02,  ..., 8.5928e-01, 9.9992e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.5515],\n        [0.2050],\n        [0.7769],\n        [0.9022],\n        [0.1066],\n        [0.6453],\n        [0.3740],\n        [0.8030],\n        [0.9355],\n        [0.8922],\n        [0.0845],\n        [0.5956],\n        [0.0758],\n        [0.7367],\n        [0.6199],\n        [0.9223],\n        [0.1130],\n        [0.4162],\n        [0.4367],\n        [0.2591],\n        [0.2569],\n        [0.0199],\n        [0.3331],\n        [0.8123],\n        [0.3657],\n        [0.9459],\n        [0.0788],\n        [0.9786],\n        [0.9167],\n        [0.5480],\n        [0.1334],\n        [0.6340],\n        [0.8727],\n        [0.2404],\n        [0.0529],\n        [0.9177],\n        [0.2879],\n        [0.4434],\n        [0.7627],\n        [0.3803],\n        [0.0102],\n        [0.8116],\n        [0.8493],\n        [0.4431],\n        [0.6770],\n        [0.9923],\n        [0.7249],\n        [0.4307],\n        [0.7503],\n        [0.4056],\n        [0.4129],\n        [0.1103],\n        [0.6370],\n        [0.6708],\n        [0.5074],\n        [0.8922],\n        [0.1531],\n        [0.7515],\n        [0.9309],\n        [0.2046],\n        [0.5800],\n        [0.9415],\n        [0.9428],\n        [0.0786],\n        [0.3130],\n        [0.8553],\n        [0.4668],\n        [0.7037],\n        [0.0748],\n        [0.8609],\n        [0.4734],\n        [0.6957],\n        [0.5366],\n        [0.1771],\n        [0.5889],\n        [0.4014],\n        [0.2405],\n        [0.0630],\n        [0.4399],\n        [0.2195],\n        [0.3917],\n        [0.3809],\n        [0.2423],\n        [0.9577],\n        [0.2944],\n        [0.4517],\n        [0.6668],\n        [0.5011],\n        [0.1120],\n        [0.1703],\n        [0.1101],\n        [0.8013],\n        [0.8050],\n        [0.8622],\n        [0.0959],\n        [0.2777],\n        [0.6783],\n        [0.0420],\n        [0.2120],\n        [0.4836],\n        [0.5213],\n        [0.3535],\n        [0.7868],\n        [0.2239],\n        [0.0376],\n        [0.2175],\n        [0.5544],\n        [0.1051],\n        [0.0199],\n        [0.3543],\n        [0.9344],\n        [0.8660],\n        [0.7016],\n        [0.9871],\n        [0.5023],\n        [0.0034],\n        [0.9560],\n        [0.5768],\n        [0.1428],\n        [0.3708],\n        [0.9375],\n        [0.4582],\n        [0.3934],\n        [0.0772],\n        [0.5898],\n        [0.2382],\n        [0.6696],\n        [0.9430],\n        [0.7239],\n        [0.7353],\n        [0.8341],\n        [0.5738],\n        [0.5383],\n        [0.6063],\n        [0.7215],\n        [0.6842],\n        [0.9461],\n        [0.2516],\n        [0.9974],\n        [0.5988],\n        [0.0602],\n        [0.9842],\n        [0.1293],\n        [0.3597],\n        [0.9793],\n        [0.4072],\n        [0.9644],\n        [0.0506],\n        [0.3462],\n        [0.3667],\n        [0.0607],\n        [0.9854],\n        [0.3852],\n        [0.9721],\n        [0.7428],\n        [0.0438],\n        [0.6725],\n        [0.8368],\n        [0.9855],\n        [0.2745],\n        [0.2830],\n        [0.2265],\n        [0.1088],\n        [0.8453],\n        [0.3988],\n        [0.7612],\n        [0.9109],\n        [0.2050],\n        [0.0075],\n        [0.2217],\n        [0.9295],\n        [0.9185],\n        [0.5791],\n        [0.0906],\n        [0.2242],\n        [0.6919],\n        [0.3258],\n        [0.4345],\n        [0.8168],\n        [0.6043],\n        [0.1367],\n        [0.1255],\n        [0.1571],\n        [0.5253],\n        [0.3607],\n        [0.3141],\n        [0.3850],\n        [0.9469],\n        [0.8480],\n        [0.8154],\n        [0.0664],\n        [0.0234],\n        [0.9560],\n        [0.2360],\n        [0.0098],\n        [0.7375],\n        [0.9661],\n        [0.9767],\n        [0.6775],\n        [0.7742],\n        [0.0499],\n        [0.0905],\n        [0.5615],\n        [0.8813],\n        [0.5116],\n        [0.1514],\n        [0.0201],\n        [0.4271],\n        [0.1902],\n        [0.4816],\n        [0.4926],\n        [0.5648],\n        [0.9272],\n        [0.6151],\n        [0.8318],\n        [0.0797],\n        [0.9117],\n        [0.0776],\n        [0.0985],\n        [0.7809],\n        [0.6075],\n        [0.8557],\n        [0.8578],\n        [0.5291],\n        [0.6153],\n        [0.3437],\n        [0.4644],\n        [0.8560],\n        [0.1311],\n        [0.4925],\n        [0.8432],\n        [0.6097],\n        [0.0452],\n        [0.6075],\n        [0.7728],\n        [0.9285],\n        [0.9252],\n        [0.2753],\n        [0.9664],\n        [0.2547],\n        [0.5873],\n        [0.3667],\n        [0.8065],\n        [0.7395],\n        [0.0453],\n        [0.5106],\n        [0.1121],\n        [0.0461],\n        [0.8033],\n        [0.5444],\n        [0.5722],\n        [0.7544],\n        [0.0093],\n        [0.2225],\n        [0.2155],\n        [0.2285],\n        [0.9128],\n        [0.3645],\n        [0.5404],\n        [0.1754],\n        [0.9583],\n        [0.9423],\n        [0.7475],\n        [0.0880],\n        [0.1153],\n        [0.4829],\n        [0.3743],\n        [0.2263],\n        [0.5450],\n        [0.1099],\n        [0.6247],\n        [0.9879],\n        [0.0245],\n        [0.3693],\n        [0.5520],\n        [0.8169],\n        [0.5421],\n        [0.0842],\n        [0.3687],\n        [0.7625],\n        [0.3382],\n        [0.4495],\n        [0.0502],\n        [0.6245],\n        [0.3083],\n        [0.7559],\n        [0.4016],\n        [0.1992],\n        [0.5324],\n        [0.5472],\n        [0.9670],\n        [0.2400],\n        [0.6629],\n        [0.6726],\n        [0.3213],\n        [0.0868],\n        [0.9965],\n        [0.0814],\n        [0.3140],\n        [0.7982],\n        [0.6531],\n        [0.0050],\n        [0.0617],\n        [0.5610],\n        [0.4759],\n        [0.3318],\n        [0.0461],\n        [0.6751],\n        [0.9833],\n        [0.0574],\n        [0.0609],\n        [0.2162]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [ True, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 1.9081,  2.1155],\n        [ 1.4174,  1.9191],\n        [ 0.1824,  0.4497],\n        ...,\n        [-2.4527,  0.8042],\n        [ 1.6655,  0.1224],\n        [-1.0616,  1.8027]]) torch.Size([9984, 2])\nsamples tensor([[ 3.4910e+00, -1.0214e+00],\n        [ 3.4039e+00, -2.7295e-01],\n        [ 2.3346e+00,  2.4065e+00],\n        [ 1.2875e+00, -7.0271e-02],\n        [ 4.2176e+00,  2.4986e-01],\n        [ 3.5399e+00,  1.1058e+00],\n        [ 2.3248e+00,  8.7043e-01],\n        [ 3.1149e+00, -8.7953e-02],\n        [ 1.3024e+00, -2.2292e-02],\n        [ 3.1385e+00,  1.7708e-01],\n        [ 2.9863e+00,  1.2411e+00],\n        [ 3.8513e+00, -2.1914e-01],\n        [-4.9084e-02, -4.3625e-01],\n        [ 1.8932e+00,  8.3057e-01],\n        [ 3.7854e+00, -3.2673e-01],\n        [ 1.5912e+00, -9.4559e-02],\n        [ 9.2092e-02, -1.5324e+00],\n        [ 4.6135e+00, -1.1176e-01],\n        [ 3.7589e+00,  1.1406e-03],\n        [ 1.8038e+00, -1.3242e+00],\n        [ 3.6852e+00, -1.8309e+00],\n        [ 7.1960e-01, -6.0245e-01],\n        [ 2.4749e+00, -4.1172e-02],\n        [ 3.7083e+00, -8.6996e-01],\n        [-1.8608e-01,  3.1384e-01],\n        [ 2.0446e+00,  4.7686e-01],\n        [ 2.4449e+00, -3.9601e-02],\n        [ 1.8766e+00,  1.8285e+00],\n        [ 2.4163e+00,  6.5195e-01],\n        [ 4.5724e+00, -7.9077e-01],\n        [ 3.4171e+00, -1.1262e-01],\n        [ 1.8170e+00,  4.5933e-01],\n        [ 9.1201e-01, -8.8020e-01],\n        [ 2.4211e+00,  7.4373e-01],\n        [ 2.3136e+00, -5.7132e-01],\n        [ 7.7536e-01,  2.3645e-01],\n        [ 3.3453e+00, -1.6011e+00],\n        [ 1.2223e+00, -2.7409e-01],\n        [ 2.6255e+00, -5.1415e-01],\n        [ 3.3324e+00, -3.1840e-01],\n        [ 1.1918e+00,  6.5407e-01],\n        [ 3.2010e+00,  1.1623e+00],\n        [ 2.1415e+00, -1.9406e+00],\n        [ 2.2928e+00, -1.8218e+00],\n        [ 4.0054e+00,  1.1516e+00],\n        [ 3.6441e+00,  9.2169e-01],\n        [ 3.3665e+00, -1.5554e+00],\n        [ 3.2944e+00,  3.3045e-01],\n        [ 2.9003e+00,  2.5989e-01],\n        [ 3.0240e+00,  7.8888e-01],\n        [ 4.0777e+00, -9.5715e-01],\n        [ 2.1092e+00, -3.6543e-01],\n        [ 2.1715e+00,  1.2089e+00],\n        [ 2.7647e+00, -1.7410e+00],\n        [ 1.5441e+00, -1.2846e+00],\n        [ 1.8629e+00, -1.1535e+00],\n        [ 3.2751e+00,  1.0648e+00],\n        [ 1.6161e+00, -2.2426e-01],\n        [ 2.2573e+00,  9.7751e-01],\n        [ 2.8799e+00,  8.0116e-01],\n        [ 2.6110e+00,  1.0596e+00],\n        [ 4.8635e+00, -5.4322e-01],\n        [ 1.1612e+00, -5.5141e-01],\n        [ 1.4003e+00, -6.8177e-01],\n        [ 2.6522e+00,  5.7432e-01],\n        [ 6.2913e-01,  9.1708e-01],\n        [ 4.6525e+00, -9.2554e-01],\n        [ 1.7068e+00, -9.7142e-02],\n        [ 1.4124e+00,  6.3037e-01],\n        [ 3.2094e+00,  3.4943e-02],\n        [ 1.4322e+00, -1.8807e+00],\n        [ 1.5560e+00,  2.0780e+00],\n        [ 1.6564e+00,  4.1225e-01],\n        [ 2.5723e+00, -2.3553e-01],\n        [ 2.4330e+00,  7.0475e-01],\n        [ 1.1404e+00, -9.6819e-01],\n        [ 1.0381e+00, -5.9237e-01],\n        [ 1.7422e+00,  9.0652e-01],\n        [ 4.0072e+00,  5.1901e-01],\n        [ 2.4029e-01, -6.5568e-02],\n        [ 4.7369e+00,  3.6778e-01],\n        [ 2.7950e+00,  3.5129e-01],\n        [ 8.9195e-01,  9.0342e-01],\n        [ 7.5095e-01,  7.0513e-01],\n        [ 1.5081e+00,  7.9827e-01],\n        [ 3.1168e+00, -1.5965e+00],\n        [ 2.5471e+00,  6.3711e-01],\n        [ 4.2538e+00,  8.3094e-01],\n        [ 4.7092e+00, -7.3433e-01],\n        [ 1.9516e+00, -4.2042e-01],\n        [ 2.9595e+00,  1.5539e+00],\n        [ 3.1917e+00, -2.6855e-02],\n        [ 1.8491e+00, -1.5392e+00],\n        [ 1.7825e+00,  3.7046e-02],\n        [ 1.2202e+00, -2.2622e-01],\n        [ 3.0266e+00, -6.8109e-01],\n        [ 2.1036e+00,  6.1134e-02],\n        [ 2.0974e+00, -7.1276e-01],\n        [ 3.4988e+00, -1.3675e+00],\n        [ 1.3236e+00,  3.1437e-01],\n        [ 2.9552e+00, -5.5463e-01],\n        [ 3.6625e+00,  7.8612e-01],\n        [ 3.7229e+00, -1.0143e-01],\n        [ 3.0639e+00,  1.9247e-01],\n        [ 1.7015e+00,  3.3108e-01],\n        [ 3.2579e+00, -1.1259e-01],\n        [ 1.1933e+00, -5.5410e-01],\n        [-5.8820e-01,  4.6463e-02],\n        [ 2.4728e+00, -9.6498e-01],\n        [ 4.2641e+00, -1.1620e+00],\n        [ 2.0482e+00,  1.2758e+00],\n        [ 1.9490e+00,  5.4661e-01],\n        [ 3.5838e+00, -1.4876e+00],\n        [ 1.9826e+00,  4.6379e-01],\n        [ 2.0541e+00,  4.2310e-01],\n        [ 1.3113e-01, -3.2591e-01],\n        [ 2.7419e+00, -1.6235e+00],\n        [ 1.6593e+00, -5.8311e-01],\n        [ 3.2042e+00, -1.1703e+00],\n        [ 2.4873e+00,  5.0757e-01],\n        [ 1.1491e+00,  2.3024e-01],\n        [ 2.9800e+00, -9.7596e-01],\n        [ 3.2843e+00,  3.7862e-02],\n        [ 3.1104e+00, -1.1027e+00],\n        [ 4.0424e+00, -2.0428e+00],\n        [ 2.9085e+00, -5.7540e-01],\n        [ 2.3900e+00, -8.8108e-01],\n        [ 1.7475e+00, -1.5868e-01],\n        [ 1.8624e+00, -8.7663e-01],\n        [ 2.7858e+00, -5.3039e-01],\n        [ 1.4902e+00, -1.5565e-01],\n        [ 2.5156e+00,  4.3528e-01],\n        [ 3.2296e+00,  2.8918e-01],\n        [ 2.4575e+00,  2.5259e-02],\n        [ 2.6074e+00, -2.9363e-01],\n        [ 7.4484e-01,  4.4004e-01],\n        [ 2.3958e+00, -7.1288e-01],\n        [ 3.3959e+00,  6.0162e-01],\n        [ 2.1878e-01,  1.0119e+00],\n        [ 2.4148e+00, -1.5629e+00],\n        [ 3.4639e+00,  6.2884e-01],\n        [ 5.9314e-02,  1.7508e+00],\n        [-1.1538e-01, -1.9342e+00],\n        [ 1.3975e+00,  4.5996e-01],\n        [ 2.4459e+00, -5.9868e-02],\n        [ 1.5271e+00, -1.4542e+00],\n        [ 2.7373e+00, -6.2065e-01],\n        [ 2.4563e+00,  1.6331e-01],\n        [ 2.8560e+00, -1.5227e+00],\n        [ 6.0167e-01,  5.7847e-01],\n        [ 1.4351e+00, -7.0172e-01],\n        [ 7.2750e-01,  4.2194e-01],\n        [ 2.6555e+00,  1.5185e-01],\n        [ 2.9630e+00,  1.2618e+00],\n        [ 1.1558e+00,  1.1803e+00],\n        [ 1.1070e+00,  1.7362e-01],\n        [ 9.1202e-02, -1.0894e+00],\n        [ 2.5898e+00,  9.3583e-01],\n        [ 8.0948e-01,  9.4554e-01],\n        [ 2.5053e+00, -1.1952e+00],\n        [ 2.5117e+00, -6.0612e-01],\n        [ 2.5785e+00, -6.3633e-01],\n        [ 2.5857e+00, -3.9342e-01],\n        [ 2.2887e+00, -7.0153e-01],\n        [ 3.6916e+00,  6.0719e-01],\n        [ 1.5299e+00,  8.9884e-01],\n        [ 1.4908e+00,  3.9284e-01],\n        [ 3.8239e+00, -2.1485e+00],\n        [ 1.0615e+00,  1.0150e+00],\n        [ 4.0547e+00, -4.2256e-01],\n        [ 2.0186e+00, -4.7741e-01],\n        [ 2.7140e+00,  1.2654e+00],\n        [ 4.3260e-01, -3.4470e-01],\n        [ 1.2497e+00, -4.8899e-02],\n        [ 2.1474e+00, -1.8907e-01],\n        [ 3.0490e+00, -1.7016e+00],\n        [ 1.2179e+00,  5.3832e-01],\n        [ 3.3932e+00, -9.2216e-01],\n        [ 3.4355e+00, -4.4809e-01],\n        [ 1.8392e+00, -9.5104e-01],\n        [ 2.9972e+00, -2.3223e+00],\n        [ 2.0642e+00, -2.1716e-01],\n        [ 2.2368e+00,  6.7824e-02],\n        [ 3.6458e+00, -1.6722e+00],\n        [ 1.9041e+00, -1.3404e+00],\n        [ 1.8744e+00, -7.3056e-01],\n        [ 3.3390e+00, -1.7242e+00],\n        [ 1.6857e+00, -9.0906e-01],\n        [ 1.5496e+00, -3.7633e-01],\n        [ 2.2925e+00, -2.6797e-01],\n        [ 3.4618e+00, -7.2592e-01],\n        [ 1.6620e+00,  1.5505e+00],\n        [ 1.3135e+00, -8.9612e-01],\n        [ 3.8239e+00, -3.7795e-01],\n        [ 1.9866e+00, -1.7202e-01],\n        [ 1.3849e+00,  1.1538e+00],\n        [ 1.6235e+00,  2.1696e-01],\n        [ 2.4163e+00, -5.4890e-01],\n        [ 2.2023e+00,  2.2190e+00],\n        [ 2.3689e+00,  7.3476e-01],\n        [ 2.3807e+00, -3.2975e-01],\n        [ 2.8979e+00,  5.6296e-02],\n        [ 2.3192e+00, -8.7893e-01],\n        [ 1.7816e+00, -1.6414e+00],\n        [ 2.8692e+00, -3.1716e-01],\n        [ 1.6844e+00, -5.5007e-01],\n        [ 2.0938e+00,  6.8984e-01],\n        [ 3.0989e+00, -1.4107e+00],\n        [ 8.5921e-01,  3.7692e-01],\n        [ 3.2510e+00,  1.5254e+00],\n        [ 3.0448e+00,  1.3045e+00],\n        [ 3.7098e+00,  8.5547e-01],\n        [ 1.4430e+00, -4.5272e-01],\n        [ 2.1730e+00, -1.7967e+00],\n        [ 3.9643e+00, -9.0256e-01],\n        [ 2.3192e+00,  8.5416e-01],\n        [ 2.6194e+00, -2.5412e+00],\n        [ 4.3972e+00, -8.4899e-01],\n        [ 3.6228e-01,  5.1934e-01],\n        [ 1.2354e+00,  1.6448e+00],\n        [ 1.9180e+00,  1.3271e+00],\n        [ 3.5124e+00, -1.2495e+00],\n        [ 1.9439e+00, -1.0221e-01],\n        [ 5.7263e-01,  9.2942e-02],\n        [ 3.0724e+00,  7.5728e-02],\n        [ 1.1517e+00,  4.3854e-01],\n        [ 3.3541e+00, -1.8273e+00],\n        [ 4.2810e+00, -1.0235e+00],\n        [ 3.7598e+00, -2.2958e+00],\n        [ 2.7517e+00, -6.4229e-01],\n        [ 2.8816e+00,  1.4803e+00],\n        [ 4.8327e+00, -1.4837e+00],\n        [ 1.3287e+00,  7.3569e-02],\n        [ 1.3872e+00,  1.7141e+00],\n        [ 4.2304e+00,  1.7187e+00],\n        [ 3.4340e+00,  1.2151e+00],\n        [ 3.0483e+00, -2.1350e-02],\n        [ 2.2719e+00, -5.7726e-01],\n        [ 1.7694e+00,  5.3153e-01],\n        [ 3.9534e+00,  2.2975e-02],\n        [ 8.3230e-01, -1.1450e-01],\n        [ 2.1145e+00,  3.3615e-01],\n        [ 3.2651e+00, -8.8922e-01],\n        [ 2.2952e+00,  2.4903e-01],\n        [ 2.7394e+00,  8.9943e-01],\n        [ 1.6045e+00, -1.4143e-01],\n        [ 1.8102e+00,  1.1255e+00],\n        [ 1.5285e+00, -4.9481e-02],\n        [ 1.4555e+00, -6.9074e-01],\n        [ 2.1476e+00,  9.7111e-01],\n        [ 3.2339e+00, -6.2839e-01],\n        [ 2.6793e+00, -1.3101e+00],\n        [ 3.1866e+00,  1.3379e+00],\n        [ 2.9149e+00, -2.5570e+00],\n        [ 2.4725e+00,  1.2256e+00],\n        [ 4.2048e+00, -1.3942e-01],\n        [ 1.8422e+00, -2.5712e-01],\n        [ 3.0517e+00, -1.1166e+00],\n        [ 2.0547e+00,  4.9988e-01],\n        [ 2.5120e-01,  1.2785e+00],\n        [ 4.8348e+00,  6.2996e-01],\n        [ 2.6165e+00,  8.7798e-02],\n        [ 1.8746e+00, -2.0197e+00],\n        [ 2.2152e+00,  7.1293e-01],\n        [ 3.4468e+00,  3.9470e-01],\n        [ 1.5884e+00, -4.0722e-01],\n        [ 1.6322e+00, -2.4750e-01],\n        [ 2.9113e+00,  2.6620e-01],\n        [ 2.7155e+00,  8.8158e-01],\n        [ 4.0361e+00, -1.2050e+00],\n        [ 2.2052e+00, -1.1035e+00],\n        [ 3.9220e+00,  7.0902e-01],\n        [ 2.5193e+00,  1.5410e+00],\n        [ 3.8928e+00,  1.6063e+00],\n        [ 4.6423e+00, -1.9194e+00],\n        [ 4.2645e-02,  1.5347e+00],\n        [ 1.3459e+00,  7.9952e-01],\n        [ 2.9572e+00, -3.0111e+00],\n        [ 2.2008e+00, -1.6214e-01],\n        [ 3.6044e+00, -9.0270e-02],\n        [ 1.2518e+00, -2.3489e-02],\n        [ 3.0019e+00, -8.5311e-01],\n        [ 1.2006e+00,  6.1982e-01],\n        [ 4.0785e+00, -2.1904e+00],\n        [ 4.5858e+00,  2.8241e-01],\n        [ 2.5740e+00, -4.1256e-01],\n        [ 1.7070e+00, -3.4894e-01],\n        [ 2.6882e+00,  1.3497e+00],\n        [ 2.8410e+00, -5.7220e-01],\n        [ 1.2267e+00, -1.4461e+00],\n        [ 4.1964e+00,  1.5485e-01],\n        [ 2.0954e+00, -6.7152e-01],\n        [ 1.9531e+00, -6.4096e-01],\n        [ 3.3861e+00,  8.3614e-01],\n        [ 2.7840e+00,  5.1296e-01],\n        [ 3.3902e+00, -5.0006e-02],\n        [ 4.0441e-01,  2.9270e-01],\n        [ 6.2647e-01, -1.5851e-01],\n        [ 1.4774e+00,  6.0640e-01],\n        [ 2.3725e+00, -1.9652e+00],\n        [ 3.3831e+00, -2.6351e+00],\n        [ 4.2987e+00,  1.4161e+00],\n        [ 2.1165e+00,  5.9748e-01],\n        [ 2.2876e+00,  5.6507e-01],\n        [ 1.2180e+00,  8.9575e-02],\n        [ 4.5850e+00, -8.2417e-01],\n        [ 1.0985e+00,  1.2025e+00],\n        [ 9.0942e-01,  2.1945e-01],\n        [ 2.5312e+00, -1.0454e+00],\n        [ 4.4510e+00,  5.5817e-01],\n        [ 2.1198e+00,  8.7577e-02],\n        [ 2.1147e+00,  5.3310e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.0480e-05, 9.1579e-03, 9.1593e-03,  ..., 7.8211e-01, 7.8211e-01,\n         1.0000e+00],\n        [2.6235e-01, 2.6918e-01, 2.6919e-01,  ..., 9.8517e-01, 9.9972e-01,\n         1.0000e+00],\n        [3.1159e-03, 3.1159e-03, 3.1179e-03,  ..., 9.8155e-01, 9.8168e-01,\n         1.0000e+00],\n        ...,\n        [1.2766e-02, 9.8028e-02, 3.2263e-01,  ..., 9.4350e-01, 9.9981e-01,\n         1.0000e+00],\n        [1.5616e-02, 2.5547e-02, 3.0690e-02,  ..., 9.7076e-01, 9.7098e-01,\n         1.0000e+00],\n        [2.4125e-02, 6.0157e-02, 8.0333e-02,  ..., 6.8885e-01, 1.0000e+00,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[1.1368e-01],\n        [8.0328e-01],\n        [7.0734e-01],\n        [6.1283e-01],\n        [6.1254e-01],\n        [9.2263e-01],\n        [1.9592e-01],\n        [2.7831e-02],\n        [9.2064e-01],\n        [7.6709e-01],\n        [5.4901e-01],\n        [5.0012e-01],\n        [6.0578e-02],\n        [7.9843e-01],\n        [8.3898e-01],\n        [5.8131e-01],\n        [5.9457e-01],\n        [4.1703e-01],\n        [3.8152e-01],\n        [5.5572e-01],\n        [1.8892e-01],\n        [8.8371e-01],\n        [8.1851e-01],\n        [2.5960e-01],\n        [2.0035e-01],\n        [6.2438e-01],\n        [4.8211e-01],\n        [4.0551e-01],\n        [3.6075e-01],\n        [1.0728e-01],\n        [5.5372e-01],\n        [8.1822e-01],\n        [7.3936e-01],\n        [8.4483e-01],\n        [2.9351e-01],\n        [7.6290e-01],\n        [5.5175e-01],\n        [7.5689e-02],\n        [4.9033e-01],\n        [9.4573e-01],\n        [3.2903e-01],\n        [4.2679e-01],\n        [6.5404e-01],\n        [6.9512e-01],\n        [4.1128e-01],\n        [2.3566e-01],\n        [9.1101e-01],\n        [6.2977e-01],\n        [8.3895e-01],\n        [1.8643e-01],\n        [6.7724e-01],\n        [6.3487e-01],\n        [8.2948e-01],\n        [9.1943e-01],\n        [5.2893e-02],\n        [2.4085e-02],\n        [3.5275e-01],\n        [2.2092e-01],\n        [5.8152e-01],\n        [4.3176e-01],\n        [4.9782e-01],\n        [7.7740e-01],\n        [6.3447e-01],\n        [3.1557e-01],\n        [5.1986e-01],\n        [4.5436e-01],\n        [4.7714e-04],\n        [1.4984e-01],\n        [4.5133e-01],\n        [8.6161e-01],\n        [9.3426e-01],\n        [3.5768e-01],\n        [2.8230e-01],\n        [2.9538e-01],\n        [7.1157e-01],\n        [3.1519e-01],\n        [3.4575e-01],\n        [3.9103e-01],\n        [8.4812e-01],\n        [8.1170e-01],\n        [2.7052e-01],\n        [5.1668e-01],\n        [3.0836e-01],\n        [8.4355e-01],\n        [6.4778e-01],\n        [3.1048e-01],\n        [4.1361e-01],\n        [2.1173e-01],\n        [4.6221e-01],\n        [2.5491e-01],\n        [5.7195e-01],\n        [3.8298e-01],\n        [8.1384e-01],\n        [7.9827e-01],\n        [5.5425e-01],\n        [3.2199e-01],\n        [8.5807e-01],\n        [6.5182e-01],\n        [5.2781e-01],\n        [5.6193e-01],\n        [5.9902e-01],\n        [6.0504e-01],\n        [1.3025e-01],\n        [3.1439e-01],\n        [9.2379e-01],\n        [9.6461e-01],\n        [8.9041e-01],\n        [7.0841e-01],\n        [5.8772e-01],\n        [3.6306e-01],\n        [2.4889e-01],\n        [1.7784e-01],\n        [4.2854e-01],\n        [2.9289e-01],\n        [5.7276e-01],\n        [4.1333e-01],\n        [2.4902e-02],\n        [2.4811e-02],\n        [1.1123e-01],\n        [1.1338e-01],\n        [7.7945e-01],\n        [9.9660e-01],\n        [3.1673e-02],\n        [7.6689e-01],\n        [3.3675e-01],\n        [1.0264e-01],\n        [6.7335e-01],\n        [3.6027e-01],\n        [3.6295e-01],\n        [1.5402e-01],\n        [4.3898e-01],\n        [2.1475e-01],\n        [7.4418e-02],\n        [8.0396e-01],\n        [7.9845e-01],\n        [2.0862e-01],\n        [7.5586e-01],\n        [4.4935e-01],\n        [8.6343e-01],\n        [8.4486e-01],\n        [4.4410e-01],\n        [6.2034e-01],\n        [5.8802e-01],\n        [9.6520e-01],\n        [7.6713e-01],\n        [7.7933e-01],\n        [8.5319e-01],\n        [1.5321e-01],\n        [2.8843e-01],\n        [7.1650e-01],\n        [9.0835e-01],\n        [1.0138e-01],\n        [4.0779e-01],\n        [6.9579e-01],\n        [1.2564e-01],\n        [6.3601e-01],\n        [1.2586e-01],\n        [8.2475e-01],\n        [7.0323e-01],\n        [4.9992e-01],\n        [3.6512e-01],\n        [7.3676e-01],\n        [6.8384e-01],\n        [8.3085e-01],\n        [6.8850e-01],\n        [1.4580e-01],\n        [7.9316e-02],\n        [3.5962e-01],\n        [9.6776e-01],\n        [8.3355e-01],\n        [2.5647e-01],\n        [2.2824e-01],\n        [6.0658e-01],\n        [9.2604e-01],\n        [9.5341e-01],\n        [5.4583e-01],\n        [8.7091e-01],\n        [3.3238e-01],\n        [9.9007e-01],\n        [8.3103e-01],\n        [4.1906e-01],\n        [2.7753e-01],\n        [4.0082e-01],\n        [1.8049e-01],\n        [5.0701e-01],\n        [5.6964e-01],\n        [9.3901e-02],\n        [4.4689e-01],\n        [3.9192e-01],\n        [2.0408e-01],\n        [6.1545e-01],\n        [6.2711e-01],\n        [2.0452e-02],\n        [2.7901e-01],\n        [8.0599e-01],\n        [9.3379e-01],\n        [2.4565e-02],\n        [7.4521e-01],\n        [2.5339e-01],\n        [1.8440e-01],\n        [4.3947e-01],\n        [6.0913e-01],\n        [8.7246e-01],\n        [8.7232e-01],\n        [3.6463e-01],\n        [9.4850e-02],\n        [1.4826e-01],\n        [5.2255e-01],\n        [1.2628e-01],\n        [7.0766e-01],\n        [3.3275e-01],\n        [8.8254e-02],\n        [2.4751e-01],\n        [4.6802e-01],\n        [3.3811e-01],\n        [4.9717e-01],\n        [8.7497e-01],\n        [7.7611e-01],\n        [6.7738e-01],\n        [9.9810e-01],\n        [6.3502e-02],\n        [8.5801e-02],\n        [6.5372e-02],\n        [3.5134e-01],\n        [8.2047e-01],\n        [7.8804e-01],\n        [4.7712e-01],\n        [8.1728e-01],\n        [7.1320e-01],\n        [9.7672e-01],\n        [3.0383e-01],\n        [2.7257e-01],\n        [7.5653e-01],\n        [5.7108e-01],\n        [8.4686e-01],\n        [2.5468e-02],\n        [9.0153e-01],\n        [7.8131e-01],\n        [2.3200e-01],\n        [3.6936e-03],\n        [4.3578e-01],\n        [6.1021e-01],\n        [5.5584e-01],\n        [8.1437e-01],\n        [6.3077e-01],\n        [3.2558e-01],\n        [3.3332e-01],\n        [3.8896e-01],\n        [6.0916e-01],\n        [3.5429e-01],\n        [2.6838e-01],\n        [9.6175e-01],\n        [4.9011e-01],\n        [5.5499e-02],\n        [6.0488e-01],\n        [4.8616e-01],\n        [4.5787e-01],\n        [5.2923e-01],\n        [5.9962e-01],\n        [6.8479e-01],\n        [9.1560e-01],\n        [2.1120e-01],\n        [5.7247e-01],\n        [6.3001e-01],\n        [9.0616e-01],\n        [2.0796e-01],\n        [8.5608e-02],\n        [8.0027e-01],\n        [9.2597e-01],\n        [7.4018e-01],\n        [4.3141e-01],\n        [8.0532e-02],\n        [5.9587e-01],\n        [7.8150e-01],\n        [6.9589e-01],\n        [2.9608e-01],\n        [5.0232e-01],\n        [6.6433e-01],\n        [9.7149e-01],\n        [1.6575e-01],\n        [3.3500e-01],\n        [5.6543e-01],\n        [3.6174e-01],\n        [9.7311e-01],\n        [8.4927e-01],\n        [1.1824e-01],\n        [8.0337e-01],\n        [7.6170e-01],\n        [4.1714e-01],\n        [9.2213e-01],\n        [7.2387e-01],\n        [3.9447e-01],\n        [4.3912e-02],\n        [8.1913e-01],\n        [9.1463e-01],\n        [8.0546e-01],\n        [1.3526e-01],\n        [2.2567e-01],\n        [3.6386e-02],\n        [7.8333e-01],\n        [8.4444e-01],\n        [5.3511e-01],\n        [1.3359e-01],\n        [7.3419e-01],\n        [6.9231e-01],\n        [8.7928e-01],\n        [3.3755e-01],\n        [8.1745e-01],\n        [1.1096e-01],\n        [6.1702e-02],\n        [3.1974e-01],\n        [6.0868e-01]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False,  True, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-1.6412,  0.2751],\n        [ 2.3041,  1.8457],\n        [-2.4406, -1.2745],\n        ...,\n        [ 0.4426,  1.5861],\n        [ 1.2182, -0.7733],\n        [-3.2265, -1.2060]]) torch.Size([9984, 2])\nsamples tensor([[ 1.6107e+00,  1.7543e+00],\n        [ 3.2679e+00,  1.1383e+00],\n        [ 3.4498e+00, -4.7116e-01],\n        [ 2.3141e+00, -6.7450e-01],\n        [ 3.1987e+00,  2.2466e-02],\n        [ 1.7621e+00, -1.1722e-01],\n        [ 2.3273e+00, -3.2082e-01],\n        [ 2.3198e+00,  4.0046e-01],\n        [ 3.1924e+00,  8.7169e-02],\n        [ 3.7909e+00, -1.7314e-03],\n        [ 1.5525e+00, -1.2259e+00],\n        [ 3.7713e+00, -1.0085e+00],\n        [ 3.7754e+00, -6.1703e-01],\n        [ 1.1943e+00, -6.7990e-01],\n        [ 3.0132e+00,  4.9080e-01],\n        [ 1.9369e+00, -2.3613e+00],\n        [ 2.5562e+00, -5.8475e-01],\n        [ 1.3281e+00, -2.2484e+00],\n        [ 1.2034e+00, -1.4812e+00],\n        [ 3.2734e+00,  2.7828e-01],\n        [ 2.4448e+00,  7.4573e-01],\n        [ 1.2557e+00,  2.1885e-01],\n        [ 3.7531e+00, -2.2179e+00],\n        [ 3.6393e+00,  1.4013e-01],\n        [ 1.9104e+00, -2.3892e-01],\n        [ 2.2398e+00,  1.5136e+00],\n        [ 3.3593e+00, -3.4195e-01],\n        [ 1.5801e+00, -1.3616e-01],\n        [ 1.2008e+00, -7.0856e-01],\n        [ 1.6018e+00,  2.0200e+00],\n        [ 2.9636e+00,  5.1524e-01],\n        [ 1.8832e+00, -1.3185e+00],\n        [ 1.8644e+00,  6.3538e-01],\n        [ 3.1734e+00, -1.0139e+00],\n        [ 1.7403e+00,  1.4841e-02],\n        [ 1.6429e+00, -1.0873e+00],\n        [ 1.4366e+00, -3.9373e-01],\n        [ 1.3022e+00,  1.0590e+00],\n        [ 1.4992e+00, -6.2495e-01],\n        [ 3.7488e+00,  5.2619e-04],\n        [ 1.5959e+00, -1.7351e+00],\n        [ 3.3969e+00,  4.6130e-02],\n        [ 2.5824e-01, -1.2673e+00],\n        [ 1.9390e+00,  6.2858e-02],\n        [ 2.2835e+00,  4.8466e-01],\n        [ 3.3559e+00,  4.2578e-01],\n        [ 1.9561e+00,  5.3418e-01],\n        [ 3.3378e+00, -5.9040e-01],\n        [ 2.9597e+00, -5.6861e-01],\n        [ 2.2736e+00,  3.2088e-01],\n        [ 1.5213e+00, -2.0211e-01],\n        [ 1.5520e+00, -3.5462e-01],\n        [ 4.1436e+00, -1.3906e+00],\n        [ 2.0833e+00,  7.9508e-01],\n        [ 3.0072e+00, -1.1925e+00],\n        [ 2.4221e-01, -1.9797e-01],\n        [ 1.8451e+00,  1.0680e+00],\n        [ 2.1035e+00, -3.9498e-01],\n        [ 1.9368e+00, -7.6734e-01],\n        [ 1.7252e+00,  2.5573e-01],\n        [ 3.4765e+00,  2.1622e-01],\n        [ 7.2219e-01,  9.5384e-01],\n        [ 1.5619e+00,  3.6627e-01],\n        [ 2.2044e+00,  9.8983e-02],\n        [ 3.8189e+00, -1.1084e+00],\n        [ 3.3714e+00,  1.1026e+00],\n        [ 4.4423e+00,  1.3989e+00],\n        [ 2.2326e+00, -1.5563e+00],\n        [ 3.5369e+00, -2.2650e+00],\n        [ 1.6899e+00,  8.6057e-01],\n        [ 8.0025e-01, -4.2684e-01],\n        [ 3.3069e+00,  2.1244e-01],\n        [ 3.5239e+00, -1.6315e+00],\n        [ 3.1478e+00, -1.0475e-01],\n        [ 2.4175e+00,  2.0234e-01],\n        [ 3.1667e+00, -1.1670e+00],\n        [ 1.4560e+00, -1.1105e+00],\n        [ 2.6674e+00,  9.2572e-01],\n        [ 1.0318e+00,  1.2334e+00],\n        [ 3.0254e+00,  1.3371e+00],\n        [ 3.1138e+00,  3.8725e-01],\n        [ 2.3333e+00,  1.8057e+00],\n        [ 3.0026e+00,  4.2380e-01],\n        [ 4.8026e+00,  8.4022e-01],\n        [ 2.5706e+00,  7.9353e-01],\n        [ 2.7205e+00, -2.9419e-01],\n        [ 1.4472e+00, -5.6805e-02],\n        [ 3.0832e+00,  2.6261e-03],\n        [ 2.1471e+00, -2.0733e-01],\n        [ 2.5620e+00, -1.5578e+00],\n        [ 3.6542e+00, -2.8502e-01],\n        [ 4.3774e+00, -2.1559e+00],\n        [ 3.0864e+00, -1.7521e-01],\n        [ 1.0201e+00,  4.8795e-01],\n        [ 2.0967e+00,  8.4569e-01],\n        [ 2.7582e+00,  1.5513e+00],\n        [ 2.0107e+00, -7.6153e-01],\n        [ 2.9285e+00,  3.8994e-01],\n        [ 2.7975e+00, -7.6225e-01],\n        [ 2.8908e+00,  3.2272e-01],\n        [ 2.1061e+00, -8.9255e-02],\n        [ 5.3723e-02, -2.0379e+00],\n        [ 1.5958e+00, -5.2836e-01],\n        [ 2.0389e+00, -1.8074e-02],\n        [ 3.8468e+00,  8.8245e-01],\n        [ 2.0026e+00,  8.8209e-01],\n        [ 3.6846e+00,  4.4218e-01],\n        [ 1.9455e-01,  1.6204e+00],\n        [ 2.6176e+00, -6.9602e-01],\n        [ 2.5337e+00, -5.1589e-01],\n        [ 2.6721e+00,  9.7708e-01],\n        [ 1.6768e+00,  9.1712e-01],\n        [ 2.7617e+00, -2.0519e+00],\n        [ 2.3114e+00,  3.9389e-01],\n        [ 2.2723e+00, -2.0674e+00],\n        [ 1.7968e+00, -2.2093e-01],\n        [ 1.5291e+00, -1.8418e-01],\n        [ 1.9780e+00,  3.7928e-01],\n        [ 1.2728e+00, -1.4303e+00],\n        [ 3.5176e+00, -2.0135e+00],\n        [ 2.8823e+00,  5.2125e-01],\n        [ 3.3429e-01,  1.3407e+00],\n        [ 1.8185e+00,  1.5795e+00],\n        [ 2.4884e+00, -5.8330e-01],\n        [ 4.1175e+00,  1.2266e+00],\n        [ 3.4770e+00,  4.0060e-01],\n        [ 1.6467e+00, -1.2096e+00],\n        [ 2.6906e+00,  8.3443e-01],\n        [ 2.7330e+00, -1.7805e+00],\n        [ 3.0516e+00,  5.3932e-01],\n        [ 2.3474e+00,  6.2744e-01],\n        [ 1.9332e+00, -1.2769e+00],\n        [ 4.0020e+00,  5.6700e-01],\n        [ 2.8113e+00, -8.0084e-02],\n        [ 2.4484e+00,  3.0130e-01],\n        [ 1.5711e+00,  7.2864e-01],\n        [ 2.6836e+00,  2.7900e-01],\n        [ 1.4628e+00,  1.1863e+00],\n        [ 3.4725e+00,  4.4775e-03],\n        [ 1.5382e+00,  2.5132e-01],\n        [ 3.4325e+00, -6.0852e-01],\n        [ 2.9709e+00, -7.0374e-01],\n        [ 2.2009e+00, -8.0923e-01],\n        [ 1.8744e+00,  3.9548e-01],\n        [ 2.9878e+00, -4.7810e-01],\n        [ 2.1707e+00, -1.9909e+00],\n        [ 2.6399e+00,  5.6728e-01],\n        [ 3.0134e+00, -8.9889e-01],\n        [ 2.3594e+00,  1.5015e+00],\n        [ 1.9802e+00, -1.5395e+00],\n        [ 2.7024e+00,  1.3127e+00],\n        [ 1.0664e+00, -1.0281e+00],\n        [-1.6546e-01, -1.2265e+00],\n        [ 2.7928e+00, -5.7746e-01],\n        [ 1.8677e+00, -6.1325e-01],\n        [ 3.2196e+00, -8.7625e-01],\n        [ 1.7288e+00, -1.6143e+00],\n        [ 4.0579e+00, -1.3518e+00],\n        [ 2.5948e+00,  4.0497e-01],\n        [ 3.9061e+00,  4.0619e-01],\n        [ 2.5777e+00, -6.7774e-01],\n        [ 6.9329e-01,  6.1501e-02],\n        [ 2.3830e+00,  4.4987e-01],\n        [ 2.5390e+00,  6.0030e-01],\n        [ 3.1441e+00, -1.1002e+00],\n        [ 2.5608e+00, -3.7566e-01],\n        [ 3.3631e+00,  5.8807e-02],\n        [ 2.1385e+00, -7.4840e-01],\n        [ 5.8238e-01, -4.3666e-01],\n        [ 2.8355e+00, -2.1869e+00],\n        [ 1.0818e+00, -6.6732e-01],\n        [ 2.8196e+00, -6.8184e-01],\n        [ 1.3662e+00, -4.3124e-01],\n        [ 1.4588e+00,  5.2219e-02],\n        [ 2.5222e+00,  1.2878e+00],\n        [ 4.1822e+00, -8.7614e-01],\n        [ 2.1167e+00,  7.1614e-01],\n        [ 1.1030e+00, -1.7384e+00],\n        [ 1.8117e+00, -1.1048e+00],\n        [ 3.2357e+00, -1.8286e-01],\n        [ 3.1457e+00, -7.6594e-01],\n        [ 2.1485e+00, -3.3992e-01],\n        [ 3.9256e+00, -2.0172e+00],\n        [ 1.0079e+00, -3.2315e-01],\n        [ 2.6506e+00, -1.0241e+00],\n        [ 3.9483e+00, -8.3570e-01],\n        [ 1.5973e+00,  2.4598e-01],\n        [ 2.5524e+00, -9.0580e-01],\n        [ 2.2254e+00, -6.8258e-01],\n        [ 2.9601e+00, -7.5528e-01],\n        [ 3.7086e+00, -1.0361e+00],\n        [ 2.2231e+00, -2.2816e+00],\n        [ 3.0982e+00, -1.6444e-01],\n        [ 4.0787e+00, -1.0035e+00],\n        [ 1.4478e+00,  3.2384e-01],\n        [ 3.2347e+00,  2.3684e-01],\n        [ 2.6978e+00,  2.0943e+00],\n        [ 1.9888e+00,  4.2139e-01],\n        [ 1.7118e+00, -2.2149e-01],\n        [ 4.0893e+00, -2.2024e-01],\n        [ 1.5785e+00, -1.0998e+00],\n        [ 2.7090e+00,  1.1220e+00],\n        [ 3.4617e+00,  2.9829e-01],\n        [ 2.1291e+00, -8.0922e-01],\n        [ 2.6078e+00, -1.3765e+00],\n        [ 1.5028e+00, -1.9084e-01],\n        [ 2.5219e+00,  1.6451e+00],\n        [ 1.9080e+00, -4.2445e-01],\n        [ 2.8100e+00, -1.7613e+00],\n        [ 2.8095e+00, -6.4911e-01],\n        [ 2.5827e+00, -2.4494e-01],\n        [ 2.2622e+00, -4.3414e-02],\n        [ 3.3067e+00,  1.1433e-03],\n        [ 2.9447e+00, -4.5461e-01],\n        [ 3.6178e+00,  5.4237e-01],\n        [ 1.9342e+00, -2.0203e-01],\n        [ 2.4615e+00, -3.5877e-01],\n        [ 1.8859e+00, -4.0405e-01],\n        [ 2.0975e+00,  8.1371e-01],\n        [ 2.3416e+00,  8.7546e-01],\n        [ 1.7639e+00,  1.0208e+00],\n        [ 1.2355e+00, -7.6153e-01],\n        [ 3.6623e+00, -1.7534e-01],\n        [ 2.7673e+00,  6.2920e-01],\n        [ 2.6957e+00, -5.9907e-01],\n        [ 1.1190e+00,  1.4035e+00],\n        [ 2.1190e+00, -1.5853e+00],\n        [ 2.8759e+00, -8.7622e-01],\n        [ 3.6747e+00, -2.5487e-01],\n        [ 2.0105e+00, -4.0623e-01],\n        [ 3.9647e+00,  6.1095e-02],\n        [ 3.0137e+00,  6.3697e-01],\n        [ 2.1328e+00,  1.1870e+00],\n        [ 1.8517e+00, -1.1201e+00],\n        [ 3.3808e+00,  4.4926e-02],\n        [ 2.5230e+00,  1.1297e+00],\n        [ 1.9082e+00, -1.4459e+00],\n        [ 3.8394e+00, -3.2500e-01],\n        [ 1.9758e+00, -2.0467e-01],\n        [ 1.9053e+00,  5.3483e-01],\n        [ 2.8421e+00,  4.5590e-01],\n        [ 2.5049e+00, -2.2377e+00],\n        [ 1.6595e+00, -6.6694e-01],\n        [ 1.1364e+00, -5.8145e-01],\n        [ 3.0586e+00,  4.1963e-01],\n        [ 1.8517e+00,  1.1950e+00],\n        [ 2.1876e+00, -7.1510e-01],\n        [ 2.3221e+00, -2.2923e+00],\n        [ 1.8872e+00, -9.9882e-01],\n        [ 1.4636e+00,  2.4124e-01],\n        [ 2.8801e+00, -1.0363e+00],\n        [ 3.2976e+00, -7.5910e-01],\n        [ 4.1600e+00, -7.5609e-01],\n        [ 1.8572e+00,  1.3352e+00],\n        [ 3.6064e+00, -1.5576e+00],\n        [ 1.8647e+00,  9.6239e-02],\n        [ 2.7008e+00,  5.3639e-01],\n        [ 1.1804e+00,  5.7341e-01],\n        [ 4.6135e+00, -2.4807e-01],\n        [ 1.5991e+00, -6.3096e-01],\n        [ 2.0637e+00,  1.1770e+00],\n        [ 2.2272e+00,  5.5170e-01],\n        [ 3.4880e+00, -1.5615e+00],\n        [ 2.8085e+00, -1.8065e+00],\n        [ 3.1385e+00,  4.5782e-01],\n        [ 1.2445e+00, -1.1284e-01],\n        [ 1.6817e+00,  3.1243e-02],\n        [ 1.9717e+00, -1.2180e-01],\n        [ 1.7078e+00,  4.5085e-02],\n        [ 2.7271e+00,  1.0352e+00],\n        [ 1.6043e+00, -7.7987e-02],\n        [ 2.4496e+00,  4.3656e-01],\n        [ 3.6313e+00, -2.4463e+00],\n        [ 2.1688e+00, -5.7722e-01],\n        [ 2.9793e+00, -4.6916e-02],\n        [ 2.6514e+00, -2.3068e+00],\n        [ 2.9126e+00, -1.2170e+00],\n        [ 3.5452e+00, -9.8789e-01],\n        [ 1.7225e+00,  8.3430e-01],\n        [ 1.2992e+00, -7.7382e-01],\n        [ 2.7251e+00, -1.5618e+00],\n        [ 2.8648e+00, -3.6411e-01],\n        [ 3.3422e+00,  2.3918e+00],\n        [ 2.4279e+00,  8.2738e-01],\n        [ 2.8353e+00,  8.3343e-01],\n        [ 2.8075e+00, -6.9591e-01],\n        [ 3.4124e+00,  7.0894e-01],\n        [ 3.6857e+00,  9.7078e-01],\n        [ 3.8209e+00, -9.5713e-01],\n        [ 3.9684e+00,  3.5995e-01],\n        [ 2.7312e+00, -9.7465e-02],\n        [ 1.0752e+00,  3.4272e-01],\n        [ 2.9635e+00,  1.7545e+00],\n        [ 9.1765e-01, -1.6185e+00],\n        [ 1.4086e+00,  7.6033e-01],\n        [ 3.1702e+00, -1.2532e+00],\n        [ 2.2659e+00, -4.3149e-01],\n        [ 2.9083e+00, -1.7388e+00],\n        [ 1.3575e+00, -7.6018e-01],\n        [ 2.6259e+00,  2.4057e-01],\n        [ 1.0884e+00,  4.1037e-01],\n        [ 2.1268e+00, -6.8240e-03],\n        [ 3.9133e+00, -9.1963e-01],\n        [ 4.1305e+00, -6.5484e-02],\n        [ 1.3200e+00, -6.4187e-01],\n        [ 1.2912e+00, -1.0940e+00],\n        [ 4.4451e-01, -9.8308e-01],\n        [ 6.7259e-01,  4.1188e-01],\n        [ 1.5337e+00,  1.6446e-01],\n        [ 1.8501e+00,  7.8987e-02],\n        [ 2.4267e+00, -2.4595e-01],\n        [ 1.4010e+00,  2.4782e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[5.6397e-03, 6.9490e-02, 9.8130e-02,  ..., 9.8910e-01, 9.9998e-01,\n         1.0000e+00],\n        [4.8861e-05, 1.4089e-02, 1.4089e-02,  ..., 8.9789e-01, 1.0000e+00,\n         1.0000e+00],\n        [9.8638e-05, 6.9199e-03, 6.9221e-03,  ..., 9.8262e-01, 9.8262e-01,\n         1.0000e+00],\n        ...,\n        [3.9622e-03, 3.9654e-03, 3.9655e-03,  ..., 9.6204e-01, 9.6204e-01,\n         1.0000e+00],\n        [2.0545e-02, 2.0651e-02, 2.0651e-02,  ..., 9.1268e-01, 9.1279e-01,\n         1.0000e+00],\n        [7.7495e-02, 7.7495e-02, 7.7496e-02,  ..., 1.0000e+00, 1.0000e+00,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.7826],\n        [0.0908],\n        [0.4442],\n        [0.1009],\n        [0.4540],\n        [0.4409],\n        [0.4720],\n        [0.3508],\n        [0.0232],\n        [0.9534],\n        [0.4378],\n        [0.4459],\n        [0.4977],\n        [0.7702],\n        [0.4494],\n        [0.8219],\n        [0.1453],\n        [0.6914],\n        [0.1475],\n        [0.6979],\n        [0.4761],\n        [0.9967],\n        [0.1074],\n        [0.4630],\n        [0.7676],\n        [0.0159],\n        [0.3910],\n        [0.1145],\n        [0.3134],\n        [0.7378],\n        [0.1970],\n        [0.1952],\n        [0.8699],\n        [0.8379],\n        [0.6548],\n        [0.7883],\n        [0.0212],\n        [0.3196],\n        [0.9320],\n        [0.3069],\n        [0.9652],\n        [0.0131],\n        [0.8530],\n        [0.2706],\n        [0.7228],\n        [0.2047],\n        [0.1445],\n        [0.4728],\n        [0.2416],\n        [0.9013],\n        [0.1897],\n        [0.2525],\n        [0.0784],\n        [0.4541],\n        [0.8440],\n        [0.9527],\n        [0.8593],\n        [0.2710],\n        [0.6986],\n        [0.6471],\n        [0.4280],\n        [0.0388],\n        [0.9031],\n        [0.0988],\n        [0.0513],\n        [0.5172],\n        [0.1278],\n        [0.8314],\n        [0.3170],\n        [0.4182],\n        [0.5175],\n        [0.4229],\n        [0.2367],\n        [0.4257],\n        [0.5417],\n        [0.7754],\n        [0.2463],\n        [0.8944],\n        [0.1181],\n        [0.0920],\n        [0.1203],\n        [0.9768],\n        [0.5212],\n        [0.4632],\n        [0.6069],\n        [0.9245],\n        [0.8549],\n        [0.0664],\n        [0.7394],\n        [0.1693],\n        [0.4704],\n        [0.5112],\n        [0.1400],\n        [0.4272],\n        [0.4801],\n        [0.9875],\n        [0.0719],\n        [0.3959],\n        [0.3078],\n        [0.1707],\n        [0.3358],\n        [0.0218],\n        [0.2003],\n        [0.4640],\n        [0.1250],\n        [0.2429],\n        [0.6638],\n        [0.0027],\n        [0.4616],\n        [0.4406],\n        [0.4455],\n        [0.9756],\n        [0.9124],\n        [0.2585],\n        [0.5544],\n        [0.9792],\n        [0.1020],\n        [0.9438],\n        [0.6551],\n        [0.0863],\n        [0.1944],\n        [0.4197],\n        [0.0741],\n        [0.0301],\n        [0.0606],\n        [0.8983],\n        [0.7689],\n        [0.2970],\n        [0.7643],\n        [0.5341],\n        [0.6755],\n        [0.2849],\n        [0.2691],\n        [0.3975],\n        [0.9590],\n        [0.7031],\n        [0.3348],\n        [0.3033],\n        [0.9060],\n        [0.4548],\n        [0.0068],\n        [0.8557],\n        [0.4473],\n        [0.7241],\n        [0.7886],\n        [0.6795],\n        [0.5135],\n        [0.8600],\n        [0.8810],\n        [0.8109],\n        [0.1614],\n        [0.7557],\n        [0.8907],\n        [0.9653],\n        [0.5187],\n        [0.8145],\n        [0.4253],\n        [0.8973],\n        [0.0511],\n        [0.4406],\n        [0.6844],\n        [0.9478],\n        [0.7415],\n        [0.3286],\n        [0.0353],\n        [0.1688],\n        [0.9588],\n        [0.6639],\n        [0.0904],\n        [0.8825],\n        [0.6955],\n        [0.9342],\n        [0.0866],\n        [0.7589],\n        [0.0438],\n        [0.5296],\n        [0.4667],\n        [0.9715],\n        [0.0556],\n        [0.7591],\n        [0.7856],\n        [0.7338],\n        [0.1834],\n        [0.0742],\n        [0.6506],\n        [0.2423],\n        [0.3906],\n        [0.8061],\n        [0.0136],\n        [0.1325],\n        [0.3006],\n        [0.9684],\n        [0.7216],\n        [0.7323],\n        [0.2179],\n        [0.6030],\n        [0.0958],\n        [0.1629],\n        [0.2441],\n        [0.6720],\n        [0.4076],\n        [0.4979],\n        [0.3306],\n        [0.3344],\n        [0.2536],\n        [0.8324],\n        [0.0460],\n        [0.1184],\n        [0.8438],\n        [0.3221],\n        [0.8448],\n        [0.6350],\n        [0.3101],\n        [0.8482],\n        [0.1561],\n        [0.6424],\n        [0.4813],\n        [0.6533],\n        [0.2131],\n        [0.8531],\n        [0.9548],\n        [0.4962],\n        [0.3795],\n        [0.8875],\n        [0.7193],\n        [0.9758],\n        [0.7595],\n        [0.5974],\n        [0.3131],\n        [0.1117],\n        [0.7958],\n        [0.9710],\n        [0.8661],\n        [0.4304],\n        [0.3944],\n        [0.1086],\n        [0.0134],\n        [0.3728],\n        [0.2980],\n        [0.4793],\n        [0.6866],\n        [0.8545],\n        [0.8717],\n        [0.6517],\n        [0.7825],\n        [0.7208],\n        [0.6274],\n        [0.7983],\n        [0.8914],\n        [0.0570],\n        [0.8931],\n        [0.2271],\n        [0.0705],\n        [0.3198],\n        [0.5320],\n        [0.3078],\n        [0.6187],\n        [0.6601],\n        [0.0860],\n        [0.0216],\n        [0.6492],\n        [0.5456],\n        [0.9024],\n        [0.5339],\n        [0.7640],\n        [0.9206],\n        [0.8972],\n        [0.8635],\n        [0.1270],\n        [0.5385],\n        [0.1668],\n        [0.8188],\n        [0.0681],\n        [0.9973],\n        [0.1026],\n        [0.4285],\n        [0.2173],\n        [0.6656],\n        [0.6196],\n        [0.0989],\n        [0.6252],\n        [0.7006],\n        [0.2936],\n        [0.3914],\n        [0.0355],\n        [0.6490],\n        [0.5693],\n        [0.0441],\n        [0.4918],\n        [0.5342],\n        [0.2316],\n        [0.4293],\n        [0.6443],\n        [0.9580],\n        [0.8219],\n        [0.0868],\n        [0.6524],\n        [0.4118],\n        [0.0308],\n        [0.3550],\n        [0.3853],\n        [0.8862],\n        [0.3825],\n        [0.7045],\n        [0.6256],\n        [0.2841],\n        [0.0703],\n        [0.9025],\n        [0.2737],\n        [0.5740],\n        [0.9866],\n        [0.9593]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ...,  True, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False,  True],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-0.1975, -0.6961],\n        [ 1.8554,  0.1248],\n        [ 1.6902,  0.6187],\n        ...,\n        [ 0.6484,  0.4715],\n        [-3.3867,  0.7532],\n        [-4.2365, -2.1256]]) torch.Size([9984, 2])\nsamples tensor([[ 3.3390, -0.6555],\n        [ 3.6977, -0.4443],\n        [ 2.6783, -1.6186],\n        [ 1.7039,  0.7320],\n        [ 2.7680,  0.7561],\n        [ 2.6001,  0.3567],\n        [ 4.2690, -0.5834],\n        [ 2.4403,  0.4944],\n        [ 1.6692, -1.7265],\n        [ 2.2107, -0.3569],\n        [ 3.1518, -0.2413],\n        [ 1.6447,  0.7189],\n        [ 3.5744,  1.8925],\n        [ 1.9363, -0.2771],\n        [ 1.1920, -0.0110],\n        [ 2.0786, -0.5000],\n        [ 1.7173, -1.2758],\n        [ 3.5905,  0.3970],\n        [ 3.4299, -1.2886],\n        [ 1.2345,  0.7771],\n        [ 3.7781,  0.1812],\n        [ 1.7465,  2.1576],\n        [ 1.3901, -0.8769],\n        [ 2.6142, -1.6993],\n        [ 2.1504, -1.3021],\n        [ 0.9436,  0.2802],\n        [ 2.6301, -0.7443],\n        [ 1.8490, -0.0351],\n        [ 3.0039, -0.6054],\n        [ 2.6748, -0.6478],\n        [ 2.2770, -0.0382],\n        [ 0.6270,  1.0858],\n        [ 0.6387,  0.6481],\n        [ 2.9078,  1.3346],\n        [ 0.6305, -1.4586],\n        [ 1.8431,  0.1695],\n        [ 3.2258,  0.0992],\n        [ 2.2244, -1.9165],\n        [ 2.7420,  0.7831],\n        [ 1.8700,  1.1849],\n        [ 2.9480, -1.5515],\n        [ 0.7147, -0.8365],\n        [ 3.4787, -0.4095],\n        [ 1.4146,  0.4172],\n        [ 1.1641,  0.1367],\n        [ 0.8204, -0.3123],\n        [ 2.3651,  0.4342],\n        [ 2.1760,  0.9465],\n        [ 2.6293, -0.5539],\n        [ 3.0281, -0.8283],\n        [ 2.1283,  2.2773],\n        [ 2.1617,  1.4459],\n        [ 1.4611, -0.1272],\n        [ 2.3369,  0.0153],\n        [ 3.4278, -0.5820],\n        [ 3.1852,  1.5111],\n        [ 3.2096, -0.7834],\n        [ 3.7386, -2.2570],\n        [ 4.3932, -1.2027],\n        [ 2.9012, -0.8286],\n        [ 3.1219, -0.8863],\n        [ 1.4527,  0.4151],\n        [-0.1533,  0.7111],\n        [ 2.5512, -0.1289],\n        [ 1.8614, -0.7276],\n        [ 3.7927, -0.0352],\n        [ 1.1778, -0.9368],\n        [ 3.8833, -1.2631],\n        [ 2.2434, -0.7342],\n        [ 2.3325,  1.3405],\n        [ 2.1982, -1.3177],\n        [ 2.1503, -0.8869],\n        [ 1.7303, -0.4506],\n        [ 3.2102,  0.5028],\n        [ 1.4488,  0.1148],\n        [ 3.8278, -1.7641],\n        [ 1.2508,  1.0992],\n        [ 2.9274, -0.7920],\n        [ 2.0544, -0.1948],\n        [ 0.9983,  0.2315],\n        [ 1.7144, -0.5259],\n        [ 3.7835,  0.9226],\n        [ 3.7486,  1.3012],\n        [ 2.5099, -1.3571],\n        [ 2.4459,  0.0527],\n        [ 3.1961,  0.8947],\n        [ 1.2140, -1.1670],\n        [ 0.6741, -0.4878],\n        [ 1.8284, -0.1677],\n        [ 1.0254,  0.1357],\n        [ 1.7428, -1.1122],\n        [ 3.2205, -1.0670],\n        [ 2.4684,  1.3559],\n        [ 4.0473, -1.5038],\n        [ 4.5443,  0.2025],\n        [ 2.6366,  0.8487],\n        [ 1.9101, -0.8038],\n        [ 3.4757, -0.3960],\n        [ 4.0467, -0.5074],\n        [ 3.1671,  0.6584],\n        [ 2.9507, -1.5662],\n        [ 2.9905, -1.9713],\n        [ 2.2340,  0.1977],\n        [ 2.0157, -0.4641],\n        [ 3.2824,  0.6428],\n        [ 3.6873,  0.3849],\n        [ 3.4161,  0.3858],\n        [ 0.7139,  0.9417],\n        [ 2.7688,  0.4018],\n        [ 2.9042, -0.0745],\n        [ 2.9054,  0.4347],\n        [ 1.2654,  0.8800],\n        [ 2.0287,  0.6845],\n        [ 3.5138,  0.3506],\n        [ 2.6633, -0.2869],\n        [ 1.8993,  0.0286],\n        [ 3.5642, -1.6073],\n        [ 2.9861, -1.0011],\n        [ 1.7665, -1.9375],\n        [ 2.4511, -0.7719],\n        [ 3.4567, -0.4797],\n        [ 2.4910,  0.4647],\n        [ 1.2827, -0.8262],\n        [ 1.6685, -1.7377],\n        [ 3.5731,  0.9656],\n        [ 3.1433,  1.3953],\n        [ 1.8938, -2.0044],\n        [ 2.7339,  0.6008],\n        [ 3.2304,  0.5812],\n        [ 2.0808, -0.4274],\n        [ 3.6678, -1.2851],\n        [ 2.5051, -1.5635],\n        [ 2.3257,  1.5543],\n        [ 2.1599, -0.6028],\n        [ 1.2295,  0.4055],\n        [ 3.3439, -0.2172],\n        [ 1.0374, -0.6039],\n        [ 3.4888,  0.6530],\n        [ 3.0730, -0.5336],\n        [ 2.8949, -1.7429],\n        [ 0.9040,  1.1458],\n        [ 0.4915, -0.6736],\n        [ 2.0709, -0.8800],\n        [ 1.9736, -0.9432],\n        [ 1.9880, -1.7728],\n        [ 1.9809, -0.3657],\n        [ 3.0125, -1.6333],\n        [ 3.0412,  0.7153],\n        [ 3.4509,  0.7865],\n        [ 0.6864, -0.4868],\n        [ 3.1259, -1.1040],\n        [ 1.8351, -0.8470],\n        [ 0.9784,  0.6568],\n        [ 0.3871, -0.7749],\n        [ 4.0384, -1.3336],\n        [ 3.7352, -0.9269],\n        [ 3.4481, -0.4668],\n        [ 2.1369,  0.5183],\n        [ 2.7998,  0.8221],\n        [ 0.4232, -1.2193],\n        [ 2.1673, -1.9495],\n        [ 1.8305,  0.6476],\n        [ 3.2110,  1.1400],\n        [ 1.1044, -0.1957],\n        [ 3.7948,  1.4272],\n        [ 2.2830,  0.2336],\n        [ 1.0761,  0.9949],\n        [ 2.4903,  0.2635],\n        [ 2.6002, -0.9455],\n        [ 3.4941, -0.6328],\n        [ 1.1801, -1.2352],\n        [ 2.4779,  1.0737],\n        [ 1.8651,  0.2770],\n        [ 3.4048,  1.2932],\n        [ 1.3425,  0.8264],\n        [ 1.4103,  0.1291],\n        [ 2.1199, -0.5565],\n        [ 3.3021,  0.5332],\n        [ 4.1333, -0.9550],\n        [ 2.5795,  0.8399],\n        [ 1.9797,  1.3616],\n        [ 2.6522, -0.3093],\n        [ 1.6128, -1.1707],\n        [ 2.1860,  0.9852],\n        [ 1.7950, -0.7769],\n        [ 0.8738, -0.1575],\n        [ 1.8283,  0.2713],\n        [ 2.5464, -0.5211],\n        [ 1.4018, -0.3517],\n        [ 1.5601,  0.1202],\n        [ 2.5103, -0.8911],\n        [ 1.4991,  0.3674],\n        [ 2.5663, -0.9983],\n        [ 1.3663, -0.2178],\n        [ 2.3682, -1.2834],\n        [ 2.6519, -0.8909],\n        [ 2.9195, -1.2489],\n        [ 3.5315, -0.6370],\n        [ 3.0102,  0.3014],\n        [ 1.0734,  0.7101],\n        [ 2.4433, -1.1437],\n        [ 3.7260, -0.3911],\n        [ 2.0608,  0.1903],\n        [ 1.7579,  0.0427],\n        [ 1.8811, -1.9150],\n        [ 2.3335, -0.8117],\n        [ 2.3162, -0.6918],\n        [ 1.0523,  0.9845],\n        [ 3.8296,  0.5876],\n        [ 4.2494, -0.4855],\n        [ 2.0086, -0.2231],\n        [ 3.1937, -1.1324],\n        [ 2.0677, -0.4425],\n        [ 2.5605, -0.0648],\n        [ 3.1764, -0.9590],\n        [ 0.4127, -0.5647],\n        [ 4.1483,  0.7848],\n        [ 2.1121,  0.3835],\n        [ 2.2447, -1.2289],\n        [ 2.0137, -0.4870],\n        [ 4.5436, -0.1784],\n        [ 2.7068, -1.3325],\n        [ 1.8324, -0.8088],\n        [ 2.4187,  0.0064],\n        [ 2.4141,  0.8154],\n        [ 2.5626,  1.4814],\n        [ 2.0766, -2.4591],\n        [ 2.7688,  0.3229],\n        [ 1.2694, -1.1933],\n        [ 2.2734, -0.0480],\n        [ 0.9197,  1.2585],\n        [ 1.4920, -0.1410],\n        [ 4.8906, -0.1523],\n        [ 2.8250, -1.7994],\n        [ 1.1578,  0.7222],\n        [ 3.5042, -0.2227],\n        [ 0.9335,  1.7886],\n        [-0.4133, -0.1154],\n        [ 1.7484, -0.7898],\n        [ 1.2694,  0.8426],\n        [ 3.5448, -1.5004],\n        [ 1.2383,  0.1198],\n        [ 2.4550,  0.5981],\n        [ 3.5047, -2.2242],\n        [ 1.7853, -0.9914],\n        [ 1.7142, -2.4116],\n        [ 2.8683, -0.4392],\n        [ 4.2186,  0.6701],\n        [ 2.4742, -0.9431],\n        [ 3.8292, -0.7366],\n        [ 3.0103,  0.0334],\n        [ 4.2905,  1.2220],\n        [ 2.0405, -0.6383],\n        [ 2.1142, -2.7764],\n        [ 4.2060, -0.1562],\n        [ 2.8158, -1.1159],\n        [ 3.8432, -0.0329],\n        [ 2.9680,  0.5791],\n        [ 1.4245, -0.6069],\n        [ 2.2240,  1.1619],\n        [ 3.2683, -1.1563],\n        [ 2.6410, -1.4789],\n        [ 2.1046, -0.7146],\n        [ 1.9557,  0.0855],\n        [ 1.4104, -0.1167],\n        [ 3.4778,  0.1062],\n        [ 0.7194, -0.4670],\n        [ 2.4459, -0.1307],\n        [ 2.3302,  0.8829],\n        [ 1.7395, -0.5261],\n        [ 2.7146, -1.7700],\n        [ 1.7540,  0.1264],\n        [ 2.8636, -0.7033],\n        [ 1.4287,  0.1211],\n        [ 2.3919,  0.1103],\n        [ 2.1515,  0.3920],\n        [ 1.2733, -0.2642],\n        [ 3.1550, -1.7088],\n        [ 1.7272,  0.0252],\n        [ 2.4042,  0.5007],\n        [ 2.0845, -0.9230],\n        [ 1.9289, -0.7350],\n        [ 3.2523,  3.3171],\n        [ 1.0281, -0.9632],\n        [ 0.8898,  0.1139],\n        [ 1.6367, -0.4815],\n        [ 2.3537, -1.3009],\n        [ 1.8105,  0.0442],\n        [ 3.2515,  2.0359],\n        [ 2.3178, -0.2730],\n        [ 1.3123, -1.0020],\n        [ 3.8512,  0.8510],\n        [ 2.3621,  0.9572],\n        [ 0.1417, -0.7695],\n        [ 1.5010, -0.5359],\n        [ 4.0066,  0.5873],\n        [ 2.5914,  0.4949],\n        [ 1.2379, -1.1537],\n        [ 2.9060, -1.2541],\n        [ 3.1054, -1.4218],\n        [ 2.9221, -3.1560],\n        [ 3.1091, -0.4441],\n        [ 1.3614, -1.3611],\n        [ 3.7345, -1.7788],\n        [ 1.9352, -0.8539],\n        [ 2.9265, -1.4872],\n        [ 3.0219,  0.0085],\n        [ 1.2999, -0.7663],\n        [ 3.4663,  1.0381],\n        [-0.2884, -0.5060],\n        [ 1.4292,  0.3242],\n        [ 1.4964, -0.0081]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.9009e-05, 3.9851e-05, 1.2733e-04,  ..., 9.9836e-01, 9.9836e-01,\n         1.0000e+00],\n        [1.5702e-01, 1.5702e-01, 1.9162e-01,  ..., 5.4812e-01, 9.5383e-01,\n         1.0000e+00],\n        [6.2417e-05, 4.8168e-03, 1.9778e-01,  ..., 9.3532e-01, 9.9999e-01,\n         1.0000e+00],\n        ...,\n        [6.8974e-02, 6.9461e-02, 6.9624e-02,  ..., 9.9951e-01, 1.0000e+00,\n         1.0000e+00],\n        [1.8067e-02, 2.3106e-02, 2.3175e-02,  ..., 9.9662e-01, 9.9960e-01,\n         1.0000e+00],\n        [1.4190e-04, 1.7694e-03, 8.3601e-03,  ..., 9.9971e-01, 9.9993e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.9818],\n        [0.4729],\n        [0.0453],\n        [0.1583],\n        [0.2304],\n        [0.0372],\n        [0.8012],\n        [0.7168],\n        [0.2055],\n        [0.5484],\n        [0.7470],\n        [0.3458],\n        [0.6307],\n        [0.2216],\n        [0.0861],\n        [0.0858],\n        [0.0886],\n        [0.0903],\n        [0.1292],\n        [0.0100],\n        [0.3511],\n        [0.3585],\n        [0.6212],\n        [0.6866],\n        [0.3236],\n        [0.5237],\n        [0.2755],\n        [0.1633],\n        [0.0315],\n        [0.5274],\n        [0.7633],\n        [0.2561],\n        [0.9843],\n        [0.1958],\n        [0.5916],\n        [0.2203],\n        [0.9996],\n        [0.0555],\n        [0.8885],\n        [0.2411],\n        [0.3737],\n        [0.6352],\n        [0.4795],\n        [0.4718],\n        [0.6776],\n        [0.0234],\n        [0.5247],\n        [0.7975],\n        [0.9808],\n        [0.5879],\n        [0.5287],\n        [0.8631],\n        [0.8367],\n        [0.9726],\n        [0.5258],\n        [0.2546],\n        [0.4174],\n        [0.0099],\n        [0.7326],\n        [0.7404],\n        [0.4282],\n        [0.3637],\n        [0.6532],\n        [0.9473],\n        [0.1323],\n        [0.2984],\n        [0.2550],\n        [0.8458],\n        [0.8991],\n        [0.1550],\n        [0.9093],\n        [0.9828],\n        [0.0654],\n        [0.1941],\n        [0.2084],\n        [0.1141],\n        [0.4400],\n        [0.8622],\n        [0.4581],\n        [0.9825],\n        [0.0954],\n        [0.8016],\n        [0.2330],\n        [0.7147],\n        [0.8919],\n        [0.8914],\n        [0.0638],\n        [0.1860],\n        [0.1566],\n        [0.6339],\n        [0.7591],\n        [0.5128],\n        [0.4034],\n        [0.0412],\n        [0.2286],\n        [0.3767],\n        [0.5044],\n        [0.0234],\n        [0.6191],\n        [0.8340],\n        [0.4947],\n        [0.0822],\n        [0.9730],\n        [0.1679],\n        [0.5003],\n        [0.3592],\n        [0.5471],\n        [0.2290],\n        [0.8100],\n        [0.6088],\n        [0.0322],\n        [0.1850],\n        [0.5777],\n        [0.1196],\n        [0.5529],\n        [0.0716],\n        [0.4748],\n        [0.4329],\n        [0.4841],\n        [0.6713],\n        [0.2012],\n        [0.0503],\n        [0.5711],\n        [0.8007],\n        [0.2903],\n        [0.0215],\n        [0.3618],\n        [0.8736],\n        [0.3412],\n        [0.0449],\n        [0.3034],\n        [0.5254],\n        [0.4058],\n        [0.0956],\n        [0.3328],\n        [0.3536],\n        [0.7657],\n        [0.1269],\n        [0.8494],\n        [0.3054],\n        [0.9263],\n        [0.8888],\n        [0.4694],\n        [0.3631],\n        [0.5243],\n        [0.4888],\n        [0.7006],\n        [0.7660],\n        [0.0948],\n        [0.8956],\n        [0.1281],\n        [0.7839],\n        [0.3320],\n        [0.2502],\n        [0.3011],\n        [0.6200],\n        [0.9473],\n        [0.2347],\n        [0.8400],\n        [0.5499],\n        [0.4948],\n        [0.6986],\n        [0.0223],\n        [0.5162],\n        [0.2191],\n        [0.2596],\n        [0.8008],\n        [0.0965],\n        [0.8842],\n        [0.8471],\n        [0.3713],\n        [0.2527],\n        [0.9597],\n        [0.6933],\n        [0.7796],\n        [0.4297],\n        [0.7593],\n        [0.6953],\n        [0.9218],\n        [0.2227],\n        [0.3942],\n        [0.1337],\n        [0.9363],\n        [0.0420],\n        [0.1025],\n        [0.4961],\n        [0.9844],\n        [0.7539],\n        [0.3347],\n        [0.6726],\n        [0.5682],\n        [0.5932],\n        [0.1910],\n        [0.0993],\n        [0.5502],\n        [0.1498],\n        [0.6192],\n        [0.0016],\n        [0.2351],\n        [0.9639],\n        [0.5953],\n        [0.9955],\n        [0.5428],\n        [0.2084],\n        [0.6798],\n        [0.2165],\n        [0.4713],\n        [0.4629],\n        [0.8548],\n        [0.2935],\n        [0.7929],\n        [0.6793],\n        [0.3143],\n        [0.9028],\n        [0.4204],\n        [0.9012],\n        [0.5473],\n        [0.8434],\n        [0.7969],\n        [0.3559],\n        [0.3299],\n        [0.7353],\n        [0.4845],\n        [0.5487],\n        [0.7038],\n        [0.4342],\n        [0.1333],\n        [0.0185],\n        [0.9145],\n        [0.1801],\n        [0.4780],\n        [0.1479],\n        [0.4745],\n        [0.6489],\n        [0.7281],\n        [0.4996],\n        [0.5769],\n        [0.7503],\n        [0.7734],\n        [0.2899],\n        [0.8852],\n        [0.4760],\n        [0.4485],\n        [0.6069],\n        [0.2103],\n        [0.5968],\n        [0.3190],\n        [0.0989],\n        [0.3247],\n        [0.6186],\n        [0.3651],\n        [0.8676],\n        [0.6147],\n        [0.0129],\n        [0.6579],\n        [0.2946],\n        [0.3014],\n        [0.1694],\n        [0.4834],\n        [0.2946],\n        [0.9297],\n        [0.4836],\n        [0.6325],\n        [0.7141],\n        [0.2197],\n        [0.5994],\n        [0.7527],\n        [0.2886],\n        [0.1561],\n        [0.2836],\n        [0.5754],\n        [0.0701],\n        [0.6432],\n        [0.4586],\n        [0.5492],\n        [0.3946],\n        [0.1961],\n        [0.8890],\n        [0.3407],\n        [0.0600],\n        [0.7567],\n        [0.7685],\n        [0.1920],\n        [0.5911],\n        [0.3185],\n        [0.5710],\n        [0.4826],\n        [0.1011],\n        [0.4222],\n        [0.1939],\n        [0.5360],\n        [0.1788],\n        [0.6662],\n        [0.5642],\n        [0.7375],\n        [0.9091],\n        [0.6605],\n        [0.0082],\n        [0.8861],\n        [0.8806],\n        [0.1618],\n        [0.1932],\n        [0.1697],\n        [0.1325],\n        [0.2913],\n        [0.3154],\n        [0.3918],\n        [0.0650],\n        [0.8180],\n        [0.1955],\n        [0.3895],\n        [0.4604]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False,  True,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-1.4748,  0.5823],\n        [-1.1687,  2.1220],\n        [-1.2317, -0.1138],\n        ...,\n        [ 0.5863,  0.1726],\n        [-0.5489,  1.3029],\n        [-0.8271,  2.0497]]) torch.Size([9984, 2])\nsamples tensor([[ 3.8296,  0.0343],\n        [ 2.0745,  0.6239],\n        [ 2.2474,  0.0779],\n        [ 1.3235,  0.1572],\n        [ 3.1680, -0.5199],\n        [ 2.1691, -0.3465],\n        [ 2.2348,  0.2173],\n        [ 1.2999,  0.7104],\n        [ 2.3340,  0.6137],\n        [ 2.8282, -0.5028],\n        [ 2.1320, -1.8904],\n        [ 3.5509, -0.7314],\n        [ 3.4143,  0.8696],\n        [ 2.9777, -1.1154],\n        [ 3.5472,  0.3414],\n        [ 1.9168,  0.4765],\n        [ 0.8444, -0.1732],\n        [ 1.6845,  0.1855],\n        [ 2.6654,  0.2497],\n        [ 2.5694, -0.5013],\n        [ 1.0225, -1.7583],\n        [ 1.7377, -0.7190],\n        [ 0.4582, -2.0044],\n        [ 1.2261,  0.1674],\n        [ 3.4085, -1.5600],\n        [ 2.2125,  0.4092],\n        [ 2.9199, -0.7909],\n        [ 3.0672,  1.2591],\n        [ 1.7773, -0.3559],\n        [ 1.7173, -2.1789],\n        [ 3.4898, -0.6967],\n        [ 3.5443, -1.1020],\n        [ 2.1419, -3.2433],\n        [ 2.4272,  0.7106],\n        [ 3.0558, -0.3240],\n        [ 3.2011,  0.1488],\n        [ 2.1247,  2.5211],\n        [ 2.2919,  0.1610],\n        [ 2.6654,  0.9468],\n        [ 1.7803, -1.4685],\n        [ 3.6618,  0.0062],\n        [ 3.5517, -1.7581],\n        [ 2.4396, -0.1649],\n        [ 2.5327,  0.1482],\n        [ 3.3751, -1.4489],\n        [ 2.0244,  0.9755],\n        [ 3.9656, -0.6047],\n        [ 3.4588, -0.2577],\n        [ 0.9035,  0.5680],\n        [ 1.4949,  0.1799],\n        [ 2.6722, -1.9947],\n        [ 0.1408, -1.7069],\n        [ 2.0968, -0.7670],\n        [ 1.2206,  1.9542],\n        [ 3.3522, -1.3852],\n        [ 2.8864, -1.1185],\n        [ 1.3733,  0.7869],\n        [ 1.8692, -0.1405],\n        [ 2.4064,  1.0613],\n        [ 1.6337,  0.7446],\n        [ 1.8411, -0.4558],\n        [ 2.2445, -2.3763],\n        [ 2.8330,  0.2482],\n        [ 0.9305,  1.6722],\n        [ 2.8169, -0.6721],\n        [ 2.5754, -0.9047],\n        [ 1.5294, -0.4974],\n        [ 2.8269, -0.2968],\n        [ 1.5319, -0.1144],\n        [ 1.8304,  1.4489],\n        [ 4.6581,  0.7743],\n        [-0.3377, -0.3132],\n        [ 2.0133,  1.2163],\n        [ 2.0146, -1.3349],\n        [ 1.6648,  0.5889],\n        [ 4.1097, -1.8171],\n        [ 0.3835, -2.2922],\n        [ 2.0086, -0.7994],\n        [ 1.4388, -0.3353],\n        [ 3.8186, -0.1606],\n        [ 3.0463,  1.5571],\n        [ 2.7942,  0.0921],\n        [ 1.7926,  1.8079],\n        [ 2.0915, -0.8563],\n        [ 2.5799,  0.1825],\n        [ 1.6125, -0.0648],\n        [ 2.9719,  0.0698],\n        [ 1.8810,  0.8784],\n        [ 1.8979,  0.3517],\n        [ 3.3899, -0.0925],\n        [ 2.0255,  0.1943],\n        [ 2.5724, -0.3042],\n        [ 3.3958, -0.1264],\n        [ 1.6554,  0.4199],\n        [ 1.3878,  1.5683],\n        [ 2.9126, -0.6100],\n        [ 2.7760, -0.5848],\n        [ 3.0745, -0.7813],\n        [ 3.9648, -1.3737],\n        [ 4.0649, -1.4866],\n        [ 3.2470,  0.9976],\n        [ 1.4551, -0.2590],\n        [ 0.4243,  0.7357],\n        [ 0.9130,  0.1271],\n        [ 0.1784,  0.2129],\n        [ 0.5715,  0.5619],\n        [ 2.2467,  1.5619],\n        [ 3.3120,  0.5801],\n        [ 2.1541, -0.2775],\n        [ 1.2081, -0.6860],\n        [ 1.8208,  0.8735],\n        [ 1.4760,  0.2057],\n        [ 1.4298, -0.6762],\n        [ 2.7880,  1.1375],\n        [ 4.2284, -1.1595],\n        [ 2.7943,  1.5195],\n        [ 2.2568, -1.1539],\n        [ 2.1691, -0.0891],\n        [ 1.1190,  0.9687],\n        [ 1.1293,  0.0241],\n        [ 2.5090, -1.7162],\n        [ 1.5872,  1.4261],\n        [ 4.1306, -0.8231],\n        [ 2.1430, -0.3544],\n        [ 3.6427, -0.3302],\n        [ 2.5224, -1.5612],\n        [ 2.3245, -0.5156],\n        [ 2.6941, -0.1473],\n        [ 1.4408, -0.6368],\n        [ 1.9931,  0.7839],\n        [ 2.3052, -2.1096],\n        [ 2.2132,  0.3384],\n        [ 4.7060, -0.8762],\n        [ 0.7824, -0.1645],\n        [ 2.5515, -1.5128],\n        [ 3.1394, -1.2986],\n        [ 1.3097,  0.3072],\n        [ 0.6217, -0.1793],\n        [ 4.2023, -0.0980],\n        [ 3.4198, -0.9059],\n        [ 2.7078,  0.3194],\n        [ 1.7533,  0.3481],\n        [ 1.4002, -0.5403],\n        [ 3.6511,  0.2969],\n        [ 2.6937, -1.1323],\n        [ 2.4533,  0.3725],\n        [ 1.7274,  1.8479],\n        [ 4.4795, -0.2281],\n        [ 3.3173, -1.8647],\n        [ 1.5969,  1.0669],\n        [ 4.3572,  0.2456],\n        [ 4.0053,  0.1089],\n        [ 3.6099,  0.5114],\n        [ 1.5685, -0.4197],\n        [ 2.4671, -1.2996],\n        [ 2.6467, -0.1056],\n        [ 1.7846, -0.1074],\n        [ 0.8999, -0.9982],\n        [ 3.1669, -0.1229],\n        [ 1.8244, -2.1554],\n        [ 4.5966, -1.4619],\n        [ 1.9934, -0.6004],\n        [ 1.3323,  0.0063],\n        [ 2.3953, -0.4629],\n        [ 4.0488, -2.3800],\n        [ 2.3433, -1.1724],\n        [ 2.1904, -0.6619],\n        [ 3.1360, -0.4988],\n        [ 2.0022, -1.2346],\n        [ 2.9688, -0.2634],\n        [ 3.7216, -0.6083],\n        [ 2.7183,  0.3894],\n        [ 1.0719, -0.1806],\n        [ 1.3221,  0.0443],\n        [ 1.4119, -0.9890],\n        [ 1.4262, -1.6419],\n        [ 3.7295,  0.5082],\n        [ 2.7093, -0.0109],\n        [ 1.7851, -0.3335],\n        [ 2.3578, -0.2087],\n        [ 4.4055, -1.3194],\n        [ 3.7019,  0.3519],\n        [ 1.5798,  1.3181],\n        [ 4.2871,  0.8921],\n        [ 2.0045,  0.2020],\n        [ 3.1624,  1.1235],\n        [ 0.0811, -0.7623],\n        [ 2.9155, -0.8479],\n        [ 2.8995,  1.9451],\n        [ 3.0547, -1.7518],\n        [ 3.0428, -0.0248],\n        [ 2.5985,  0.0823],\n        [ 3.1872, -2.3368],\n        [ 2.3491,  0.7127],\n        [ 2.4144, -0.6718],\n        [ 2.0556,  0.2756],\n        [ 3.5238,  0.5471],\n        [ 0.8240,  2.8788],\n        [ 4.4009,  0.9733],\n        [ 3.8646,  0.8826],\n        [ 2.4038,  0.2062],\n        [ 0.1843,  1.1979],\n        [ 3.8462, -0.0630],\n        [ 1.2278,  0.0660],\n        [ 1.1262, -0.6438],\n        [ 1.1170,  0.4442],\n        [ 3.3224,  0.4609],\n        [ 2.1315, -1.7203],\n        [ 0.0822,  0.8654],\n        [ 2.9526, -0.7122],\n        [ 1.5624,  0.8633],\n        [ 3.3055,  0.8896],\n        [ 2.2119, -0.9159],\n        [ 3.2946, -1.6101],\n        [ 2.2987, -0.4832],\n        [ 2.8205, -0.6481],\n        [ 1.7051, -0.7754],\n        [ 2.2625,  2.4633],\n        [ 1.3485,  1.6794],\n        [ 2.4338, -0.7713],\n        [ 1.6705, -0.7917],\n        [ 2.5596,  0.0083],\n        [ 2.6712, -0.3213],\n        [ 3.2777, -0.7190],\n        [ 1.6490,  0.3308],\n        [ 2.2135, -0.8757],\n        [ 2.4423,  0.9059],\n        [ 0.5619,  0.1831],\n        [ 0.9888, -0.3529],\n        [ 3.4890,  0.4072],\n        [ 3.5036, -0.9744],\n        [ 3.9063, -0.1710],\n        [ 1.6424, -0.7940],\n        [ 3.3527, -0.2982],\n        [ 1.7538, -0.4672],\n        [ 3.0190,  1.5292],\n        [ 3.0426,  1.4920],\n        [ 1.6220,  0.0691],\n        [ 2.3168, -1.1412],\n        [ 2.5884,  0.7068],\n        [ 2.0928,  0.6037],\n        [ 2.1160,  0.1181],\n        [ 1.8729, -1.4674],\n        [ 0.8758, -1.1253],\n        [ 4.6661, -1.9904],\n        [ 2.9295,  0.4893],\n        [ 3.2401,  0.1667],\n        [ 2.5424, -0.4833],\n        [ 3.0473, -0.1996],\n        [ 1.4382,  0.4205],\n        [ 2.7121, -1.8060],\n        [ 3.0666, -0.1828],\n        [ 1.0951, -1.3569],\n        [-0.1043,  0.2223],\n        [ 2.2561,  0.8115],\n        [ 1.6046, -1.0389],\n        [ 0.3723, -1.2681],\n        [ 3.0223, -0.9723],\n        [ 2.0124,  0.2285],\n        [ 3.9224, -0.8614],\n        [ 2.9321, -0.7975],\n        [ 2.4105, -0.4866],\n        [ 3.6442, -2.3562],\n        [ 1.5032, -1.3106],\n        [ 3.9605, -0.8801],\n        [ 1.7597, -1.2436],\n        [ 4.1847, -0.7777],\n        [ 1.2384,  0.3388],\n        [ 2.2980,  0.4346],\n        [ 1.8613, -1.8122],\n        [ 3.5366, -0.5567],\n        [ 1.8579, -0.7339],\n        [ 2.1261, -0.2404],\n        [ 0.9793, -0.0568],\n        [ 2.7548, -0.9992],\n        [ 1.4421, -1.2440],\n        [ 0.8182, -1.3018],\n        [ 3.7993, -0.9611],\n        [ 0.6471, -0.7378],\n        [ 1.8781,  0.7037],\n        [ 2.3249, -1.8673],\n        [ 0.9613,  1.5319],\n        [ 2.1306,  1.1091],\n        [ 1.0712, -0.0791],\n        [ 2.9737,  0.2267],\n        [ 3.0649,  0.1527],\n        [ 3.4148, -0.2448],\n        [ 2.6914,  0.0075],\n        [ 3.8512,  1.1334],\n        [ 3.8083,  0.5901],\n        [ 1.9114,  1.9635],\n        [ 3.5368, -3.4048],\n        [ 1.3980, -1.6603],\n        [ 2.7586, -1.4160],\n        [ 1.9115, -0.6871],\n        [ 1.4159,  0.3872],\n        [ 2.7761, -0.2450],\n        [ 1.0728,  0.3563],\n        [ 2.8812, -0.4919],\n        [ 3.8481, -1.1987],\n        [ 2.0792, -0.8706],\n        [ 4.0543,  1.4809],\n        [ 3.1431, -1.0976],\n        [ 2.3414, -0.0654],\n        [ 1.7224, -1.8210],\n        [ 1.8914,  1.1483],\n        [ 2.5195, -0.8388],\n        [ 3.0594,  0.1268],\n        [ 2.9865, -1.1868],\n        [ 1.7337,  0.2231],\n        [ 2.6260, -0.6473],\n        [ 3.7161, -1.0011]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.4764e-06, 4.9913e-03, 4.9940e-03,  ..., 9.6819e-01, 1.0000e+00,\n         1.0000e+00],\n        [3.5097e-05, 1.1944e-04, 1.9666e-03,  ..., 9.9312e-01, 1.0000e+00,\n         1.0000e+00],\n        [1.4991e-08, 2.3417e-01, 2.3444e-01,  ..., 8.0033e-01, 8.0044e-01,\n         1.0000e+00],\n        ...,\n        [4.1289e-03, 5.5454e-03, 2.5080e-02,  ..., 8.0757e-01, 9.9523e-01,\n         1.0000e+00],\n        [6.8802e-11, 2.1033e-01, 2.2005e-01,  ..., 8.6326e-01, 9.9845e-01,\n         1.0000e+00],\n        [3.7046e-02, 3.7201e-02, 2.9872e-01,  ..., 8.2104e-01, 9.9831e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[7.6437e-01],\n        [3.7008e-01],\n        [7.6155e-01],\n        [9.6648e-01],\n        [8.0871e-01],\n        [1.6877e-01],\n        [2.6704e-01],\n        [5.8707e-01],\n        [9.3961e-01],\n        [4.3517e-01],\n        [1.6043e-01],\n        [9.5886e-01],\n        [1.4350e-01],\n        [9.1048e-01],\n        [3.4258e-03],\n        [3.8203e-01],\n        [5.3942e-01],\n        [8.2861e-01],\n        [1.2916e-01],\n        [5.1084e-01],\n        [7.2814e-01],\n        [3.4317e-01],\n        [4.3471e-01],\n        [8.4752e-01],\n        [4.6122e-01],\n        [9.5521e-01],\n        [8.5796e-01],\n        [4.3893e-01],\n        [8.4336e-01],\n        [8.5594e-01],\n        [7.6512e-01],\n        [9.0143e-01],\n        [3.1971e-01],\n        [9.8050e-02],\n        [7.6821e-01],\n        [3.5172e-01],\n        [7.0311e-01],\n        [2.1182e-01],\n        [7.7301e-01],\n        [8.6748e-01],\n        [9.0467e-01],\n        [5.1062e-01],\n        [7.4790e-01],\n        [2.7130e-01],\n        [7.6700e-01],\n        [2.7198e-02],\n        [5.1151e-01],\n        [5.3348e-01],\n        [8.8127e-01],\n        [5.8615e-01],\n        [9.1320e-01],\n        [6.1374e-01],\n        [7.9277e-01],\n        [3.3929e-01],\n        [5.0212e-03],\n        [3.8750e-01],\n        [2.0866e-01],\n        [7.6421e-01],\n        [5.4456e-01],\n        [2.2257e-01],\n        [5.8907e-01],\n        [8.1336e-02],\n        [4.7314e-01],\n        [4.8959e-01],\n        [2.2143e-01],\n        [3.7809e-01],\n        [9.2481e-01],\n        [9.3903e-01],\n        [2.6836e-01],\n        [9.4745e-01],\n        [5.3366e-01],\n        [4.0082e-01],\n        [2.8621e-01],\n        [2.5919e-02],\n        [3.4248e-01],\n        [9.0190e-01],\n        [5.9174e-01],\n        [4.7430e-01],\n        [8.9775e-01],\n        [5.7943e-01],\n        [4.7336e-01],\n        [9.0694e-02],\n        [6.8778e-01],\n        [2.8645e-01],\n        [9.2571e-01],\n        [9.1145e-02],\n        [4.4042e-01],\n        [4.2001e-01],\n        [8.1102e-01],\n        [6.0643e-01],\n        [7.1917e-01],\n        [4.8091e-01],\n        [6.9772e-01],\n        [2.7779e-01],\n        [8.9117e-01],\n        [9.2924e-01],\n        [5.7261e-01],\n        [3.1558e-02],\n        [5.4026e-01],\n        [9.6521e-01],\n        [5.0925e-01],\n        [7.5173e-02],\n        [8.5199e-01],\n        [4.5936e-01],\n        [8.4240e-01],\n        [9.1614e-02],\n        [3.2777e-01],\n        [9.5617e-02],\n        [1.4506e-01],\n        [7.3607e-01],\n        [2.2773e-02],\n        [4.7580e-01],\n        [8.0478e-01],\n        [5.9757e-01],\n        [7.7738e-01],\n        [6.5120e-01],\n        [3.6425e-01],\n        [1.7842e-01],\n        [2.4909e-01],\n        [3.6885e-01],\n        [7.2452e-01],\n        [5.9392e-02],\n        [1.3957e-02],\n        [3.5614e-01],\n        [1.8901e-01],\n        [1.6680e-01],\n        [1.1978e-01],\n        [1.1563e-05],\n        [4.5100e-02],\n        [3.7624e-01],\n        [9.3784e-01],\n        [5.6677e-01],\n        [7.3420e-01],\n        [5.4531e-01],\n        [5.9350e-01],\n        [3.2600e-01],\n        [4.1058e-01],\n        [6.4756e-01],\n        [5.9710e-01],\n        [9.4538e-01],\n        [2.4673e-01],\n        [8.2147e-01],\n        [7.3262e-02],\n        [9.4656e-01],\n        [8.7340e-01],\n        [3.3416e-01],\n        [5.4552e-01],\n        [1.0287e-01],\n        [9.6386e-02],\n        [7.6132e-01],\n        [8.5923e-01],\n        [5.1107e-01],\n        [3.0148e-01],\n        [6.9382e-01],\n        [3.5273e-01],\n        [9.6469e-01],\n        [4.0103e-01],\n        [3.1003e-02],\n        [3.6635e-01],\n        [1.7494e-01],\n        [2.3431e-01],\n        [4.2669e-03],\n        [5.9687e-01],\n        [8.6149e-01],\n        [1.1718e-01],\n        [6.8200e-01],\n        [9.9247e-01],\n        [9.2106e-01],\n        [1.2572e-01],\n        [4.1426e-01],\n        [9.7218e-01],\n        [9.4450e-01],\n        [9.5333e-01],\n        [9.2124e-01],\n        [8.8980e-01],\n        [4.0629e-01],\n        [9.6491e-01],\n        [3.8127e-01],\n        [5.7320e-01],\n        [9.9294e-01],\n        [7.7627e-01],\n        [2.5583e-01],\n        [7.0096e-01],\n        [2.6332e-01],\n        [8.3009e-01],\n        [7.9138e-01],\n        [3.8876e-01],\n        [4.0831e-01],\n        [6.7509e-01],\n        [8.5885e-01],\n        [3.3794e-01],\n        [1.5322e-01],\n        [7.0481e-01],\n        [5.1953e-02],\n        [5.3477e-01],\n        [6.9682e-01],\n        [9.2567e-01],\n        [1.8437e-01],\n        [7.6013e-02],\n        [7.3913e-01],\n        [9.2612e-01],\n        [5.0288e-01],\n        [2.7325e-01],\n        [4.9695e-01],\n        [3.3359e-01],\n        [4.2151e-01],\n        [4.4843e-01],\n        [9.5452e-02],\n        [1.6070e-01],\n        [3.6349e-01],\n        [2.5064e-01],\n        [7.2307e-02],\n        [6.6338e-01],\n        [7.7319e-01],\n        [4.6809e-01],\n        [3.5545e-02],\n        [7.6185e-01],\n        [6.9815e-01],\n        [2.4950e-01],\n        [8.4247e-01],\n        [2.0281e-01],\n        [6.1349e-01],\n        [2.0847e-02],\n        [7.5221e-01],\n        [4.9356e-01],\n        [1.7126e-01],\n        [9.2133e-01],\n        [7.8323e-01],\n        [4.2434e-01],\n        [3.8487e-01],\n        [3.2300e-01],\n        [8.2351e-01],\n        [3.8430e-01],\n        [5.5644e-01],\n        [6.0977e-01],\n        [5.5576e-01],\n        [2.1146e-01],\n        [2.8560e-01],\n        [7.1439e-01],\n        [1.5165e-01],\n        [6.7428e-01],\n        [8.8644e-01],\n        [4.8006e-01],\n        [5.3019e-01],\n        [8.4109e-01],\n        [5.8886e-01],\n        [4.1431e-01],\n        [7.3815e-01],\n        [7.6808e-01],\n        [9.2969e-01],\n        [7.3576e-02],\n        [5.0190e-01],\n        [2.5106e-01],\n        [9.4249e-01],\n        [5.6339e-01],\n        [3.8293e-01],\n        [4.4601e-01],\n        [9.8132e-01],\n        [7.9424e-02],\n        [8.6079e-01],\n        [6.6289e-01],\n        [5.8360e-01],\n        [4.3303e-01],\n        [5.4897e-01],\n        [3.0297e-01],\n        [3.0659e-01],\n        [2.9852e-01],\n        [8.5157e-02],\n        [6.2865e-01],\n        [7.3585e-01],\n        [9.1314e-01],\n        [3.4587e-01],\n        [4.7125e-01],\n        [5.5556e-01],\n        [9.4055e-01],\n        [4.3972e-01],\n        [8.8846e-01],\n        [6.8061e-01],\n        [3.5135e-01],\n        [2.2280e-01],\n        [8.1447e-01],\n        [9.4905e-01],\n        [5.0695e-01],\n        [3.0212e-01],\n        [3.5145e-01],\n        [5.9865e-01],\n        [1.0430e-01],\n        [5.0982e-01],\n        [8.4201e-01],\n        [1.1505e-01],\n        [6.0726e-01],\n        [9.1814e-01],\n        [2.2972e-01],\n        [5.3832e-01],\n        [8.6030e-01],\n        [1.2457e-01],\n        [8.4382e-01],\n        [7.8676e-02],\n        [7.7052e-01],\n        [1.1393e-01],\n        [3.1710e-01],\n        [7.3137e-01],\n        [7.0454e-01],\n        [3.2746e-01],\n        [8.5307e-01],\n        [5.0154e-01],\n        [3.1778e-01],\n        [1.7383e-01],\n        [8.1495e-01],\n        [3.7348e-01],\n        [2.0359e-02],\n        [8.1984e-01]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False,  True, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-2.4127,  0.0125],\n        [ 0.6448,  1.3712],\n        [-1.9227,  2.0855],\n        ...,\n        [-0.8400,  0.4430],\n        [ 3.8974, -0.6023],\n        [-0.0893,  0.1132]]) torch.Size([9984, 2])\nsamples tensor([[ 2.9154e+00, -1.1147e+00],\n        [ 2.5138e+00, -1.9887e-01],\n        [ 1.5275e+00, -5.7573e-01],\n        [ 1.2427e-01,  8.5127e-01],\n        [ 2.5183e+00,  8.1407e-01],\n        [ 1.4535e+00, -1.9816e-01],\n        [ 1.1743e+00, -1.2943e+00],\n        [ 1.0935e+00, -2.8163e-01],\n        [ 3.9513e-01, -1.0184e+00],\n        [ 2.1402e+00, -6.3080e-01],\n        [ 1.9882e+00, -3.0916e-01],\n        [ 2.4446e+00,  1.3779e+00],\n        [ 1.6414e+00,  5.3263e-01],\n        [ 2.3768e+00, -5.8180e-01],\n        [ 4.3829e-02, -6.9819e-01],\n        [ 2.5639e+00,  1.4681e-01],\n        [ 1.9820e+00, -1.1688e+00],\n        [ 3.7666e-01,  9.4831e-01],\n        [ 1.8062e+00, -1.9838e+00],\n        [ 4.0085e+00,  2.5109e-01],\n        [ 2.3885e+00,  1.2023e+00],\n        [ 3.0152e+00,  5.0419e-01],\n        [ 3.2235e+00, -1.6976e+00],\n        [ 1.5385e+00,  3.7255e-01],\n        [ 1.7990e+00, -2.9030e-01],\n        [ 3.0928e+00,  8.5351e-01],\n        [ 9.6914e-01, -5.2224e-01],\n        [ 9.1107e-01, -9.7714e-02],\n        [ 1.9646e+00, -3.6262e-01],\n        [ 3.2037e+00, -1.2935e+00],\n        [ 1.6902e+00, -4.4638e-01],\n        [ 2.8859e+00, -3.2569e-01],\n        [ 4.4588e+00, -7.1289e-01],\n        [ 2.0108e+00, -1.7612e-02],\n        [ 3.4250e+00, -7.4901e-01],\n        [ 2.3891e+00, -7.9899e-01],\n        [ 3.1766e+00,  5.5768e-01],\n        [ 2.6666e+00, -2.3079e+00],\n        [ 2.2828e+00, -1.1203e+00],\n        [ 6.6977e-01, -1.2891e+00],\n        [ 2.5787e+00, -4.3556e-01],\n        [ 2.3361e+00,  8.7673e-01],\n        [ 1.5645e+00, -6.3480e-01],\n        [ 4.5785e+00,  4.1089e-03],\n        [ 5.3378e-01,  2.3495e-01],\n        [ 2.0115e+00,  1.1116e+00],\n        [ 2.7491e+00, -8.0485e-01],\n        [ 2.3331e+00,  3.3483e-01],\n        [ 1.2925e+00,  2.9222e-01],\n        [ 2.8095e+00,  6.3001e-01],\n        [ 1.8620e+00,  1.0614e+00],\n        [ 2.3587e+00, -1.9810e+00],\n        [ 3.6370e+00, -1.0407e-01],\n        [ 3.2214e+00, -3.7381e-01],\n        [ 1.3879e+00,  9.2538e-01],\n        [ 3.3711e+00,  6.6298e-01],\n        [ 3.6207e+00, -1.5781e+00],\n        [ 3.1294e+00, -1.2672e+00],\n        [ 2.6843e+00, -5.8977e-01],\n        [ 3.1601e+00, -7.3573e-02],\n        [ 2.6756e+00,  5.4888e-01],\n        [ 3.3679e+00,  2.8125e-01],\n        [ 2.7247e+00,  1.0914e+00],\n        [ 1.7930e+00, -1.2166e+00],\n        [ 1.7726e+00, -7.2047e-01],\n        [ 1.4980e+00,  2.7524e-01],\n        [ 2.0510e+00, -2.2077e-01],\n        [ 1.9512e+00,  3.9894e-01],\n        [ 3.5209e+00, -1.1573e-01],\n        [ 6.8236e-01, -2.6598e-02],\n        [ 2.6781e+00,  4.7586e-01],\n        [ 1.6048e+00, -5.2297e-03],\n        [ 2.9625e+00, -3.1533e-02],\n        [ 1.2237e+00, -1.6057e-01],\n        [ 2.5779e+00,  4.4984e-02],\n        [ 3.2030e+00,  8.9023e-01],\n        [ 1.4212e+00,  1.2710e+00],\n        [ 2.1048e+00, -7.4354e-02],\n        [ 2.5223e+00,  2.3769e-02],\n        [ 9.5785e-01,  7.0107e-01],\n        [ 1.7748e+00, -2.1704e-01],\n        [ 1.5786e+00, -1.0077e+00],\n        [ 3.5722e+00, -1.3845e+00],\n        [ 2.5543e+00,  4.3813e-01],\n        [ 2.7465e+00, -8.9712e-01],\n        [ 4.2892e+00, -7.7342e-01],\n        [ 2.4267e+00, -7.1780e-01],\n        [ 2.1278e+00, -2.8876e-01],\n        [ 2.2358e+00,  1.6467e-01],\n        [ 2.1157e+00, -6.9341e-01],\n        [ 8.0262e-01, -1.0846e+00],\n        [ 2.4147e+00,  8.9948e-01],\n        [ 1.7140e+00, -1.7383e+00],\n        [ 1.4578e+00, -8.4659e-01],\n        [ 1.5233e+00, -6.9036e-01],\n        [ 3.2526e+00, -1.4551e+00],\n        [ 3.5308e+00, -4.9682e-01],\n        [ 4.1570e+00, -2.1219e+00],\n        [ 2.4617e+00,  1.5159e-01],\n        [ 2.3182e+00,  1.7673e+00],\n        [ 3.7393e+00,  1.5523e+00],\n        [ 3.0316e+00,  3.8643e-02],\n        [ 2.3947e+00,  4.8437e-01],\n        [ 1.0605e+00,  1.1138e+00],\n        [ 1.5329e+00, -1.4011e-01],\n        [ 1.5921e+00,  3.2652e-01],\n        [ 1.0330e+00,  8.0847e-01],\n        [ 1.6596e+00, -1.7496e+00],\n        [ 2.6242e+00,  9.7787e-01],\n        [ 4.3385e+00, -1.2848e+00],\n        [ 1.6672e+00,  1.8513e+00],\n        [ 7.6163e-01, -1.2384e+00],\n        [ 1.4557e+00,  2.2114e+00],\n        [ 1.5769e+00, -1.1141e+00],\n        [ 3.2047e+00, -8.6375e-02],\n        [ 1.7931e+00, -2.1673e+00],\n        [ 4.4698e+00, -7.0039e-01],\n        [ 1.6836e+00, -5.0195e-01],\n        [ 3.0965e+00, -3.1700e-01],\n        [ 1.7424e+00, -4.7220e-01],\n        [ 1.8758e+00, -6.5669e-01],\n        [ 2.3604e+00, -2.1122e-01],\n        [ 1.0698e+00, -1.0814e+00],\n        [ 2.9981e+00,  3.7886e-01],\n        [ 3.5736e+00, -1.8374e-01],\n        [ 1.3321e+00, -8.2630e-01],\n        [ 3.8677e+00,  1.1583e+00],\n        [ 5.4609e-01,  2.4758e+00],\n        [ 1.4660e+00,  1.0212e+00],\n        [ 2.5302e+00, -1.6013e+00],\n        [ 4.2201e+00, -9.0801e-01],\n        [ 1.9985e+00, -1.3201e+00],\n        [ 1.9423e+00,  1.1560e+00],\n        [ 2.4685e+00, -4.3492e-01],\n        [ 2.4892e+00, -1.2629e+00],\n        [ 3.6645e+00, -2.4596e+00],\n        [ 3.7178e+00,  1.8134e+00],\n        [ 1.4808e+00,  3.9307e-01],\n        [ 1.3856e+00, -2.4860e-01],\n        [ 2.3389e+00, -1.1632e+00],\n        [ 2.5795e+00,  8.7814e-01],\n        [ 2.2366e+00,  3.1932e-02],\n        [ 2.2105e+00, -3.1563e-01],\n        [ 3.0109e+00,  2.0959e-01],\n        [ 2.3133e+00,  1.5218e-01],\n        [ 1.5424e+00, -1.0569e+00],\n        [ 2.7930e+00,  3.8707e-01],\n        [ 2.3843e+00, -1.0494e+00],\n        [ 1.3602e+00, -7.1424e-01],\n        [ 2.6283e+00, -2.8043e+00],\n        [ 1.2417e+00, -1.1358e+00],\n        [ 1.6345e+00,  4.9214e-01],\n        [ 1.2601e+00, -1.8975e+00],\n        [ 1.8845e+00,  1.0258e+00],\n        [ 2.8619e+00, -6.7091e-01],\n        [ 7.8672e-01,  9.7017e-01],\n        [ 2.0948e+00,  1.3023e+00],\n        [ 3.6859e-01, -6.2385e-01],\n        [ 7.3587e-01,  6.3263e-01],\n        [ 4.8809e+00, -4.2142e-01],\n        [ 1.9746e+00,  1.1717e-02],\n        [ 1.9353e+00,  1.5620e+00],\n        [ 1.9040e-01,  2.1542e-01],\n        [ 2.8647e+00,  1.7487e+00],\n        [ 3.1655e+00, -7.4393e-01],\n        [ 6.4156e-01, -8.6656e-01],\n        [ 3.9467e-01, -1.4481e-01],\n        [ 1.9136e+00, -3.9622e-01],\n        [ 3.7966e+00, -1.4662e-01],\n        [ 2.0725e+00,  3.4562e-01],\n        [ 1.0730e+00, -4.0471e-02],\n        [ 2.7082e+00,  1.1950e+00],\n        [ 3.3891e+00, -9.8295e-01],\n        [ 1.5446e+00, -5.3675e-01],\n        [ 3.7703e+00,  4.5385e-01],\n        [ 2.5751e+00,  7.9110e-01],\n        [ 3.1025e+00, -1.8224e-02],\n        [ 1.5187e+00,  2.6981e-01],\n        [ 2.8971e+00,  2.2031e+00],\n        [ 8.7840e-01, -5.6193e-01],\n        [ 1.5651e+00, -6.6087e-01],\n        [ 2.0562e+00,  6.5938e-01],\n        [ 3.2120e+00, -2.1574e+00],\n        [ 1.9579e+00, -2.2832e+00],\n        [ 3.3252e+00,  9.7624e-01],\n        [ 3.8502e+00, -2.0162e+00],\n        [ 1.9663e+00,  2.7084e-01],\n        [ 1.9940e+00,  1.7064e+00],\n        [ 2.8268e+00, -2.1000e+00],\n        [ 3.4826e+00,  1.9525e+00],\n        [ 3.2091e+00,  5.9003e-01],\n        [ 3.9875e+00, -1.6823e+00],\n        [ 1.9363e+00,  1.7909e+00],\n        [ 3.8926e+00, -5.1747e-01],\n        [ 1.4659e+00, -1.3762e+00],\n        [ 2.0575e+00, -9.4607e-01],\n        [ 3.0450e+00,  1.0696e+00],\n        [ 1.6949e+00, -1.1965e+00],\n        [ 1.2354e+00,  1.3220e+00],\n        [ 2.7754e+00,  1.7449e-02],\n        [ 2.5933e+00, -1.0004e-01],\n        [ 3.4017e+00, -1.3096e+00],\n        [ 1.9072e+00, -1.3918e+00],\n        [ 1.9786e+00, -4.6527e-01],\n        [ 3.0143e+00, -4.7479e-01],\n        [ 1.3557e+00,  6.5306e-01],\n        [ 3.0885e+00, -6.6642e-01],\n        [ 4.3674e+00,  1.1853e+00],\n        [ 3.4751e+00, -1.9477e+00],\n        [ 3.3801e+00,  2.4268e-01],\n        [ 2.8891e+00, -8.3483e-01],\n        [ 3.8126e+00,  6.2986e-01],\n        [ 2.6752e+00,  8.2829e-01],\n        [ 2.4740e+00, -1.3250e+00],\n        [ 2.6372e+00, -6.2915e-01],\n        [ 1.3884e+00, -7.6629e-01],\n        [ 1.5266e+00,  1.6032e-01],\n        [ 2.4023e+00, -2.1489e+00],\n        [ 2.0721e+00, -1.2721e+00],\n        [ 3.4678e+00,  1.0246e+00],\n        [ 3.0551e+00, -4.9164e-01],\n        [ 2.5231e+00, -4.4351e-01],\n        [ 2.1583e+00,  9.5597e-01],\n        [ 2.8406e+00, -1.4480e+00],\n        [ 3.6025e+00,  8.3289e-02],\n        [ 2.6915e+00, -1.8585e+00],\n        [ 1.8948e-01, -1.8767e+00],\n        [ 1.7624e+00,  8.5475e-02],\n        [ 2.7426e+00, -8.3100e-01],\n        [ 1.9038e+00,  4.9687e-01],\n        [ 2.2625e-01, -6.4178e-01],\n        [ 2.8622e+00,  4.8320e-01],\n        [ 7.7169e-01,  9.8914e-02],\n        [ 1.8306e+00, -1.0853e+00],\n        [ 1.6353e+00, -4.7624e-01],\n        [ 2.1949e+00, -1.4866e+00],\n        [ 1.4952e+00,  7.6974e-01],\n        [ 2.5029e+00,  2.4402e-01],\n        [ 3.2695e+00, -1.1241e-01],\n        [ 2.5116e+00, -1.4112e+00],\n        [ 2.4091e+00, -4.2111e-01],\n        [ 1.0275e+00, -8.3054e-01],\n        [ 1.5514e+00,  7.5599e-01],\n        [ 2.6747e+00,  1.3814e+00],\n        [ 4.2442e+00, -7.4956e-01],\n        [ 3.1577e+00,  4.5051e-01],\n        [ 3.2644e+00,  6.5486e-01],\n        [ 1.5494e+00, -1.2060e+00],\n        [ 2.1954e+00,  9.7872e-01],\n        [ 3.0990e+00,  4.7143e-01],\n        [ 2.6165e+00,  8.5457e-01],\n        [ 1.7254e+00, -5.6815e-01],\n        [ 2.8121e+00,  1.8216e+00],\n        [ 3.4172e+00,  5.9134e-01],\n        [ 2.9585e+00, -7.1745e-01],\n        [ 8.1047e-01, -2.1799e+00],\n        [ 2.9588e+00, -1.9043e+00],\n        [ 2.5244e+00,  2.1683e+00],\n        [ 2.0966e+00,  2.2168e-01],\n        [ 2.6091e+00,  1.3447e-01],\n        [ 2.2558e+00, -6.5299e-01],\n        [ 1.9317e+00, -6.2048e-01],\n        [ 2.7399e+00, -3.3260e-01],\n        [ 4.7660e+00, -1.1163e+00],\n        [ 3.0244e+00,  1.1320e+00],\n        [ 3.9174e+00, -7.1865e-01],\n        [ 2.2859e+00,  3.6959e-01],\n        [ 2.6671e+00, -7.8180e-01],\n        [ 1.9178e+00, -1.4292e+00],\n        [ 4.5866e+00, -5.0831e-01],\n        [ 2.0390e+00,  1.4112e-01],\n        [ 1.2206e+00, -1.0388e+00],\n        [ 1.8533e+00,  8.6248e-01],\n        [ 2.6119e+00, -2.5382e-01],\n        [ 2.2307e+00, -7.4737e-01],\n        [ 1.9130e+00, -1.6935e+00],\n        [ 2.9628e+00, -1.7294e+00],\n        [ 1.8120e+00,  1.4477e+00],\n        [ 2.8339e+00, -7.3319e-01],\n        [ 3.5135e+00,  1.6549e-01],\n        [ 3.5385e+00, -2.7562e+00],\n        [ 3.0189e+00, -9.8457e-01],\n        [ 3.6727e+00,  6.1228e-01],\n        [ 1.7613e+00, -1.5316e+00],\n        [ 2.2832e+00, -1.5133e+00],\n        [ 3.3223e+00, -6.2639e-01],\n        [ 3.5899e+00,  8.3277e-01],\n        [ 2.8375e+00, -1.1077e+00],\n        [ 2.1764e+00, -9.6561e-03],\n        [ 1.3971e+00, -5.3625e-01],\n        [ 1.4108e+00, -2.4159e+00],\n        [ 1.6264e+00, -1.0556e+00],\n        [ 2.8749e+00, -2.6482e-01],\n        [ 1.1971e+00, -3.4742e-01],\n        [ 9.8789e-01,  8.2471e-02],\n        [ 2.8799e+00, -2.4708e-01],\n        [ 1.5923e+00, -7.1421e-01],\n        [ 3.2975e+00, -9.8036e-01],\n        [ 2.8070e+00, -2.6347e+00],\n        [ 3.7840e+00, -1.5413e+00],\n        [ 2.1583e+00, -8.3219e-01],\n        [ 1.8735e+00, -1.2826e-01],\n        [ 1.7423e+00, -1.5099e+00],\n        [ 3.0631e+00,  1.2982e-01],\n        [ 1.9795e+00, -3.5662e-01],\n        [ 3.3177e+00,  2.9263e-01],\n        [ 2.3193e+00,  5.4573e-01],\n        [ 3.0059e+00, -8.5417e-01],\n        [ 1.2870e+00, -1.5354e-01],\n        [ 1.9492e+00,  1.9587e+00],\n        [ 3.1434e+00,  1.3759e-01],\n        [ 1.3131e+00,  2.3977e+00]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[3.3378e-01, 3.3378e-01, 3.3897e-01,  ..., 9.0414e-01, 9.8803e-01,\n         1.0000e+00],\n        [1.5182e-03, 4.1415e-02, 5.0087e-02,  ..., 9.5888e-01, 9.6228e-01,\n         1.0000e+00],\n        [1.2603e-05, 1.2618e-05, 3.3791e-02,  ..., 9.8945e-01, 9.9991e-01,\n         1.0000e+00],\n        ...,\n        [5.8308e-02, 6.0533e-02, 9.1436e-02,  ..., 9.3163e-01, 9.7782e-01,\n         1.0000e+00],\n        [2.0527e-05, 1.1146e-01, 1.1536e-01,  ..., 9.8621e-01, 9.9245e-01,\n         1.0000e+00],\n        [3.2710e-03, 3.2710e-03, 4.4582e-03,  ..., 5.6362e-01, 9.9989e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.6054],\n        [0.3237],\n        [0.4830],\n        [0.4285],\n        [0.5214],\n        [0.6048],\n        [0.6707],\n        [0.0924],\n        [0.3563],\n        [0.4746],\n        [0.3600],\n        [0.1290],\n        [0.9496],\n        [0.5129],\n        [0.3299],\n        [0.7369],\n        [0.5549],\n        [0.1613],\n        [0.6392],\n        [0.3401],\n        [0.7644],\n        [0.6027],\n        [0.8432],\n        [0.6981],\n        [0.0272],\n        [0.6304],\n        [0.4830],\n        [0.6533],\n        [0.1688],\n        [0.4866],\n        [0.4310],\n        [0.9792],\n        [0.7697],\n        [0.8708],\n        [0.5051],\n        [0.7953],\n        [0.4231],\n        [0.3108],\n        [0.3049],\n        [0.0255],\n        [0.2050],\n        [0.1936],\n        [0.4694],\n        [0.7300],\n        [0.3388],\n        [0.0605],\n        [0.7168],\n        [0.9075],\n        [0.6871],\n        [0.0067],\n        [0.4248],\n        [0.1624],\n        [0.3457],\n        [0.6591],\n        [0.6874],\n        [0.6595],\n        [0.0058],\n        [0.6421],\n        [0.5400],\n        [0.4430],\n        [0.2299],\n        [0.4165],\n        [0.6327],\n        [0.7768],\n        [0.9159],\n        [0.3877],\n        [0.3141],\n        [0.9646],\n        [0.8184],\n        [0.4624],\n        [0.5094],\n        [0.2276],\n        [0.8896],\n        [0.4930],\n        [0.7467],\n        [0.7528],\n        [0.7902],\n        [0.2851],\n        [0.0265],\n        [0.0158],\n        [0.5330],\n        [0.4240],\n        [0.8513],\n        [0.7079],\n        [0.4460],\n        [0.6045],\n        [0.6174],\n        [0.5700],\n        [0.8925],\n        [0.1183],\n        [0.9071],\n        [0.4230],\n        [0.3170],\n        [0.9163],\n        [0.4693],\n        [0.6510],\n        [0.4420],\n        [0.3522],\n        [0.0014],\n        [0.9402],\n        [0.7441],\n        [0.2006],\n        [0.8634],\n        [0.4436],\n        [0.6430],\n        [0.6856],\n        [0.1893],\n        [0.5036],\n        [0.1201],\n        [0.7405],\n        [0.7301],\n        [0.4308],\n        [0.1167],\n        [0.0535],\n        [0.2474],\n        [0.4073],\n        [0.2692],\n        [0.5681],\n        [0.6093],\n        [0.2943],\n        [0.9076],\n        [0.6208],\n        [0.2874],\n        [0.7909],\n        [0.7077],\n        [0.1564],\n        [0.7481],\n        [0.5625],\n        [0.6379],\n        [0.3294],\n        [0.1513],\n        [0.5926],\n        [0.3544],\n        [0.3976],\n        [0.2987],\n        [0.6684],\n        [0.3501],\n        [0.8445],\n        [0.9894],\n        [0.1700],\n        [0.1976],\n        [0.7719],\n        [0.5476],\n        [0.6330],\n        [0.8361],\n        [0.5998],\n        [0.7902],\n        [0.8138],\n        [0.2842],\n        [0.0905],\n        [0.6646],\n        [0.0238],\n        [0.5389],\n        [0.6696],\n        [0.4768],\n        [0.8665],\n        [0.3817],\n        [0.8774],\n        [0.8200],\n        [0.9904],\n        [0.3439],\n        [0.1573],\n        [0.1519],\n        [0.0844],\n        [0.8866],\n        [0.3740],\n        [0.7062],\n        [0.2102],\n        [0.9357],\n        [0.3166],\n        [0.8145],\n        [0.9317],\n        [0.6571],\n        [0.6014],\n        [0.9005],\n        [0.8755],\n        [0.8775],\n        [0.8914],\n        [0.0047],\n        [0.7118],\n        [0.8988],\n        [0.6686],\n        [0.8879],\n        [0.3060],\n        [0.8661],\n        [0.0225],\n        [0.5200],\n        [0.3213],\n        [0.9559],\n        [0.9735],\n        [0.6352],\n        [0.7309],\n        [0.8415],\n        [0.6445],\n        [0.6079],\n        [0.9387],\n        [0.3827],\n        [0.0240],\n        [0.1002],\n        [0.9009],\n        [0.2543],\n        [0.5491],\n        [0.0980],\n        [0.5234],\n        [0.8258],\n        [0.0883],\n        [0.1573],\n        [0.0246],\n        [0.2031],\n        [0.5181],\n        [0.2556],\n        [0.2358],\n        [0.6016],\n        [0.5113],\n        [0.5818],\n        [0.6349],\n        [0.4784],\n        [0.6300],\n        [0.4406],\n        [0.6061],\n        [0.3294],\n        [0.7612],\n        [0.1809],\n        [0.6119],\n        [0.3848],\n        [0.7116],\n        [0.5834],\n        [0.2486],\n        [0.9514],\n        [0.1721],\n        [0.4359],\n        [0.5258],\n        [0.3163],\n        [0.5731],\n        [0.5803],\n        [0.0444],\n        [0.8131],\n        [0.6127],\n        [0.7848],\n        [0.6964],\n        [0.7193],\n        [0.7629],\n        [0.1845],\n        [0.5879],\n        [0.9118],\n        [0.7360],\n        [0.4270],\n        [0.7450],\n        [0.9978],\n        [0.7288],\n        [0.8031],\n        [0.7058],\n        [0.7534],\n        [0.0934],\n        [0.1444],\n        [0.4827],\n        [0.2851],\n        [0.0688],\n        [0.8393],\n        [0.4336],\n        [0.1438],\n        [0.9883],\n        [0.3351],\n        [0.2169],\n        [0.4253],\n        [0.1140],\n        [0.5947],\n        [0.4866],\n        [0.4142],\n        [0.0080],\n        [0.1295],\n        [0.8545],\n        [0.9613],\n        [0.4901],\n        [0.9242],\n        [0.1417],\n        [0.1567],\n        [0.8206],\n        [0.4748],\n        [0.8282],\n        [0.5074],\n        [0.9923],\n        [0.7947],\n        [0.1939],\n        [0.6113],\n        [0.4002],\n        [0.0850],\n        [0.3159],\n        [0.1493],\n        [0.3872],\n        [0.0853],\n        [0.8605],\n        [0.1215],\n        [0.0363],\n        [0.6068],\n        [0.6096],\n        [0.2019],\n        [0.9709],\n        [0.4269],\n        [0.8286],\n        [0.4506],\n        [0.8020],\n        [0.4350],\n        [0.2880],\n        [0.4859],\n        [0.9578],\n        [0.3627],\n        [0.3919],\n        [0.7149],\n        [0.2087],\n        [0.6906],\n        [0.0819]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 2.2583, -1.2859],\n        [-2.3290,  0.9433],\n        [ 0.9494,  1.3667],\n        ...,\n        [ 1.1004,  1.5744],\n        [ 2.8758, -0.1972],\n        [-1.0834,  1.0219]]) torch.Size([9984, 2])\nsamples tensor([[ 1.9667, -0.3441],\n        [ 2.4330, -0.6400],\n        [ 2.1845,  0.0144],\n        [ 2.6215, -1.7794],\n        [ 2.9919,  0.7827],\n        [ 2.0676,  1.8002],\n        [ 1.9479, -0.4202],\n        [ 2.8259,  0.5239],\n        [ 2.2259, -0.3024],\n        [ 1.2655,  0.1556],\n        [ 3.9312, -0.5818],\n        [ 4.7625, -0.9572],\n        [ 1.5149,  1.4935],\n        [ 4.6009, -1.4266],\n        [ 3.6047, -1.0462],\n        [ 2.5188, -0.4731],\n        [ 2.0713, -0.1481],\n        [ 2.3516,  0.3258],\n        [ 1.9864, -0.1750],\n        [ 2.6881,  0.9106],\n        [ 1.7035, -0.2965],\n        [ 1.9643, -0.9687],\n        [ 2.9900,  0.4292],\n        [ 2.4009, -0.3780],\n        [ 0.9859, -0.0279],\n        [ 2.8826, -0.6575],\n        [ 2.8228, -1.2831],\n        [ 3.2575, -2.3666],\n        [ 2.7296, -0.2679],\n        [ 2.4208, -0.0198],\n        [ 2.8482, -0.5938],\n        [ 2.9129, -0.0977],\n        [ 1.4365, -0.5869],\n        [ 3.3231, -0.1144],\n        [ 3.8486, -0.9878],\n        [ 1.8419,  1.3092],\n        [ 3.7652,  0.9802],\n        [ 1.2952, -1.0030],\n        [ 1.9738, -0.6014],\n        [ 2.5930,  1.3656],\n        [ 3.4219, -0.6178],\n        [ 3.0250,  0.9286],\n        [ 1.3712,  0.9422],\n        [ 3.5780,  0.0670],\n        [ 1.9039,  0.2970],\n        [ 1.9097, -0.7144],\n        [ 2.2054,  0.7161],\n        [ 2.0844, -0.3651],\n        [ 3.1573, -0.3622],\n        [ 2.0708,  2.3124],\n        [ 2.5618, -1.2361],\n        [ 3.0257,  0.4540],\n        [ 2.9665,  1.0700],\n        [ 0.8717,  0.6470],\n        [ 2.2201, -1.2577],\n        [ 1.7214,  1.1592],\n        [ 0.0992, -0.3121],\n        [ 1.1158, -2.4725],\n        [ 2.5984, -0.2203],\n        [ 3.4147, -0.5659],\n        [ 2.1241,  0.0338],\n        [ 3.8757, -0.1296],\n        [ 0.9765, -0.0228],\n        [ 3.1047, -1.0931],\n        [ 3.4356,  2.3990],\n        [ 2.2620, -0.5016],\n        [ 2.7747, -0.2627],\n        [ 1.5277,  0.6504],\n        [ 1.8165, -1.3634],\n        [ 3.8225, -1.9706],\n        [ 1.7425, -0.6980],\n        [ 2.2953,  0.0571],\n        [ 1.1452,  1.0257],\n        [ 1.8231,  0.1838],\n        [ 1.8648,  0.1551],\n        [ 3.5851, -1.7252],\n        [ 1.4919, -0.1447],\n        [ 3.7744, -1.5017],\n        [ 1.5677, -0.8753],\n        [ 1.5156,  2.6682],\n        [ 3.0599, -1.0512],\n        [ 2.5135, -1.0585],\n        [ 2.9114, -0.3778],\n        [ 4.0800,  1.0894],\n        [ 3.0821,  0.5700],\n        [ 1.8003, -0.8698],\n        [ 3.5749, -1.0242],\n        [ 4.0784,  1.1294],\n        [ 2.9046,  0.6174],\n        [ 2.8046, -0.2352],\n        [ 2.5992,  0.3227],\n        [ 1.8586, -1.3364],\n        [ 1.7249, -0.5397],\n        [ 2.5549,  0.1639],\n        [-0.2158, -0.6566],\n        [ 2.7059, -0.8253],\n        [ 3.8375,  0.1252],\n        [ 1.0876, -0.7181],\n        [ 1.5462,  0.8833],\n        [ 3.8501,  0.1278],\n        [ 2.0950,  0.4892],\n        [ 1.7778,  0.3757],\n        [ 2.2641,  0.0997],\n        [ 1.1419,  0.1365],\n        [ 2.2673, -2.2586],\n        [ 1.2848,  0.8463],\n        [ 3.4703,  0.3490],\n        [ 3.8560,  1.1179],\n        [ 0.9000, -0.3342],\n        [ 0.4741,  1.6692],\n        [ 1.8633, -0.5492],\n        [ 3.9884, -0.6847],\n        [ 2.2185,  1.3232],\n        [ 1.3018, -0.5127],\n        [ 2.3558,  1.5323],\n        [ 1.4620, -0.2229],\n        [ 1.8380,  0.6198],\n        [ 1.9884, -1.8710],\n        [ 3.2215, -0.4766],\n        [ 3.1439,  0.3484],\n        [ 2.2657,  0.3577],\n        [ 2.9623, -0.6897],\n        [ 1.8116, -0.6296],\n        [ 1.7292, -0.7201],\n        [ 0.4341,  1.4018],\n        [ 0.2566, -0.1232],\n        [ 2.4393,  0.2094],\n        [ 2.3215, -0.1005],\n        [-0.4939,  0.7165],\n        [ 3.9334, -1.6673],\n        [ 0.5590, -0.9132],\n        [ 3.1503, -1.5634],\n        [ 2.0150,  0.2536],\n        [ 3.0967, -2.1268],\n        [ 2.4091, -1.0277],\n        [ 2.1307, -0.6206],\n        [ 3.1310, -0.5044],\n        [ 2.2932,  0.4099],\n        [ 3.6469, -0.5211],\n        [ 2.1170, -0.2275],\n        [ 2.8824,  1.6719],\n        [ 3.6186, -0.6070],\n        [ 2.8200,  1.0344],\n        [ 2.8758, -0.9713],\n        [ 2.4120, -1.3588],\n        [ 1.7962,  0.4678],\n        [ 3.3376, -0.3899],\n        [ 2.7107,  0.3570],\n        [ 1.3527, -0.0913],\n        [ 1.8225, -0.2262],\n        [ 2.0813, -1.6306],\n        [ 2.4338,  0.7406],\n        [ 3.3333,  1.2467],\n        [ 1.6594, -1.0442],\n        [ 2.3853, -0.8186],\n        [ 2.8160, -0.0179],\n        [ 1.6495, -1.3900],\n        [ 1.9607, -2.4444],\n        [ 3.2629, -0.3678],\n        [ 2.8107,  1.2797],\n        [ 1.5637, -1.4142],\n        [ 2.4063,  1.4785],\n        [ 2.9953, -0.4202],\n        [ 1.5265, -0.9949],\n        [ 1.7890,  0.2144],\n        [ 2.0210, -0.9893],\n        [ 3.7142, -0.6855],\n        [ 3.1431, -0.0723],\n        [ 0.8499,  0.1468],\n        [ 2.0542,  0.7120],\n        [ 1.3029,  0.3466],\n        [ 2.1430,  0.8286],\n        [ 3.4080,  0.8544],\n        [ 2.7241, -0.3594],\n        [ 0.5272, -0.0434],\n        [ 2.2124, -1.0335],\n        [ 2.1405,  0.0913],\n        [ 2.2336, -0.8491],\n        [ 2.9668,  2.2955],\n        [ 3.6295, -2.1881],\n        [ 1.8939, -0.3270],\n        [ 2.0627,  2.0307],\n        [ 4.0016, -1.6862],\n        [ 2.2255, -1.1083],\n        [ 3.0338, -1.6939],\n        [ 2.2689,  0.2854],\n        [ 2.2644,  0.3119],\n        [ 0.7097, -1.8758],\n        [ 3.5829, -0.2749],\n        [ 1.1995,  1.2961],\n        [ 2.1998, -0.0361],\n        [ 1.7080, -0.5253],\n        [ 1.2093,  0.1052],\n        [ 1.0307,  0.4790],\n        [ 1.3989,  0.4175],\n        [ 2.2817,  0.9495],\n        [ 1.3370,  0.0782],\n        [ 2.2157,  1.7695],\n        [ 1.2890, -0.3956],\n        [ 3.7034,  0.8676],\n        [ 3.3123, -1.2890],\n        [ 2.9372,  1.7967],\n        [ 2.3836, -0.7830],\n        [ 0.8852,  0.0192],\n        [ 1.7344, -0.9968],\n        [ 1.5585,  0.6863],\n        [ 1.4124, -0.3540],\n        [ 1.2745,  0.0946],\n        [ 1.8941, -0.5876],\n        [ 2.0599,  0.8465],\n        [-0.2454,  0.4163],\n        [ 2.1659,  0.3958],\n        [ 3.4518, -0.6045],\n        [ 2.7237, -1.1660],\n        [ 1.7607, -0.5171],\n        [ 1.9961, -0.0418],\n        [ 3.6018,  0.3499],\n        [ 2.8700, -1.3005],\n        [ 1.4712, -0.2431],\n        [ 2.0491, -2.3844],\n        [ 3.5424, -1.7017],\n        [ 2.2419,  0.1637],\n        [ 0.8524,  0.9923],\n        [ 2.7424,  1.0494],\n        [ 1.6606,  1.5116],\n        [ 2.5152,  1.7980],\n        [ 3.3078, -1.2446],\n        [ 2.4340, -0.2246],\n        [-0.3093, -1.3151],\n        [ 4.8413,  0.2065],\n        [ 1.1954, -0.0904],\n        [ 3.7054, -1.2177],\n        [ 2.0709, -1.2656],\n        [ 0.1255, -2.2068],\n        [ 2.4332,  0.2123],\n        [ 3.4782,  1.3128],\n        [ 2.0052,  0.7258],\n        [ 3.6725, -0.6561],\n        [ 3.7448, -0.3790],\n        [ 4.2799, -1.3575],\n        [ 2.5520, -0.7444],\n        [ 2.5926, -0.7954],\n        [ 0.8668,  1.4067],\n        [ 3.8148,  0.2600],\n        [ 2.5462, -0.9884],\n        [ 3.0394, -0.2864],\n        [ 2.1651,  0.9824],\n        [ 2.5558, -0.4910],\n        [ 3.2476,  2.0025],\n        [ 3.3788,  0.1170],\n        [ 2.8318,  1.2219],\n        [ 3.4819, -1.4178],\n        [ 2.1657, -1.0967],\n        [ 1.3618, -1.4176],\n        [-0.0852, -1.7778],\n        [ 2.4715, -0.5707],\n        [ 0.6935, -1.7783],\n        [ 1.4971,  0.3318],\n        [ 1.3318, -0.7199],\n        [ 2.6475, -0.8391],\n        [ 2.0768,  0.7564],\n        [ 2.4435,  0.8676],\n        [ 1.6627, -0.9171],\n        [ 1.7295, -0.7534],\n        [ 0.6732,  0.1899],\n        [ 3.3480, -0.6215],\n        [ 2.5038,  0.9942],\n        [ 1.9798,  0.1261],\n        [ 3.4497, -0.9849],\n        [ 1.7019,  1.4712],\n        [ 2.4859, -0.1672],\n        [ 3.2719, -0.4002],\n        [ 1.5948, -0.2130],\n        [ 1.7272,  0.4377],\n        [ 1.4809, -0.2554],\n        [ 3.3612, -0.1724],\n        [ 2.0069,  0.5267],\n        [ 2.9107,  0.8383],\n        [ 0.9342,  0.0815],\n        [ 2.4069,  1.1339],\n        [ 3.0328,  0.1535],\n        [ 3.4584,  2.0677],\n        [ 2.0729,  0.2484],\n        [ 1.6123,  0.1836],\n        [ 1.7457, -1.3028],\n        [ 4.1647,  0.4514],\n        [ 1.4538,  0.3861],\n        [ 0.6179, -1.2548],\n        [ 3.1405,  1.6754],\n        [ 1.9820,  1.6247],\n        [ 2.2688, -0.8261],\n        [ 1.9119,  0.2298],\n        [ 2.0406, -0.4262],\n        [ 1.8561,  0.0227],\n        [ 3.0070,  1.6537],\n        [ 2.9270,  0.9101],\n        [ 3.0711, -0.1947],\n        [ 4.5161,  0.3590],\n        [ 1.5227, -1.5817],\n        [ 3.3480,  0.0766],\n        [ 2.5304,  2.9907],\n        [ 2.2029, -0.4029],\n        [ 2.4962,  0.7478],\n        [ 4.0666, -0.0103],\n        [ 3.5057,  0.7046],\n        [ 3.0840,  0.3112],\n        [ 3.2363,  0.4371],\n        [ 2.0409, -3.5024],\n        [ 1.7077,  0.4815],\n        [ 1.9391,  0.5000],\n        [ 2.8601, -0.0812],\n        [ 1.6963,  0.1370]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.1152e-04, 1.1270e-04, 3.4227e-02,  ..., 9.5519e-01, 9.9971e-01,\n         1.0000e+00],\n        [3.9503e-08, 7.1822e-08, 1.1688e-07,  ..., 8.5425e-01, 9.0357e-01,\n         1.0000e+00],\n        [1.3824e-02, 1.9124e-02, 1.9242e-02,  ..., 9.9847e-01, 9.9903e-01,\n         1.0000e+00],\n        ...,\n        [3.9010e-01, 3.9321e-01, 3.9462e-01,  ..., 7.9528e-01, 9.9937e-01,\n         1.0000e+00],\n        [6.7782e-03, 6.3294e-02, 9.3413e-02,  ..., 9.9840e-01, 9.9990e-01,\n         1.0000e+00],\n        [1.1124e-05, 3.1000e-04, 3.1110e-04,  ..., 9.9578e-01, 9.9960e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.5059],\n        [0.2129],\n        [0.4640],\n        [0.6371],\n        [0.9537],\n        [0.6182],\n        [0.7141],\n        [0.0470],\n        [0.7700],\n        [0.4732],\n        [0.9037],\n        [0.1460],\n        [0.9293],\n        [0.8171],\n        [0.7578],\n        [0.9639],\n        [0.0251],\n        [0.6925],\n        [0.4623],\n        [0.3116],\n        [0.5459],\n        [0.6562],\n        [0.9415],\n        [0.0602],\n        [0.1916],\n        [0.5851],\n        [0.2833],\n        [0.4094],\n        [0.6525],\n        [0.3846],\n        [0.9695],\n        [0.2479],\n        [0.7576],\n        [0.2800],\n        [0.8645],\n        [0.3234],\n        [0.0705],\n        [0.2441],\n        [0.3513],\n        [0.7798],\n        [0.2639],\n        [0.2890],\n        [0.2198],\n        [0.5580],\n        [0.0103],\n        [0.0274],\n        [0.7972],\n        [0.4200],\n        [0.4250],\n        [0.2869],\n        [0.9614],\n        [0.9727],\n        [0.7532],\n        [0.6442],\n        [0.2555],\n        [0.3940],\n        [0.2867],\n        [0.6468],\n        [0.0372],\n        [0.4611],\n        [0.8339],\n        [0.1553],\n        [0.8225],\n        [0.1519],\n        [0.3307],\n        [0.6783],\n        [0.4751],\n        [0.0596],\n        [0.4151],\n        [0.2944],\n        [0.4259],\n        [0.9728],\n        [0.3114],\n        [0.5727],\n        [0.2332],\n        [0.8927],\n        [0.1448],\n        [0.1074],\n        [0.7626],\n        [0.1256],\n        [0.6812],\n        [0.9184],\n        [0.6746],\n        [0.0501],\n        [0.7589],\n        [0.5067],\n        [0.1010],\n        [0.3027],\n        [0.6905],\n        [0.7431],\n        [0.8024],\n        [0.3198],\n        [0.4081],\n        [0.8318],\n        [0.5959],\n        [0.4723],\n        [0.2922],\n        [0.8234],\n        [0.7939],\n        [0.4373],\n        [0.6571],\n        [0.0927],\n        [0.8368],\n        [0.4902],\n        [0.3758],\n        [0.8599],\n        [0.4384],\n        [0.2913],\n        [0.2804],\n        [0.9963],\n        [0.5291],\n        [0.6115],\n        [0.2554],\n        [0.1595],\n        [0.6128],\n        [0.3204],\n        [0.8595],\n        [0.4312],\n        [0.7683],\n        [0.0252],\n        [0.6044],\n        [0.9278],\n        [0.1847],\n        [0.0756],\n        [0.3987],\n        [0.0236],\n        [0.7732],\n        [0.1656],\n        [0.5420],\n        [0.5166],\n        [0.6768],\n        [0.3207],\n        [0.8368],\n        [0.0693],\n        [0.7865],\n        [0.0361],\n        [0.2842],\n        [0.5547],\n        [0.4177],\n        [0.8280],\n        [0.5332],\n        [0.1694],\n        [0.2266],\n        [0.6558],\n        [0.5964],\n        [0.7944],\n        [0.1860],\n        [0.1848],\n        [0.2248],\n        [0.5011],\n        [0.6753],\n        [0.3679],\n        [0.8869],\n        [0.5018],\n        [0.9005],\n        [0.2583],\n        [0.1823],\n        [0.1285],\n        [0.3665],\n        [0.9485],\n        [0.2762],\n        [0.0067],\n        [0.9229],\n        [0.9637],\n        [0.4157],\n        [0.1197],\n        [0.0749],\n        [0.5982],\n        [0.3981],\n        [0.8243],\n        [0.8489],\n        [0.0541],\n        [0.8551],\n        [0.2568],\n        [0.6215],\n        [0.5221],\n        [0.7189],\n        [0.2119],\n        [0.1824],\n        [0.2095],\n        [0.3431],\n        [0.6963],\n        [0.5436],\n        [0.2718],\n        [0.1138],\n        [0.9098],\n        [0.0322],\n        [0.2600],\n        [0.7470],\n        [0.8935],\n        [0.9955],\n        [0.0588],\n        [0.5847],\n        [0.8087],\n        [0.6275],\n        [0.9646],\n        [0.6942],\n        [0.6874],\n        [0.8084],\n        [0.6779],\n        [0.3514],\n        [0.2585],\n        [0.3164],\n        [0.0788],\n        [0.5351],\n        [0.0081],\n        [0.0716],\n        [0.5316],\n        [0.8059],\n        [0.7365],\n        [0.9149],\n        [0.3092],\n        [0.4211],\n        [0.8484],\n        [0.3579],\n        [0.1021],\n        [0.2964],\n        [0.0963],\n        [0.0738],\n        [0.2869],\n        [0.6206],\n        [0.7677],\n        [0.7096],\n        [0.8557],\n        [0.2326],\n        [0.9420],\n        [0.6053],\n        [0.0795],\n        [0.1423],\n        [0.5693],\n        [0.9141],\n        [0.1995],\n        [0.7236],\n        [0.5105],\n        [0.7536],\n        [0.4904],\n        [0.5135],\n        [0.5160],\n        [0.7631],\n        [0.1691],\n        [0.2199],\n        [0.2340],\n        [0.6359],\n        [0.0660],\n        [0.0066],\n        [0.9735],\n        [0.9624],\n        [0.9996],\n        [0.9937],\n        [0.2522],\n        [0.1391],\n        [0.4762],\n        [0.5462],\n        [0.5112],\n        [0.2751],\n        [0.7898],\n        [0.9345],\n        [0.1149],\n        [0.6306],\n        [0.0251],\n        [0.4573],\n        [0.7994],\n        [0.5625],\n        [0.6668],\n        [0.8031],\n        [0.7929],\n        [0.4375],\n        [0.5467],\n        [0.0943],\n        [0.5477],\n        [0.4961],\n        [0.0220],\n        [0.7359],\n        [0.1204],\n        [0.5912],\n        [0.5394],\n        [0.9339],\n        [0.5487],\n        [0.9535],\n        [0.2078],\n        [0.4885],\n        [0.9420],\n        [0.1564],\n        [0.3352],\n        [0.2381],\n        [0.9904],\n        [0.5018],\n        [0.0439],\n        [0.0377],\n        [0.5588],\n        [0.9296],\n        [0.2467],\n        [0.2730],\n        [0.1031],\n        [0.2059],\n        [0.1770],\n        [0.1910],\n        [0.6217],\n        [0.1968],\n        [0.6763],\n        [0.4711],\n        [0.8082],\n        [0.4903],\n        [0.0256],\n        [0.7923],\n        [0.9018],\n        [0.7750],\n        [0.6118],\n        [0.0050],\n        [0.1444],\n        [0.0266],\n        [0.9775]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [ True, False, False,  ..., False, False, False],\n        [False,  True, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-1.2856,  0.1764],\n        [-2.3470,  0.9328],\n        [ 2.7224,  1.7669],\n        ...,\n        [ 0.7833,  0.6550],\n        [ 0.8992,  2.1191],\n        [-0.6031,  0.8031]]) torch.Size([9984, 2])\nsamples tensor([[ 5.1506e-01,  3.4665e-01],\n        [ 2.0021e+00, -4.9508e-01],\n        [ 1.3426e+00, -2.2087e+00],\n        [ 1.2332e+00,  9.3672e-01],\n        [ 3.8524e+00, -1.8289e+00],\n        [ 6.5353e-01, -5.2019e-01],\n        [ 1.1718e+00, -6.8337e-01],\n        [ 2.2419e+00,  8.4405e-01],\n        [ 2.4629e+00, -6.9910e-01],\n        [ 1.3326e+00, -9.8512e-01],\n        [ 1.7853e+00,  3.5495e-01],\n        [ 1.8985e+00, -2.8182e-02],\n        [ 3.3018e+00,  1.3368e+00],\n        [ 2.8032e+00,  1.6744e+00],\n        [ 3.8948e-01,  1.5820e-01],\n        [ 2.7186e+00, -2.7641e+00],\n        [ 2.0486e+00,  4.1934e-01],\n        [ 3.1274e+00,  2.9284e-01],\n        [ 1.6885e+00, -2.8407e-01],\n        [ 2.6567e+00, -2.2527e+00],\n        [ 3.8508e+00, -6.2335e-01],\n        [ 1.8575e+00,  1.4681e-01],\n        [ 3.3159e+00, -9.1140e-02],\n        [ 1.8701e+00, -1.2243e-01],\n        [ 2.4081e+00,  1.0430e-02],\n        [ 3.9949e+00, -4.7340e-02],\n        [ 2.3231e+00, -6.5343e-01],\n        [ 3.3595e+00,  3.9047e-01],\n        [ 2.4919e+00, -1.0703e-01],\n        [ 2.7374e+00,  4.8721e-01],\n        [ 2.9183e+00,  8.0813e-01],\n        [ 1.9063e+00, -4.4793e-01],\n        [ 9.0589e-01, -2.5617e-01],\n        [ 2.2049e+00, -8.8016e-01],\n        [ 1.9447e+00, -1.0767e+00],\n        [ 1.8335e+00, -1.7532e-02],\n        [ 2.0996e+00,  7.1530e-02],\n        [ 8.9480e-01,  5.5396e-01],\n        [ 2.0604e+00, -2.4015e+00],\n        [ 1.4456e+00,  5.6282e-01],\n        [ 1.9781e+00, -1.0010e+00],\n        [ 2.2493e+00, -2.0557e-01],\n        [ 1.6291e+00,  1.6058e+00],\n        [ 2.1382e+00,  4.8175e-01],\n        [ 1.1945e+00,  5.5066e-01],\n        [ 2.8647e+00, -3.4455e-01],\n        [ 2.2323e+00, -3.3268e-01],\n        [ 2.5250e+00,  3.0491e-01],\n        [ 1.6105e+00, -1.3472e+00],\n        [ 3.8744e+00, -1.2267e+00],\n        [ 2.3604e+00,  1.1132e+00],\n        [ 1.6861e+00,  1.6406e+00],\n        [-4.7687e-01, -6.3156e-02],\n        [ 3.8597e+00, -5.4731e-02],\n        [ 1.7458e+00, -7.3101e-01],\n        [ 2.2339e+00,  1.2074e+00],\n        [ 2.4402e+00, -7.1639e-01],\n        [ 3.9108e+00,  2.3835e-01],\n        [ 2.4413e+00,  5.6576e-01],\n        [ 4.3996e+00, -7.9333e-01],\n        [ 3.9496e+00, -1.9236e-01],\n        [ 2.0407e+00,  7.9631e-01],\n        [ 4.1749e-03, -4.7054e-02],\n        [ 3.0400e+00,  5.9048e-01],\n        [ 6.9695e-03, -7.0783e-01],\n        [ 3.4772e+00, -1.9842e+00],\n        [ 1.6955e+00,  8.8785e-01],\n        [ 3.0265e+00,  2.9077e-01],\n        [ 2.8147e+00,  1.3583e-01],\n        [ 1.8596e+00, -1.2664e+00],\n        [ 3.2969e+00, -8.2434e-01],\n        [ 2.0520e+00, -2.2918e-01],\n        [ 2.2624e+00, -1.7358e+00],\n        [ 4.9091e-01, -1.2205e+00],\n        [ 1.7899e+00, -1.0485e+00],\n        [ 2.5265e+00, -1.3673e+00],\n        [ 1.9323e+00, -1.7250e+00],\n        [ 2.0164e+00,  1.2427e-01],\n        [ 3.2325e+00, -6.9230e-01],\n        [ 2.2280e+00,  1.6270e+00],\n        [ 2.9395e+00, -4.2287e-01],\n        [ 1.2307e+00,  1.0421e+00],\n        [ 2.0160e+00, -3.3279e-01],\n        [ 2.9155e+00,  3.1117e-01],\n        [-3.5250e-01, -1.4841e+00],\n        [ 4.9485e+00, -1.1870e+00],\n        [ 2.9451e+00, -3.1781e-01],\n        [ 2.8741e+00, -4.7892e-01],\n        [ 2.2731e+00, -3.2376e-01],\n        [ 4.0655e+00, -1.1528e-01],\n        [ 2.1168e+00, -1.8562e+00],\n        [ 3.0339e+00,  7.7677e-01],\n        [ 3.5381e+00,  9.7507e-01],\n        [ 3.3857e+00,  1.7305e-01],\n        [ 3.6884e-01,  5.8931e-01],\n        [ 2.4510e+00,  3.5053e-01],\n        [ 1.9764e+00,  1.3089e-01],\n        [ 4.6274e-01,  1.6121e-01],\n        [ 1.6325e+00,  9.5897e-02],\n        [ 1.8812e+00, -3.4542e-01],\n        [ 2.7255e+00, -2.3348e-01],\n        [ 1.3709e-01,  9.6237e-01],\n        [ 1.9910e+00, -6.3727e-01],\n        [ 3.4969e+00, -1.7092e+00],\n        [ 2.0599e+00,  7.3449e-01],\n        [ 3.9518e+00,  1.4657e-01],\n        [ 2.5211e+00,  1.0830e+00],\n        [ 1.5748e+00,  1.2077e+00],\n        [ 1.4825e+00, -7.4905e-01],\n        [ 2.9460e+00, -1.1835e-02],\n        [ 3.3682e+00,  4.6352e-01],\n        [ 1.9376e+00, -8.1862e-01],\n        [ 3.4387e+00,  3.7330e-01],\n        [ 1.6351e+00,  8.5621e-01],\n        [ 2.9087e+00, -9.1786e-01],\n        [ 3.5648e+00, -2.7981e-01],\n        [ 2.3806e+00, -2.1203e+00],\n        [ 3.0296e+00, -2.2571e+00],\n        [ 3.4818e+00, -7.1026e-01],\n        [ 1.1939e+00,  1.3691e+00],\n        [ 1.4148e+00, -4.5654e-01],\n        [ 2.1283e+00,  1.4530e+00],\n        [ 2.7186e+00, -7.6624e-02],\n        [ 3.3736e+00, -1.0961e+00],\n        [ 3.0098e+00, -1.2505e-01],\n        [ 2.4063e+00, -1.3977e+00],\n        [ 3.6772e+00,  6.0680e-01],\n        [ 2.7670e+00, -6.5587e-01],\n        [ 1.6723e+00, -1.1756e+00],\n        [ 3.4727e+00, -5.3464e-01],\n        [ 2.3632e+00, -1.5125e-01],\n        [ 2.5014e+00, -4.1747e-01],\n        [ 2.4294e+00,  1.4915e+00],\n        [ 3.5689e+00,  1.4285e+00],\n        [ 2.5779e+00,  7.9535e-01],\n        [ 1.3544e+00,  1.2757e+00],\n        [ 2.7027e+00,  1.6822e+00],\n        [ 3.3103e+00,  6.4760e-01],\n        [ 1.8467e+00,  5.7624e-02],\n        [ 1.5213e+00,  3.0813e-05],\n        [ 3.1212e+00,  1.6568e+00],\n        [ 9.4687e-01, -5.6741e-01],\n        [ 2.2717e+00,  9.3913e-01],\n        [ 3.6988e+00,  2.1680e+00],\n        [ 4.1944e+00, -7.0799e-01],\n        [ 1.0802e+00, -1.0311e+00],\n        [ 2.6291e+00,  6.2336e-01],\n        [ 2.0572e+00, -8.2700e-01],\n        [ 2.0576e+00,  1.3589e+00],\n        [ 2.6962e+00, -5.8369e-01],\n        [ 2.0997e+00, -1.7269e+00],\n        [ 1.1791e+00, -9.2667e-01],\n        [ 3.3734e+00, -1.6252e+00],\n        [ 1.6087e+00, -7.5291e-01],\n        [ 2.3930e+00, -1.8769e+00],\n        [ 3.5039e+00,  4.0164e-01],\n        [ 1.9115e+00, -2.0874e+00],\n        [ 1.3064e+00, -8.1765e-01],\n        [ 1.3345e+00,  1.6946e-02],\n        [ 1.9042e+00, -3.2867e-01],\n        [ 2.1736e+00, -6.9352e-01],\n        [ 1.0597e+00,  1.2926e+00],\n        [ 1.9932e+00, -8.7115e-01],\n        [ 1.9223e+00,  6.2083e-01],\n        [ 2.4244e+00, -1.4770e+00],\n        [ 2.7457e+00,  4.0549e-01],\n        [ 1.6386e+00, -3.3836e-01],\n        [ 4.0021e+00, -4.1119e-01],\n        [ 1.3449e+00, -1.5441e-01],\n        [ 3.0869e+00, -1.4807e+00],\n        [ 1.6522e+00,  1.5692e+00],\n        [-6.2334e-01, -8.6582e-01],\n        [ 1.7992e+00,  5.0860e-01],\n        [ 2.3393e+00,  1.1549e+00],\n        [ 3.7811e+00, -1.1584e+00],\n        [ 1.0229e+00,  9.7765e-01],\n        [ 4.2852e+00,  3.3821e-01],\n        [ 3.3171e+00,  2.2558e+00],\n        [ 4.1695e+00, -4.1586e-01],\n        [ 3.4841e+00,  6.5448e-01],\n        [ 3.5988e+00, -1.5612e+00],\n        [ 2.0581e+00, -1.0265e+00],\n        [ 3.7902e+00, -1.8776e+00],\n        [ 2.3121e+00, -3.9663e-01],\n        [ 4.6286e+00, -1.6658e-01],\n        [ 2.7564e+00, -2.1713e-02],\n        [ 2.1534e+00,  7.2090e-01],\n        [ 2.2612e+00,  4.4246e-01],\n        [ 2.4599e+00,  5.6830e-01],\n        [ 2.7693e+00, -2.2482e-01],\n        [ 3.0181e+00, -4.4292e-02],\n        [ 8.9954e-01, -6.0140e-01],\n        [ 1.9559e+00,  2.2035e-01],\n        [ 2.5460e+00,  6.0719e-01],\n        [ 2.0070e+00, -1.4074e+00],\n        [ 3.3768e+00,  1.7265e+00],\n        [ 3.4118e+00,  3.5674e-01],\n        [ 2.8925e+00,  1.5609e+00],\n        [ 2.0403e+00, -1.1309e+00],\n        [ 2.1326e+00, -2.9411e+00],\n        [ 3.6234e+00, -9.9923e-02],\n        [ 3.4571e+00,  6.3712e-02],\n        [ 2.6432e+00, -2.0022e+00],\n        [ 1.9473e+00,  2.2736e-01],\n        [ 2.6772e+00,  4.2183e-01],\n        [ 2.2012e+00,  1.2773e+00],\n        [ 1.1216e+00, -1.3644e+00],\n        [ 2.5129e+00, -2.9592e-01],\n        [ 1.4595e+00,  3.2162e-02],\n        [ 1.8861e+00, -8.5870e-02],\n        [ 3.1445e+00, -8.1383e-03],\n        [ 2.1758e+00,  1.3738e+00],\n        [ 2.7761e+00, -1.1563e+00],\n        [ 2.7649e+00,  4.1992e-01],\n        [ 2.4544e+00, -5.5215e-01],\n        [ 1.9341e+00, -3.6639e-01],\n        [ 2.9533e+00, -5.2839e-01],\n        [ 1.9706e+00,  9.1857e-01],\n        [ 1.6994e+00, -7.5203e-01],\n        [ 2.3209e+00, -1.2259e+00],\n        [ 3.5582e+00, -1.0217e-01],\n        [ 8.1951e-01, -1.0190e-01],\n        [ 2.0198e+00, -1.0676e-01],\n        [ 3.9817e+00,  1.5458e+00],\n        [ 2.5361e+00, -5.3271e-01],\n        [ 1.4512e+00,  2.2548e-01],\n        [ 2.0500e-01, -6.7410e-01],\n        [ 1.2384e+00, -1.1150e+00],\n        [ 3.3144e+00, -1.7389e+00],\n        [ 2.7783e+00,  9.2863e-01],\n        [ 4.3958e+00, -1.1588e+00],\n        [ 1.6882e+00, -6.9570e-01],\n        [ 2.0556e+00,  1.1055e+00],\n        [ 3.8716e+00,  2.9754e-01],\n        [ 2.2091e-02, -3.9938e-01],\n        [ 3.9254e+00, -2.0885e+00],\n        [ 9.2647e-01,  4.7011e-01],\n        [ 3.4499e+00,  1.4718e+00],\n        [ 2.4338e+00, -3.9640e-01],\n        [ 1.3856e+00,  1.6626e+00],\n        [ 1.2114e+00, -1.5580e+00],\n        [ 7.8953e-01,  1.8192e-01],\n        [ 1.9003e+00,  8.9485e-01],\n        [ 1.5904e+00,  1.1042e-01],\n        [ 3.4718e+00,  2.0597e+00],\n        [ 2.4339e+00, -1.2198e-01],\n        [ 2.1050e+00,  9.0814e-01],\n        [ 1.7794e+00,  1.5867e+00],\n        [ 1.0900e+00,  1.2553e-01],\n        [ 1.8492e+00,  4.2855e-01],\n        [ 3.9403e+00, -1.1860e+00],\n        [ 3.0693e+00,  5.2146e-01],\n        [ 2.2654e+00,  5.4045e-01],\n        [ 3.2685e+00, -1.7681e+00],\n        [ 4.5641e+00, -1.5326e+00],\n        [ 3.0374e+00, -1.2416e+00],\n        [ 1.0002e+00, -5.9463e-01],\n        [ 3.1746e+00, -3.1059e-01],\n        [ 1.1195e+00,  8.4642e-01],\n        [ 2.4911e+00,  8.6121e-01],\n        [ 1.8901e+00, -9.6535e-01],\n        [ 2.1381e+00, -4.2530e-01],\n        [ 1.0364e+00,  2.2946e-01],\n        [ 1.5708e+00,  4.4776e-01],\n        [ 3.6426e+00,  2.1358e+00],\n        [ 4.2361e+00, -8.1435e-01],\n        [ 4.5228e+00,  1.5397e+00],\n        [ 1.2055e+00, -1.3141e+00],\n        [ 1.1148e+00, -9.4993e-02],\n        [ 2.9895e+00,  7.1128e-02],\n        [ 3.2991e+00, -1.0161e+00],\n        [ 3.6693e+00,  1.5628e+00],\n        [ 2.6515e+00, -1.6150e-01],\n        [ 2.2667e+00, -2.0898e+00],\n        [ 2.0042e+00, -1.3775e-01],\n        [ 2.7214e+00, -2.2640e-01],\n        [ 1.7709e+00, -1.4838e+00],\n        [ 2.2047e+00, -9.0473e-01],\n        [ 3.4493e+00, -1.8295e-01],\n        [ 2.5290e+00,  3.9547e-02],\n        [ 1.8019e+00,  2.1723e-02],\n        [ 1.2831e+00, -2.7741e-02],\n        [ 2.9025e+00, -4.0863e-01],\n        [ 1.5585e+00, -4.0986e-01],\n        [ 4.7798e+00, -5.7036e-01],\n        [ 8.8649e-01, -1.0285e+00],\n        [ 4.4586e-01, -1.8721e+00],\n        [ 1.7398e+00, -1.8506e-01],\n        [ 1.2902e+00,  9.1257e-01],\n        [ 3.0065e+00,  1.0786e-01],\n        [ 2.3085e+00,  1.4918e+00],\n        [ 2.0072e+00,  4.8902e-01],\n        [ 2.9234e+00,  7.5458e-03],\n        [ 2.7572e+00, -3.3705e-01],\n        [ 1.8369e+00, -1.0935e-01],\n        [ 3.7976e+00,  7.1052e-01],\n        [ 2.8979e+00, -8.3950e-01],\n        [ 2.7680e+00,  6.2929e-02],\n        [ 1.0285e+00,  1.1065e+00],\n        [ 7.6610e-01, -1.4545e+00],\n        [ 1.8851e+00,  1.8519e+00],\n        [ 3.8000e+00,  1.3887e-01],\n        [ 1.9458e+00, -9.4978e-01],\n        [ 1.4139e+00,  1.6051e-01],\n        [ 2.2958e+00,  4.7624e-01],\n        [ 1.9833e+00, -1.0599e+00],\n        [ 1.6407e+00, -2.1449e-01],\n        [ 3.2318e+00,  6.1744e-01],\n        [ 4.5529e+00,  1.6176e+00],\n        [ 2.3411e+00, -2.2642e+00],\n        [ 1.4638e+00,  4.5912e-01],\n        [ 1.7310e+00,  1.2007e+00]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.3988e-01, 2.7526e-01, 2.7811e-01,  ..., 9.9941e-01, 9.9999e-01,\n         1.0000e+00],\n        [4.2633e-03, 5.1806e-03, 5.2177e-03,  ..., 9.9267e-01, 1.0000e+00,\n         1.0000e+00],\n        [6.2214e-07, 6.2291e-07, 2.6938e-02,  ..., 9.0391e-01, 9.8770e-01,\n         1.0000e+00],\n        ...,\n        [2.0393e-08, 6.7118e-03, 1.0529e-02,  ..., 9.7701e-01, 9.9330e-01,\n         1.0000e+00],\n        [9.1115e-03, 9.1742e-03, 3.6169e-01,  ..., 9.9910e-01, 9.9940e-01,\n         1.0000e+00],\n        [3.2407e-06, 3.3290e-04, 1.3447e-03,  ..., 8.4644e-01, 9.9889e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.1215],\n        [0.3513],\n        [0.2325],\n        [0.9850],\n        [0.7181],\n        [0.5638],\n        [0.3893],\n        [0.5010],\n        [0.8238],\n        [0.0295],\n        [0.3031],\n        [0.0115],\n        [0.0853],\n        [0.4381],\n        [0.0968],\n        [0.4308],\n        [0.4845],\n        [0.3498],\n        [0.1390],\n        [0.8666],\n        [0.3555],\n        [0.9787],\n        [0.9544],\n        [0.4466],\n        [0.9181],\n        [0.4044],\n        [0.5098],\n        [0.8866],\n        [0.4480],\n        [0.9123],\n        [0.9020],\n        [0.3869],\n        [0.6372],\n        [0.7997],\n        [0.3245],\n        [0.8314],\n        [0.9680],\n        [0.8849],\n        [0.7149],\n        [0.2779],\n        [0.7106],\n        [0.8229],\n        [0.7343],\n        [0.6162],\n        [0.6238],\n        [0.5672],\n        [0.0213],\n        [0.4712],\n        [0.5766],\n        [0.4736],\n        [0.1485],\n        [0.8255],\n        [0.5429],\n        [0.3391],\n        [0.1160],\n        [0.6804],\n        [0.1566],\n        [0.3595],\n        [0.6157],\n        [0.0065],\n        [0.6089],\n        [0.2674],\n        [0.6867],\n        [0.7924],\n        [0.0649],\n        [0.1737],\n        [0.2584],\n        [0.6698],\n        [0.5580],\n        [0.2269],\n        [0.9173],\n        [0.5335],\n        [0.8494],\n        [0.8813],\n        [0.4151],\n        [0.4175],\n        [0.9020],\n        [0.1900],\n        [0.5443],\n        [0.5589],\n        [0.3049],\n        [0.3520],\n        [0.4473],\n        [0.8117],\n        [0.9020],\n        [0.5424],\n        [0.4146],\n        [0.2406],\n        [0.8161],\n        [0.1445],\n        [0.0735],\n        [0.7580],\n        [0.2979],\n        [0.0160],\n        [0.6612],\n        [0.4639],\n        [0.7953],\n        [0.4036],\n        [0.3688],\n        [0.3923],\n        [0.3993],\n        [0.2230],\n        [0.5124],\n        [0.1953],\n        [0.3839],\n        [0.7062],\n        [0.3357],\n        [0.1017],\n        [0.6258],\n        [0.5928],\n        [0.8846],\n        [0.7619],\n        [0.6089],\n        [0.5267],\n        [0.7847],\n        [0.2160],\n        [0.0159],\n        [0.1041],\n        [0.2581],\n        [0.0076],\n        [0.7168],\n        [0.8952],\n        [0.5464],\n        [0.1726],\n        [0.1732],\n        [0.4546],\n        [0.0841],\n        [0.0866],\n        [0.3561],\n        [0.2209],\n        [0.8622],\n        [0.8087],\n        [0.9282],\n        [0.8595],\n        [0.2355],\n        [0.8259],\n        [0.0235],\n        [0.5695],\n        [0.7125],\n        [0.6446],\n        [0.4395],\n        [0.5835],\n        [0.6949],\n        [0.0176],\n        [0.8529],\n        [0.8681],\n        [0.5044],\n        [0.6685],\n        [0.8515],\n        [0.8871],\n        [0.5138],\n        [0.7466],\n        [0.6670],\n        [0.4888],\n        [0.3705],\n        [0.2474],\n        [0.9502],\n        [0.3385],\n        [0.9740],\n        [0.3541],\n        [0.6611],\n        [0.3191],\n        [0.7686],\n        [0.7824],\n        [0.6674],\n        [0.4602],\n        [0.3422],\n        [0.7948],\n        [0.8886],\n        [0.2475],\n        [0.1280],\n        [0.6954],\n        [0.3411],\n        [0.1727],\n        [0.0927],\n        [0.2484],\n        [0.1060],\n        [0.8213],\n        [0.8192],\n        [0.9266],\n        [0.0400],\n        [0.3383],\n        [0.1212],\n        [0.7974],\n        [0.5413],\n        [0.1244],\n        [0.0937],\n        [0.6453],\n        [0.6618],\n        [0.1376],\n        [0.6468],\n        [0.9754],\n        [0.9280],\n        [0.5176],\n        [0.4030],\n        [0.4740],\n        [0.0887],\n        [0.4321],\n        [0.3347],\n        [0.9737],\n        [0.3313],\n        [0.3856],\n        [0.0885],\n        [0.7460],\n        [0.0311],\n        [0.6357],\n        [0.5677],\n        [0.0547],\n        [0.9715],\n        [0.3011],\n        [0.5984],\n        [0.6718],\n        [0.7718],\n        [0.6009],\n        [0.1752],\n        [0.7774],\n        [0.2190],\n        [0.6329],\n        [0.6928],\n        [0.7029],\n        [0.1350],\n        [0.4117],\n        [0.0938],\n        [0.6204],\n        [0.4792],\n        [0.9669],\n        [0.2451],\n        [0.6399],\n        [0.6881],\n        [0.6905],\n        [0.1867],\n        [0.9524],\n        [0.4288],\n        [0.4651],\n        [0.3760],\n        [0.3267],\n        [0.7880],\n        [0.4924],\n        [0.3517],\n        [0.0189],\n        [0.6783],\n        [0.7662],\n        [0.9741],\n        [0.7077],\n        [0.6514],\n        [0.0572],\n        [0.3690],\n        [0.0198],\n        [0.2083],\n        [0.6459],\n        [0.9061],\n        [0.0850],\n        [0.9382],\n        [0.9571],\n        [0.0128],\n        [0.2031],\n        [0.4888],\n        [0.7420],\n        [0.1525],\n        [0.6997],\n        [0.5073],\n        [0.0043],\n        [0.0576],\n        [0.7742],\n        [0.5410],\n        [0.5389],\n        [0.9669],\n        [0.9972],\n        [0.0955],\n        [0.4313],\n        [0.4467],\n        [0.3025],\n        [0.6086],\n        [0.5561],\n        [0.5783],\n        [0.9802],\n        [0.2652],\n        [0.0106],\n        [0.0903],\n        [0.7621],\n        [0.7795],\n        [0.3401],\n        [0.6732],\n        [0.0237],\n        [0.0318],\n        [0.7682],\n        [0.4253],\n        [0.7791],\n        [0.9553],\n        [0.5355],\n        [0.8165],\n        [0.1659],\n        [0.2367],\n        [0.8399],\n        [0.4165],\n        [0.0454],\n        [0.1743],\n        [0.2630],\n        [0.6311],\n        [0.0566],\n        [0.1107],\n        [0.0690],\n        [0.3906],\n        [0.4268],\n        [0.1269],\n        [0.4800],\n        [0.1945],\n        [0.5649],\n        [0.4162],\n        [0.2674],\n        [0.0839],\n        [0.2066]]) torch.Size([312, 1])\nmask tensor([[ True, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False,  True,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 2.4284,  0.4415],\n        [ 1.9357,  0.0957],\n        [ 0.3693,  1.2637],\n        ...,\n        [-2.0347,  1.2922],\n        [ 2.0894, -0.3232],\n        [ 0.3961,  2.0878]]) torch.Size([9984, 2])\nsamples tensor([[ 2.4284e+00,  4.4147e-01],\n        [ 1.8005e+00, -9.4111e-01],\n        [ 2.7945e+00,  9.9437e-02],\n        [ 2.4656e+00,  2.5182e+00],\n        [ 3.8378e+00, -9.2235e-01],\n        [ 1.4698e+00, -2.4068e+00],\n        [ 2.8156e+00, -9.6662e-01],\n        [ 2.7668e+00,  9.4257e-01],\n        [ 2.0141e+00,  1.3472e+00],\n        [ 2.7535e-01, -3.7750e-02],\n        [ 3.7059e+00, -2.5330e+00],\n        [-4.4246e-01,  9.7576e-01],\n        [ 1.7362e+00,  8.7901e-01],\n        [ 2.1959e+00,  4.4159e-01],\n        [ 1.7982e+00,  9.1974e-01],\n        [ 2.5463e+00,  1.7560e+00],\n        [-2.6137e-02,  2.0490e-01],\n        [ 1.7490e+00, -1.7120e-01],\n        [ 3.5091e+00,  1.0751e+00],\n        [ 2.6952e+00,  2.1500e+00],\n        [ 2.0329e+00,  4.6007e-01],\n        [ 1.9672e+00,  1.3196e+00],\n        [ 1.2455e+00,  1.0554e+00],\n        [ 1.6673e+00, -1.5843e+00],\n        [ 3.6109e+00, -1.4225e-01],\n        [ 4.4998e+00, -2.1217e+00],\n        [ 1.9206e+00,  1.1050e+00],\n        [ 3.4584e+00, -1.5364e+00],\n        [ 2.3978e+00, -7.1998e-02],\n        [ 2.2475e+00, -3.7546e-01],\n        [ 2.4165e+00, -2.4229e-01],\n        [ 2.9350e+00, -4.9382e-01],\n        [ 1.1098e+00, -2.5399e-01],\n        [ 1.8408e+00,  1.7554e+00],\n        [ 3.6985e+00, -3.2490e-04],\n        [ 3.9133e+00,  8.6593e-01],\n        [ 5.8949e-01, -3.2574e-01],\n        [ 7.4049e-01,  1.7719e+00],\n        [ 3.2172e+00, -1.6874e+00],\n        [ 2.9376e+00, -9.2428e-01],\n        [ 8.5926e-01, -4.8115e-01],\n        [ 1.2282e+00,  2.7879e-01],\n        [ 3.7640e+00,  4.6727e-01],\n        [ 3.8149e-01,  1.4848e+00],\n        [ 3.2692e+00,  1.8806e+00],\n        [ 2.4491e+00, -2.0240e+00],\n        [ 2.1300e+00,  4.3893e-01],\n        [ 1.8535e+00, -2.3643e-01],\n        [ 1.8520e-01, -8.1088e-01],\n        [ 2.1003e+00, -9.3956e-01],\n        [ 7.6290e-01, -3.4114e-01],\n        [ 2.5586e+00,  1.6519e-02],\n        [ 2.5263e+00,  7.0955e-01],\n        [ 2.6076e+00, -8.7950e-02],\n        [ 2.6408e+00,  8.6204e-01],\n        [ 3.5984e+00,  9.0649e-01],\n        [ 2.8218e+00,  7.0561e-01],\n        [ 2.4972e+00, -1.3557e+00],\n        [ 1.9680e+00, -8.2427e-01],\n        [ 5.0028e-01, -3.9081e-01],\n        [ 3.5005e+00,  3.8157e-01],\n        [ 1.6375e+00, -4.5231e-01],\n        [ 3.2594e+00, -1.7154e-01],\n        [ 3.4257e+00, -9.3122e-01],\n        [ 2.9115e+00,  9.4802e-01],\n        [ 2.7722e+00,  2.5718e-02],\n        [ 2.5168e-01, -9.9863e-01],\n        [ 3.9754e+00, -7.5197e-01],\n        [ 4.2814e+00,  6.9104e-01],\n        [ 9.5264e-01, -1.5521e+00],\n        [ 3.4454e+00, -5.7516e-01],\n        [ 4.0263e+00, -3.3284e-01],\n        [ 1.5805e+00,  1.1308e+00],\n        [ 3.9531e+00, -9.5538e-01],\n        [ 3.7211e+00,  8.3460e-02],\n        [ 1.7539e+00,  1.7082e+00],\n        [ 2.1842e+00, -6.0604e-01],\n        [ 2.0899e+00,  3.1127e-01],\n        [ 2.8415e+00, -1.5863e+00],\n        [ 9.6315e-01, -1.3252e+00],\n        [ 1.0117e+00,  1.3826e-01],\n        [ 2.1995e+00,  8.9366e-01],\n        [ 2.2549e+00, -3.3163e-02],\n        [ 3.2544e+00, -5.5605e-01],\n        [ 2.9964e+00, -1.0047e+00],\n        [ 2.9040e+00, -1.2580e-01],\n        [ 1.9629e+00,  3.9698e-01],\n        [ 1.4588e+00,  3.1636e-01],\n        [ 2.3410e+00,  3.6282e-01],\n        [ 1.4427e+00,  6.3626e-01],\n        [ 1.5995e+00,  1.4260e-01],\n        [ 7.3521e-01, -2.0211e-01],\n        [ 3.0001e+00, -8.7900e-01],\n        [ 3.3976e+00,  1.4854e+00],\n        [ 3.1263e+00, -1.4551e+00],\n        [ 3.3018e+00,  1.2832e+00],\n        [ 1.1798e+00, -1.9058e-01],\n        [ 3.0660e+00,  3.7229e-01],\n        [ 3.0009e+00,  1.5784e+00],\n        [ 3.5698e+00,  5.9156e-01],\n        [ 1.5028e+00,  3.6351e-01],\n        [ 1.6803e+00, -1.5688e+00],\n        [ 2.7020e-01, -5.6287e-01],\n        [ 7.9767e-01, -7.5497e-01],\n        [ 4.4625e+00, -7.9427e-01],\n        [ 2.0313e+00, -5.1022e-01],\n        [ 1.1736e+00, -7.9999e-01],\n        [ 1.7817e+00,  5.0923e-01],\n        [ 1.6121e+00,  3.6167e-02],\n        [ 6.1422e-01, -6.1770e-01],\n        [ 3.8603e+00, -1.2189e+00],\n        [ 1.5867e+00,  6.6039e-02],\n        [ 1.7426e+00, -3.3209e-01],\n        [ 2.5014e+00,  1.1966e+00],\n        [ 2.4536e+00, -5.4327e-01],\n        [ 2.5317e+00, -2.0314e+00],\n        [ 2.6207e+00,  1.4110e+00],\n        [ 3.4129e+00, -1.3149e-01],\n        [ 3.0927e+00,  4.6125e-01],\n        [ 3.6803e+00,  7.3403e-01],\n        [ 2.0034e+00,  7.1812e-01],\n        [ 3.1299e+00, -1.4304e+00],\n        [ 1.5377e+00,  5.0106e-02],\n        [ 2.6009e+00,  3.8439e-01],\n        [ 2.4326e+00, -7.8388e-01],\n        [ 2.4410e+00, -3.4818e-01],\n        [ 1.8059e+00,  1.3973e+00],\n        [ 1.6118e+00, -7.7441e-02],\n        [ 2.5179e+00, -1.8970e-01],\n        [ 3.7863e+00, -7.6497e-01],\n        [ 1.5478e+00,  7.0153e-01],\n        [ 2.9152e+00,  5.7322e-01],\n        [ 3.6691e+00,  2.3109e+00],\n        [ 4.1605e+00,  6.8639e-01],\n        [ 3.0399e+00,  2.2812e-01],\n        [ 9.3853e-01, -1.3746e+00],\n        [ 7.6083e-01, -4.5945e-02],\n        [ 2.6240e+00, -1.3537e+00],\n        [ 3.6478e+00, -1.8972e-01],\n        [ 1.9346e+00,  1.7022e-01],\n        [ 2.8800e+00, -8.3831e-01],\n        [ 4.0258e+00,  5.5603e-02],\n        [ 1.3973e+00, -8.1436e-02],\n        [ 2.1904e+00,  1.4114e+00],\n        [ 2.3240e+00,  8.4348e-01],\n        [ 6.8724e-01,  8.7228e-01],\n        [ 1.6461e+00,  1.3432e+00],\n        [ 2.6157e+00, -6.9654e-01],\n        [ 2.4506e+00, -1.6415e+00],\n        [ 1.3944e+00, -6.4990e-01],\n        [ 6.6707e-01,  5.3819e-02],\n        [ 3.0661e+00,  3.3817e-01],\n        [ 1.0621e+00,  1.0098e+00],\n        [ 3.8841e+00,  1.4277e+00],\n        [ 2.0736e+00, -1.2312e+00],\n        [ 3.7010e+00, -1.0145e+00],\n        [ 7.2293e-01,  4.7879e-01],\n        [ 2.1530e+00,  1.2069e-01],\n        [ 1.7356e+00,  8.0489e-01],\n        [ 3.6908e+00, -1.5732e+00],\n        [ 7.2508e-01, -2.9306e-01],\n        [ 2.0420e-01,  7.6190e-01],\n        [ 3.0251e+00, -1.5670e+00],\n        [ 2.2118e+00, -2.7635e+00],\n        [ 8.7628e-01,  1.4884e+00],\n        [ 8.6703e-01, -2.6454e-01],\n        [ 1.8469e+00, -2.2767e+00],\n        [ 2.5646e+00,  1.8054e-01],\n        [ 2.7169e+00, -2.1695e+00],\n        [ 1.5504e+00, -6.7182e-01],\n        [ 8.3257e-01, -5.5502e-01],\n        [ 4.0784e+00,  1.2350e+00],\n        [ 4.4419e+00,  1.9840e-01],\n        [ 2.8835e+00, -8.4324e-01],\n        [ 2.0748e+00,  3.5660e-02],\n        [ 3.8820e+00,  6.6648e-01],\n        [ 1.3753e+00,  1.2540e+00],\n        [ 2.6476e+00,  9.6211e-01],\n        [ 4.4036e+00, -9.2292e-01],\n        [ 3.1931e+00,  8.1548e-01],\n        [ 2.7322e+00,  5.5641e-01],\n        [ 4.0327e+00,  1.4243e+00],\n        [ 3.6988e+00,  3.2446e-01],\n        [ 3.2756e+00, -1.0024e+00],\n        [ 1.1407e+00, -1.9262e+00],\n        [ 3.3187e+00, -8.4543e-02],\n        [ 2.5199e+00,  1.2694e+00],\n        [ 2.0095e+00, -5.7753e-01],\n        [ 2.6004e+00, -3.6238e-01],\n        [ 2.4811e+00, -1.2367e+00],\n        [ 2.7990e+00, -9.4177e-01],\n        [ 3.1120e+00, -6.0180e-01],\n        [ 2.1768e+00,  1.2769e+00],\n        [ 1.5683e+00, -3.6013e-02],\n        [ 2.6912e+00,  5.4506e-01],\n        [ 3.2691e+00, -1.9637e+00],\n        [ 2.5551e+00, -1.2974e+00],\n        [ 3.0402e+00,  1.0464e+00],\n        [ 2.9389e+00, -7.9072e-01],\n        [ 1.6771e+00,  4.2810e-01],\n        [ 3.7523e+00, -1.9073e+00],\n        [ 2.5323e+00, -1.9907e+00],\n        [ 2.0541e+00, -3.4918e-01],\n        [ 2.8858e+00, -1.0039e+00],\n        [ 5.7118e-01, -3.2785e-01],\n        [ 3.0724e+00, -8.4690e-01],\n        [ 2.0656e+00, -1.5098e+00],\n        [ 1.1237e+00,  1.2477e-01],\n        [ 2.2506e+00,  4.7226e-01],\n        [ 1.3489e+00, -1.0709e+00],\n        [ 2.3938e+00,  5.4302e-01],\n        [ 3.0981e+00, -1.6982e+00],\n        [ 1.4196e+00,  8.8132e-01],\n        [ 4.1498e+00, -2.5181e-01],\n        [ 2.3911e+00, -2.4789e-01],\n        [ 2.6216e+00, -3.4779e-01],\n        [ 3.6905e+00, -5.1463e-01],\n        [ 1.3660e+00, -6.7700e-01],\n        [ 1.5319e+00, -1.2509e+00],\n        [ 1.6379e+00,  1.1651e+00],\n        [ 3.2269e+00,  6.2750e-01],\n        [ 3.1905e+00, -1.6881e+00],\n        [ 3.8091e+00, -7.9297e-01],\n        [ 1.4358e+00, -6.3175e-01],\n        [ 3.1174e+00, -9.0077e-01],\n        [ 1.8761e+00,  4.4373e-01],\n        [ 3.2689e+00,  1.0833e+00],\n        [ 1.7721e+00, -7.2570e-02],\n        [ 3.1599e+00, -5.4269e-01],\n        [ 2.5623e+00,  2.8168e-01],\n        [ 2.9538e+00, -1.0296e+00],\n        [ 1.9316e+00,  8.7313e-01],\n        [ 2.2526e+00, -3.1955e-01],\n        [ 2.7226e+00, -1.9761e+00],\n        [ 2.7100e+00, -3.4409e-01],\n        [ 2.2218e+00, -2.1120e-01],\n        [ 3.8547e+00, -3.4042e+00],\n        [ 4.1148e+00, -1.0449e+00],\n        [ 1.2561e+00,  2.2030e-01],\n        [ 1.8483e+00,  1.0173e+00],\n        [ 2.8667e+00, -1.3120e+00],\n        [ 1.9048e+00, -4.0383e-01],\n        [ 1.7834e+00,  4.0059e-01],\n        [ 2.0997e+00,  5.6811e-01],\n        [ 1.9978e+00, -1.2060e+00],\n        [ 1.7960e+00,  3.6339e-01],\n        [ 2.8216e+00, -3.8730e-01],\n        [ 1.8770e+00,  1.0654e+00],\n        [ 2.4235e+00, -1.2395e+00],\n        [ 3.5628e+00,  6.2980e-01],\n        [ 2.3397e+00, -1.2716e-01],\n        [ 2.6664e-01, -1.6922e+00],\n        [ 1.3634e+00, -4.1072e-01],\n        [ 2.3503e+00,  9.1747e-01],\n        [ 2.9622e+00,  1.9962e+00],\n        [ 2.5811e+00,  1.3259e+00],\n        [ 4.2905e+00, -9.9507e-02],\n        [ 1.7792e+00, -7.5298e-01],\n        [ 2.3880e+00,  1.2279e+00],\n        [ 2.5648e+00, -1.2082e+00],\n        [ 2.7957e+00, -3.9447e-01],\n        [ 1.1227e+00,  1.8064e+00],\n        [ 2.8389e+00,  1.1406e+00],\n        [ 8.9440e-01,  3.7964e-01],\n        [ 2.6028e-01, -1.5960e+00],\n        [ 2.4487e+00, -3.4535e-01],\n        [ 1.7938e+00, -4.5060e-01],\n        [ 1.5724e+00,  1.7055e+00],\n        [ 2.4870e+00, -4.2131e-02],\n        [ 3.1036e+00, -5.7308e-03],\n        [ 3.8601e+00, -4.9236e-03],\n        [ 2.5286e+00, -6.6110e-01],\n        [ 3.4520e+00, -8.9595e-01],\n        [ 3.4655e+00, -1.7037e+00],\n        [ 9.3596e-01,  1.9069e-01],\n        [-4.3686e-02, -4.1413e-01],\n        [ 1.9856e+00,  4.5942e-01],\n        [ 2.9055e+00,  1.6943e+00],\n        [ 4.1629e+00, -1.9704e+00],\n        [ 2.4049e+00, -1.9538e+00],\n        [ 4.9882e-01,  4.1684e-01],\n        [ 4.8058e+00, -4.5641e-01],\n        [ 1.8427e+00, -2.5114e-01],\n        [ 1.1357e+00,  2.3032e-01],\n        [ 2.1599e+00,  1.1228e+00],\n        [ 2.9721e+00, -3.0270e-01],\n        [ 2.6849e+00, -7.6494e-01],\n        [ 2.1944e-01, -1.2697e+00],\n        [ 1.6487e+00, -4.5607e-02],\n        [ 3.3403e+00, -1.0210e+00],\n        [ 1.4143e+00, -7.0883e-01],\n        [ 7.4679e-01, -7.1061e-01],\n        [ 2.7401e+00, -5.3879e-01],\n        [ 2.7180e+00, -9.4648e-01],\n        [ 3.2019e+00, -1.1327e+00],\n        [ 2.7569e+00, -8.1751e-01],\n        [ 3.2624e+00,  9.4096e-01],\n        [ 2.8455e+00,  4.4745e-01],\n        [ 3.0119e+00, -4.7753e-01],\n        [ 1.5944e+00, -8.2489e-02],\n        [ 1.2773e+00, -4.8832e-01],\n        [ 2.4708e+00,  2.2710e-01],\n        [ 1.1369e+00,  3.9788e-01],\n        [ 1.0087e+00,  2.0698e-01],\n        [ 2.1021e+00, -1.3122e-01],\n        [ 1.4593e+00, -2.4659e+00],\n        [ 2.4191e+00,  1.0348e+00],\n        [ 3.8084e+00,  6.0623e-01],\n        [ 1.3135e+00, -7.5097e-01],\n        [ 2.6244e+00, -1.9823e-01],\n        [ 3.2436e+00, -1.2575e+00],\n        [ 2.6871e+00, -1.0244e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[2.8220e-04, 3.0330e-04, 3.1546e-04,  ..., 8.6007e-01, 8.6430e-01,\n         1.0000e+00],\n        [2.9395e-03, 1.0207e-02, 1.0304e-02,  ..., 9.9096e-01, 9.9286e-01,\n         1.0000e+00],\n        [3.2607e-04, 3.2796e-04, 2.6677e-02,  ..., 8.3987e-01, 8.3987e-01,\n         1.0000e+00],\n        ...,\n        [3.6706e-10, 1.1878e-05, 1.7809e-01,  ..., 9.7405e-01, 9.8946e-01,\n         1.0000e+00],\n        [2.7234e-03, 3.1426e-01, 3.1426e-01,  ..., 9.9999e-01, 9.9999e-01,\n         1.0000e+00],\n        [1.3059e-04, 4.8825e-03, 9.4988e-03,  ..., 9.8985e-01, 9.9969e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[3.9658e-01],\n        [1.4348e-01],\n        [9.8412e-01],\n        [2.8307e-01],\n        [8.2284e-01],\n        [9.4636e-01],\n        [1.2660e-01],\n        [7.8941e-01],\n        [6.4850e-01],\n        [1.9245e-01],\n        [4.3459e-01],\n        [4.4697e-01],\n        [6.9247e-01],\n        [3.5812e-01],\n        [7.2403e-01],\n        [1.1244e-01],\n        [3.9952e-01],\n        [4.7314e-01],\n        [3.3337e-02],\n        [8.4402e-01],\n        [9.0742e-01],\n        [9.1490e-01],\n        [1.8429e-01],\n        [6.0554e-02],\n        [4.1434e-01],\n        [6.4015e-01],\n        [1.7982e-01],\n        [9.1889e-01],\n        [7.7073e-01],\n        [4.4345e-01],\n        [3.0685e-01],\n        [2.5095e-01],\n        [2.9747e-01],\n        [1.3677e-01],\n        [2.3502e-01],\n        [1.5475e-01],\n        [9.3984e-01],\n        [6.6299e-01],\n        [5.4710e-01],\n        [6.3947e-01],\n        [3.3923e-01],\n        [2.0400e-01],\n        [5.0081e-01],\n        [6.6418e-01],\n        [5.2340e-01],\n        [7.9784e-01],\n        [9.8962e-01],\n        [3.2587e-01],\n        [9.2955e-01],\n        [9.1911e-01],\n        [4.4531e-01],\n        [3.0090e-01],\n        [8.0559e-01],\n        [6.6545e-01],\n        [4.1821e-01],\n        [7.2450e-02],\n        [5.7479e-01],\n        [1.1314e-01],\n        [6.5409e-01],\n        [6.7316e-01],\n        [5.8368e-01],\n        [6.5646e-01],\n        [4.1129e-01],\n        [5.2842e-01],\n        [5.8851e-01],\n        [8.3066e-01],\n        [8.4846e-01],\n        [8.0279e-01],\n        [8.8795e-01],\n        [7.8261e-01],\n        [5.7585e-02],\n        [5.7481e-01],\n        [1.5990e-01],\n        [7.3849e-01],\n        [3.1629e-01],\n        [1.5427e-01],\n        [4.8989e-01],\n        [8.8989e-01],\n        [4.9441e-01],\n        [2.8968e-01],\n        [6.2564e-01],\n        [5.0186e-01],\n        [3.3045e-01],\n        [4.0369e-01],\n        [8.9071e-01],\n        [9.2349e-01],\n        [6.2443e-01],\n        [3.7611e-01],\n        [2.5485e-01],\n        [1.0685e-01],\n        [8.4584e-01],\n        [8.3128e-01],\n        [6.9564e-01],\n        [1.7080e-01],\n        [7.1333e-01],\n        [7.5933e-01],\n        [6.6714e-01],\n        [7.7735e-01],\n        [9.0777e-02],\n        [3.1618e-01],\n        [9.4356e-01],\n        [3.9641e-01],\n        [3.7560e-01],\n        [1.4411e-01],\n        [6.1342e-01],\n        [2.1467e-01],\n        [6.3620e-01],\n        [1.9957e-01],\n        [3.1187e-01],\n        [6.3651e-01],\n        [4.7085e-01],\n        [9.4248e-01],\n        [6.2696e-02],\n        [1.6058e-01],\n        [5.5555e-01],\n        [9.2466e-01],\n        [9.9214e-01],\n        [4.4012e-01],\n        [4.9625e-01],\n        [7.2302e-01],\n        [1.3604e-01],\n        [7.9169e-01],\n        [9.9345e-01],\n        [5.8564e-01],\n        [2.2399e-01],\n        [3.0743e-01],\n        [3.5241e-02],\n        [6.4386e-01],\n        [9.9724e-01],\n        [5.7219e-01],\n        [2.1815e-01],\n        [4.0244e-01],\n        [1.0825e-01],\n        [9.2274e-01],\n        [7.3998e-01],\n        [1.4509e-01],\n        [7.2995e-01],\n        [4.7498e-02],\n        [8.2993e-01],\n        [3.5450e-01],\n        [2.1467e-01],\n        [3.1511e-01],\n        [6.4488e-01],\n        [1.3220e-01],\n        [1.8045e-01],\n        [6.5802e-01],\n        [8.8241e-01],\n        [2.6304e-01],\n        [9.5333e-02],\n        [4.3337e-01],\n        [2.8463e-01],\n        [3.4746e-01],\n        [2.1651e-02],\n        [4.0330e-01],\n        [6.1874e-01],\n        [9.1542e-01],\n        [8.1889e-01],\n        [7.9288e-01],\n        [6.2604e-01],\n        [4.6038e-01],\n        [6.8036e-01],\n        [4.2120e-01],\n        [3.9243e-01],\n        [6.8164e-01],\n        [5.1529e-02],\n        [6.6533e-01],\n        [8.6060e-01],\n        [8.4753e-01],\n        [4.6876e-01],\n        [3.6008e-01],\n        [6.6343e-01],\n        [5.7860e-01],\n        [1.1958e-01],\n        [1.4587e-01],\n        [2.3011e-01],\n        [4.2771e-02],\n        [6.8061e-02],\n        [5.9210e-01],\n        [6.5660e-01],\n        [6.4789e-01],\n        [8.8819e-01],\n        [2.6945e-01],\n        [5.4836e-01],\n        [1.1513e-01],\n        [5.3365e-01],\n        [4.4625e-01],\n        [8.0820e-01],\n        [6.5001e-01],\n        [7.9075e-02],\n        [9.5467e-01],\n        [6.4179e-01],\n        [5.3883e-01],\n        [6.7269e-01],\n        [9.7091e-01],\n        [7.6701e-01],\n        [2.5433e-02],\n        [6.2224e-01],\n        [9.6329e-01],\n        [7.4151e-01],\n        [9.0289e-01],\n        [8.4353e-01],\n        [7.5043e-01],\n        [8.0511e-01],\n        [2.6253e-01],\n        [8.8034e-01],\n        [5.4317e-01],\n        [5.3387e-01],\n        [1.9579e-01],\n        [4.3638e-01],\n        [1.2477e-01],\n        [2.8889e-01],\n        [8.7326e-01],\n        [4.5852e-01],\n        [8.2340e-01],\n        [2.3453e-01],\n        [1.2124e-01],\n        [9.0511e-01],\n        [1.8483e-01],\n        [4.9513e-01],\n        [2.1746e-01],\n        [5.8947e-01],\n        [3.0441e-01],\n        [2.1032e-01],\n        [1.2935e-01],\n        [5.3200e-01],\n        [9.9085e-01],\n        [5.6419e-01],\n        [2.8953e-01],\n        [7.2960e-01],\n        [3.0517e-01],\n        [4.3783e-01],\n        [9.1864e-01],\n        [4.1760e-01],\n        [6.7870e-02],\n        [1.0123e-02],\n        [1.1247e-04],\n        [2.1638e-02],\n        [9.4735e-01],\n        [8.0877e-01],\n        [1.7017e-01],\n        [4.3632e-01],\n        [6.7382e-01],\n        [8.0421e-01],\n        [3.9731e-01],\n        [2.8739e-01],\n        [2.1309e-01],\n        [8.5938e-01],\n        [7.7110e-01],\n        [3.9159e-01],\n        [4.4587e-01],\n        [7.9362e-01],\n        [2.3526e-01],\n        [3.4207e-01],\n        [5.3163e-02],\n        [9.6853e-01],\n        [1.6482e-02],\n        [9.2614e-01],\n        [9.8726e-01],\n        [6.4590e-01],\n        [1.6528e-02],\n        [9.0729e-01],\n        [7.3849e-01],\n        [7.1975e-01],\n        [3.0669e-01],\n        [4.2977e-02],\n        [2.3151e-02],\n        [3.6460e-01],\n        [2.4548e-02],\n        [5.1638e-02],\n        [5.7131e-01],\n        [7.2929e-01],\n        [2.6113e-01],\n        [5.6228e-02],\n        [3.7492e-01],\n        [7.5003e-01],\n        [9.1598e-01],\n        [5.3681e-01],\n        [1.7353e-01],\n        [8.6741e-01],\n        [7.6456e-01],\n        [4.4732e-01],\n        [6.6569e-01],\n        [2.3427e-01],\n        [8.1458e-01],\n        [3.4212e-01],\n        [3.6794e-02],\n        [3.4897e-02],\n        [2.2556e-01],\n        [8.9128e-01],\n        [1.7224e-01],\n        [4.0921e-02],\n        [7.3878e-01],\n        [6.4530e-01],\n        [6.7460e-01],\n        [7.1279e-01],\n        [4.0640e-01],\n        [2.8746e-01],\n        [7.7826e-01],\n        [7.0893e-01],\n        [2.5979e-01],\n        [5.3256e-01],\n        [4.5154e-01],\n        [5.8069e-01],\n        [3.8786e-02],\n        [1.0988e-01],\n        [8.0776e-01],\n        [5.0349e-01],\n        [1.5551e-01],\n        [6.4590e-01],\n        [5.7419e-01],\n        [7.6577e-01],\n        [6.4893e-01]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False,  True],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-0.7720,  2.3805],\n        [-1.6508,  1.6977],\n        [-1.7359,  2.1554],\n        ...,\n        [-0.8962,  1.8369],\n        [ 0.5081,  0.4378],\n        [-0.5548,  1.3839]]) torch.Size([9984, 2])\nsamples tensor([[ 2.4291e+00,  6.9529e-01],\n        [ 2.6621e+00, -2.6540e-01],\n        [ 2.0900e+00,  3.7689e-01],\n        [ 2.1228e+00, -5.0926e-01],\n        [ 2.9194e+00,  7.0339e-02],\n        [ 1.6321e+00, -9.4037e-01],\n        [ 2.5017e+00, -9.5704e-01],\n        [ 1.8201e+00,  1.8835e+00],\n        [ 1.7498e+00,  1.3610e-02],\n        [ 1.7330e+00, -2.5898e-02],\n        [ 2.1846e+00,  7.5876e-01],\n        [ 9.4465e-01,  5.7052e-01],\n        [ 1.0003e+00, -6.6266e-01],\n        [ 2.1996e+00, -6.3326e-01],\n        [ 2.4646e+00, -4.1351e-01],\n        [ 3.8598e+00,  6.8688e-01],\n        [ 1.0453e+00,  6.0685e-01],\n        [ 7.2263e-01, -1.2067e+00],\n        [ 2.9733e+00,  6.9249e-01],\n        [ 2.0275e+00, -3.3354e-01],\n        [ 1.1604e+00, -5.3967e-01],\n        [ 2.3296e+00,  2.3377e-01],\n        [ 1.0498e+00,  3.4940e-01],\n        [ 2.5047e+00,  1.7776e+00],\n        [ 1.6771e+00, -2.1661e-01],\n        [ 3.8341e+00,  6.4085e-01],\n        [ 3.2012e+00,  2.2813e+00],\n        [ 2.0751e+00,  9.5329e-02],\n        [ 3.1962e+00,  9.7189e-01],\n        [ 3.1977e+00, -1.1772e+00],\n        [ 4.4778e+00, -5.8848e-01],\n        [ 1.3033e+00, -9.9320e-02],\n        [ 2.5045e+00,  4.0159e-01],\n        [ 3.0398e+00, -1.1432e-01],\n        [ 1.1853e+00,  7.7194e-01],\n        [ 1.8693e+00, -4.3651e-01],\n        [ 2.3758e+00,  2.3997e-01],\n        [ 2.2374e+00,  8.3355e-01],\n        [ 8.5599e-01, -1.8235e+00],\n        [ 2.7076e+00,  1.2512e+00],\n        [ 3.4415e+00,  1.3386e+00],\n        [ 9.4802e-01,  1.3545e-01],\n        [ 1.9646e+00, -1.7799e+00],\n        [ 2.8315e+00, -1.3758e+00],\n        [ 2.1259e+00, -8.6525e-01],\n        [ 1.9037e+00,  1.7113e+00],\n        [ 9.8185e-01,  3.9340e-01],\n        [ 2.9652e+00, -1.5666e+00],\n        [ 1.6474e+00,  1.5696e-01],\n        [ 1.9522e+00,  5.9202e-01],\n        [ 3.0060e+00,  2.9928e-01],\n        [ 2.5178e+00, -2.1352e+00],\n        [ 2.5395e+00, -9.7331e-02],\n        [ 2.9384e+00,  5.8239e-01],\n        [ 3.6632e+00, -1.8945e+00],\n        [ 2.2983e+00, -1.6999e-01],\n        [ 4.8339e+00, -1.4749e-01],\n        [ 1.1092e+00, -1.3450e+00],\n        [ 2.5863e+00, -2.3169e+00],\n        [-8.5725e-01,  4.9469e-01],\n        [ 1.3167e+00, -3.4387e-01],\n        [ 2.4839e+00, -5.8003e-01],\n        [ 2.5974e+00, -7.9735e-01],\n        [ 1.0273e+00, -3.8088e-01],\n        [ 2.4968e+00, -2.5000e-01],\n        [ 1.3631e+00,  6.6559e-01],\n        [ 3.2835e+00, -1.6057e-01],\n        [ 1.5698e+00, -8.9840e-01],\n        [ 2.1748e+00,  4.9574e-01],\n        [ 3.0358e+00, -6.5488e-01],\n        [ 2.3747e+00, -9.4489e-01],\n        [ 2.7025e+00, -9.5588e-01],\n        [ 1.4145e+00,  1.2651e-01],\n        [ 2.9615e+00,  1.6157e+00],\n        [ 3.6903e+00,  2.0958e+00],\n        [ 2.6234e+00,  1.7536e+00],\n        [ 2.8584e+00,  2.5960e-01],\n        [ 1.4236e+00, -2.6015e-01],\n        [ 2.0648e+00, -2.8053e-01],\n        [ 3.2493e+00, -1.2982e+00],\n        [ 2.3896e+00, -1.4448e+00],\n        [ 2.2872e+00, -6.1432e-01],\n        [ 2.0758e+00, -2.1685e-01],\n        [ 2.2372e+00,  2.2712e-01],\n        [ 2.3669e+00, -3.5301e-01],\n        [ 1.9386e+00,  1.8114e+00],\n        [ 2.1614e+00, -1.2069e+00],\n        [ 2.7356e+00,  7.5201e-01],\n        [ 2.0240e+00,  7.6140e-01],\n        [ 1.2193e+00, -6.7835e-01],\n        [ 3.2504e+00, -3.2131e-01],\n        [ 3.2214e+00,  4.7043e-02],\n        [ 2.0778e+00, -1.3119e+00],\n        [ 2.5031e+00,  1.3069e+00],\n        [ 3.1948e+00,  2.2995e-01],\n        [ 3.1811e+00,  1.1590e+00],\n        [ 2.8874e+00, -1.4409e-02],\n        [ 2.9224e+00,  7.2065e-01],\n        [ 1.7351e+00,  2.5143e-01],\n        [ 2.2776e+00, -5.9093e-01],\n        [ 2.8865e+00,  1.3327e+00],\n        [ 4.1165e+00, -7.4095e-02],\n        [ 1.7106e+00,  1.4761e+00],\n        [ 1.9249e+00,  1.4002e+00],\n        [ 2.6849e+00,  5.7687e-01],\n        [ 2.3569e+00,  8.5259e-02],\n        [ 2.0880e+00, -1.6723e+00],\n        [ 3.1430e+00, -2.0290e+00],\n        [ 2.5128e+00, -1.5007e+00],\n        [ 1.1202e+00,  4.2956e-01],\n        [ 3.2210e+00, -1.6258e+00],\n        [ 1.5592e+00, -2.3232e-02],\n        [ 3.5240e+00,  8.8288e-01],\n        [ 2.6273e+00, -1.2901e-01],\n        [ 8.8670e-01,  1.5290e+00],\n        [ 1.8114e+00, -1.0707e+00],\n        [ 2.1466e+00,  1.9389e+00],\n        [ 2.2314e+00, -5.8581e-01],\n        [-1.5694e-01,  1.5079e-01],\n        [ 2.2784e+00,  1.5962e+00],\n        [ 2.0914e+00, -1.6563e-01],\n        [ 2.5728e+00, -6.4431e-02],\n        [ 2.3600e+00,  3.8600e-02],\n        [ 3.0487e+00, -2.2950e+00],\n        [ 5.4060e-01,  5.6297e-01],\n        [ 3.0885e+00, -1.3541e+00],\n        [ 3.0526e+00,  5.4334e-01],\n        [ 2.3986e+00, -1.7688e+00],\n        [ 3.5586e+00, -5.9623e-01],\n        [ 1.3055e+00,  1.0769e-01],\n        [ 2.0913e+00, -1.4648e-02],\n        [ 2.2150e+00,  7.7385e-01],\n        [ 2.3126e+00, -4.7194e-01],\n        [ 2.5370e+00,  4.2484e-03],\n        [ 3.5708e+00,  3.6257e-01],\n        [ 2.0084e+00,  3.0871e-01],\n        [ 3.1933e+00, -3.5165e-02],\n        [ 1.9507e+00,  1.1861e+00],\n        [ 1.5828e+00,  3.2767e-01],\n        [ 2.8349e+00,  4.2469e-01],\n        [ 2.9093e+00, -1.2803e+00],\n        [ 3.4477e+00, -1.5972e+00],\n        [ 3.6467e+00, -1.3840e+00],\n        [ 1.0640e+00, -1.8609e+00],\n        [ 3.2280e+00,  1.1186e+00],\n        [ 2.0299e+00, -1.5105e+00],\n        [ 3.0090e+00,  4.0436e-01],\n        [ 2.2507e+00,  8.9812e-01],\n        [ 3.6476e+00,  1.4610e+00],\n        [ 1.3090e+00,  1.4058e+00],\n        [ 8.6788e-01,  4.8082e-01],\n        [ 2.4862e+00, -5.5862e-01],\n        [ 3.6297e+00, -2.7253e+00],\n        [ 2.9094e+00, -6.7618e-02],\n        [ 7.9176e-01, -5.2870e-01],\n        [ 4.3788e+00,  7.9606e-01],\n        [ 3.7625e+00, -5.8012e-01],\n        [ 1.3727e+00, -1.2263e+00],\n        [ 2.1809e+00, -2.1148e-01],\n        [ 1.7914e+00, -1.0187e+00],\n        [ 3.0024e+00, -6.1637e-01],\n        [ 2.2011e+00, -9.2083e-01],\n        [ 2.0700e+00,  2.7536e-01],\n        [ 2.1457e+00, -2.1956e+00],\n        [ 4.0524e+00, -1.1347e+00],\n        [ 3.9669e+00, -1.5780e+00],\n        [ 3.2608e+00, -1.4902e-02],\n        [ 2.4707e+00,  4.3752e-01],\n        [ 5.2033e-01, -8.0129e-01],\n        [ 1.8680e+00, -7.8695e-01],\n        [ 2.1115e+00, -6.5372e-01],\n        [ 3.5144e+00,  3.4925e-01],\n        [ 1.7534e+00, -6.9358e-01],\n        [ 3.7856e+00, -2.0072e+00],\n        [ 2.9980e+00, -1.5405e+00],\n        [ 3.1363e+00, -3.3390e+00],\n        [ 1.5475e+00,  3.9785e-01],\n        [ 1.7395e+00, -8.0545e-01],\n        [ 9.9599e-01, -1.0085e-01],\n        [ 3.0004e+00,  3.2630e-01],\n        [ 4.4673e-01, -1.3268e+00],\n        [ 1.2906e+00,  6.9387e-01],\n        [ 1.8369e+00,  1.5442e+00],\n        [ 3.6160e+00, -4.1583e-01],\n        [ 2.3427e+00, -4.5789e-01],\n        [ 2.9072e+00,  1.2640e+00],\n        [ 1.8679e+00, -1.6454e+00],\n        [ 2.6632e+00,  1.8271e+00],\n        [ 1.3330e+00,  1.5595e+00],\n        [ 2.8886e+00,  5.8128e-01],\n        [ 3.1419e+00, -7.4987e-01],\n        [ 1.6637e+00, -1.3783e-01],\n        [ 1.3835e+00, -9.7951e-01],\n        [ 2.2521e+00,  1.0592e+00],\n        [ 3.8340e+00,  6.3888e-02],\n        [ 3.5899e+00,  2.8332e-01],\n        [ 2.5282e+00,  5.9373e-01],\n        [ 1.1775e+00,  1.2121e-01],\n        [ 1.7309e+00, -2.9302e-01],\n        [ 2.7668e+00,  4.6217e-01],\n        [ 3.3402e+00, -1.0462e+00],\n        [ 4.1079e+00, -7.1038e-01],\n        [ 2.5308e+00,  4.0190e-01],\n        [ 2.8251e+00, -9.0767e-01],\n        [ 2.0805e+00, -5.2472e-01],\n        [ 1.1039e+00, -1.2670e-01],\n        [ 8.4986e-01,  1.7366e-02],\n        [ 2.4731e+00,  2.1879e+00],\n        [ 2.9668e+00,  2.1049e+00],\n        [ 1.7468e+00,  7.8952e-01],\n        [ 2.7259e+00, -1.9115e+00],\n        [ 2.6073e-01,  3.2961e-01],\n        [ 1.3860e+00, -2.6865e-02],\n        [ 1.3943e+00, -5.6730e-01],\n        [ 1.2733e+00,  2.0157e-01],\n        [ 2.1338e+00,  3.2679e-01],\n        [ 3.0509e+00, -4.6194e-01],\n        [ 3.9766e+00, -2.4982e-01],\n        [ 3.3120e+00,  1.5320e-01],\n        [ 1.6993e+00, -2.7153e-01],\n        [ 3.2165e+00, -4.2832e-01],\n        [ 2.5400e+00, -2.2985e-01],\n        [ 1.9394e+00, -5.4004e-01],\n        [ 2.4310e+00, -1.9803e+00],\n        [ 3.8291e+00, -1.6693e+00],\n        [ 1.1799e+00,  1.5122e+00],\n        [ 3.2873e+00, -2.4993e-01],\n        [ 3.1653e+00, -6.8119e-01],\n        [ 2.1498e+00,  5.6245e-01],\n        [ 2.2188e+00, -2.1075e+00],\n        [ 1.2005e+00, -3.5600e-01],\n        [ 1.0825e+00, -1.3392e-01],\n        [ 2.0345e+00,  1.0439e+00],\n        [ 4.0149e+00,  6.4808e-01],\n        [ 7.2774e-01, -4.1916e-01],\n        [ 1.7262e+00,  7.9313e-01],\n        [ 8.9609e-01,  3.3160e-02],\n        [ 1.5539e+00, -6.5417e-01],\n        [ 9.1075e-01, -1.2557e+00],\n        [ 2.0244e+00, -5.8445e-02],\n        [ 3.5745e+00, -5.3745e-01],\n        [ 2.4713e+00, -2.7284e-01],\n        [ 2.3272e+00, -2.8308e-01],\n        [ 3.8948e+00, -5.2976e-01],\n        [ 1.0965e+00, -1.8475e+00],\n        [ 1.4599e+00, -9.7348e-01],\n        [ 1.5546e+00, -6.6453e-01],\n        [ 2.4727e+00,  3.2752e-01],\n        [ 2.2525e+00, -1.8136e+00],\n        [ 1.0962e+00, -6.1178e-01],\n        [ 2.6255e+00, -9.6623e-01],\n        [ 2.1760e+00, -6.2941e-01],\n        [ 2.0308e+00, -9.4468e-02],\n        [ 1.0117e+00, -1.0446e+00],\n        [ 2.9149e+00,  1.0759e+00],\n        [ 8.8687e-01,  6.3811e-01],\n        [ 4.2950e+00,  5.0499e-01],\n        [ 1.5434e+00,  1.9503e-01],\n        [ 2.4896e+00,  5.8359e-02],\n        [ 2.2531e+00,  1.4105e+00],\n        [ 2.1013e+00, -4.2672e-01],\n        [ 2.2008e+00,  1.0456e+00],\n        [ 7.1597e-01, -4.1625e-01],\n        [ 2.7776e+00, -9.2978e-02],\n        [ 1.8367e+00,  1.5702e+00],\n        [ 1.3597e+00,  6.1811e-01],\n        [ 1.6451e+00,  4.4317e-01],\n        [ 2.0928e+00,  6.3781e-02],\n        [ 1.3808e+00,  1.2961e-01],\n        [ 4.3493e+00,  5.5402e-01],\n        [ 3.4133e+00, -6.2588e-01],\n        [ 1.9187e+00,  1.5428e-01],\n        [ 3.6824e+00, -1.7491e+00],\n        [ 1.9156e+00,  4.4027e-01],\n        [ 3.6026e+00,  2.4775e-01],\n        [ 1.2495e+00, -7.6757e-02],\n        [ 9.8599e-01, -2.9373e-01],\n        [ 2.1774e+00, -6.3958e-01],\n        [ 2.9113e+00, -9.0830e-02],\n        [ 2.6390e+00,  7.2301e-02],\n        [ 1.5381e+00, -1.1105e+00],\n        [ 7.0527e-01, -1.7167e+00],\n        [ 1.9373e+00,  8.4936e-01],\n        [ 2.8048e+00, -1.8146e+00],\n        [ 3.1518e+00,  8.9033e-01],\n        [ 2.8264e+00, -4.1352e-01],\n        [ 1.4989e+00, -5.7349e-01],\n        [ 1.7965e+00,  1.0074e+00],\n        [ 1.1394e+00,  3.2728e-01],\n        [ 2.6033e+00, -5.9746e-02],\n        [ 6.4050e-01, -3.6759e-01],\n        [ 4.0355e+00,  5.3413e-01],\n        [ 2.1147e+00,  2.7531e-01],\n        [ 2.9727e+00, -2.3493e-01],\n        [ 3.2277e+00,  3.9746e-01],\n        [ 2.3295e+00, -1.4309e-01],\n        [ 3.2691e+00, -5.8294e-01],\n        [ 2.2936e+00, -2.6953e+00],\n        [ 1.5272e+00,  8.0464e-02],\n        [ 4.7447e+00, -5.1496e-01],\n        [ 2.4335e+00, -3.9417e-01],\n        [ 3.0541e+00,  1.6558e+00],\n        [ 2.1259e+00, -2.6180e-01],\n        [ 1.1217e+00,  1.7438e-01],\n        [ 2.8454e+00,  2.7035e-01],\n        [ 3.2138e+00,  1.3040e+00],\n        [ 2.3178e+00,  1.6696e-01],\n        [ 2.5609e+00,  4.9978e-01],\n        [ 2.8944e+00, -2.4825e-01],\n        [ 1.2788e+00, -6.4154e-01],\n        [ 4.3844e+00, -1.7461e-01],\n        [ 2.2480e+00,  4.3266e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[2.6829e-02, 2.9475e-02, 1.0153e-01,  ..., 9.8127e-01, 9.9888e-01,\n         1.0000e+00],\n        [7.9384e-04, 4.7925e-03, 2.4097e-02,  ..., 9.4131e-01, 9.8316e-01,\n         1.0000e+00],\n        [2.5669e-01, 2.5669e-01, 2.5670e-01,  ..., 9.9931e-01, 1.0000e+00,\n         1.0000e+00],\n        ...,\n        [8.1842e-05, 3.7627e-02, 3.7627e-02,  ..., 9.9997e-01, 9.9999e-01,\n         1.0000e+00],\n        [9.5189e-03, 3.0082e-02, 3.0082e-02,  ..., 9.9811e-01, 9.9825e-01,\n         1.0000e+00],\n        [4.5347e-05, 4.5347e-05, 4.6539e-05,  ..., 9.8860e-01, 9.9879e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.2496],\n        [0.4289],\n        [0.9689],\n        [0.4675],\n        [0.4255],\n        [0.6002],\n        [0.8062],\n        [0.9069],\n        [0.2765],\n        [0.8659],\n        [0.3870],\n        [0.6962],\n        [0.7387],\n        [0.9072],\n        [0.5132],\n        [0.3331],\n        [0.6887],\n        [0.6379],\n        [0.8255],\n        [0.2663],\n        [0.5015],\n        [0.4580],\n        [0.4749],\n        [0.7059],\n        [0.7932],\n        [0.8745],\n        [0.8865],\n        [0.1313],\n        [0.7215],\n        [0.9089],\n        [0.6793],\n        [0.2937],\n        [0.7083],\n        [0.4030],\n        [0.6300],\n        [0.7125],\n        [0.8380],\n        [0.4214],\n        [0.9860],\n        [0.5570],\n        [0.1053],\n        [0.5953],\n        [0.1758],\n        [0.1076],\n        [0.9486],\n        [0.8529],\n        [0.1256],\n        [0.1220],\n        [0.6611],\n        [0.0827],\n        [0.1944],\n        [0.3307],\n        [0.0462],\n        [0.2365],\n        [0.3261],\n        [0.1967],\n        [0.7562],\n        [0.6265],\n        [0.2024],\n        [0.4973],\n        [0.3566],\n        [0.3292],\n        [0.3122],\n        [0.9602],\n        [0.7403],\n        [0.6396],\n        [0.8857],\n        [0.4896],\n        [0.8915],\n        [0.1644],\n        [0.4381],\n        [0.3822],\n        [0.3534],\n        [0.9843],\n        [0.1669],\n        [0.5164],\n        [0.7082],\n        [0.0398],\n        [0.7299],\n        [0.1329],\n        [0.9275],\n        [0.7149],\n        [0.7464],\n        [0.1089],\n        [0.7120],\n        [0.9815],\n        [0.1255],\n        [0.7264],\n        [0.8504],\n        [0.6232],\n        [0.1514],\n        [0.3612],\n        [0.2712],\n        [0.8230],\n        [0.3901],\n        [0.5558],\n        [0.3448],\n        [0.5723],\n        [0.2658],\n        [0.9443],\n        [0.3232],\n        [0.8244],\n        [0.6647],\n        [0.7503],\n        [0.4166],\n        [0.5439],\n        [0.9405],\n        [0.7508],\n        [0.4304],\n        [0.2511],\n        [0.9797],\n        [0.9412],\n        [0.9206],\n        [0.3260],\n        [0.7431],\n        [0.9226],\n        [0.0111],\n        [0.7556],\n        [0.0370],\n        [0.4492],\n        [0.5758],\n        [0.8635],\n        [0.5318],\n        [0.8250],\n        [0.5284],\n        [0.9264],\n        [0.4455],\n        [0.9368],\n        [0.6718],\n        [0.2223],\n        [0.6359],\n        [0.5478],\n        [0.8055],\n        [0.4676],\n        [0.5812],\n        [0.2729],\n        [0.8619],\n        [0.9233],\n        [0.1673],\n        [0.6945],\n        [0.9026],\n        [0.8809],\n        [0.7538],\n        [0.6485],\n        [0.3066],\n        [0.0527],\n        [0.0690],\n        [0.9280],\n        [0.2838],\n        [0.6377],\n        [0.1160],\n        [0.6298],\n        [0.7149],\n        [0.7560],\n        [0.6752],\n        [0.7061],\n        [0.3966],\n        [0.0113],\n        [0.9307],\n        [0.2388],\n        [0.9790],\n        [0.8863],\n        [0.1375],\n        [0.4845],\n        [0.3706],\n        [0.6025],\n        [0.2909],\n        [0.1574],\n        [0.7833],\n        [0.9315],\n        [0.2094],\n        [0.3337],\n        [0.8431],\n        [0.7055],\n        [0.8305],\n        [0.0029],\n        [0.3603],\n        [0.3179],\n        [0.3794],\n        [0.3598],\n        [0.6316],\n        [0.4847],\n        [0.7408],\n        [0.3943],\n        [0.3482],\n        [0.7633],\n        [0.4604],\n        [0.2603],\n        [0.4109],\n        [0.0507],\n        [0.1122],\n        [0.7343],\n        [0.4215],\n        [0.3366],\n        [0.7993],\n        [0.4904],\n        [0.8729],\n        [0.0350],\n        [0.7719],\n        [0.2244],\n        [0.1043],\n        [0.1832],\n        [0.3629],\n        [0.4381],\n        [0.2935],\n        [0.8839],\n        [0.6822],\n        [0.0374],\n        [0.7824],\n        [0.3732],\n        [0.2291],\n        [0.7834],\n        [0.7369],\n        [0.7664],\n        [0.2355],\n        [0.4681],\n        [0.3719],\n        [0.4757],\n        [0.5103],\n        [0.7006],\n        [0.7613],\n        [0.9990],\n        [0.5802],\n        [0.7761],\n        [0.8013],\n        [0.8940],\n        [0.2414],\n        [0.0560],\n        [0.7105],\n        [0.7483],\n        [0.6331],\n        [0.8638],\n        [0.6960],\n        [0.3521],\n        [0.6043],\n        [0.5028],\n        [0.1741],\n        [0.1728],\n        [0.3524],\n        [0.4886],\n        [0.9159],\n        [0.6557],\n        [0.9770],\n        [0.8967],\n        [0.8092],\n        [0.5702],\n        [0.7415],\n        [0.5008],\n        [0.9056],\n        [0.6660],\n        [0.7951],\n        [0.2235],\n        [0.2251],\n        [0.9759],\n        [0.6654],\n        [0.3679],\n        [0.8146],\n        [0.8841],\n        [0.3304],\n        [0.6085],\n        [0.1600],\n        [0.4533],\n        [0.5423],\n        [0.9045],\n        [0.0271],\n        [0.1262],\n        [0.8767],\n        [0.8901],\n        [0.2377],\n        [0.5829],\n        [0.2184],\n        [0.1458],\n        [0.8390],\n        [0.1000],\n        [0.2748],\n        [0.2590],\n        [0.0686],\n        [0.4081],\n        [0.7439],\n        [0.1905],\n        [0.7577],\n        [0.9005],\n        [0.7656],\n        [0.4554],\n        [0.8869],\n        [0.7786],\n        [0.8146],\n        [0.2386],\n        [0.5684],\n        [0.2836],\n        [0.2319],\n        [0.5535],\n        [0.2860],\n        [0.7974],\n        [0.6854],\n        [0.1468],\n        [0.8234],\n        [0.7022],\n        [0.2643],\n        [0.8823],\n        [0.3601],\n        [0.7931],\n        [0.5469],\n        [0.4412],\n        [0.1408],\n        [0.5926],\n        [0.2537],\n        [0.4775],\n        [0.5090],\n        [0.8218],\n        [0.4049],\n        [0.7845]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ...,  True, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ...,  True, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 0.7931, -0.1914],\n        [ 1.1115,  2.5199],\n        [ 2.4975,  0.6721],\n        ...,\n        [-1.8712, -1.0377],\n        [ 0.7771,  0.8825],\n        [-0.0026,  1.4443]]) torch.Size([9984, 2])\nsamples tensor([[ 3.7219, -0.8718],\n        [ 1.7594, -1.5174],\n        [ 0.9024, -1.0041],\n        [ 2.4318,  0.8536],\n        [ 2.7030,  0.4548],\n        [ 4.1593, -1.2760],\n        [ 2.1603,  1.6436],\n        [ 1.9454,  0.2851],\n        [ 3.8422,  0.9963],\n        [ 2.4503, -0.4428],\n        [ 3.2329, -0.8647],\n        [ 1.6890,  0.0098],\n        [ 1.4329, -1.1358],\n        [ 2.7681, -1.1507],\n        [ 3.8895,  0.4776],\n        [ 3.6750, -0.6322],\n        [ 4.1030,  0.2334],\n        [ 1.5171, -1.4751],\n        [ 2.3089, -0.9170],\n        [-0.9025, -1.2112],\n        [ 3.9311, -0.8433],\n        [ 2.3291,  1.2620],\n        [ 2.3491, -1.5256],\n        [ 3.7403,  0.1578],\n        [ 2.4108, -0.1063],\n        [ 1.1198, -0.7902],\n        [ 2.7261, -1.4860],\n        [ 2.5132,  0.5642],\n        [ 1.2514, -1.5749],\n        [ 4.9267, -0.7367],\n        [ 2.6687, -0.2667],\n        [ 2.2671,  1.4873],\n        [ 2.9481, -0.3052],\n        [ 2.9033, -0.2122],\n        [ 3.2244, -1.1622],\n        [ 2.8872, -0.3923],\n        [ 2.1727,  0.2269],\n        [ 2.1193,  2.0285],\n        [ 1.3869,  1.8442],\n        [ 3.8630,  0.0409],\n        [-0.3359, -1.9507],\n        [ 2.3130, -0.0562],\n        [ 2.9567,  0.7174],\n        [ 1.9450, -0.6008],\n        [ 1.6857, -0.3490],\n        [ 4.6055, -1.9888],\n        [ 3.3060,  1.2251],\n        [ 2.2498, -0.3167],\n        [ 2.9040,  0.2238],\n        [ 3.4573,  0.3350],\n        [ 2.0521,  0.2612],\n        [ 2.2633,  0.0596],\n        [ 0.9692,  0.3288],\n        [ 2.5257, -0.1593],\n        [ 1.9859, -1.4289],\n        [ 3.4550,  0.9667],\n        [ 4.2066, -0.0051],\n        [ 2.2838, -0.3185],\n        [ 3.8132,  1.7676],\n        [ 3.9099, -1.2411],\n        [ 2.3617,  1.2281],\n        [ 2.3268,  0.7352],\n        [ 2.3609,  0.6926],\n        [ 3.2874, -0.7025],\n        [ 2.7595, -0.2918],\n        [ 2.9108, -0.8662],\n        [ 3.2390, -1.7805],\n        [ 2.9423, -0.0919],\n        [ 1.3469,  0.0138],\n        [ 2.0791, -0.4785],\n        [ 1.7684, -0.6041],\n        [ 3.1432,  0.7155],\n        [ 1.7353,  0.5844],\n        [ 3.0153,  0.4032],\n        [ 3.2366,  0.6122],\n        [ 3.1312,  1.9567],\n        [ 0.9987,  1.5176],\n        [ 1.2687,  0.4716],\n        [ 1.6982, -2.0431],\n        [ 1.0169, -0.1961],\n        [ 2.0362,  1.0877],\n        [ 3.3772,  1.2262],\n        [ 4.0006, -0.9955],\n        [ 2.9062, -2.2851],\n        [ 1.5013, -0.9359],\n        [ 1.7682,  1.4111],\n        [ 2.0642, -0.0096],\n        [ 4.2362,  0.1696],\n        [ 2.9752,  1.1502],\n        [ 3.8934, -0.5823],\n        [ 1.5166, -1.5097],\n        [ 3.1628,  1.8390],\n        [ 3.8149, -1.3072],\n        [ 3.1986,  0.0370],\n        [ 2.3991, -2.5353],\n        [ 4.0160, -1.3619],\n        [ 1.5606, -0.1640],\n        [ 2.2427, -0.1873],\n        [ 2.1331,  0.6669],\n        [ 3.1646,  1.2692],\n        [ 2.0215, -0.1585],\n        [ 2.4805, -0.3463],\n        [ 0.7831, -0.3169],\n        [ 1.1888, -1.0257],\n        [ 1.8790, -0.1135],\n        [ 2.3402, -1.0601],\n        [ 2.7871,  0.0176],\n        [ 2.3743,  0.6987],\n        [ 1.5524, -0.5534],\n        [ 2.5481, -0.1224],\n        [ 1.5871,  0.8857],\n        [ 2.0134,  1.2132],\n        [ 1.9124, -1.4195],\n        [ 3.5845, -0.1613],\n        [ 1.9934, -0.6097],\n        [ 2.5198,  1.0840],\n        [ 3.2617,  1.9061],\n        [ 2.9160, -0.0606],\n        [ 1.0550, -0.2788],\n        [ 1.6605, -1.5546],\n        [ 2.5536, -1.1923],\n        [ 2.2045,  0.3843],\n        [-0.1830, -1.1350],\n        [ 2.3391,  0.4564],\n        [ 1.5227,  0.1807],\n        [ 1.6079, -0.4995],\n        [ 2.2400, -1.4583],\n        [ 1.4568, -0.9924],\n        [ 3.4138,  0.7997],\n        [ 1.1852,  1.2034],\n        [ 3.9265, -1.0330],\n        [ 2.7975, -1.3669],\n        [ 2.0394,  2.3060],\n        [ 1.6437,  0.7579],\n        [ 1.4861, -1.8314],\n        [ 2.6678,  0.1379],\n        [ 2.4160,  0.1626],\n        [ 4.5770, -2.1256],\n        [ 0.9292, -1.4625],\n        [ 2.0355,  0.2501],\n        [ 1.2459,  0.1651],\n        [ 2.6616, -0.3366],\n        [ 2.1365, -0.4317],\n        [ 2.8960, -1.3652],\n        [ 1.9596,  0.0279],\n        [ 3.6628, -2.9988],\n        [ 4.4581, -0.1889],\n        [ 3.3801,  0.7341],\n        [ 1.9949,  0.0136],\n        [ 3.0836, -0.6010],\n        [ 3.5217,  0.0500],\n        [ 1.8072, -1.7200],\n        [ 1.4876, -0.0578],\n        [ 2.0611,  1.0858],\n        [ 1.6495,  0.7641],\n        [ 2.1323, -1.3029],\n        [ 2.2922,  1.0011],\n        [ 3.3525,  1.9411],\n        [ 1.2743, -0.5343],\n        [ 2.6742, -0.5663],\n        [ 0.4782, -0.9788],\n        [ 4.3751, -0.6883],\n        [ 1.7807,  0.5539],\n        [ 2.1547, -0.3216],\n        [ 2.4912, -1.3705],\n        [ 1.7956, -0.4432],\n        [ 1.0584, -0.3703],\n        [ 2.0536,  0.3366],\n        [ 3.5162,  1.5359],\n        [ 0.7568, -0.0578],\n        [ 2.6179,  0.1614],\n        [ 0.6809, -0.3003],\n        [ 3.8995, -0.2742],\n        [ 2.2130, -0.6041],\n        [ 4.4298, -2.5710],\n        [ 1.1672,  0.6543],\n        [ 4.3836, -0.6713],\n        [ 0.6046, -0.6790],\n        [ 1.0840,  0.7407],\n        [ 2.1373, -2.1166],\n        [ 2.8618, -0.3523],\n        [ 3.0540, -0.0064],\n        [ 2.4211,  0.1138],\n        [ 2.9914, -1.3965],\n        [ 1.5560,  0.2222],\n        [ 3.6383,  1.1408],\n        [-0.4623, -0.5834],\n        [ 3.0360, -1.2194],\n        [ 2.1847, -0.2319],\n        [ 4.2992, -1.6789],\n        [ 3.1927, -0.7081],\n        [ 1.9288,  0.6344],\n        [ 1.3634, -1.8396],\n        [ 3.8250,  1.3725],\n        [ 2.8182,  2.1420],\n        [ 4.3607,  1.4372],\n        [ 1.6832,  0.4162],\n        [ 0.1397, -0.0471],\n        [ 1.0036,  0.9973],\n        [ 2.3730, -1.3005],\n        [ 3.1379, -1.6159],\n        [ 3.1400, -0.5216],\n        [ 0.9453, -1.6909],\n        [ 3.4804,  2.7577],\n        [ 2.9620, -0.3116],\n        [ 0.8264,  0.5589],\n        [ 2.4671, -0.4569],\n        [ 1.7674,  1.9516],\n        [ 2.6801, -0.7391],\n        [ 2.2838,  0.3075],\n        [ 2.0715, -0.6620],\n        [ 1.5895,  0.1925],\n        [ 2.0192, -0.9832],\n        [ 3.4230,  0.0621],\n        [ 1.5758,  1.4901],\n        [ 1.2831,  0.3042],\n        [ 3.3973, -0.6760],\n        [ 3.4792, -0.8934],\n        [ 2.6764, -0.4576],\n        [ 3.4894,  0.3328],\n        [ 0.8933,  0.6061],\n        [ 3.1256,  0.5452],\n        [ 1.6229, -2.1027],\n        [ 3.6147, -1.7540],\n        [ 2.6696,  0.3410],\n        [ 2.9255,  0.3416],\n        [ 1.5887, -0.2336],\n        [ 3.0923, -1.7866],\n        [ 2.9236, -0.8209],\n        [ 3.0795, -2.4146],\n        [ 2.6430, -0.8184],\n        [ 2.9440,  0.3589],\n        [ 1.5369,  0.1083],\n        [ 3.9066,  0.0710],\n        [ 2.4510,  0.5327],\n        [ 3.1533, -1.7787],\n        [ 3.1403, -0.2525],\n        [ 3.8791,  0.1222],\n        [ 4.7282,  1.8827],\n        [ 2.6041, -0.1864],\n        [ 4.0424, -0.1042],\n        [ 0.7530, -0.3885],\n        [ 3.3612,  2.0269],\n        [ 1.1705,  0.0323],\n        [ 1.3820,  0.6186],\n        [ 2.8125, -0.5431],\n        [ 1.8391,  0.1080],\n        [ 2.2265,  1.9802],\n        [ 1.3091, -0.5141],\n        [ 2.6551, -2.1858],\n        [ 1.6408,  1.1582],\n        [ 2.9807, -0.6466],\n        [ 3.1479,  0.8821],\n        [ 3.5953, -2.1451],\n        [ 3.2349, -0.3113],\n        [ 1.7759,  0.6651],\n        [ 2.7579, -1.0053],\n        [ 3.3833,  0.6322],\n        [ 3.2621, -0.9139],\n        [ 0.8691, -1.2655],\n        [ 1.7296, -1.0685],\n        [ 1.4945,  0.4137],\n        [ 2.6975, -0.5728],\n        [ 2.9779, -0.1558],\n        [ 3.1439, -0.1960],\n        [ 2.5251,  2.1939],\n        [ 0.2797, -1.4822],\n        [ 1.1593, -0.5407],\n        [ 3.1648, -0.7485],\n        [ 1.9555, -1.2416],\n        [ 3.9289,  1.2978],\n        [ 2.6960, -0.6278],\n        [ 1.5125, -0.5406],\n        [ 1.4741,  1.2660],\n        [ 0.3843, -0.4432],\n        [ 0.9652, -0.7011],\n        [ 2.0174,  0.7104],\n        [ 4.1251, -0.7391],\n        [ 3.5097,  1.5516],\n        [ 2.0582,  0.1333],\n        [ 3.5520, -0.3527],\n        [ 1.1641,  0.1343],\n        [ 3.0437, -0.6221],\n        [ 3.0594, -3.3468],\n        [ 2.3403, -0.0707],\n        [ 3.2847, -0.9657],\n        [ 1.6799, -0.8914],\n        [ 2.8318, -0.1817],\n        [ 1.3199, -0.8807],\n        [ 0.2073,  0.1595],\n        [ 0.8407,  0.0514],\n        [ 3.1142, -0.6504],\n        [ 2.7692, -0.2135],\n        [ 2.4761,  0.1160],\n        [ 0.4848, -1.4035],\n        [ 0.8593,  0.2907],\n        [ 1.8925, -0.3551],\n        [ 1.7183,  0.5257],\n        [ 3.6375, -0.4760],\n        [ 2.6761,  1.1950],\n        [ 3.6603, -0.4404],\n        [ 0.0947,  1.1755],\n        [ 1.9453,  1.0125],\n        [ 3.2448, -0.2449],\n        [ 3.0624,  1.5597],\n        [ 1.7797,  0.1977],\n        [ 3.1007, -1.0227],\n        [ 1.5286, -0.9493],\n        [ 1.2473, -0.3294],\n        [ 2.3807, -0.3078],\n        [ 3.6631, -1.8102],\n        [ 2.5860, -1.0657]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[4.3517e-06, 4.4223e-06, 2.3965e-01,  ..., 9.9705e-01, 9.9709e-01,\n         1.0000e+00],\n        [1.6780e-07, 7.4497e-02, 7.8425e-02,  ..., 9.9570e-01, 9.9935e-01,\n         1.0000e+00],\n        [4.9843e-06, 1.5934e-03, 1.2692e-01,  ..., 8.1363e-01, 8.1726e-01,\n         1.0000e+00],\n        ...,\n        [4.5703e-08, 8.1236e-03, 3.3101e-01,  ..., 9.9895e-01, 1.0000e+00,\n         1.0000e+00],\n        [4.4434e-02, 5.9721e-02, 5.9722e-02,  ..., 8.5534e-01, 9.6069e-01,\n         1.0000e+00],\n        [1.1209e-04, 1.4046e-04, 2.7648e-03,  ..., 8.9039e-01, 9.0414e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[2.1037e-01],\n        [4.8854e-01],\n        [4.4504e-01],\n        [7.1581e-02],\n        [5.8959e-01],\n        [3.9871e-01],\n        [8.5341e-01],\n        [8.8302e-01],\n        [8.3237e-01],\n        [8.3777e-01],\n        [8.2625e-01],\n        [3.3264e-01],\n        [7.4715e-01],\n        [5.1705e-01],\n        [6.1588e-01],\n        [9.1550e-01],\n        [1.3513e-01],\n        [6.1782e-01],\n        [1.8897e-02],\n        [1.6533e-02],\n        [8.9690e-01],\n        [2.2649e-01],\n        [7.3524e-02],\n        [7.6176e-01],\n        [2.7376e-01],\n        [7.6249e-01],\n        [2.3532e-01],\n        [7.0580e-01],\n        [1.1907e-01],\n        [5.6403e-02],\n        [2.3343e-01],\n        [7.2509e-02],\n        [3.5645e-01],\n        [4.8117e-01],\n        [9.5673e-01],\n        [7.5768e-01],\n        [4.2933e-01],\n        [4.2643e-01],\n        [2.2119e-01],\n        [3.2539e-01],\n        [4.8334e-01],\n        [9.3229e-01],\n        [9.5642e-01],\n        [1.1203e-01],\n        [7.5093e-01],\n        [1.8174e-01],\n        [7.7094e-01],\n        [1.2379e-01],\n        [6.8595e-01],\n        [4.7776e-02],\n        [3.0353e-01],\n        [4.5487e-01],\n        [9.4829e-01],\n        [9.3815e-01],\n        [3.9943e-01],\n        [4.8051e-02],\n        [3.7880e-01],\n        [6.1655e-01],\n        [7.4880e-01],\n        [3.6849e-01],\n        [3.3861e-04],\n        [5.9983e-01],\n        [9.1660e-01],\n        [8.2217e-01],\n        [6.9957e-01],\n        [8.0093e-01],\n        [1.9628e-01],\n        [2.7102e-01],\n        [5.1448e-01],\n        [6.4342e-02],\n        [8.4603e-01],\n        [2.9947e-01],\n        [4.1205e-01],\n        [1.0280e-01],\n        [4.0771e-02],\n        [4.9100e-01],\n        [3.2951e-01],\n        [9.5586e-01],\n        [3.7909e-01],\n        [7.7966e-02],\n        [1.5669e-01],\n        [5.3861e-01],\n        [1.8582e-01],\n        [7.5168e-01],\n        [7.1459e-01],\n        [2.3138e-01],\n        [8.9844e-01],\n        [9.7256e-01],\n        [9.4563e-01],\n        [5.3358e-03],\n        [6.5132e-01],\n        [4.0805e-01],\n        [9.2408e-01],\n        [3.1659e-01],\n        [6.4864e-01],\n        [5.8018e-01],\n        [7.8632e-01],\n        [4.5453e-01],\n        [9.2273e-01],\n        [4.2277e-01],\n        [1.1947e-01],\n        [3.7762e-01],\n        [3.9350e-01],\n        [6.3092e-02],\n        [4.7504e-01],\n        [6.0460e-01],\n        [3.7535e-01],\n        [4.8729e-01],\n        [5.2638e-01],\n        [4.0154e-01],\n        [7.7971e-01],\n        [3.9719e-01],\n        [3.6690e-01],\n        [6.9017e-01],\n        [1.5678e-01],\n        [5.8848e-01],\n        [7.5023e-01],\n        [3.8071e-01],\n        [9.1890e-01],\n        [6.2945e-01],\n        [8.6131e-01],\n        [5.4474e-01],\n        [3.0162e-01],\n        [3.0632e-01],\n        [4.4039e-01],\n        [9.9176e-01],\n        [8.0190e-01],\n        [8.0688e-01],\n        [5.0358e-01],\n        [9.3834e-02],\n        [7.6853e-01],\n        [5.1049e-01],\n        [7.8085e-01],\n        [9.7044e-01],\n        [4.2326e-01],\n        [9.4551e-01],\n        [2.2519e-01],\n        [4.5035e-01],\n        [9.4036e-01],\n        [1.2902e-01],\n        [3.1161e-01],\n        [4.1557e-01],\n        [6.5790e-02],\n        [2.9341e-01],\n        [4.4749e-01],\n        [8.3459e-01],\n        [1.7991e-02],\n        [8.5244e-01],\n        [8.5527e-01],\n        [3.6592e-01],\n        [7.4595e-01],\n        [5.5871e-02],\n        [3.5273e-01],\n        [1.3846e-01],\n        [7.3337e-01],\n        [1.7460e-01],\n        [5.0715e-01],\n        [8.1707e-01],\n        [3.1265e-01],\n        [9.1762e-01],\n        [1.7263e-01],\n        [9.7916e-02],\n        [6.7575e-01],\n        [3.9367e-02],\n        [3.5627e-01],\n        [9.0088e-01],\n        [6.3131e-01],\n        [2.9765e-01],\n        [5.0700e-01],\n        [5.3722e-01],\n        [4.5097e-01],\n        [8.7448e-01],\n        [7.7629e-01],\n        [1.3425e-01],\n        [5.5863e-01],\n        [1.8039e-01],\n        [7.6831e-01],\n        [9.8027e-02],\n        [1.2482e-01],\n        [8.6248e-01],\n        [5.8548e-01],\n        [5.8874e-01],\n        [9.9348e-01],\n        [8.0047e-02],\n        [9.1270e-01],\n        [2.8080e-01],\n        [7.0776e-01],\n        [4.6631e-01],\n        [1.1233e-02],\n        [8.0687e-01],\n        [2.4108e-01],\n        [4.1544e-01],\n        [9.4475e-01],\n        [5.3446e-02],\n        [9.0789e-01],\n        [2.1428e-01],\n        [3.2541e-01],\n        [3.3688e-01],\n        [2.4120e-01],\n        [7.9610e-01],\n        [9.9777e-01],\n        [5.7759e-01],\n        [9.1718e-01],\n        [5.5086e-01],\n        [3.9482e-01],\n        [1.2890e-02],\n        [8.6824e-01],\n        [9.8210e-02],\n        [9.0589e-01],\n        [3.3627e-01],\n        [4.0868e-01],\n        [1.0497e-01],\n        [1.9461e-01],\n        [8.7000e-02],\n        [6.5221e-01],\n        [4.5843e-02],\n        [8.1883e-01],\n        [5.7446e-01],\n        [7.3601e-01],\n        [5.9427e-01],\n        [9.7043e-01],\n        [6.4272e-01],\n        [2.5351e-01],\n        [2.4622e-01],\n        [9.9018e-01],\n        [9.6498e-01],\n        [6.7486e-01],\n        [9.0827e-01],\n        [6.8858e-01],\n        [6.6951e-01],\n        [3.4685e-01],\n        [7.1775e-01],\n        [1.9525e-01],\n        [7.6256e-01],\n        [3.1869e-01],\n        [1.6449e-01],\n        [1.2471e-01],\n        [1.8396e-01],\n        [5.2263e-01],\n        [6.9715e-01],\n        [2.6097e-03],\n        [4.0688e-01],\n        [1.1804e-01],\n        [2.8683e-01],\n        [4.0999e-01],\n        [7.7148e-01],\n        [2.0320e-01],\n        [9.3724e-01],\n        [2.6624e-01],\n        [3.9884e-01],\n        [6.5505e-01],\n        [7.4481e-02],\n        [4.6693e-01],\n        [8.8670e-01],\n        [7.5587e-01],\n        [4.9412e-01],\n        [6.9958e-01],\n        [2.2323e-02],\n        [4.1224e-01],\n        [8.0303e-01],\n        [5.6828e-01],\n        [3.2912e-01],\n        [6.1094e-01],\n        [4.5778e-01],\n        [2.2615e-01],\n        [4.4012e-01],\n        [2.3109e-01],\n        [9.0056e-01],\n        [2.4104e-01],\n        [9.7923e-01],\n        [4.4216e-01],\n        [9.0755e-01],\n        [9.8490e-01],\n        [2.0070e-01],\n        [6.3072e-01],\n        [7.6129e-01],\n        [3.9394e-02],\n        [3.6697e-01],\n        [3.3485e-01],\n        [7.0514e-01],\n        [7.7112e-01],\n        [1.4571e-01],\n        [4.1619e-01],\n        [2.4014e-01],\n        [2.5525e-01],\n        [2.6464e-01],\n        [7.7031e-01],\n        [7.9947e-01],\n        [9.8569e-01],\n        [4.5948e-01],\n        [8.3280e-02],\n        [9.6963e-01],\n        [1.3469e-01],\n        [8.2635e-02],\n        [3.2166e-01],\n        [4.9774e-01],\n        [9.0870e-01],\n        [7.4595e-01],\n        [4.1036e-01],\n        [4.6415e-02],\n        [1.1195e-01],\n        [6.6350e-02],\n        [2.1883e-01],\n        [9.2840e-01],\n        [9.1674e-01],\n        [7.1054e-02],\n        [7.2479e-01],\n        [3.7579e-01],\n        [7.6348e-01],\n        [8.4930e-01],\n        [6.8681e-01],\n        [7.8554e-01]]) torch.Size([312, 1])\nmask tensor([[False, False,  True,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-2.0856,  0.4660],\n        [-3.1418, -0.6112],\n        [ 1.7676, -0.4125],\n        ...,\n        [ 0.5638,  1.8056],\n        [ 0.9554,  0.4793],\n        [ 1.4010, -0.4634]]) torch.Size([9984, 2])\nsamples tensor([[ 1.7676e+00, -4.1251e-01],\n        [ 2.5938e+00, -1.0952e+00],\n        [ 2.0026e+00, -1.5548e+00],\n        [ 2.8278e+00, -7.6848e-01],\n        [ 3.7389e+00, -2.2837e-01],\n        [ 3.2131e+00, -1.1453e+00],\n        [ 1.7190e+00,  5.4816e-01],\n        [ 1.4591e+00,  1.4404e+00],\n        [ 2.3463e+00,  3.0761e-01],\n        [-7.3575e-01,  3.0214e-01],\n        [ 2.4887e+00,  3.9944e-01],\n        [ 1.7453e+00, -2.3816e-01],\n        [ 2.3057e+00, -6.8954e-01],\n        [ 2.1761e+00, -1.5035e+00],\n        [ 4.4022e+00,  1.0799e+00],\n        [ 1.6688e+00,  9.1043e-02],\n        [ 2.0426e+00, -1.2529e-01],\n        [ 1.2847e+00,  3.5041e-01],\n        [ 9.2053e-01, -2.6910e-01],\n        [ 1.5874e+00,  5.4099e-01],\n        [ 2.3394e+00, -2.2912e-01],\n        [ 3.7073e+00, -7.9178e-01],\n        [ 2.2028e+00,  1.8559e+00],\n        [ 1.1718e+00,  2.3100e-01],\n        [ 2.7729e+00,  9.7828e-01],\n        [ 1.3083e+00, -8.4443e-01],\n        [ 2.8853e+00, -2.1196e-02],\n        [ 3.0062e+00, -1.3166e+00],\n        [ 1.0303e+00,  4.9189e-01],\n        [ 2.9839e+00, -1.1216e+00],\n        [ 3.4211e+00, -1.7135e+00],\n        [ 1.4620e+00,  4.1472e-01],\n        [ 1.2172e+00, -1.1040e+00],\n        [ 4.2240e+00, -3.8732e-01],\n        [ 2.3505e+00, -4.1669e-01],\n        [ 3.1280e+00,  1.4215e+00],\n        [-3.7267e-01,  1.1438e+00],\n        [ 2.2530e+00,  5.2940e-01],\n        [ 3.9741e+00, -5.5078e-01],\n        [ 2.9051e+00,  3.3330e-01],\n        [ 2.3256e+00,  1.6450e+00],\n        [ 2.1658e+00, -1.0183e+00],\n        [ 1.9232e+00,  1.3325e+00],\n        [ 3.0928e+00, -1.1885e+00],\n        [ 2.7404e+00,  2.2901e+00],\n        [ 4.5315e+00, -1.8403e+00],\n        [-2.9713e-01,  8.3846e-01],\n        [ 2.8749e+00, -1.6902e+00],\n        [ 3.5342e+00,  1.5920e+00],\n        [ 3.5463e+00,  1.1560e-01],\n        [ 1.3455e+00,  2.5929e-02],\n        [ 3.0341e+00, -1.5303e+00],\n        [ 1.6552e+00,  3.0904e-02],\n        [ 2.6062e+00,  7.0470e-01],\n        [ 1.6564e+00, -2.7676e+00],\n        [ 2.5879e+00, -9.8246e-01],\n        [ 2.8517e+00, -1.3628e+00],\n        [ 4.0663e-01, -1.7387e-01],\n        [ 2.0891e+00,  6.9317e-01],\n        [ 1.8859e+00,  3.9791e-02],\n        [ 1.6461e+00,  1.7486e+00],\n        [ 2.2105e+00,  4.0216e-01],\n        [ 2.9295e+00,  4.2004e-01],\n        [ 2.8251e+00, -2.3053e-01],\n        [-4.8669e-01,  4.0887e-01],\n        [ 2.9060e+00, -2.0693e+00],\n        [ 3.2290e+00,  1.0398e+00],\n        [ 2.1610e+00,  4.4260e-01],\n        [ 3.2264e+00, -3.0186e-01],\n        [ 1.0451e+00, -7.2903e-01],\n        [ 3.1906e+00,  9.1579e-01],\n        [ 2.4522e+00, -7.4163e-01],\n        [ 3.4515e+00, -5.4261e-01],\n        [ 3.2381e+00, -1.6064e-01],\n        [ 3.9998e-01,  6.9183e-02],\n        [ 4.8035e+00,  1.5122e+00],\n        [ 3.4141e+00, -6.1158e-02],\n        [ 2.3443e+00, -9.1713e-02],\n        [ 2.2315e+00, -1.3386e+00],\n        [ 1.4393e+00, -1.3596e+00],\n        [ 2.3058e+00, -3.9166e-01],\n        [ 3.7262e+00, -3.4545e-01],\n        [ 1.3748e+00, -8.7994e-01],\n        [ 2.7557e+00,  2.0743e-01],\n        [ 1.0865e+00,  1.9935e-01],\n        [ 3.7034e+00,  3.6216e-01],\n        [ 1.9101e+00,  1.7899e-01],\n        [ 2.6644e+00,  9.7736e-01],\n        [ 7.3567e-01, -7.1720e-01],\n        [ 6.6121e-01,  2.8869e-01],\n        [ 3.8975e+00,  1.4375e-02],\n        [ 2.4139e+00,  1.3863e+00],\n        [ 1.6108e+00,  6.5598e-01],\n        [ 3.3110e+00, -1.1902e+00],\n        [ 3.8915e+00,  3.5566e-01],\n        [ 3.6388e+00, -7.8590e-03],\n        [ 2.2355e+00, -1.8865e+00],\n        [ 1.0951e+00, -1.7488e+00],\n        [ 1.5440e+00,  4.2326e-01],\n        [ 3.3070e+00, -5.2392e-01],\n        [ 3.3434e+00,  1.3344e+00],\n        [ 2.8905e+00, -9.9079e-01],\n        [ 2.4444e+00,  2.3501e-01],\n        [ 2.0770e+00,  4.1993e-01],\n        [ 3.2522e+00, -3.2167e-01],\n        [ 3.5257e+00,  3.8851e-01],\n        [ 1.3422e+00, -1.0184e+00],\n        [ 3.1081e+00,  1.3343e-02],\n        [ 1.0673e+00,  1.7021e+00],\n        [ 2.3094e+00,  1.1506e-01],\n        [ 2.1362e+00, -7.2751e-02],\n        [ 2.0304e+00, -1.6588e-01],\n        [ 2.2371e+00, -2.0707e-01],\n        [ 9.8623e-01, -1.6243e+00],\n        [ 2.5385e+00,  7.4058e-01],\n        [ 3.5370e+00, -1.4965e+00],\n        [ 2.6533e+00,  1.1914e+00],\n        [ 2.4081e+00,  3.8281e-01],\n        [ 2.4481e+00,  1.0356e+00],\n        [ 2.2239e+00, -8.3720e-01],\n        [ 1.5595e+00,  1.7746e-01],\n        [ 2.8108e+00, -1.8106e+00],\n        [ 2.2894e+00, -2.1657e+00],\n        [ 1.9393e+00,  3.8117e-01],\n        [ 2.7111e+00, -1.7914e+00],\n        [ 2.0513e+00,  2.7951e-01],\n        [ 3.0616e-01, -3.4300e-01],\n        [ 1.7245e+00,  2.8350e-01],\n        [ 4.2029e+00, -1.5653e+00],\n        [ 2.7235e+00,  1.0800e+00],\n        [ 2.0819e+00, -1.6674e+00],\n        [ 1.9206e+00, -1.0463e+00],\n        [ 1.2628e+00,  6.0530e-01],\n        [ 1.3814e+00, -4.0041e-01],\n        [ 2.4897e+00, -7.2267e-01],\n        [ 1.0150e+00, -4.9370e-01],\n        [ 2.6954e+00,  1.3649e+00],\n        [ 3.7775e+00, -9.3651e-01],\n        [ 9.3042e-01,  3.1633e-01],\n        [ 1.7392e+00, -3.0390e-01],\n        [ 1.6443e+00,  1.8412e+00],\n        [ 3.9419e+00,  3.0990e-01],\n        [ 1.6279e+00,  5.9072e-01],\n        [ 9.5206e-01, -1.1740e+00],\n        [ 3.1288e+00,  1.5643e+00],\n        [ 4.3021e+00, -2.9873e+00],\n        [ 2.3939e+00, -3.3270e-01],\n        [ 2.3976e+00, -9.7165e-02],\n        [ 1.0257e+00, -8.1170e-02],\n        [ 3.7767e+00, -3.8825e-01],\n        [ 2.7247e+00, -7.0204e-01],\n        [ 1.9406e+00,  2.2178e-01],\n        [ 2.6049e+00, -8.3770e-01],\n        [ 2.1722e+00, -2.7254e-01],\n        [ 1.8344e+00,  1.1062e+00],\n        [ 1.1861e-01, -1.2681e+00],\n        [ 3.5651e+00, -1.9854e-01],\n        [ 3.4023e+00,  1.7014e+00],\n        [ 2.7379e+00, -9.5601e-01],\n        [ 2.5846e+00, -1.8082e+00],\n        [ 2.7615e+00,  1.1168e-01],\n        [ 1.2159e+00, -2.3159e-02],\n        [ 1.8090e+00,  9.4059e-01],\n        [ 1.1594e+00,  1.8024e-01],\n        [ 3.2490e+00,  7.7297e-01],\n        [ 3.2020e+00,  4.2882e-01],\n        [ 2.0403e+00, -1.4960e-01],\n        [ 1.7479e+00, -1.1402e-01],\n        [ 2.8088e+00,  1.7890e+00],\n        [ 3.3078e+00, -5.9760e-01],\n        [ 1.8359e+00, -5.0887e-01],\n        [ 8.8215e-01, -1.0736e+00],\n        [ 2.6752e+00,  1.2430e-01],\n        [ 1.9904e+00,  1.1069e-01],\n        [ 3.7872e+00, -2.3820e+00],\n        [ 2.4760e+00, -1.8380e+00],\n        [ 1.4592e+00,  2.2391e-01],\n        [ 4.0948e+00,  7.1280e-01],\n        [ 1.8776e+00,  4.2448e-01],\n        [ 4.2041e+00, -1.6806e+00],\n        [ 4.1409e+00,  7.6078e-01],\n        [ 3.9677e+00, -1.5119e-01],\n        [ 7.5223e-01, -1.4547e-01],\n        [ 1.8095e+00,  4.5538e-02],\n        [ 2.9026e+00,  1.0284e+00],\n        [ 2.7449e+00,  1.2002e+00],\n        [ 2.0182e+00, -1.2883e+00],\n        [ 3.3603e+00, -1.3927e+00],\n        [ 1.6647e-01, -2.7325e-01],\n        [ 3.5993e+00,  7.0837e-01],\n        [ 1.6826e+00, -6.2602e-01],\n        [ 2.0766e+00, -1.8822e+00],\n        [ 1.1833e+00,  8.1610e-01],\n        [ 2.8237e+00,  1.9755e-01],\n        [-5.6852e-01, -4.6325e-01],\n        [ 1.4925e+00, -1.0806e+00],\n        [ 3.3276e+00,  3.5245e-01],\n        [ 2.8026e+00,  2.4920e-01],\n        [ 1.9590e+00,  3.2402e-01],\n        [ 2.7718e+00,  7.9773e-01],\n        [ 1.8239e+00,  9.6325e-01],\n        [ 1.1120e+00, -1.4783e+00],\n        [ 3.2677e+00,  4.1759e-01],\n        [ 2.3482e+00,  3.8567e-01],\n        [ 1.7898e+00, -5.1259e-01],\n        [ 2.1392e+00,  4.5747e-01],\n        [ 3.9744e+00,  1.5796e+00],\n        [ 2.6226e+00, -7.7542e-01],\n        [ 2.6849e+00,  1.5246e-01],\n        [ 2.2120e+00,  2.4019e-01],\n        [ 3.0892e+00, -2.6263e-01],\n        [ 3.3071e+00,  1.3262e+00],\n        [ 6.8177e-01,  1.1874e-01],\n        [ 3.4266e+00,  2.3234e+00],\n        [ 2.0144e+00, -1.2577e+00],\n        [ 2.6501e-01,  5.8203e-01],\n        [ 1.9638e+00, -5.0931e-01],\n        [ 3.0678e+00, -1.7381e+00],\n        [ 3.3150e+00,  2.2236e-01],\n        [ 3.7158e+00, -2.8314e-01],\n        [ 1.0451e+00,  4.5177e-01],\n        [ 2.4421e+00,  1.0403e+00],\n        [ 9.6264e-01, -1.0578e+00],\n        [ 3.6555e+00, -4.0401e-01],\n        [ 7.9030e-01,  9.4187e-01],\n        [ 3.2204e+00,  1.3480e+00],\n        [ 2.8151e+00, -1.2582e-01],\n        [ 1.5386e+00,  5.2453e-01],\n        [ 2.9030e+00,  5.4366e-01],\n        [ 2.8480e+00,  9.4363e-01],\n        [ 2.7235e+00,  1.2166e+00],\n        [ 2.1633e+00, -3.5691e-01],\n        [ 2.2607e+00, -1.2586e-01],\n        [ 1.8923e+00, -3.3203e-01],\n        [ 2.7605e+00, -4.7940e-01],\n        [ 1.8894e+00,  9.5265e-02],\n        [ 3.2086e+00,  3.8209e-01],\n        [ 3.6998e+00, -1.8715e+00],\n        [ 2.2448e+00, -7.4437e-01],\n        [ 2.5077e+00,  1.1412e+00],\n        [ 2.9339e+00,  8.0682e-02],\n        [ 2.9589e+00,  3.6497e-03],\n        [ 4.2392e+00, -7.4090e-01],\n        [ 2.3694e+00, -7.2869e-01],\n        [ 2.3371e+00, -1.9640e+00],\n        [ 1.3909e+00, -4.3752e-01],\n        [ 3.8230e+00, -1.8761e+00],\n        [ 2.3748e+00,  5.7548e-01],\n        [ 1.0710e+00,  1.7504e+00],\n        [ 2.2333e+00, -2.4027e+00],\n        [ 2.4576e+00, -6.3757e-02],\n        [ 3.5163e+00, -1.2092e+00],\n        [ 2.0781e+00, -2.5204e+00],\n        [ 2.9398e+00, -6.5961e-01],\n        [ 1.4907e+00,  4.9196e-03],\n        [ 1.8282e+00, -8.8925e-01],\n        [ 3.1241e+00, -6.8857e-01],\n        [ 3.7101e+00, -7.4022e-01],\n        [ 2.4682e+00, -5.8698e-02],\n        [ 1.3557e+00, -8.7940e-01],\n        [ 1.1957e+00,  1.5220e-01],\n        [ 2.5985e+00, -1.8752e+00],\n        [ 2.7635e+00, -1.5724e+00],\n        [ 2.4310e+00, -3.3787e-01],\n        [ 1.3264e+00,  6.3022e-01],\n        [ 4.4399e+00,  2.0249e-01],\n        [ 4.0961e+00, -6.9291e-01],\n        [ 1.3489e+00, -5.6259e-02],\n        [ 1.8480e+00,  7.8112e-01],\n        [ 3.2531e+00,  1.2177e-01],\n        [ 1.3027e+00, -1.8670e+00],\n        [ 2.6204e+00, -1.1875e-01],\n        [ 1.9151e+00, -3.2138e-01],\n        [ 1.8393e+00,  2.8444e-01],\n        [ 2.5535e+00, -7.8824e-01],\n        [ 3.4511e+00, -1.3167e+00],\n        [ 1.4395e+00,  5.9375e-01],\n        [ 2.3886e+00, -1.0381e+00],\n        [ 2.1961e+00, -8.4131e-01],\n        [ 2.8538e+00, -7.4549e-01],\n        [ 2.2938e+00, -6.0476e-01],\n        [ 2.2303e+00, -1.6669e+00],\n        [ 1.5670e+00, -8.6134e-01],\n        [ 2.3380e+00,  2.1553e-01],\n        [ 2.5943e+00,  2.9833e-01],\n        [ 2.7669e+00, -7.9258e-01],\n        [ 3.9142e+00,  2.0127e+00],\n        [ 3.0623e+00, -1.3814e+00],\n        [ 3.2633e+00,  1.6573e+00],\n        [ 3.1006e+00,  4.5774e-01],\n        [ 9.9503e-01, -2.8637e-01],\n        [ 4.1317e+00, -9.3386e-01],\n        [ 3.5793e+00, -3.3736e-01],\n        [ 6.4580e-01, -7.2377e-01],\n        [ 1.9598e+00, -6.8549e-01],\n        [ 1.2630e+00, -1.1451e+00],\n        [ 3.0289e+00, -1.2295e+00],\n        [ 2.8975e+00, -1.3314e+00],\n        [ 2.6146e+00, -1.4047e+00],\n        [ 2.5372e+00,  9.2588e-01],\n        [ 2.4079e+00,  2.2401e-01],\n        [ 3.9022e+00, -1.6361e+00],\n        [ 3.3869e+00, -1.1193e+00],\n        [ 1.4410e+00,  3.3962e-02],\n        [ 2.5664e+00,  2.3202e-01],\n        [ 1.6592e+00, -5.1198e-02],\n        [ 9.6390e-01,  7.3182e-01],\n        [ 5.6925e-01, -5.7454e-01],\n        [ 4.2066e+00,  1.1461e-01],\n        [ 3.4055e+00,  1.1065e+00],\n        [ 3.1725e+00,  5.1258e-01],\n        [ 2.5813e+00, -1.9140e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[8.4637e-02, 1.3141e-01, 1.3141e-01,  ..., 1.0000e+00, 1.0000e+00,\n         1.0000e+00],\n        [5.4851e-02, 5.7199e-02, 5.7199e-02,  ..., 9.9748e-01, 9.9970e-01,\n         1.0000e+00],\n        [4.9304e-04, 5.0665e-04, 5.5242e-03,  ..., 6.6598e-01, 7.2646e-01,\n         1.0000e+00],\n        ...,\n        [6.7540e-04, 6.8146e-04, 3.8608e-02,  ..., 9.6814e-01, 9.9943e-01,\n         1.0000e+00],\n        [1.0673e-01, 1.1138e-01, 1.1138e-01,  ..., 9.9807e-01, 9.9844e-01,\n         1.0000e+00],\n        [3.0962e-05, 2.5729e-03, 2.5758e-03,  ..., 9.7827e-01, 9.7956e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.3381],\n        [0.1599],\n        [0.2598],\n        [0.0865],\n        [0.8470],\n        [0.2724],\n        [0.9985],\n        [0.0105],\n        [0.7858],\n        [0.0711],\n        [0.2804],\n        [0.9489],\n        [0.9893],\n        [0.5091],\n        [0.7275],\n        [0.4595],\n        [0.1491],\n        [0.4142],\n        [0.4651],\n        [0.8418],\n        [0.6773],\n        [0.2884],\n        [0.8434],\n        [0.5582],\n        [0.1992],\n        [0.8842],\n        [0.9178],\n        [0.5503],\n        [0.1544],\n        [0.6855],\n        [0.3810],\n        [0.4386],\n        [0.9708],\n        [0.6938],\n        [0.0932],\n        [0.6538],\n        [0.8657],\n        [0.7843],\n        [0.0898],\n        [0.6591],\n        [0.1652],\n        [0.0134],\n        [0.4579],\n        [0.3445],\n        [0.3872],\n        [0.0392],\n        [0.9443],\n        [0.9387],\n        [0.3418],\n        [0.9494],\n        [0.8503],\n        [0.5080],\n        [0.9388],\n        [0.8968],\n        [0.4015],\n        [0.8718],\n        [0.9001],\n        [0.4860],\n        [0.7896],\n        [0.4112],\n        [0.8219],\n        [0.8467],\n        [0.8084],\n        [0.3810],\n        [0.7615],\n        [0.8081],\n        [0.6041],\n        [0.8375],\n        [0.1001],\n        [0.6860],\n        [0.7276],\n        [0.6934],\n        [0.1707],\n        [0.9116],\n        [0.8910],\n        [0.6966],\n        [0.7026],\n        [0.1576],\n        [0.0226],\n        [0.7424],\n        [0.3396],\n        [0.2089],\n        [0.8604],\n        [0.7970],\n        [0.0958],\n        [0.7405],\n        [0.7911],\n        [0.1529],\n        [0.5383],\n        [0.7093],\n        [0.1838],\n        [0.6000],\n        [0.8980],\n        [0.2806],\n        [0.9596],\n        [0.3384],\n        [0.2118],\n        [0.5982],\n        [0.2940],\n        [0.0658],\n        [0.4094],\n        [0.7848],\n        [0.0596],\n        [0.3559],\n        [0.7072],\n        [0.3777],\n        [0.5357],\n        [0.2535],\n        [0.6420],\n        [0.7601],\n        [0.4188],\n        [0.1677],\n        [0.4312],\n        [0.2697],\n        [0.7529],\n        [0.0337],\n        [0.1959],\n        [0.0913],\n        [0.6196],\n        [0.5408],\n        [0.3403],\n        [0.2110],\n        [0.5829],\n        [0.6846],\n        [0.9022],\n        [0.8170],\n        [0.2394],\n        [0.6163],\n        [0.1963],\n        [0.9874],\n        [0.7808],\n        [0.8303],\n        [0.2447],\n        [0.9939],\n        [0.8685],\n        [0.3606],\n        [0.2404],\n        [0.8383],\n        [0.4470],\n        [0.0306],\n        [0.5175],\n        [0.9117],\n        [0.0187],\n        [0.4973],\n        [0.8985],\n        [0.6400],\n        [0.3463],\n        [0.1456],\n        [0.3045],\n        [0.1302],\n        [0.3881],\n        [0.3018],\n        [0.2870],\n        [0.6084],\n        [0.4803],\n        [0.0776],\n        [0.4647],\n        [0.5551],\n        [0.6345],\n        [0.4559],\n        [0.5743],\n        [0.0146],\n        [0.6169],\n        [0.0575],\n        [0.2035],\n        [0.4543],\n        [0.3858],\n        [0.3532],\n        [0.6566],\n        [0.2827],\n        [0.6410],\n        [0.9508],\n        [0.0064],\n        [0.5960],\n        [0.5597],\n        [0.1496],\n        [0.6219],\n        [0.6828],\n        [0.9000],\n        [0.7197],\n        [0.5957],\n        [0.5551],\n        [0.3828],\n        [0.7106],\n        [0.2067],\n        [0.4871],\n        [0.7339],\n        [0.0920],\n        [0.3629],\n        [0.3334],\n        [0.6104],\n        [0.6545],\n        [0.9855],\n        [0.1682],\n        [0.2843],\n        [0.6788],\n        [0.0710],\n        [0.4575],\n        [0.6314],\n        [0.6754],\n        [0.4968],\n        [0.0534],\n        [0.1107],\n        [0.9316],\n        [0.0414],\n        [0.7278],\n        [0.9681],\n        [0.6119],\n        [0.0583],\n        [0.5179],\n        [0.6966],\n        [0.4283],\n        [0.3959],\n        [0.7533],\n        [0.5842],\n        [0.8673],\n        [0.7026],\n        [0.8783],\n        [0.4198],\n        [0.2078],\n        [0.9699],\n        [0.4172],\n        [0.2445],\n        [0.6507],\n        [0.0875],\n        [0.5372],\n        [0.7517],\n        [0.8282],\n        [0.6805],\n        [0.1866],\n        [0.8664],\n        [0.8623],\n        [0.6069],\n        [0.8548],\n        [0.9998],\n        [0.1102],\n        [0.4830],\n        [0.2847],\n        [0.4917],\n        [0.5445],\n        [0.3318],\n        [0.8146],\n        [0.3314],\n        [0.5906],\n        [0.0277],\n        [0.1958],\n        [0.4869],\n        [0.7677],\n        [0.8128],\n        [0.2547],\n        [0.8368],\n        [0.9580],\n        [0.3748],\n        [0.8023],\n        [0.3361],\n        [0.5316],\n        [0.4568],\n        [0.7140],\n        [0.3403],\n        [0.2384],\n        [0.4600],\n        [0.9277],\n        [0.9641],\n        [0.6584],\n        [0.6750],\n        [0.0785],\n        [0.4361],\n        [0.7242],\n        [0.2793],\n        [0.4713],\n        [0.0242],\n        [0.8198],\n        [0.7271],\n        [0.8731],\n        [0.9039],\n        [0.7498],\n        [0.7680],\n        [0.3902],\n        [0.2982],\n        [0.3170],\n        [0.4247],\n        [0.7543],\n        [0.4440],\n        [0.2985],\n        [0.9391],\n        [0.9204],\n        [0.3548],\n        [0.9245],\n        [0.1633],\n        [0.7353],\n        [0.5275],\n        [0.5243],\n        [0.6496],\n        [0.1251],\n        [0.7941],\n        [0.8336],\n        [0.1924],\n        [0.6277],\n        [0.2910],\n        [0.5536],\n        [0.4394],\n        [0.6841],\n        [0.8890],\n        [0.3888],\n        [0.0854],\n        [0.8007],\n        [0.6155],\n        [0.3736],\n        [0.9842],\n        [0.5408],\n        [0.0221],\n        [0.6949]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [ True, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 1.9107,  0.0454],\n        [ 1.4246,  0.0681],\n        [-2.8463, -0.6833],\n        ...,\n        [ 0.8323,  2.5397],\n        [-0.1457,  1.6798],\n        [ 0.7927,  0.7674]]) torch.Size([9984, 2])\nsamples tensor([[ 2.6042e+00, -1.4496e+00],\n        [ 1.0513e+00, -8.4349e-01],\n        [ 2.5342e+00,  1.0227e+00],\n        [ 4.1542e+00,  5.9749e-01],\n        [ 1.8992e+00, -1.1540e+00],\n        [ 1.9268e+00, -9.1223e-01],\n        [ 1.8250e+00,  5.8396e-01],\n        [ 2.1919e+00,  1.0435e+00],\n        [ 2.1221e+00, -1.6983e-02],\n        [ 3.9510e+00, -8.3803e-02],\n        [ 1.9462e+00,  1.3549e+00],\n        [ 2.3189e+00,  3.1638e-01],\n        [ 4.6997e+00, -4.8014e-02],\n        [ 3.9513e+00,  3.0180e-01],\n        [ 1.0768e+00, -7.7335e-01],\n        [ 1.2278e+00, -7.3012e-02],\n        [ 2.6436e+00,  7.5959e-01],\n        [ 1.6075e+00, -8.0281e-01],\n        [ 4.7205e+00,  5.9395e-01],\n        [ 2.6240e+00, -9.7499e-01],\n        [ 8.9731e-01,  8.1150e-01],\n        [ 3.6419e+00, -5.7176e-01],\n        [ 2.5735e+00,  2.4680e-02],\n        [ 1.8915e+00, -1.5777e+00],\n        [ 2.3628e+00,  6.8664e-01],\n        [ 2.0864e+00,  5.4159e-01],\n        [ 1.7871e+00, -8.8223e-01],\n        [ 2.5515e+00, -2.1845e-01],\n        [ 8.6843e-01, -9.1649e-01],\n        [ 2.3147e+00, -1.1955e+00],\n        [ 2.2003e+00,  1.0785e+00],\n        [ 3.1535e+00, -9.1649e-01],\n        [ 1.0058e+00,  4.3986e-01],\n        [ 1.9140e+00,  4.5783e-01],\n        [ 3.2367e+00, -1.4102e+00],\n        [ 6.1828e-01, -8.5051e-01],\n        [ 1.1012e+00, -5.5409e-01],\n        [ 3.6779e+00, -1.1648e+00],\n        [ 6.0309e-01, -1.0012e-01],\n        [ 1.6653e+00, -1.2674e+00],\n        [ 1.0581e+00, -5.5236e-01],\n        [ 3.2149e+00, -5.5492e-02],\n        [ 2.5942e+00, -7.5595e-02],\n        [ 2.6046e+00, -1.4758e-01],\n        [ 1.9102e+00, -5.9775e-01],\n        [ 7.0022e-01, -3.6506e-02],\n        [ 3.0586e-01,  1.6291e+00],\n        [ 2.0465e+00,  1.0376e+00],\n        [ 3.3167e+00, -2.9043e-01],\n        [ 1.7115e+00,  3.6580e-01],\n        [ 1.9399e+00, -5.3446e-01],\n        [ 9.0999e-01,  7.9957e-01],\n        [ 1.9977e+00,  9.3724e-02],\n        [ 2.1785e+00,  6.8509e-01],\n        [ 2.1807e+00,  2.9298e-01],\n        [ 1.7572e+00,  1.3410e+00],\n        [ 1.4425e+00,  1.0049e+00],\n        [ 2.7947e+00, -3.8877e-01],\n        [ 2.7704e+00, -1.0677e+00],\n        [ 2.6553e+00,  2.6068e+00],\n        [ 2.5265e+00,  9.4926e-02],\n        [ 2.6499e+00,  1.9561e+00],\n        [ 3.1487e+00,  8.4965e-01],\n        [ 1.4057e+00, -1.4259e-01],\n        [ 1.6797e+00,  1.7982e-01],\n        [ 1.8592e+00,  5.7409e-01],\n        [ 1.4301e+00, -1.0280e+00],\n        [-3.8347e-01,  1.6525e-01],\n        [ 2.8505e+00,  5.9791e-01],\n        [ 3.4237e+00,  3.7868e-02],\n        [ 3.5538e+00, -3.1478e-01],\n        [ 1.5532e+00,  1.1582e+00],\n        [ 1.0615e+00, -5.1210e-01],\n        [ 3.1058e+00,  9.2633e-01],\n        [ 2.8770e+00,  1.4694e+00],\n        [ 1.6454e+00,  1.0186e+00],\n        [ 2.2003e+00, -4.3925e-01],\n        [ 3.7442e+00, -6.6456e-01],\n        [ 1.8468e+00,  7.6307e-01],\n        [ 2.1580e+00,  5.8564e-01],\n        [ 1.4617e+00,  6.3733e-01],\n        [ 4.4767e+00,  1.4669e+00],\n        [ 2.4884e+00,  1.5594e-01],\n        [ 3.5352e+00, -3.2659e-01],\n        [ 2.2056e+00,  2.2465e-01],\n        [ 3.0220e+00,  1.1864e-01],\n        [ 2.9049e+00,  5.1586e-01],\n        [ 1.6735e+00, -1.9149e-01],\n        [ 4.9301e+00,  2.2958e-01],\n        [ 2.1438e+00, -7.6436e-01],\n        [ 7.1652e-01, -1.0114e+00],\n        [ 2.0462e+00, -4.2699e-01],\n        [ 6.8319e-01, -5.8974e-02],\n        [ 3.5891e+00, -1.9274e+00],\n        [ 2.3323e+00,  4.4161e-01],\n        [ 2.6206e+00, -5.4645e-01],\n        [ 3.8424e+00, -1.6196e+00],\n        [ 2.2104e+00,  5.8888e-01],\n        [ 3.6120e+00, -1.2274e+00],\n        [ 1.7967e+00,  6.6922e-01],\n        [ 1.4960e+00, -4.3909e-01],\n        [ 1.8257e+00, -1.6126e-01],\n        [ 3.0592e+00,  8.0077e-01],\n        [ 2.7477e+00, -1.5115e+00],\n        [ 3.9208e+00, -2.6167e-01],\n        [ 2.0500e+00, -9.8951e-01],\n        [ 2.3499e+00,  4.9846e-01],\n        [ 3.2877e+00, -1.6464e+00],\n        [ 2.6948e+00, -9.7651e-01],\n        [ 2.0129e+00,  4.8839e-01],\n        [ 2.9282e+00, -4.9509e-01],\n        [ 2.6989e+00,  8.4669e-01],\n        [ 2.9794e+00,  7.4105e-01],\n        [ 2.4417e+00, -1.3507e+00],\n        [ 1.7151e+00,  3.1098e-01],\n        [ 2.9236e+00,  3.5987e-01],\n        [ 2.4049e+00, -1.9511e-01],\n        [ 2.7024e+00,  7.3281e-01],\n        [ 8.8184e-01, -5.6103e-01],\n        [ 3.0856e+00,  1.7042e-01],\n        [ 2.8568e+00,  7.7464e-01],\n        [ 2.3357e+00, -2.1906e-01],\n        [ 3.7158e+00, -5.2212e-01],\n        [ 1.2439e+00, -8.3641e-01],\n        [ 2.8486e+00,  1.8967e-01],\n        [ 8.4452e-01, -1.1300e+00],\n        [ 2.3591e+00, -1.3767e+00],\n        [ 1.6382e+00, -7.1707e-01],\n        [ 4.0456e+00,  2.2315e-01],\n        [ 7.6835e-01,  1.4255e-01],\n        [ 1.3807e+00, -9.5592e-01],\n        [ 1.4477e+00, -6.2697e-01],\n        [ 2.5389e+00, -8.4457e-01],\n        [ 7.6253e-01,  8.7125e-01],\n        [ 2.3991e+00,  1.7656e-01],\n        [ 2.2424e+00,  1.5667e-01],\n        [ 3.8075e-01, -1.8099e-01],\n        [ 1.9358e+00,  9.5552e-01],\n        [ 2.8433e+00, -9.3175e-01],\n        [ 2.5121e+00,  1.6296e+00],\n        [ 2.8445e+00, -3.1281e+00],\n        [ 1.6745e+00,  9.5745e-01],\n        [ 1.0975e+00, -5.2532e-01],\n        [ 3.6458e+00, -6.1774e-01],\n        [ 3.7015e+00,  8.0548e-01],\n        [ 2.0604e+00, -1.2707e+00],\n        [ 3.5175e+00, -2.3669e+00],\n        [ 1.5418e+00, -4.2362e-01],\n        [ 5.9735e-01,  4.6804e-01],\n        [ 1.1346e+00,  8.8221e-01],\n        [ 1.7907e+00,  3.9347e-01],\n        [ 2.4115e+00, -6.6999e-01],\n        [ 1.7835e+00, -8.8887e-01],\n        [ 2.8346e+00, -5.5101e-01],\n        [ 3.7253e+00, -5.3443e-01],\n        [ 2.2460e+00,  2.7956e-01],\n        [ 2.4336e+00, -1.4861e+00],\n        [ 2.8815e+00, -1.4335e+00],\n        [ 1.0486e+00,  1.0258e+00],\n        [ 1.2125e+00,  8.2057e-01],\n        [ 1.7265e+00,  1.0011e+00],\n        [ 1.8186e+00,  1.1254e+00],\n        [ 3.5175e+00,  1.3986e-01],\n        [ 9.4082e-01, -9.3632e-01],\n        [ 2.9099e+00, -1.8659e+00],\n        [ 4.0070e+00,  3.7028e-01],\n        [ 2.1784e+00,  2.5336e-01],\n        [ 1.1887e+00, -2.2345e+00],\n        [ 3.5544e+00, -4.9113e-01],\n        [ 2.7813e+00,  7.6787e-01],\n        [ 3.2560e+00,  4.8341e-01],\n        [ 2.6008e+00, -4.2901e-01],\n        [ 1.4921e+00,  6.3825e-01],\n        [ 2.8053e+00,  8.8530e-02],\n        [ 3.3611e+00, -9.1370e-01],\n        [ 2.1949e+00, -8.9601e-01],\n        [ 3.4342e+00,  1.3109e+00],\n        [ 2.1813e+00,  4.7091e-01],\n        [ 2.7704e+00, -2.9109e-01],\n        [ 3.0844e+00, -2.1990e+00],\n        [ 3.4467e+00, -5.0475e-01],\n        [ 2.5971e+00,  1.0182e-01],\n        [ 4.0222e+00, -2.9147e+00],\n        [ 2.3133e-01, -9.4506e-01],\n        [ 3.0179e+00, -1.2489e-01],\n        [ 7.0606e-01, -1.4707e+00],\n        [ 2.3505e+00, -5.5221e-02],\n        [ 4.2797e+00,  1.6619e+00],\n        [ 1.4939e+00, -1.7402e+00],\n        [ 1.9737e+00, -1.0928e+00],\n        [ 2.8444e+00, -2.6073e+00],\n        [ 2.5242e+00, -2.1011e+00],\n        [ 1.5474e+00,  3.0573e-01],\n        [ 8.3977e-01,  1.1089e-01],\n        [ 1.4855e+00, -4.3469e-01],\n        [ 2.4767e+00, -9.8900e-01],\n        [ 3.3612e+00, -1.1080e+00],\n        [ 2.3689e+00,  1.5565e-01],\n        [ 1.5601e+00, -3.6810e-01],\n        [ 2.2134e+00,  3.7359e-01],\n        [ 2.8787e+00, -9.8528e-01],\n        [ 2.0584e+00,  4.1823e-01],\n        [ 2.2504e+00,  6.3528e-01],\n        [ 4.7004e+00, -8.2454e-01],\n        [ 1.3574e+00, -5.0954e-01],\n        [ 1.9302e+00, -1.2251e+00],\n        [ 1.4902e+00,  1.1031e+00],\n        [ 2.8450e+00,  1.6023e+00],\n        [ 2.3056e+00,  1.6459e+00],\n        [ 1.2085e+00, -2.1727e+00],\n        [ 1.0084e+00, -1.0797e+00],\n        [ 3.0182e+00, -4.3144e-01],\n        [ 3.3464e+00,  7.9583e-01],\n        [ 5.8528e-01,  2.0323e-01],\n        [ 1.9903e+00, -7.2009e-01],\n        [ 2.4652e+00, -1.1150e-01],\n        [ 1.9213e+00,  1.3791e-01],\n        [ 2.6338e+00, -1.8465e-01],\n        [ 2.5239e+00, -1.8469e+00],\n        [ 1.3663e+00, -9.8077e-01],\n        [ 2.5271e+00,  3.8454e-01],\n        [ 2.6294e+00,  8.8766e-01],\n        [ 4.7398e+00, -2.3283e+00],\n        [ 3.4204e+00, -2.5390e-02],\n        [ 2.2984e+00,  1.4006e+00],\n        [ 4.3952e+00, -6.6069e-01],\n        [ 2.5789e+00,  2.4957e-01],\n        [ 3.7670e+00,  1.2617e-01],\n        [ 1.9500e+00, -2.1182e-01],\n        [ 8.4636e-01,  1.3778e-01],\n        [ 3.7497e+00, -4.3354e-01],\n        [ 3.5826e+00, -1.1341e+00],\n        [ 3.4701e+00, -7.8835e-01],\n        [ 2.3154e+00,  8.6931e-01],\n        [ 6.0742e-01,  1.5952e+00],\n        [ 2.1839e+00, -2.5440e-01],\n        [ 1.7999e+00, -3.0603e-01],\n        [ 1.9031e+00, -1.1451e+00],\n        [ 1.5975e+00, -1.5217e+00],\n        [ 1.8680e+00, -9.8447e-01],\n        [ 4.1627e+00,  1.4700e+00],\n        [ 2.2031e+00, -3.6975e-01],\n        [ 3.1543e+00, -1.6625e-01],\n        [ 1.3204e+00, -1.3641e+00],\n        [ 4.6415e+00, -7.3796e-01],\n        [ 2.3985e+00, -5.8678e-01],\n        [ 2.7564e+00, -3.4336e-02],\n        [ 2.0020e+00,  2.7541e-03],\n        [ 1.7732e+00, -1.9650e-01],\n        [ 3.4695e+00, -9.7171e-01],\n        [ 2.0746e+00,  1.2712e+00],\n        [ 1.4844e+00,  1.3446e+00],\n        [ 8.6967e-01,  6.7957e-01],\n        [ 2.6697e+00, -6.8598e-01],\n        [ 3.2701e+00, -4.2439e-01],\n        [ 2.0472e+00, -4.8200e-01],\n        [ 4.7346e-01, -8.2082e-01],\n        [-3.7845e-01, -7.0053e-02],\n        [ 2.5993e+00, -8.2360e-01],\n        [ 3.0706e+00,  8.0829e-01],\n        [ 4.4538e+00,  4.8948e-01],\n        [ 4.3196e+00,  2.2696e-01],\n        [ 3.7050e+00,  1.0713e-01],\n        [ 3.4725e+00, -1.5638e+00],\n        [ 3.1317e+00,  3.6318e-01],\n        [ 3.2349e+00, -1.5738e-01],\n        [ 2.6056e+00, -7.7249e-01],\n        [ 1.2843e+00,  2.5492e-01],\n        [ 1.4723e+00,  1.1765e+00],\n        [ 3.2031e+00, -6.3489e-01],\n        [ 2.2796e+00,  2.3002e-01],\n        [ 1.5794e+00, -7.1935e-01],\n        [ 2.2051e+00, -5.6554e-01],\n        [ 2.0034e+00, -5.8623e-01],\n        [ 2.5969e+00, -1.4272e+00],\n        [ 2.3246e+00, -1.3620e-01],\n        [ 1.1267e-01,  4.7927e-01],\n        [ 2.1406e+00,  1.1122e+00],\n        [ 2.7624e+00, -1.0338e+00],\n        [ 2.3900e+00, -2.0711e-01],\n        [ 1.7202e+00, -2.3752e+00],\n        [ 3.2788e+00, -1.0224e+00],\n        [ 1.0759e+00, -1.1993e+00],\n        [ 2.2929e+00, -3.7038e-01],\n        [ 2.8437e+00, -1.6597e+00],\n        [ 2.6937e+00,  4.4158e-01],\n        [ 1.5526e+00, -5.3538e-01],\n        [ 8.2894e-01,  1.9676e-01],\n        [ 2.3513e+00, -4.0210e-01],\n        [ 2.5547e+00, -1.1165e+00],\n        [ 1.5624e+00,  3.4994e-01],\n        [ 2.8453e+00,  5.4266e-01],\n        [ 2.1371e+00,  5.0818e-02],\n        [ 1.7051e+00, -2.8625e-01],\n        [ 3.0177e+00,  2.4293e-02],\n        [ 1.7647e+00, -3.7133e-01],\n        [ 3.1057e+00, -7.3970e-01],\n        [ 1.9404e+00,  3.9310e-01],\n        [ 2.2872e+00, -1.1568e+00],\n        [ 4.0304e+00,  9.1820e-02],\n        [ 2.0813e+00, -1.3102e+00],\n        [ 3.6297e+00,  2.0136e+00],\n        [ 3.0170e+00,  5.2937e-01],\n        [ 4.3269e+00, -3.2492e-01],\n        [ 1.9891e+00,  6.2406e-02],\n        [ 2.0692e+00,  3.6819e-01],\n        [ 3.0797e+00,  1.1227e+00],\n        [ 3.1613e+00, -8.7979e-01],\n        [ 2.2060e+00,  6.9043e-01],\n        [ 2.0416e+00, -9.2795e-01],\n        [ 1.7313e+00, -1.4181e-01],\n        [ 3.0921e+00, -5.9370e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.7709e-02, 1.5511e-01, 1.6176e-01,  ..., 6.4857e-01, 7.0064e-01,\n         1.0000e+00],\n        [3.9815e-03, 6.8253e-02, 6.8925e-02,  ..., 9.9772e-01, 9.9774e-01,\n         1.0000e+00],\n        [1.6320e-06, 1.5979e-01, 1.9948e-01,  ..., 9.5373e-01, 9.9988e-01,\n         1.0000e+00],\n        ...,\n        [1.9567e-03, 2.0724e-03, 2.1235e-03,  ..., 9.9925e-01, 9.9927e-01,\n         1.0000e+00],\n        [2.0624e-03, 2.5707e-03, 5.0051e-03,  ..., 9.9391e-01, 1.0000e+00,\n         1.0000e+00],\n        [4.8200e-03, 4.8200e-03, 4.8248e-03,  ..., 9.8334e-01, 9.9996e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.8187],\n        [0.0504],\n        [0.5061],\n        [0.0279],\n        [0.0583],\n        [0.1164],\n        [0.1138],\n        [0.2897],\n        [0.7543],\n        [0.0849],\n        [0.6442],\n        [0.1292],\n        [0.3348],\n        [0.9150],\n        [0.8142],\n        [0.7597],\n        [0.1321],\n        [0.5692],\n        [0.8756],\n        [0.9405],\n        [0.5298],\n        [0.3638],\n        [0.4819],\n        [0.5247],\n        [0.5392],\n        [0.9142],\n        [0.9284],\n        [0.3124],\n        [0.8814],\n        [0.0813],\n        [0.2285],\n        [0.6687],\n        [0.7564],\n        [0.0027],\n        [0.1090],\n        [0.2370],\n        [0.6590],\n        [0.0738],\n        [0.9729],\n        [0.7963],\n        [0.3378],\n        [0.0473],\n        [0.9176],\n        [0.3263],\n        [0.0935],\n        [0.6575],\n        [0.2524],\n        [0.3882],\n        [0.0074],\n        [0.7247],\n        [0.7901],\n        [0.3896],\n        [0.8022],\n        [0.6347],\n        [0.5619],\n        [0.2334],\n        [0.4923],\n        [0.4220],\n        [0.6413],\n        [0.4110],\n        [0.4628],\n        [0.7732],\n        [0.5931],\n        [0.6346],\n        [0.6370],\n        [0.2046],\n        [0.9033],\n        [0.7490],\n        [0.8491],\n        [0.2408],\n        [0.9107],\n        [0.8523],\n        [0.8443],\n        [0.1645],\n        [0.2407],\n        [0.9620],\n        [0.6987],\n        [0.9230],\n        [0.7254],\n        [0.5226],\n        [0.2572],\n        [0.5404],\n        [0.8194],\n        [0.4160],\n        [0.6866],\n        [0.9968],\n        [0.4569],\n        [0.2897],\n        [0.3942],\n        [0.8058],\n        [0.8863],\n        [0.1059],\n        [0.3052],\n        [0.4496],\n        [0.4817],\n        [0.9387],\n        [0.3206],\n        [0.6845],\n        [0.9979],\n        [0.2661],\n        [0.0051],\n        [0.2396],\n        [0.3264],\n        [0.6596],\n        [0.4048],\n        [0.0929],\n        [0.5897],\n        [0.8785],\n        [0.2675],\n        [0.3964],\n        [0.6791],\n        [0.0175],\n        [0.2706],\n        [0.9272],\n        [0.8270],\n        [0.5431],\n        [0.7847],\n        [0.9286],\n        [0.2193],\n        [0.3521],\n        [0.0549],\n        [0.4114],\n        [0.9879],\n        [0.9118],\n        [0.1603],\n        [0.8752],\n        [0.9122],\n        [0.0679],\n        [0.8358],\n        [0.6299],\n        [0.1921],\n        [0.2019],\n        [0.2514],\n        [0.3535],\n        [0.7981],\n        [0.3606],\n        [0.5189],\n        [0.6620],\n        [0.3843],\n        [0.5416],\n        [0.9844],\n        [0.5289],\n        [0.4262],\n        [0.7435],\n        [0.8108],\n        [0.4648],\n        [0.9687],\n        [0.1680],\n        [0.2586],\n        [0.0012],\n        [0.4668],\n        [0.9994],\n        [0.4424],\n        [0.3845],\n        [0.3889],\n        [0.8964],\n        [0.7669],\n        [0.7627],\n        [0.7541],\n        [0.6932],\n        [0.2989],\n        [0.6214],\n        [0.0164],\n        [0.8776],\n        [0.6757],\n        [0.5563],\n        [0.6492],\n        [0.3094],\n        [0.3305],\n        [0.5489],\n        [0.9645],\n        [0.2718],\n        [0.3178],\n        [0.5946],\n        [0.7924],\n        [0.8112],\n        [0.0068],\n        [0.7427],\n        [0.3478],\n        [0.1964],\n        [0.0054],\n        [0.0618],\n        [0.4325],\n        [0.3601],\n        [0.2213],\n        [0.3646],\n        [0.1753],\n        [0.2259],\n        [0.4137],\n        [0.1746],\n        [0.5243],\n        [0.0606],\n        [0.3450],\n        [0.5231],\n        [0.2469],\n        [0.7888],\n        [0.7775],\n        [0.7296],\n        [0.0043],\n        [0.2497],\n        [0.4896],\n        [0.8630],\n        [0.0187],\n        [0.7576],\n        [0.8440],\n        [0.9635],\n        [0.2561],\n        [0.4751],\n        [0.9347],\n        [0.1191],\n        [0.4703],\n        [0.0876],\n        [0.1700],\n        [0.1585],\n        [0.6042],\n        [0.2911],\n        [0.2412],\n        [0.3552],\n        [0.5114],\n        [0.0657],\n        [0.4488],\n        [0.4962],\n        [0.7756],\n        [0.9162],\n        [0.8178],\n        [0.0291],\n        [0.1158],\n        [0.6987],\n        [0.5338],\n        [0.6396],\n        [0.4497],\n        [0.4787],\n        [0.3213],\n        [0.7876],\n        [0.2848],\n        [0.0903],\n        [0.1886],\n        [0.8100],\n        [0.7958],\n        [0.6396],\n        [0.0620],\n        [0.8267],\n        [0.3121],\n        [0.6954],\n        [0.3475],\n        [0.1641],\n        [0.1685],\n        [0.0926],\n        [0.3224],\n        [0.8536],\n        [0.8631],\n        [0.3862],\n        [0.2547],\n        [0.9962],\n        [0.4269],\n        [0.9311],\n        [0.2286],\n        [0.4958],\n        [0.2833],\n        [0.7552],\n        [0.4608],\n        [0.2411],\n        [0.8101],\n        [0.2917],\n        [0.5653],\n        [0.3801],\n        [0.0685],\n        [0.4056],\n        [0.6668],\n        [0.9413],\n        [0.2707],\n        [0.1163],\n        [0.8200],\n        [0.1448],\n        [0.6739],\n        [0.0482],\n        [0.6988],\n        [0.4282],\n        [0.2267],\n        [0.5095],\n        [0.0941],\n        [0.0491],\n        [0.2598],\n        [0.1071],\n        [0.3130],\n        [0.1613],\n        [0.7650],\n        [0.0068],\n        [0.9718],\n        [0.1979],\n        [0.4199],\n        [0.8764],\n        [0.3576],\n        [0.2995],\n        [0.3099],\n        [0.3359],\n        [0.5178],\n        [0.3669],\n        [0.5876],\n        [0.7011],\n        [0.6440],\n        [0.9495],\n        [0.1088],\n        [0.7525],\n        [0.6026],\n        [0.5925],\n        [0.6861],\n        [0.4366],\n        [0.9195],\n        [0.1668],\n        [0.5746],\n        [0.5466]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False,  True],\n        [False,  True, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 3.7652,  1.7633],\n        [ 3.4599,  0.4480],\n        [ 4.1341,  2.1321],\n        ...,\n        [-0.7013,  0.1884],\n        [ 2.3236,  1.9197],\n        [-1.2032,  1.2192]]) torch.Size([9984, 2])\nsamples tensor([[ 2.9952, -1.0862],\n        [ 1.4273,  0.2182],\n        [ 3.4027, -0.0802],\n        [ 1.2499,  0.7111],\n        [ 3.8468, -0.5971],\n        [ 2.7606, -0.6305],\n        [ 3.8465, -0.8395],\n        [ 1.1333,  0.9986],\n        [ 2.7170, -0.4703],\n        [ 1.4879,  0.1437],\n        [ 1.8775,  0.3976],\n        [ 3.2659,  0.0739],\n        [ 2.2845, -0.9248],\n        [ 0.8568,  0.0685],\n        [ 1.6192,  0.1177],\n        [ 1.3677,  0.3289],\n        [ 2.6127,  0.6180],\n        [ 2.2968,  0.8999],\n        [ 2.2909, -0.5680],\n        [ 1.1085,  0.1085],\n        [ 4.7850, -3.4998],\n        [ 2.9989,  0.9320],\n        [ 1.4950,  0.6227],\n        [ 1.0852, -1.6124],\n        [ 1.7908, -0.7279],\n        [ 3.1961,  0.3506],\n        [ 2.9120, -0.0344],\n        [ 2.6435, -0.5491],\n        [ 0.8096,  1.5642],\n        [ 1.3947,  0.3305],\n        [ 3.1328,  0.7572],\n        [ 1.5822,  1.3093],\n        [ 0.4280, -0.1310],\n        [ 2.7352, -0.4554],\n        [ 1.9264, -0.8222],\n        [ 1.1851,  1.3581],\n        [ 2.6658, -0.6803],\n        [ 4.6541, -0.2798],\n        [ 2.8093,  1.2265],\n        [ 3.5428,  0.3318],\n        [ 4.5960, -1.1362],\n        [ 2.7326, -0.2217],\n        [ 1.3521, -1.0868],\n        [ 1.4440,  0.2901],\n        [ 1.8934,  0.0829],\n        [ 0.7464, -0.9529],\n        [ 1.5011,  0.3542],\n        [ 1.7767,  0.3896],\n        [ 2.3515,  1.1554],\n        [ 1.5895, -0.1956],\n        [ 1.6355,  0.0786],\n        [ 0.8787, -0.9338],\n        [ 1.5970, -0.3445],\n        [ 2.7360, -0.2856],\n        [ 3.4694, -0.0837],\n        [ 2.6843, -0.5910],\n        [ 2.2367,  0.1539],\n        [ 3.3523,  0.9816],\n        [ 1.7946,  0.5980],\n        [ 1.9733, -0.3315],\n        [ 1.4469, -1.5918],\n        [ 2.8716, -0.8145],\n        [ 4.3617, -0.7914],\n        [ 0.4413, -2.6331],\n        [ 2.6590,  0.1472],\n        [ 1.8563,  0.7432],\n        [ 2.0976,  0.1747],\n        [ 2.8380, -0.1669],\n        [ 3.6208, -1.2359],\n        [ 1.4226,  3.2702],\n        [ 1.7964,  0.3757],\n        [ 0.9719,  0.4728],\n        [ 2.2827, -0.5273],\n        [ 3.9776, -0.4067],\n        [ 3.8576,  0.2676],\n        [ 2.3530,  0.7376],\n        [ 1.5861, -0.8609],\n        [ 2.6017,  1.1066],\n        [ 2.5674,  0.0933],\n        [ 1.9027, -0.7221],\n        [ 1.5627, -0.1184],\n        [ 2.1380, -0.7703],\n        [ 1.9647, -0.7728],\n        [ 4.0398, -0.9418],\n        [ 2.4769, -2.1454],\n        [ 1.0219,  0.8022],\n        [ 1.7345,  0.0897],\n        [ 3.9900,  0.0538],\n        [ 0.7604,  1.2944],\n        [ 3.3798,  0.3234],\n        [ 1.7547,  1.0200],\n        [ 4.3116, -0.3197],\n        [ 3.2325, -0.0727],\n        [ 3.0642,  0.7998],\n        [ 2.0691, -0.3080],\n        [ 3.2140,  0.0742],\n        [ 2.5829,  1.3456],\n        [ 2.7834,  0.1352],\n        [ 3.2707,  0.9722],\n        [ 2.4784, -0.4668],\n        [ 1.1466, -0.5296],\n        [ 2.5863,  0.1934],\n        [ 2.3576,  0.4486],\n        [ 2.6519, -0.5139],\n        [ 0.7537, -1.3295],\n        [ 0.9004, -0.7694],\n        [ 3.1478, -0.5590],\n        [ 3.8085, -0.5786],\n        [ 1.2310, -0.2513],\n        [ 1.8048, -0.1910],\n        [ 2.1348,  1.2608],\n        [ 1.3846,  1.7270],\n        [ 2.1881, -0.7942],\n        [ 2.9268, -0.6503],\n        [ 2.8910, -0.9518],\n        [ 2.4120,  0.5134],\n        [ 1.6585,  0.0753],\n        [ 3.0613,  0.9955],\n        [ 1.9072, -0.5136],\n        [ 2.7189, -2.0318],\n        [ 1.3660, -0.0783],\n        [ 2.1423,  0.9600],\n        [ 3.2166, -3.2106],\n        [ 1.6462,  0.9878],\n        [ 3.8222, -1.1055],\n        [ 2.6115, -1.4342],\n        [ 3.0667, -0.4853],\n        [ 4.7574,  0.8712],\n        [ 3.5145, -1.2031],\n        [ 1.6293,  2.3807],\n        [ 3.0658,  0.7727],\n        [ 1.4911,  0.6453],\n        [ 2.0301,  0.1865],\n        [ 1.9746, -0.2835],\n        [ 2.0853, -0.9746],\n        [ 1.2056,  0.3994],\n        [ 2.1257,  0.1357],\n        [ 4.0617, -1.8080],\n        [ 1.5489, -1.6231],\n        [ 3.0942,  0.8717],\n        [ 1.9009,  0.7867],\n        [ 3.4770,  1.3732],\n        [ 0.9736, -0.3509],\n        [ 2.0073, -1.0761],\n        [ 3.0215,  2.5827],\n        [ 3.0109,  0.0771],\n        [ 0.5571,  0.6573],\n        [ 2.0313,  1.4272],\n        [ 3.2574, -2.2257],\n        [ 0.7411,  0.8039],\n        [ 1.4282, -0.4328],\n        [ 3.3028, -0.5181],\n        [ 2.4454, -0.3989],\n        [ 3.7152,  3.0625],\n        [ 3.3404, -0.4426],\n        [ 3.4423,  0.2102],\n        [ 2.1598,  0.7568],\n        [ 0.2660, -0.8571],\n        [ 3.5916,  0.1223],\n        [ 3.4472, -1.8100],\n        [ 2.5454,  0.7566],\n        [ 1.9484, -0.4918],\n        [ 1.2038,  0.0091],\n        [ 2.9408, -1.7127],\n        [ 2.7193,  1.2274],\n        [ 1.2832, -0.1470],\n        [ 1.8798,  1.6000],\n        [ 1.3180,  0.0316],\n        [ 3.6624, -0.8660],\n        [ 2.1255,  2.2051],\n        [ 2.1707,  1.6028],\n        [ 3.0182,  0.0946],\n        [ 1.2738, -1.4633],\n        [ 2.3967, -0.6454],\n        [ 2.2378,  0.1378],\n        [ 2.6749, -0.1563],\n        [ 1.3258,  0.1412],\n        [ 3.8576, -1.0062],\n        [ 1.3007, -0.8418],\n        [ 2.2742,  0.3500],\n        [ 3.5391,  2.1910],\n        [ 2.5781,  0.9898],\n        [ 3.6436, -1.3915],\n        [ 2.1585, -0.3897],\n        [ 2.2877, -1.1320],\n        [ 2.3334,  0.1004],\n        [ 3.1971, -0.9501],\n        [ 2.3547, -0.3575],\n        [ 2.6892, -0.4705],\n        [ 1.3389, -0.7279],\n        [ 2.0136,  1.4965],\n        [ 1.9559,  0.5896],\n        [ 2.8679, -0.3928],\n        [ 3.8447, -0.5424],\n        [ 2.6332, -0.6287],\n        [ 2.4689,  1.6043],\n        [ 2.8509,  0.5685],\n        [ 2.1884,  0.1993],\n        [ 0.7771,  1.0364],\n        [ 2.8785,  0.4683],\n        [ 2.9707,  0.4510],\n        [ 3.4285,  0.1447],\n        [ 1.7482, -0.2957],\n        [ 3.4545,  0.1711],\n        [ 2.9630, -0.0814],\n        [ 2.3594,  1.5340],\n        [ 0.7758,  1.5405],\n        [ 4.6180, -1.4085],\n        [ 1.7411,  0.0638],\n        [ 3.1144, -1.6683],\n        [ 3.3934, -1.2663],\n        [ 1.7104,  1.4756],\n        [ 3.5152,  0.3447],\n        [ 2.5540, -0.8615],\n        [ 2.5628,  0.8618],\n        [ 3.0243, -1.3827],\n        [ 2.7805,  1.5500],\n        [ 3.5285, -0.0103],\n        [ 2.6680,  0.5504],\n        [ 3.6198, -0.5692],\n        [ 0.7526, -0.8611],\n        [ 2.1456, -1.6468],\n        [ 2.4896,  0.4989],\n        [ 1.3722,  0.5351],\n        [ 1.4184,  0.9133],\n        [ 0.9099, -0.5482],\n        [ 2.1302,  0.4331],\n        [ 2.2890, -1.0109],\n        [ 3.8425, -0.5788],\n        [ 2.2814,  1.2714],\n        [ 1.8938,  0.0590],\n        [ 2.7470, -2.4510],\n        [ 1.6399, -0.5341],\n        [ 4.4454, -1.7279],\n        [ 2.2990, -1.8792],\n        [ 2.6799,  0.9307],\n        [ 1.7319,  2.1287],\n        [ 1.4454, -0.8846],\n        [ 2.4793,  0.2558],\n        [ 2.7702, -0.0650],\n        [ 3.3109,  0.1989],\n        [ 1.1138, -0.7247],\n        [ 2.2939, -0.6997],\n        [ 0.7806,  1.1867],\n        [ 3.8503, -1.1382],\n        [ 2.3780,  0.5753],\n        [ 2.2378,  0.1261],\n        [ 2.3090,  0.0939],\n        [ 2.1884, -0.4262],\n        [ 2.4949, -1.8268],\n        [ 2.7617, -0.4481],\n        [ 3.7880,  1.3879],\n        [ 2.9941,  0.9134],\n        [ 3.5486,  2.4444],\n        [ 1.9023, -0.0560],\n        [ 3.3412,  1.6310],\n        [ 0.5363, -0.4195],\n        [ 1.6879,  1.1740],\n        [ 2.1551, -0.8660],\n        [ 2.1057,  2.0761],\n        [ 3.8369, -0.6811],\n        [ 3.3803,  0.5401],\n        [ 2.1532, -0.0623],\n        [ 0.6029, -1.7327],\n        [ 3.1263,  1.0081],\n        [ 2.7858, -1.1567],\n        [ 1.9201,  1.0414],\n        [ 2.9241, -1.3890],\n        [ 1.5669,  1.5193],\n        [ 2.1186,  0.2558],\n        [ 3.2931, -0.2742],\n        [ 3.0269, -0.4697],\n        [ 2.0364,  0.3754],\n        [ 2.7134, -1.4778],\n        [ 2.1564,  0.2741],\n        [ 2.5481,  0.7634],\n        [ 1.3914, -0.8258],\n        [ 2.4283,  0.0598],\n        [ 2.5144,  0.5984],\n        [ 2.2701,  1.3919],\n        [ 3.2452,  0.4352],\n        [ 3.5869, -1.1362],\n        [ 3.2525, -1.0119],\n        [ 1.8785, -0.5878],\n        [ 1.4012,  0.5854],\n        [ 2.7979,  0.0937],\n        [ 1.8194, -0.6679],\n        [ 0.9872,  0.8435],\n        [ 3.4329, -0.3060],\n        [ 2.9934,  1.2392],\n        [ 1.5803,  0.3156],\n        [ 2.4062,  1.5642],\n        [ 3.5750,  1.2494],\n        [ 2.4400, -0.3427],\n        [ 3.5520, -0.5656],\n        [ 2.1085, -0.4127],\n        [ 3.3105,  1.5867],\n        [ 2.6539, -0.1363],\n        [ 3.4125,  0.7517],\n        [ 1.7529,  0.6254],\n        [ 3.0962, -1.1991],\n        [ 3.5522,  0.0942],\n        [ 2.3300, -0.1432],\n        [ 1.4858, -0.6154],\n        [ 1.7292,  0.4575],\n        [ 3.5772, -0.6090],\n        [ 1.9623,  0.7855],\n        [ 3.3971, -0.2633],\n        [ 0.3628, -0.3553],\n        [ 1.1335,  1.0272],\n        [ 0.9061, -1.2872],\n        [ 3.7348, -0.2579]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[7.8219e-03, 3.3860e-02, 3.3860e-02,  ..., 5.1607e-01, 5.1607e-01,\n         1.0000e+00],\n        [1.0493e-03, 1.1866e-03, 1.4165e-02,  ..., 8.9989e-01, 9.9694e-01,\n         1.0000e+00],\n        [5.6317e-02, 6.0984e-02, 7.2622e-02,  ..., 9.9350e-01, 9.9654e-01,\n         1.0000e+00],\n        ...,\n        [7.5808e-09, 3.5348e-03, 4.6114e-01,  ..., 9.3117e-01, 9.5163e-01,\n         1.0000e+00],\n        [2.6927e-03, 2.6945e-03, 2.7432e-01,  ..., 8.8075e-01, 9.9956e-01,\n         1.0000e+00],\n        [3.5665e-01, 3.6573e-01, 3.6574e-01,  ..., 8.1538e-01, 9.9924e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.1832],\n        [0.2478],\n        [0.8597],\n        [0.7866],\n        [0.7421],\n        [0.4785],\n        [0.3269],\n        [0.7327],\n        [0.0114],\n        [0.8252],\n        [0.4683],\n        [0.3325],\n        [0.7850],\n        [0.4739],\n        [0.5421],\n        [0.1546],\n        [0.3862],\n        [0.5695],\n        [0.8941],\n        [0.2997],\n        [0.2649],\n        [0.5276],\n        [0.9479],\n        [0.4496],\n        [0.3862],\n        [0.5412],\n        [0.1671],\n        [0.5646],\n        [0.1445],\n        [0.0191],\n        [0.6624],\n        [0.5385],\n        [0.7729],\n        [0.3949],\n        [0.9292],\n        [0.5450],\n        [0.8517],\n        [0.4808],\n        [0.4878],\n        [0.8175],\n        [0.2433],\n        [0.1151],\n        [0.3079],\n        [0.2333],\n        [0.8478],\n        [0.2687],\n        [0.0453],\n        [0.8641],\n        [0.9082],\n        [0.6559],\n        [0.5860],\n        [0.3232],\n        [0.5628],\n        [0.6335],\n        [0.7705],\n        [0.9478],\n        [0.9176],\n        [0.1220],\n        [0.4301],\n        [0.7979],\n        [0.9837],\n        [0.6883],\n        [0.5065],\n        [0.5035],\n        [0.9536],\n        [0.5982],\n        [0.5463],\n        [0.6702],\n        [0.1982],\n        [0.8094],\n        [0.2883],\n        [0.5920],\n        [0.3699],\n        [0.7933],\n        [0.4885],\n        [0.9141],\n        [0.0288],\n        [0.0719],\n        [0.2020],\n        [0.8459],\n        [0.9417],\n        [0.7660],\n        [0.3011],\n        [0.2869],\n        [0.9306],\n        [0.2336],\n        [0.5946],\n        [0.7806],\n        [0.8152],\n        [0.3334],\n        [0.5289],\n        [0.6504],\n        [0.6440],\n        [0.0814],\n        [0.6502],\n        [0.6516],\n        [0.6938],\n        [0.5389],\n        [0.5976],\n        [0.1374],\n        [0.1640],\n        [0.9282],\n        [0.0268],\n        [0.8798],\n        [0.7668],\n        [0.4519],\n        [0.9498],\n        [0.9737],\n        [0.3227],\n        [0.1417],\n        [0.7186],\n        [0.7668],\n        [0.0172],\n        [0.3059],\n        [0.1081],\n        [0.7833],\n        [0.2309],\n        [0.7685],\n        [0.7088],\n        [0.3732],\n        [0.5476],\n        [0.8016],\n        [0.9569],\n        [0.8087],\n        [0.0385],\n        [0.9648],\n        [0.4256],\n        [0.4489],\n        [0.1051],\n        [0.2934],\n        [0.1442],\n        [0.7329],\n        [0.2177],\n        [0.9538],\n        [0.8919],\n        [0.3991],\n        [0.0700],\n        [0.3329],\n        [0.1524],\n        [0.4426],\n        [0.6916],\n        [0.3297],\n        [0.1987],\n        [0.8401],\n        [0.1239],\n        [0.4210],\n        [0.8885],\n        [0.5517],\n        [0.2547],\n        [0.0162],\n        [0.4693],\n        [0.2618],\n        [0.3514],\n        [0.9731],\n        [0.4882],\n        [0.1720],\n        [0.8215],\n        [0.8492],\n        [0.9346],\n        [0.0183],\n        [0.9978],\n        [0.2336],\n        [0.8945],\n        [0.1844],\n        [0.8353],\n        [0.6834],\n        [0.1720],\n        [0.4597],\n        [0.4047],\n        [0.2846],\n        [0.8777],\n        [0.2568],\n        [0.6578],\n        [0.2645],\n        [0.8147],\n        [0.7817],\n        [0.5196],\n        [0.3147],\n        [0.4388],\n        [0.4394],\n        [0.2191],\n        [0.5501],\n        [0.2805],\n        [0.5709],\n        [0.8372],\n        [0.8436],\n        [0.8264],\n        [0.1796],\n        [0.6292],\n        [0.3052],\n        [0.9531],\n        [0.2539],\n        [0.9577],\n        [0.5165],\n        [0.5623],\n        [0.1363],\n        [0.6963],\n        [0.2866],\n        [0.0621],\n        [0.0606],\n        [0.3020],\n        [0.4248],\n        [0.1670],\n        [0.4857],\n        [0.4864],\n        [0.6044],\n        [0.0125],\n        [0.1375],\n        [0.0515],\n        [0.1721],\n        [0.2113],\n        [0.3231],\n        [0.1388],\n        [0.5797],\n        [0.4875],\n        [0.3911],\n        [0.2705],\n        [0.6872],\n        [0.4289],\n        [0.7394],\n        [0.7805],\n        [0.3489],\n        [0.9382],\n        [0.2216],\n        [0.9041],\n        [0.8978],\n        [0.1711],\n        [0.2677],\n        [0.9569],\n        [0.1782],\n        [0.6118],\n        [0.5216],\n        [0.8224],\n        [0.9111],\n        [0.6422],\n        [0.0667],\n        [0.7259],\n        [0.7174],\n        [0.1524],\n        [0.1398],\n        [0.3565],\n        [0.8657],\n        [0.5730],\n        [0.9899],\n        [0.6911],\n        [0.1028],\n        [0.6894],\n        [0.7877],\n        [0.6174],\n        [0.6950],\n        [0.3708],\n        [0.6466],\n        [0.6800],\n        [0.4795],\n        [0.3491],\n        [0.6416],\n        [0.5048],\n        [0.3781],\n        [0.1631],\n        [0.2355],\n        [0.7978],\n        [0.0807],\n        [0.2714],\n        [0.5513],\n        [0.3929],\n        [0.3456],\n        [0.4764],\n        [0.1051],\n        [0.3723],\n        [0.9660],\n        [0.6701],\n        [0.3278],\n        [0.8705],\n        [0.9062],\n        [0.1896],\n        [0.7523],\n        [0.1158],\n        [0.1928],\n        [0.7666],\n        [0.3151],\n        [0.7220],\n        [0.2864],\n        [0.1862],\n        [0.7535],\n        [0.5315],\n        [0.8047],\n        [0.4552],\n        [0.4774],\n        [0.0751],\n        [0.7766],\n        [0.6535],\n        [0.2226],\n        [0.9070],\n        [0.5963],\n        [0.7545],\n        [0.9239],\n        [0.6179],\n        [0.2444],\n        [0.0100],\n        [0.4917],\n        [0.3195],\n        [0.7911],\n        [0.7717],\n        [0.5531],\n        [0.2894],\n        [0.3275],\n        [0.0909],\n        [0.2144],\n        [0.2620],\n        [0.0505],\n        [0.4832],\n        [0.4993]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False,  True,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 3.7588,  2.3660],\n        [ 2.3851,  1.6753],\n        [-3.1976,  0.8242],\n        ...,\n        [-1.6766, -1.1614],\n        [ 2.6885,  0.4121],\n        [-0.7690, -0.1165]]) torch.Size([9984, 2])\nsamples tensor([[ 2.4312e+00,  1.7555e-01],\n        [ 3.3847e+00, -8.8001e-01],\n        [ 6.3097e-01, -2.4619e-01],\n        [ 3.0858e+00, -7.1434e-02],\n        [ 1.8396e+00, -8.6225e-01],\n        [ 3.1537e+00, -4.7060e-01],\n        [ 3.5045e+00, -4.3078e-01],\n        [ 2.3210e+00, -1.0115e+00],\n        [ 1.9813e+00, -9.0364e-01],\n        [ 2.0753e+00,  9.9933e-01],\n        [-5.5097e-02, -3.0721e-02],\n        [ 2.1968e+00, -1.1153e+00],\n        [ 3.9285e+00, -1.9756e+00],\n        [ 3.1277e+00,  4.4278e-01],\n        [ 4.1212e+00, -2.6847e-01],\n        [ 2.0677e+00, -1.7306e+00],\n        [ 2.5629e+00,  9.5934e-01],\n        [ 1.7087e+00, -5.1190e-01],\n        [ 1.1244e+00, -8.5055e-01],\n        [ 2.2078e+00, -1.9327e+00],\n        [ 3.5908e+00, -2.0888e+00],\n        [ 1.0708e+00, -2.3294e+00],\n        [ 2.3610e+00, -6.2180e-01],\n        [ 1.9010e+00, -1.3765e-01],\n        [ 1.2693e+00, -1.6941e+00],\n        [ 1.9936e+00,  6.6521e-01],\n        [ 1.9013e+00,  7.0413e-01],\n        [ 2.6312e+00,  4.3242e-01],\n        [ 3.9232e+00, -9.3165e-01],\n        [ 1.1315e+00, -2.2636e-01],\n        [ 4.4538e+00,  6.0345e-01],\n        [ 1.9626e+00, -1.7253e+00],\n        [ 4.0078e+00, -1.1920e+00],\n        [ 1.8011e+00,  1.0659e+00],\n        [ 2.0990e+00, -1.7037e-01],\n        [ 2.8604e+00, -1.5373e+00],\n        [ 3.3216e+00,  1.6477e+00],\n        [ 3.0627e+00, -6.7492e-01],\n        [ 1.8901e+00,  9.9901e-01],\n        [ 2.4203e+00, -1.2663e+00],\n        [ 3.3165e+00, -1.7185e-01],\n        [ 1.9362e+00,  6.9278e-01],\n        [ 2.2198e+00, -1.1448e+00],\n        [ 1.6543e+00,  1.7043e+00],\n        [ 2.1787e+00,  3.1648e-01],\n        [ 3.8973e+00,  1.9089e-01],\n        [ 1.0435e+00, -5.5347e-02],\n        [ 2.4027e+00, -1.7060e+00],\n        [ 1.0224e+00,  2.7463e-01],\n        [ 1.0151e+00,  1.1556e+00],\n        [ 1.9029e+00, -4.1269e-01],\n        [ 3.6447e+00,  1.4064e-01],\n        [ 7.4991e-01,  1.3837e-01],\n        [ 1.5746e+00, -1.4045e+00],\n        [ 1.7493e+00, -8.7155e-01],\n        [ 2.3916e+00,  8.3637e-01],\n        [ 3.3436e+00,  4.1832e-01],\n        [ 1.9485e+00,  1.3545e+00],\n        [ 1.8005e+00, -1.0464e+00],\n        [ 3.3971e+00,  7.3422e-01],\n        [ 1.1038e+00,  7.4895e-01],\n        [ 3.1039e+00,  1.8744e-02],\n        [ 3.2469e+00, -7.1908e-01],\n        [ 4.0728e+00,  6.0640e-01],\n        [ 2.0189e+00,  8.0150e-02],\n        [ 3.0900e+00, -1.5456e+00],\n        [ 3.0144e+00,  6.1740e-01],\n        [ 2.6567e+00, -3.6962e-01],\n        [ 3.3376e+00,  1.2647e+00],\n        [ 3.2425e+00,  1.8737e-01],\n        [ 2.2136e+00,  8.0368e-01],\n        [ 2.8200e+00,  8.3222e-01],\n        [ 2.8947e+00,  1.2718e+00],\n        [ 2.3586e+00,  4.8772e-02],\n        [ 4.2629e+00, -7.9319e-01],\n        [ 1.0194e+00,  5.2689e-01],\n        [ 3.5836e+00, -3.9113e-01],\n        [ 1.2598e+00,  2.9110e-02],\n        [ 3.1182e+00, -4.5907e-01],\n        [ 3.1940e+00,  6.2111e-01],\n        [ 2.0449e+00,  3.9258e-01],\n        [ 2.2271e+00,  2.2709e+00],\n        [ 3.6268e+00, -1.2029e+00],\n        [ 4.0089e+00,  2.9437e-01],\n        [ 5.8396e-01, -1.1297e-01],\n        [ 4.0875e+00, -2.4683e-01],\n        [ 1.8525e+00,  9.8961e-01],\n        [ 2.4258e+00,  1.5695e-01],\n        [ 3.2627e+00, -1.1159e+00],\n        [ 2.9474e+00, -6.0164e-01],\n        [ 1.1665e+00, -2.1653e-01],\n        [ 3.7017e+00,  1.4255e+00],\n        [ 3.4385e+00,  1.3058e-01],\n        [ 3.5547e+00,  6.9808e-01],\n        [ 2.6846e+00, -3.4181e-02],\n        [ 1.4556e+00, -3.8778e-01],\n        [ 2.8339e+00, -1.3898e+00],\n        [ 2.0918e+00,  1.9080e+00],\n        [ 3.0725e+00,  1.6244e+00],\n        [ 1.3910e+00, -1.8840e+00],\n        [-1.1841e+00, -5.4764e-01],\n        [ 3.7836e+00,  1.1325e+00],\n        [ 1.6406e+00,  1.3465e+00],\n        [ 9.4995e-01, -6.6508e-01],\n        [ 2.4508e+00,  1.8686e-01],\n        [ 1.7199e+00, -5.3127e-01],\n        [ 3.7247e+00,  1.4112e-01],\n        [ 2.3814e+00,  1.7017e+00],\n        [ 1.4487e+00, -1.3886e-01],\n        [ 1.5584e+00, -1.8177e+00],\n        [ 2.0500e+00, -5.5059e-01],\n        [ 3.0220e+00,  6.5052e-01],\n        [ 4.3486e+00, -9.5523e-01],\n        [ 4.4805e+00, -2.4870e-01],\n        [ 2.5571e+00, -5.0547e-01],\n        [ 3.4344e+00, -3.7785e-01],\n        [ 1.0395e+00, -1.4474e+00],\n        [ 3.3938e+00, -2.5924e+00],\n        [ 2.7806e+00, -7.8692e-01],\n        [ 4.1686e+00, -2.1921e+00],\n        [ 3.2572e+00,  2.7586e+00],\n        [ 1.1701e+00, -3.7402e-01],\n        [ 2.6859e+00,  1.5745e+00],\n        [ 1.1426e+00, -8.9663e-02],\n        [ 1.0716e+00, -1.6153e-01],\n        [ 2.2919e+00, -6.5030e-02],\n        [ 2.2787e+00, -1.1478e+00],\n        [ 3.0295e+00, -1.0083e+00],\n        [ 2.5662e+00,  3.2678e-01],\n        [ 1.7469e+00,  3.7807e-01],\n        [ 1.9634e+00,  1.9165e+00],\n        [ 2.7492e+00,  4.4846e-02],\n        [ 1.9418e+00,  5.2567e-01],\n        [ 1.3498e+00, -3.4796e-01],\n        [ 2.7318e+00, -6.4319e-01],\n        [ 2.5183e-01,  1.9270e-01],\n        [ 1.7289e+00,  3.0860e-01],\n        [ 3.7901e+00,  2.8976e-01],\n        [ 2.8722e+00, -5.5821e-01],\n        [ 1.6035e+00,  3.1917e-01],\n        [ 6.8396e-01, -8.5179e-02],\n        [ 7.5286e-01,  4.2447e-01],\n        [ 2.3101e+00,  9.3672e-02],\n        [ 3.4840e+00, -6.3842e-01],\n        [ 2.4712e+00,  6.8801e-01],\n        [ 3.1352e+00, -2.5935e-01],\n        [ 2.6109e+00, -1.5130e+00],\n        [ 2.5449e+00,  6.3307e-01],\n        [ 2.1482e+00, -3.1387e-01],\n        [ 8.2475e-02,  1.6038e-01],\n        [ 2.2745e+00, -1.0115e+00],\n        [ 2.0451e+00, -9.1123e-01],\n        [ 1.6348e+00,  1.6469e-01],\n        [ 9.7812e-01, -6.2545e-01],\n        [ 7.4635e-01, -2.6317e+00],\n        [ 1.8412e+00, -2.6821e-01],\n        [ 1.7233e+00,  4.0174e-01],\n        [ 2.2724e+00, -5.3195e-01],\n        [ 9.3418e-01, -4.4156e-01],\n        [ 1.7260e+00,  1.4701e+00],\n        [ 6.9052e-01,  1.1887e+00],\n        [ 2.1306e+00,  1.6791e+00],\n        [ 3.2164e+00,  5.9150e-01],\n        [ 2.2410e+00, -1.8031e-01],\n        [ 3.6494e+00,  5.2694e-01],\n        [ 2.0607e+00,  7.2014e-02],\n        [ 2.6238e+00,  2.3233e-01],\n        [ 2.2781e+00,  4.9652e-01],\n        [ 3.0273e+00,  1.7531e-01],\n        [ 4.9635e+00, -1.0556e-01],\n        [ 6.3580e-01, -6.2600e-01],\n        [ 2.0838e+00,  5.5702e-01],\n        [ 1.1163e+00, -9.4582e-01],\n        [ 2.9684e+00, -1.2205e-01],\n        [ 6.7835e-01,  6.1673e-01],\n        [ 3.6828e+00, -7.1154e-02],\n        [ 4.2096e+00, -2.4291e+00],\n        [ 3.0955e+00, -1.2460e+00],\n        [ 2.5100e+00,  3.6756e-01],\n        [ 3.2601e+00, -7.0806e-01],\n        [ 1.9557e+00,  1.7227e+00],\n        [ 2.8571e+00, -5.6653e-01],\n        [ 3.0052e+00,  1.0519e+00],\n        [ 2.5309e+00, -2.5223e+00],\n        [ 3.1989e+00,  4.0976e-04],\n        [-2.0361e-01, -2.5346e-01],\n        [ 2.0477e+00,  2.2585e-01],\n        [ 2.4178e+00,  2.0193e+00],\n        [ 2.3626e+00, -4.6282e-01],\n        [ 2.6121e+00, -5.9601e-02],\n        [ 2.3783e+00,  7.2936e-01],\n        [ 3.2522e+00,  1.6154e-01],\n        [ 8.2277e-01,  2.2641e+00],\n        [ 2.5473e+00,  7.8647e-01],\n        [ 3.9047e-01, -2.3478e-01],\n        [ 2.0419e+00, -6.0717e-01],\n        [ 3.4612e+00, -1.5166e+00],\n        [ 2.4649e+00,  2.1185e+00],\n        [ 3.1327e+00,  9.5875e-01],\n        [ 1.8567e+00, -7.9201e-01],\n        [ 1.9120e+00,  1.6978e+00],\n        [ 5.9917e-01, -1.3818e+00],\n        [ 2.0144e+00, -8.9961e-01],\n        [ 2.9634e+00,  1.7454e+00],\n        [ 2.3454e+00, -1.4828e+00],\n        [ 1.2853e+00, -1.3923e+00],\n        [ 9.5396e-01,  5.3710e-02],\n        [ 1.9307e+00,  3.1064e-01],\n        [ 3.3113e+00,  2.4639e+00],\n        [ 2.0108e+00, -1.3567e-01],\n        [ 2.8946e+00,  4.3050e-01],\n        [ 2.4954e+00,  4.2148e-02],\n        [ 2.1089e+00,  3.4401e-02],\n        [ 8.2967e-01, -5.1864e-01],\n        [ 2.4063e+00,  9.5084e-01],\n        [ 2.6328e+00, -3.3959e-01],\n        [ 2.7715e+00, -3.1970e-01],\n        [ 1.9837e+00,  1.3082e+00],\n        [ 4.0659e+00, -1.6417e+00],\n        [ 3.3121e+00,  2.1338e-01],\n        [ 1.9931e+00, -2.8100e+00],\n        [ 3.8701e+00,  1.4516e+00],\n        [ 1.4317e+00,  1.3186e+00],\n        [ 3.3456e+00, -6.7934e-01],\n        [ 2.3322e+00,  1.8209e+00],\n        [ 2.8705e+00,  6.2054e-01],\n        [ 2.7556e+00,  2.1015e+00],\n        [ 2.8716e+00, -1.2385e+00],\n        [ 1.7397e+00, -3.3432e-01],\n        [ 2.1377e+00, -2.4455e+00],\n        [ 4.1102e+00,  5.4473e-01],\n        [ 1.4972e+00,  1.1803e+00],\n        [ 4.1288e+00,  1.0342e+00],\n        [ 3.6850e+00, -1.2111e+00],\n        [ 1.9250e+00, -6.1972e-01],\n        [ 2.3777e+00,  1.5009e+00],\n        [ 1.3891e+00, -1.2208e-01],\n        [ 2.8675e+00, -3.9024e-01],\n        [ 9.2281e-01,  1.2407e+00],\n        [ 4.0773e+00,  4.9057e-01],\n        [ 3.7077e+00,  1.2386e+00],\n        [ 3.3312e-02, -1.7263e+00],\n        [ 2.0699e+00, -8.1459e-01],\n        [ 2.0271e+00,  1.1151e+00],\n        [ 2.7548e+00,  9.4252e-01],\n        [ 2.0984e+00,  4.0350e-01],\n        [ 2.5181e+00,  4.1609e-01],\n        [ 2.1296e+00, -3.8541e-02],\n        [ 4.1465e+00, -2.3936e-01],\n        [ 1.9372e+00, -1.3744e+00],\n        [ 1.2483e+00,  3.3924e-01],\n        [ 1.4533e+00, -1.5256e+00],\n        [ 1.5340e+00,  2.0843e-01],\n        [ 2.7496e-01, -2.9180e-01],\n        [ 5.6772e-01, -1.5792e+00],\n        [ 8.7363e-01, -1.5101e-02],\n        [ 2.5951e+00, -9.9796e-01],\n        [ 3.4414e+00,  4.6904e-01],\n        [ 2.3085e+00,  4.8992e-01],\n        [ 3.0663e+00, -5.9098e-01],\n        [ 2.3983e-01, -1.8970e-02],\n        [ 3.3180e+00, -2.3160e+00],\n        [ 1.5002e+00, -1.4109e+00],\n        [ 2.5054e+00, -4.6364e-01],\n        [ 4.1813e+00, -2.2486e-01],\n        [ 3.6030e+00,  2.7145e-02],\n        [ 3.0994e+00, -3.6393e+00],\n        [ 2.1093e+00,  6.4116e-01],\n        [ 2.9506e+00,  1.1066e-01],\n        [ 2.3167e+00, -1.7617e+00],\n        [ 3.0502e+00, -9.8700e-01],\n        [ 1.5604e+00, -1.6374e+00],\n        [ 3.1633e+00, -6.1596e-01],\n        [ 2.5201e+00, -2.3338e-01],\n        [ 2.9903e+00, -8.3184e-01],\n        [ 9.3266e-01, -3.2016e-01],\n        [ 2.6155e+00,  1.6472e+00],\n        [ 2.5687e+00,  3.4367e-01],\n        [ 2.5496e+00,  1.1961e+00],\n        [ 8.1697e-01,  3.3816e-01],\n        [ 2.6948e+00, -6.4277e-01],\n        [ 3.2429e+00, -1.5001e+00],\n        [ 1.6556e+00, -3.3857e-01],\n        [ 3.4679e+00, -6.9329e-01],\n        [ 1.4590e+00, -6.9176e-01],\n        [ 2.7890e+00, -1.0634e+00],\n        [ 4.3553e+00,  7.7961e-01],\n        [ 2.8217e+00,  1.9112e-01],\n        [ 2.5173e+00, -1.3542e+00],\n        [ 3.5408e+00,  6.7350e-01],\n        [ 1.2055e+00, -8.3769e-01],\n        [ 2.6817e+00, -2.1447e+00],\n        [ 2.7979e+00, -1.7105e-01],\n        [ 3.9159e-01, -2.7754e-01],\n        [ 1.3383e+00, -2.2139e+00],\n        [ 1.2002e+00, -1.6005e-01],\n        [ 1.6731e+00, -6.7375e-02],\n        [ 1.8941e+00, -2.0555e-01],\n        [ 1.5957e+00,  6.2620e-01],\n        [ 3.1577e+00, -2.5145e-01],\n        [ 3.7425e+00,  1.2068e+00],\n        [ 1.0511e+00,  1.5226e+00],\n        [ 3.4698e+00, -2.3224e+00],\n        [ 3.4744e+00, -3.0785e-01],\n        [ 2.1409e+00, -1.7861e+00],\n        [ 2.6594e+00, -1.8287e+00],\n        [ 3.7606e+00, -6.1912e-01],\n        [ 2.5626e+00,  3.5665e-01],\n        [ 2.2332e+00,  6.1253e-01],\n        [ 2.3171e+00, -1.4612e-01],\n        [ 3.4204e+00,  8.4991e-01],\n        [ 8.9755e-01,  1.3082e-03]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[3.5359e-05, 1.1546e-02, 2.5151e-02,  ..., 9.7997e-01, 9.9029e-01,\n         1.0000e+00],\n        [1.0394e-01, 1.2660e-01, 1.3062e-01,  ..., 9.8731e-01, 9.9985e-01,\n         1.0000e+00],\n        [2.5046e-01, 2.5098e-01, 2.5924e-01,  ..., 8.8428e-01, 9.9961e-01,\n         1.0000e+00],\n        ...,\n        [2.9732e-10, 4.3704e-03, 3.8974e-02,  ..., 9.9837e-01, 1.0000e+00,\n         1.0000e+00],\n        [4.4452e-04, 4.0721e-02, 4.7601e-02,  ..., 9.9529e-01, 9.9656e-01,\n         1.0000e+00],\n        [6.0668e-04, 1.2737e-03, 2.7009e-02,  ..., 9.9817e-01, 9.9830e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.7183],\n        [0.8884],\n        [0.9204],\n        [0.9044],\n        [0.7173],\n        [0.9545],\n        [0.8702],\n        [0.4546],\n        [0.3683],\n        [0.9843],\n        [0.6255],\n        [0.6919],\n        [0.4114],\n        [0.8414],\n        [0.5259],\n        [0.1235],\n        [0.5509],\n        [0.0322],\n        [0.0039],\n        [0.6811],\n        [0.6942],\n        [0.0300],\n        [0.7092],\n        [0.5935],\n        [0.4279],\n        [0.3517],\n        [0.1826],\n        [0.6736],\n        [0.7949],\n        [0.6249],\n        [0.6748],\n        [0.8504],\n        [0.0145],\n        [0.1943],\n        [0.6319],\n        [0.8012],\n        [0.9693],\n        [0.7432],\n        [0.5097],\n        [0.2447],\n        [0.6019],\n        [0.3043],\n        [0.9024],\n        [0.9269],\n        [0.6470],\n        [0.6828],\n        [0.9043],\n        [0.7321],\n        [0.3051],\n        [0.7009],\n        [0.5421],\n        [0.6070],\n        [0.4153],\n        [0.5438],\n        [0.2129],\n        [0.8491],\n        [0.7330],\n        [0.0821],\n        [0.1488],\n        [0.4460],\n        [0.1419],\n        [0.9111],\n        [0.2834],\n        [0.2098],\n        [0.7604],\n        [0.5494],\n        [0.6953],\n        [0.8331],\n        [0.0760],\n        [0.4436],\n        [0.9004],\n        [0.1254],\n        [0.4044],\n        [0.7094],\n        [0.2039],\n        [0.4801],\n        [0.4188],\n        [0.1895],\n        [0.7621],\n        [0.9805],\n        [0.2420],\n        [0.9029],\n        [0.0345],\n        [0.4329],\n        [0.4400],\n        [0.5737],\n        [0.0743],\n        [0.6102],\n        [0.7617],\n        [0.4295],\n        [0.7591],\n        [0.1323],\n        [0.4869],\n        [0.1787],\n        [0.2815],\n        [0.4880],\n        [0.4920],\n        [0.3439],\n        [0.8026],\n        [0.7721],\n        [0.6984],\n        [0.3893],\n        [0.6297],\n        [0.3003],\n        [0.0133],\n        [0.7733],\n        [0.9877],\n        [0.6765],\n        [0.2951],\n        [0.7468],\n        [0.1348],\n        [0.0134],\n        [0.4897],\n        [0.2932],\n        [0.7232],\n        [0.4018],\n        [0.5172],\n        [0.1804],\n        [0.8243],\n        [0.0110],\n        [0.7214],\n        [0.0160],\n        [0.9690],\n        [0.0661],\n        [0.8582],\n        [0.2377],\n        [0.1282],\n        [0.4168],\n        [0.8929],\n        [0.4066],\n        [0.8175],\n        [0.2726],\n        [0.7539],\n        [0.9247],\n        [0.7088],\n        [0.0013],\n        [0.6706],\n        [0.9934],\n        [0.4176],\n        [0.7449],\n        [0.3566],\n        [0.7014],\n        [0.9404],\n        [0.2896],\n        [0.8476],\n        [0.1200],\n        [0.2913],\n        [0.5076],\n        [0.7594],\n        [0.2302],\n        [0.2065],\n        [0.1495],\n        [0.9981],\n        [0.1863],\n        [0.7272],\n        [0.9307],\n        [0.4636],\n        [0.5078],\n        [0.9957],\n        [0.4482],\n        [0.1667],\n        [0.9992],\n        [0.4733],\n        [0.7781],\n        [0.9093],\n        [0.2527],\n        [0.9302],\n        [0.2235],\n        [0.0994],\n        [0.5801],\n        [0.1998],\n        [0.5693],\n        [0.8167],\n        [0.2685],\n        [0.8435],\n        [0.7218],\n        [0.5904],\n        [0.6950],\n        [0.3159],\n        [0.3219],\n        [0.6366],\n        [0.2415],\n        [0.7840],\n        [0.1577],\n        [0.4582],\n        [0.4219],\n        [0.7288],\n        [0.4303],\n        [0.6708],\n        [0.7839],\n        [0.7972],\n        [0.9412],\n        [0.9614],\n        [0.9448],\n        [0.6805],\n        [0.1511],\n        [0.1183],\n        [0.2485],\n        [0.9604],\n        [0.9384],\n        [0.2819],\n        [0.9976],\n        [0.8374],\n        [0.5731],\n        [0.8243],\n        [0.8032],\n        [0.2161],\n        [0.9426],\n        [0.9461],\n        [0.3661],\n        [0.7982],\n        [0.6275],\n        [0.4923],\n        [0.0047],\n        [0.9961],\n        [0.3320],\n        [0.4009],\n        [0.2706],\n        [0.0388],\n        [0.0431],\n        [0.8954],\n        [0.6620],\n        [0.6674],\n        [0.5645],\n        [0.0198],\n        [0.8694],\n        [0.0130],\n        [0.5212],\n        [0.7630],\n        [0.4569],\n        [0.1089],\n        [0.0243],\n        [0.0927],\n        [0.7604],\n        [0.8162],\n        [0.8061],\n        [0.7696],\n        [0.2705],\n        [0.5888],\n        [0.4556],\n        [0.6378],\n        [0.3707],\n        [0.5437],\n        [0.1346],\n        [0.4670],\n        [0.5126],\n        [0.0649],\n        [0.8202],\n        [0.7022],\n        [0.6246],\n        [0.9776],\n        [0.1892],\n        [0.1788],\n        [0.9226],\n        [0.8081],\n        [0.5404],\n        [0.7209],\n        [0.3352],\n        [0.1450],\n        [0.1594],\n        [0.8688],\n        [0.6550],\n        [0.7857],\n        [0.1463],\n        [0.1521],\n        [0.8446],\n        [0.8691],\n        [0.0134],\n        [0.7356],\n        [0.7981],\n        [0.2212],\n        [0.8324],\n        [0.6278],\n        [0.4123],\n        [0.7467],\n        [0.8659],\n        [0.5169],\n        [0.6889],\n        [0.9054],\n        [0.2225],\n        [0.4377],\n        [0.2263],\n        [0.5676],\n        [0.8674],\n        [0.4071],\n        [0.1896],\n        [0.9522],\n        [0.1551],\n        [0.1211],\n        [0.6566],\n        [0.0611],\n        [0.8046],\n        [0.1334],\n        [0.2200],\n        [0.4404],\n        [0.9311],\n        [0.8640],\n        [0.1579],\n        [0.3940],\n        [0.3332],\n        [0.0968],\n        [0.9941],\n        [0.0157],\n        [0.5711],\n        [0.3592],\n        [0.9216],\n        [0.1751],\n        [0.5038],\n        [0.2176],\n        [0.5646],\n        [0.8435],\n        [0.2960]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False,  True, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-1.6479,  0.2295],\n        [ 1.6360,  2.3120],\n        [ 1.2292,  1.6280],\n        ...,\n        [ 2.7354,  1.8032],\n        [-1.1907,  0.7326],\n        [ 0.0988,  2.3570]]) torch.Size([9984, 2])\nsamples tensor([[ 3.4567e+00,  1.4552e+00],\n        [ 3.6880e+00, -7.9217e-03],\n        [ 2.0414e+00,  3.0894e-01],\n        [ 3.4730e+00, -4.8907e-02],\n        [ 2.5830e+00, -6.3850e-01],\n        [ 2.0810e+00,  1.1815e+00],\n        [ 2.9867e+00, -9.0312e-01],\n        [ 2.7644e+00, -1.9348e+00],\n        [ 4.3073e+00, -5.0855e-01],\n        [ 3.6008e+00,  5.8360e-01],\n        [ 2.2534e+00,  8.2204e-01],\n        [ 4.5944e+00, -5.3046e-02],\n        [ 3.6310e+00, -1.8216e+00],\n        [ 2.4995e+00, -8.2070e-01],\n        [ 2.1637e+00, -4.9929e-01],\n        [ 3.2540e+00, -4.7367e-01],\n        [ 2.7957e+00,  1.0014e+00],\n        [ 3.0897e+00,  3.7827e-01],\n        [ 6.2831e-01,  9.5748e-01],\n        [ 1.8633e+00,  6.0844e-01],\n        [ 2.6460e+00,  4.8831e-01],\n        [ 1.9909e+00,  8.6637e-02],\n        [ 2.7587e+00, -1.1915e+00],\n        [ 2.4349e+00, -6.8945e-01],\n        [ 1.3273e+00, -4.1693e-01],\n        [ 2.0877e+00,  6.0859e-01],\n        [ 2.4801e+00, -3.3452e-01],\n        [ 3.3328e+00, -6.6265e-01],\n        [ 2.0874e+00, -3.9942e-01],\n        [ 3.9317e+00,  2.8289e-02],\n        [ 1.2700e+00, -5.8706e-02],\n        [ 2.5360e+00, -4.7741e-01],\n        [ 2.7243e+00, -1.0269e-01],\n        [ 2.3190e+00, -9.5850e-01],\n        [ 2.4744e+00, -3.1946e-02],\n        [ 3.7091e+00,  1.1333e-01],\n        [ 2.8754e+00,  1.3256e+00],\n        [ 1.2078e+00,  8.1279e-01],\n        [ 2.0134e+00, -1.4129e+00],\n        [ 1.0271e+00,  8.5834e-01],\n        [ 2.4082e+00, -1.8034e+00],\n        [ 2.8926e+00, -6.2979e-01],\n        [ 1.0055e+00, -6.2087e-01],\n        [ 2.7686e+00,  4.6082e-01],\n        [ 2.6699e+00,  1.3944e-01],\n        [ 2.9992e+00, -1.7060e+00],\n        [ 2.2328e+00,  9.0794e-01],\n        [ 1.5426e+00, -2.2851e-01],\n        [ 3.0517e+00, -7.3135e-01],\n        [ 1.4042e+00, -7.7765e-01],\n        [ 1.9145e+00,  1.1145e+00],\n        [ 3.0858e+00, -9.4590e-01],\n        [ 2.3511e+00,  7.4137e-01],\n        [ 2.0187e+00,  1.4195e-01],\n        [ 2.2710e+00, -1.4541e+00],\n        [ 1.8082e+00, -2.9937e-01],\n        [ 3.2200e+00, -1.5378e+00],\n        [ 1.3461e+00,  1.9496e-01],\n        [ 3.9020e+00, -6.6685e-02],\n        [ 2.9293e+00,  1.1790e+00],\n        [ 3.1140e-02, -1.1065e+00],\n        [ 3.1809e+00, -2.9335e+00],\n        [ 1.6052e+00,  6.9121e-01],\n        [ 1.3936e+00, -2.2921e+00],\n        [ 1.6073e+00,  7.1867e-01],\n        [ 1.4179e+00, -1.9944e-01],\n        [ 1.6062e+00,  8.9394e-01],\n        [ 2.6682e+00,  8.9195e-01],\n        [ 2.1135e+00,  1.2602e+00],\n        [ 4.1896e+00, -6.6332e-01],\n        [ 2.4490e+00, -2.7805e-01],\n        [ 1.7273e+00,  5.0680e-01],\n        [ 2.8745e+00,  1.2989e+00],\n        [ 3.8832e+00, -1.4613e+00],\n        [ 1.2119e+00,  8.5432e-01],\n        [ 2.1650e+00,  7.2547e-01],\n        [ 1.8735e+00,  7.4634e-02],\n        [ 3.6024e+00,  6.0463e-02],\n        [ 3.1282e+00, -1.5305e+00],\n        [ 2.0608e+00,  8.5730e-01],\n        [ 2.8126e+00,  1.2137e+00],\n        [ 1.3809e+00, -2.5495e-01],\n        [ 2.3055e+00,  1.8283e+00],\n        [ 5.6272e-01, -6.2259e-01],\n        [ 4.2258e+00, -9.8498e-01],\n        [ 2.6642e+00,  1.4029e+00],\n        [ 1.1659e+00, -9.8209e-01],\n        [ 4.1166e+00,  7.2524e-01],\n        [ 3.4693e+00, -1.6652e-01],\n        [ 1.7874e+00,  1.6716e+00],\n        [ 3.1510e+00, -1.1016e-01],\n        [ 3.1089e+00, -1.3060e+00],\n        [ 3.2771e+00, -1.9388e-01],\n        [ 1.2172e+00, -2.3747e-01],\n        [ 2.8754e+00, -2.4995e-01],\n        [ 2.3591e+00, -1.1767e-01],\n        [ 1.0838e+00, -1.4866e+00],\n        [ 3.1896e+00, -1.3312e+00],\n        [ 2.9611e+00, -7.0170e-01],\n        [ 1.1280e+00, -6.4487e-01],\n        [ 3.3393e+00,  5.5501e-02],\n        [ 2.0544e+00, -2.0575e-01],\n        [ 3.5186e+00, -5.7861e-01],\n        [ 3.4917e+00, -1.1094e+00],\n        [ 1.8449e+00,  2.7385e+00],\n        [ 4.6278e-01, -3.5952e-01],\n        [ 2.1236e+00,  1.2994e+00],\n        [ 3.1620e+00, -1.9652e-01],\n        [ 2.7519e+00, -1.5726e+00],\n        [ 2.6756e+00,  4.4851e-01],\n        [ 2.3067e+00,  1.3583e-01],\n        [ 1.0474e+00,  4.9125e-01],\n        [ 3.1751e+00, -7.9804e-01],\n        [ 1.0555e+00,  1.2564e-01],\n        [ 2.5015e+00, -1.8700e+00],\n        [ 3.8245e+00,  1.6533e+00],\n        [ 2.8703e+00,  5.3258e-01],\n        [ 1.4497e+00, -9.6449e-01],\n        [ 2.2484e+00,  4.3679e-02],\n        [ 1.9433e+00,  1.2060e+00],\n        [ 1.0716e+00,  6.1291e-01],\n        [ 3.4174e+00,  1.4179e+00],\n        [ 2.9928e+00,  1.6830e+00],\n        [ 2.8030e+00,  1.1639e+00],\n        [ 2.6710e+00,  1.1518e+00],\n        [ 8.1590e-01,  1.9542e-01],\n        [ 4.0667e+00, -3.5272e-01],\n        [ 2.4218e+00, -5.4849e-01],\n        [ 3.7088e+00,  3.3172e-01],\n        [ 2.7684e+00, -9.8245e-01],\n        [ 2.1047e+00, -1.8105e+00],\n        [ 3.5556e+00, -2.7479e-01],\n        [ 2.8371e+00,  2.1823e-01],\n        [ 2.6569e+00, -2.2819e-01],\n        [ 2.3944e+00, -7.7268e-01],\n        [ 4.0278e+00,  1.6355e+00],\n        [ 3.7609e+00, -1.9487e+00],\n        [ 2.9697e+00,  6.7132e-01],\n        [ 3.7890e+00,  4.2191e-01],\n        [ 3.4264e+00, -5.6567e-01],\n        [ 1.8488e+00, -7.6448e-01],\n        [ 1.7550e+00,  1.1755e+00],\n        [ 1.4880e+00, -3.6590e-01],\n        [ 2.8810e+00, -1.9533e-01],\n        [ 1.7867e+00, -1.0726e-01],\n        [ 1.0066e+00, -9.9895e-01],\n        [ 2.1830e+00, -1.2036e+00],\n        [ 3.4851e+00, -8.6662e-01],\n        [ 2.9829e+00, -1.7051e-01],\n        [ 4.5185e+00, -2.1583e+00],\n        [ 3.0285e+00,  2.4131e-01],\n        [ 1.1826e+00, -8.0249e-01],\n        [ 3.1989e-01,  3.3927e-01],\n        [ 2.8652e+00, -1.4373e+00],\n        [ 1.9210e+00, -1.2904e+00],\n        [ 3.0353e+00, -4.4095e-01],\n        [ 2.0886e+00, -2.7746e-01],\n        [ 3.3877e+00, -4.7921e-02],\n        [ 3.3462e+00,  1.0043e+00],\n        [ 2.1396e+00, -6.8547e-01],\n        [ 3.9256e+00,  9.7077e-01],\n        [ 2.0227e+00, -4.9915e-01],\n        [ 2.9927e+00,  6.7816e-02],\n        [ 1.5613e+00, -1.1958e+00],\n        [ 2.7560e+00,  1.7392e-01],\n        [ 3.2055e+00, -1.4444e+00],\n        [ 2.5683e+00, -2.0444e-01],\n        [ 3.5348e+00, -2.0684e+00],\n        [ 1.6146e+00,  2.3813e-01],\n        [ 3.4020e+00,  6.0926e-01],\n        [ 3.5298e-01, -6.8941e-01],\n        [ 1.7731e+00, -8.7128e-01],\n        [ 2.2136e+00,  8.3606e-01],\n        [ 2.0121e+00, -5.1170e-01],\n        [ 2.6084e+00, -2.6417e+00],\n        [ 1.5830e+00,  3.2409e-01],\n        [ 1.1758e+00,  5.9727e-01],\n        [ 2.8347e+00, -1.0514e+00],\n        [ 2.0098e+00,  2.2255e-01],\n        [ 3.4880e+00, -6.6496e-01],\n        [ 2.2982e+00,  1.0240e-01],\n        [ 2.5768e+00, -1.2480e-01],\n        [ 3.9382e-01, -1.0288e+00],\n        [ 1.9240e+00, -9.6880e-01],\n        [ 3.1923e+00, -1.3479e+00],\n        [ 1.3444e+00, -1.0048e+00],\n        [ 1.6494e+00,  2.3344e-01],\n        [ 2.7901e+00, -2.0152e+00],\n        [ 4.1251e+00, -1.9675e-01],\n        [ 1.6127e+00,  1.9279e-01],\n        [ 2.1041e+00,  3.1719e-01],\n        [ 3.0326e+00, -2.2220e+00],\n        [ 7.5553e-01, -5.5758e-01],\n        [ 2.3456e+00, -7.8179e-01],\n        [ 2.6391e+00, -9.0838e-01],\n        [ 2.3379e+00, -1.6884e+00],\n        [ 2.8003e+00,  1.1005e+00],\n        [ 5.4523e-01, -6.2320e-01],\n        [ 2.3319e+00,  5.3285e-01],\n        [ 1.8868e+00,  6.6219e-02],\n        [ 7.6646e-01, -4.7930e-01],\n        [ 2.1187e+00,  9.8558e-01],\n        [ 2.6901e+00, -3.9654e-01],\n        [ 1.3061e+00,  3.7091e-02],\n        [ 3.1959e+00,  4.0031e-01],\n        [ 4.8619e-01,  8.3730e-01],\n        [ 4.4244e-01, -4.2380e-01],\n        [ 3.8376e+00,  9.1702e-01],\n        [ 2.9997e+00,  7.3122e-01],\n        [ 2.2580e+00, -6.7977e-01],\n        [ 3.5842e+00,  1.0513e+00],\n        [ 1.7834e+00, -1.6751e-01],\n        [ 2.8751e+00,  1.0499e-01],\n        [ 1.5042e+00,  1.4758e-01],\n        [ 2.3134e+00,  8.1429e-01],\n        [ 4.2056e+00,  8.1628e-02],\n        [ 3.3664e+00,  6.6222e-01],\n        [ 1.5002e+00,  6.7311e-01],\n        [ 4.4449e+00,  9.8825e-01],\n        [ 3.7455e+00,  1.3324e+00],\n        [ 3.1724e+00,  6.3259e-01],\n        [ 1.7658e+00, -1.3258e+00],\n        [ 2.7334e+00, -4.8480e-01],\n        [ 3.5234e+00, -7.9773e-01],\n        [ 7.2987e-01, -4.4223e-01],\n        [ 4.4623e+00, -1.1644e-01],\n        [ 1.8294e+00,  1.1774e+00],\n        [ 1.0275e+00, -1.2237e+00],\n        [ 6.7150e-01, -2.3626e-01],\n        [ 3.8931e+00, -9.9404e-01],\n        [ 3.3560e+00,  1.3699e+00],\n        [ 1.4816e-01, -4.7799e-01],\n        [-6.1734e-01,  3.4413e-01],\n        [ 2.4861e+00, -1.2015e+00],\n        [ 2.3389e+00,  2.9245e-02],\n        [ 2.1249e+00,  4.6333e-02],\n        [ 3.2716e+00, -3.1660e+00],\n        [ 1.1157e+00, -3.5521e-01],\n        [ 2.2849e+00, -6.4577e-01],\n        [ 2.2599e+00, -1.8498e-01],\n        [ 1.9888e+00,  1.1764e+00],\n        [ 2.0994e+00, -1.3519e+00],\n        [ 1.8353e+00, -4.3618e-02],\n        [ 1.1594e+00,  4.1946e-01],\n        [ 1.6679e+00,  1.9112e+00],\n        [ 4.5180e+00,  1.1139e-01],\n        [ 1.9325e+00,  1.8409e-01],\n        [ 4.0279e+00, -1.4250e-01],\n        [ 1.3287e+00,  8.2690e-02],\n        [ 1.0684e+00, -9.8048e-01],\n        [ 2.6053e+00,  2.4171e-01],\n        [ 2.1219e+00, -2.2376e-02],\n        [ 3.0078e+00,  2.7731e-01],\n        [ 1.3985e+00, -1.4289e-01],\n        [ 1.3338e+00, -1.1790e+00],\n        [ 1.9121e+00,  3.3354e-02],\n        [-3.1300e-01, -1.3686e+00],\n        [ 3.2818e+00, -1.6374e-01],\n        [ 2.3198e+00,  5.9873e-01],\n        [ 1.7958e+00, -4.2905e-01],\n        [ 1.7733e+00, -1.4257e+00],\n        [ 4.1008e+00, -7.3301e-01],\n        [ 2.6814e+00, -3.6600e-01],\n        [ 7.6710e-01,  2.8924e-01],\n        [ 1.8282e+00, -1.4030e+00],\n        [ 3.1065e+00, -2.1802e+00],\n        [ 2.9815e+00, -4.1414e-01],\n        [ 1.4501e+00, -1.9739e-01],\n        [ 3.0099e+00, -1.5725e-01],\n        [ 1.8753e+00, -4.9348e-01],\n        [ 2.5145e+00,  2.4194e-01],\n        [ 3.2844e+00,  2.2148e-01],\n        [ 1.9619e+00,  4.7683e-02],\n        [ 1.6683e+00, -6.1086e-02],\n        [ 2.3100e+00,  2.5753e-01],\n        [ 3.7590e+00, -1.8406e+00],\n        [ 1.4919e+00, -1.8438e+00],\n        [ 1.9487e+00,  6.0092e-01],\n        [ 1.5658e+00, -2.8678e+00],\n        [ 2.4781e+00,  1.6518e-01],\n        [ 1.7647e+00, -4.9345e-01],\n        [ 4.0584e+00, -1.7451e+00],\n        [ 2.8408e+00, -2.3626e+00],\n        [ 1.8063e+00, -1.1191e-01],\n        [ 2.7133e+00, -7.2946e-01],\n        [ 3.0024e+00,  1.2011e+00],\n        [ 2.1803e+00, -4.6080e-01],\n        [ 9.7589e-01,  1.8675e+00],\n        [ 1.7222e+00, -3.5779e-01],\n        [ 2.9970e+00, -2.4930e-01],\n        [ 4.0747e+00,  1.4268e+00],\n        [ 2.0343e+00,  1.6912e+00],\n        [ 2.7911e+00, -5.1468e-01],\n        [ 2.4613e+00, -1.7313e-02],\n        [ 1.6646e+00,  8.6131e-01],\n        [ 2.8848e+00, -2.2140e-01],\n        [ 2.2899e+00, -8.2249e-01],\n        [ 2.4406e+00, -1.5558e-01],\n        [ 2.8145e+00, -6.9653e-01],\n        [ 3.4887e+00, -3.5723e-01],\n        [ 3.7759e-01,  4.2599e-02],\n        [-2.7882e-01, -7.9728e-01],\n        [ 2.8178e+00,  5.9704e-01],\n        [ 2.2875e+00, -1.9701e+00],\n        [ 2.8397e+00, -1.1967e+00],\n        [ 1.6801e+00, -4.2064e-03],\n        [ 1.9752e+00, -1.7543e+00],\n        [ 3.1044e+00, -1.2175e+00],\n        [ 2.7227e+00,  9.9896e-01],\n        [ 2.6221e+00, -1.7397e+00],\n        [ 5.9340e-01, -1.2074e+00],\n        [ 1.7962e+00,  1.1241e+00]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[6.1802e-02, 6.1802e-02, 6.1803e-02,  ..., 8.1975e-01, 1.0000e+00,\n         1.0000e+00],\n        [8.2960e-05, 2.1281e-03, 1.0889e-02,  ..., 9.8656e-01, 9.8725e-01,\n         1.0000e+00],\n        [4.4877e-04, 3.7715e-03, 3.7902e-03,  ..., 9.6182e-01, 9.6827e-01,\n         1.0000e+00],\n        ...,\n        [8.1340e-02, 1.1658e-01, 1.1676e-01,  ..., 9.9770e-01, 1.0000e+00,\n         1.0000e+00],\n        [6.3426e-08, 1.7719e-02, 3.6272e-02,  ..., 9.9872e-01, 1.0000e+00,\n         1.0000e+00],\n        [9.6729e-04, 9.6772e-04, 1.2273e-02,  ..., 8.1724e-01, 8.6160e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[5.2613e-01],\n        [4.5516e-01],\n        [6.0646e-01],\n        [3.8505e-01],\n        [1.6708e-01],\n        [3.2892e-01],\n        [9.6692e-01],\n        [7.3104e-01],\n        [5.3326e-01],\n        [9.2708e-01],\n        [4.3891e-01],\n        [4.0451e-01],\n        [7.8770e-01],\n        [8.3291e-01],\n        [7.7638e-01],\n        [6.6582e-01],\n        [7.1125e-01],\n        [5.2091e-01],\n        [7.0823e-01],\n        [7.1671e-01],\n        [3.9634e-01],\n        [9.2987e-01],\n        [1.6078e-01],\n        [4.8171e-01],\n        [4.6960e-01],\n        [3.8201e-01],\n        [4.2938e-01],\n        [3.4550e-01],\n        [7.3630e-01],\n        [9.7434e-02],\n        [5.4260e-01],\n        [5.4021e-01],\n        [5.5701e-01],\n        [8.5849e-01],\n        [3.1799e-01],\n        [2.7855e-01],\n        [2.9003e-01],\n        [8.8759e-01],\n        [8.8032e-01],\n        [7.6009e-01],\n        [6.7659e-01],\n        [5.4887e-01],\n        [9.9868e-01],\n        [9.5407e-02],\n        [8.0853e-01],\n        [6.5139e-01],\n        [8.2822e-01],\n        [6.7177e-01],\n        [4.8873e-01],\n        [7.6757e-01],\n        [1.6804e-01],\n        [5.6033e-01],\n        [8.1107e-01],\n        [8.1845e-01],\n        [3.8168e-01],\n        [7.0205e-01],\n        [7.0323e-01],\n        [2.3525e-01],\n        [7.9486e-01],\n        [4.2244e-01],\n        [8.7368e-01],\n        [5.2165e-01],\n        [5.8594e-02],\n        [3.4994e-01],\n        [7.1120e-01],\n        [3.9287e-01],\n        [1.0837e-01],\n        [7.9898e-01],\n        [3.2039e-01],\n        [5.8402e-01],\n        [1.3706e-01],\n        [2.3139e-01],\n        [2.0292e-02],\n        [6.3401e-01],\n        [8.7265e-01],\n        [4.7800e-01],\n        [9.4546e-01],\n        [1.9345e-01],\n        [8.8259e-01],\n        [1.7264e-01],\n        [4.2886e-04],\n        [4.9556e-01],\n        [7.9077e-01],\n        [1.1977e-01],\n        [7.7246e-02],\n        [5.5694e-01],\n        [8.9517e-01],\n        [2.2661e-01],\n        [7.5578e-01],\n        [3.5797e-01],\n        [3.4410e-01],\n        [6.7766e-01],\n        [2.9110e-01],\n        [6.7305e-01],\n        [3.9525e-01],\n        [4.1215e-02],\n        [8.6284e-01],\n        [2.6184e-01],\n        [1.9036e-01],\n        [5.1805e-01],\n        [4.8990e-02],\n        [2.8267e-01],\n        [4.5338e-01],\n        [1.5260e-01],\n        [3.8499e-01],\n        [2.1344e-02],\n        [4.5066e-01],\n        [6.6112e-01],\n        [1.4513e-01],\n        [1.0997e-01],\n        [2.6934e-01],\n        [8.7892e-01],\n        [6.2314e-01],\n        [7.3754e-01],\n        [2.9562e-02],\n        [4.5966e-01],\n        [6.9227e-02],\n        [1.9237e-01],\n        [4.2342e-01],\n        [2.1241e-01],\n        [1.2164e-01],\n        [3.2897e-01],\n        [8.6006e-01],\n        [8.9637e-01],\n        [1.3514e-01],\n        [5.3308e-01],\n        [4.2366e-01],\n        [9.4112e-01],\n        [7.6135e-01],\n        [4.7405e-01],\n        [8.1826e-01],\n        [5.0854e-01],\n        [8.6876e-01],\n        [2.4601e-01],\n        [8.2600e-01],\n        [9.6243e-01],\n        [2.1640e-01],\n        [8.8086e-01],\n        [6.0001e-02],\n        [9.9842e-01],\n        [9.9259e-02],\n        [8.0165e-01],\n        [8.8724e-01],\n        [7.5733e-01],\n        [6.2691e-01],\n        [2.2041e-01],\n        [1.1253e-01],\n        [4.6847e-02],\n        [1.8515e-01],\n        [9.7336e-01],\n        [2.0384e-01],\n        [2.7578e-01],\n        [2.1542e-01],\n        [6.7180e-01],\n        [7.8105e-01],\n        [9.5787e-01],\n        [3.2830e-01],\n        [9.7462e-01],\n        [3.5882e-01],\n        [6.4222e-01],\n        [3.5741e-01],\n        [1.5435e-02],\n        [8.7004e-01],\n        [8.2177e-01],\n        [7.2508e-01],\n        [4.1000e-01],\n        [3.4572e-01],\n        [5.4057e-01],\n        [4.9183e-01],\n        [4.3581e-01],\n        [3.0924e-01],\n        [4.5954e-01],\n        [4.9231e-01],\n        [6.3539e-01],\n        [9.0735e-01],\n        [8.4775e-02],\n        [3.3372e-03],\n        [4.1116e-01],\n        [6.1050e-02],\n        [8.9116e-01],\n        [6.1395e-01],\n        [1.8212e-01],\n        [8.2007e-01],\n        [7.1532e-01],\n        [3.2195e-01],\n        [5.3967e-01],\n        [1.0153e-01],\n        [2.5049e-01],\n        [1.7490e-02],\n        [3.2963e-01],\n        [7.3881e-01],\n        [6.6731e-01],\n        [4.3842e-01],\n        [1.4249e-01],\n        [9.2297e-01],\n        [9.0623e-01],\n        [5.1757e-01],\n        [2.3453e-01],\n        [9.9178e-01],\n        [5.4343e-01],\n        [7.9141e-01],\n        [4.8173e-01],\n        [3.1005e-01],\n        [4.6247e-01],\n        [9.2919e-01],\n        [8.6323e-01],\n        [2.6390e-01],\n        [6.4234e-01],\n        [9.1376e-01],\n        [7.5121e-01],\n        [4.4745e-01],\n        [9.7219e-01],\n        [6.9291e-01],\n        [7.0572e-01],\n        [4.2523e-01],\n        [4.0466e-01],\n        [6.0642e-01],\n        [2.4068e-01],\n        [9.0618e-01],\n        [7.8630e-01],\n        [4.7038e-01],\n        [4.5283e-01],\n        [7.5427e-01],\n        [9.7650e-01],\n        [6.6197e-01],\n        [5.6345e-01],\n        [3.0408e-01],\n        [7.0103e-01],\n        [3.5544e-01],\n        [6.6308e-01],\n        [4.3735e-01],\n        [8.4075e-02],\n        [2.4705e-01],\n        [5.6926e-01],\n        [8.1321e-01],\n        [9.6026e-02],\n        [8.5656e-01],\n        [9.1918e-01],\n        [1.0983e-01],\n        [9.2366e-01],\n        [3.2051e-01],\n        [8.7737e-02],\n        [8.9286e-01],\n        [2.9622e-02],\n        [1.4830e-01],\n        [1.0815e-01],\n        [3.9350e-02],\n        [2.5077e-01],\n        [9.8488e-01],\n        [4.2937e-02],\n        [5.3616e-02],\n        [3.2361e-01],\n        [4.1761e-01],\n        [4.3419e-01],\n        [1.6032e-01],\n        [4.2656e-01],\n        [4.3096e-01],\n        [7.3966e-01],\n        [9.1456e-01],\n        [7.1302e-02],\n        [9.4614e-01],\n        [5.4972e-01],\n        [9.2154e-01],\n        [4.4624e-01],\n        [4.6394e-01],\n        [6.6167e-01],\n        [8.9759e-01],\n        [1.5267e-01],\n        [3.4882e-01],\n        [2.0467e-01],\n        [5.1082e-02],\n        [2.7043e-01],\n        [9.9900e-01],\n        [3.9846e-01],\n        [3.0331e-01],\n        [3.4011e-01],\n        [4.0827e-01],\n        [3.6855e-01],\n        [4.2120e-01],\n        [7.9991e-01],\n        [7.5055e-01],\n        [6.0239e-01],\n        [3.8838e-01],\n        [1.1033e-01],\n        [6.1403e-01],\n        [7.6617e-01],\n        [1.8745e-01],\n        [4.3858e-01],\n        [6.9873e-01],\n        [1.5091e-01],\n        [1.8192e-02],\n        [4.3818e-01],\n        [5.9954e-01],\n        [6.4163e-01],\n        [8.4388e-01],\n        [9.3404e-01],\n        [4.0058e-01],\n        [6.8101e-01],\n        [4.3303e-01],\n        [8.1436e-01],\n        [2.9779e-01],\n        [8.2584e-01],\n        [8.9297e-02],\n        [9.2222e-01],\n        [9.7618e-01],\n        [8.2668e-01],\n        [7.5478e-01],\n        [4.5382e-01],\n        [5.2496e-01],\n        [2.5056e-01],\n        [7.8926e-01],\n        [8.9486e-02]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 2.5857,  1.2894],\n        [-3.5413, -0.1099],\n        [-2.3857,  0.9843],\n        ...,\n        [ 0.9819,  1.1510],\n        [ 2.3765,  0.9025],\n        [ 3.1206, -2.2974]]) torch.Size([9984, 2])\nsamples tensor([[ 1.8616e+00, -8.8611e-01],\n        [ 2.6299e+00, -2.0551e+00],\n        [ 2.3886e+00, -8.1473e-01],\n        [ 2.8907e+00, -2.0013e+00],\n        [ 3.0510e+00,  7.0179e-01],\n        [ 2.2422e+00,  1.5500e+00],\n        [ 2.2505e+00,  9.3066e-01],\n        [ 1.8608e+00, -1.3454e+00],\n        [ 2.6543e+00,  3.5974e-01],\n        [ 6.4221e-01, -1.1444e+00],\n        [ 3.7129e+00, -1.3737e+00],\n        [ 2.1191e+00,  2.9416e-01],\n        [ 2.7738e+00, -9.6166e-01],\n        [ 1.3567e+00,  1.6037e+00],\n        [ 1.9312e+00,  5.7518e-01],\n        [ 1.9850e+00, -1.4156e+00],\n        [ 3.3750e+00, -2.4198e-01],\n        [ 2.8330e+00,  1.6518e+00],\n        [ 9.0365e-01, -1.4916e+00],\n        [ 3.1516e+00, -1.5109e+00],\n        [ 2.5307e+00, -1.0783e+00],\n        [ 1.9311e+00,  8.5311e-02],\n        [ 1.8698e+00,  1.4348e+00],\n        [ 2.2955e+00,  6.4547e-01],\n        [ 2.3617e+00, -3.0728e-03],\n        [ 4.0511e+00,  1.1091e+00],\n        [ 3.7999e+00, -2.5224e+00],\n        [ 1.8855e+00,  1.1737e+00],\n        [ 3.0169e+00, -4.6078e-02],\n        [ 3.3351e+00,  1.9427e-01],\n        [ 1.7693e+00, -1.7820e+00],\n        [ 4.8944e-01, -3.4079e-01],\n        [ 2.5427e+00, -1.1206e+00],\n        [ 3.0444e+00, -9.8259e-01],\n        [ 2.2243e+00, -3.2134e-01],\n        [ 2.3434e+00, -1.6976e+00],\n        [ 2.7639e+00, -3.5912e-01],\n        [ 2.2953e+00, -7.6107e-01],\n        [ 3.1205e+00, -8.0860e-01],\n        [ 1.8851e+00,  7.3198e-01],\n        [ 3.3835e+00,  5.5338e-01],\n        [ 2.1148e+00, -1.0710e+00],\n        [ 1.2014e+00, -3.2844e-01],\n        [ 3.0481e+00,  3.8131e-01],\n        [ 3.3904e+00, -1.5233e+00],\n        [ 1.7336e+00,  1.2556e-01],\n        [ 2.4449e+00, -1.6005e+00],\n        [ 2.0193e+00, -1.2045e+00],\n        [ 2.7704e+00,  3.1961e-01],\n        [ 3.6896e+00,  1.4188e+00],\n        [ 2.2914e+00, -1.1096e-01],\n        [ 2.0266e+00, -5.9804e-01],\n        [ 1.5071e+00, -3.7755e-01],\n        [ 7.1552e-01,  2.1808e-01],\n        [ 1.8347e+00, -1.0579e+00],\n        [ 2.0173e+00, -5.1040e-01],\n        [ 4.3750e+00, -1.3719e+00],\n        [ 4.9193e+00,  2.3747e-01],\n        [ 2.7146e+00, -1.1698e+00],\n        [ 3.2562e+00,  9.8469e-02],\n        [ 1.5885e+00, -2.9767e-01],\n        [ 3.2680e+00, -1.0582e+00],\n        [ 2.9435e-01,  1.0992e+00],\n        [ 2.5537e+00,  3.4574e-01],\n        [ 2.3961e+00, -4.1680e-01],\n        [ 2.8740e+00,  2.0035e+00],\n        [ 2.4535e+00, -1.3453e+00],\n        [ 2.8067e+00, -1.0811e+00],\n        [ 2.3146e+00, -3.8588e-01],\n        [ 3.2507e+00,  2.2033e-01],\n        [ 3.0089e+00,  1.2058e+00],\n        [ 3.4366e+00,  8.9391e-01],\n        [ 4.9147e-01, -3.4603e-01],\n        [ 1.9505e+00, -1.3849e+00],\n        [ 8.6355e-01,  1.2723e-01],\n        [ 6.6515e-01, -9.2696e-01],\n        [ 2.9750e+00,  1.4081e+00],\n        [ 1.2405e+00, -3.8644e-01],\n        [ 6.9251e-01, -5.9053e-01],\n        [ 2.7869e+00,  1.1530e+00],\n        [ 4.0224e+00, -7.4635e-01],\n        [ 9.0815e-01, -5.0523e-01],\n        [ 2.3845e+00,  2.3237e+00],\n        [ 4.4671e+00, -4.2304e-01],\n        [ 2.1522e+00,  1.3804e-01],\n        [ 1.6305e+00,  6.6667e-01],\n        [ 2.8308e+00,  2.0956e+00],\n        [ 4.1165e+00,  3.5110e-01],\n        [ 4.5903e+00, -5.9373e-01],\n        [ 2.0751e+00,  1.2292e+00],\n        [ 3.2683e+00, -1.1358e+00],\n        [ 1.9763e+00,  1.9424e-02],\n        [ 3.6969e+00,  2.4115e-01],\n        [ 2.1114e+00, -2.4397e-01],\n        [ 2.3513e+00, -8.4121e-01],\n        [ 3.6763e+00,  8.2027e-01],\n        [ 2.7479e+00,  9.4468e-01],\n        [ 1.2965e+00, -3.8251e-01],\n        [ 2.0030e+00, -1.0555e+00],\n        [ 2.1925e+00, -1.5110e+00],\n        [ 1.9609e+00, -6.5370e-01],\n        [ 2.8829e+00, -8.4642e-01],\n        [ 3.3404e+00,  5.9428e-01],\n        [ 4.4678e+00,  4.4957e-01],\n        [ 3.0944e+00,  3.4459e-01],\n        [ 6.1926e-01, -3.3383e-01],\n        [ 3.0753e-02,  2.2379e-01],\n        [ 3.7790e+00, -2.2496e-01],\n        [ 3.9448e+00,  4.0281e-01],\n        [ 2.0760e+00, -5.5589e-01],\n        [ 1.0995e+00,  7.6483e-02],\n        [ 2.2823e+00, -1.0170e+00],\n        [ 4.3522e+00, -3.2316e-02],\n        [ 2.2895e+00,  4.7114e-01],\n        [ 2.1883e+00, -3.7021e-02],\n        [ 2.4231e+00, -1.2931e+00],\n        [ 1.8333e+00,  6.1556e-01],\n        [ 2.6073e+00,  1.1622e+00],\n        [ 2.4660e+00, -4.7637e-01],\n        [ 3.8983e+00, -2.2604e-02],\n        [ 1.8933e+00,  3.0077e-01],\n        [ 7.8297e-01, -2.0953e-01],\n        [ 3.3182e+00,  4.4036e-01],\n        [ 2.3751e+00,  6.3245e-02],\n        [ 1.8841e+00, -4.5967e-01],\n        [ 1.4242e+00,  8.4496e-01],\n        [ 3.6130e+00,  1.2840e+00],\n        [ 2.8555e+00, -1.8207e+00],\n        [ 3.0938e+00, -4.1147e-01],\n        [ 3.3840e+00, -9.4678e-01],\n        [ 3.4544e+00, -1.2276e+00],\n        [ 4.1599e+00,  5.6370e-01],\n        [ 2.4147e+00,  7.2496e-01],\n        [ 5.6563e-01,  2.6669e-01],\n        [ 3.8557e+00,  3.5077e-01],\n        [ 3.1940e+00,  5.7569e-01],\n        [ 4.1209e+00,  1.7039e-01],\n        [ 2.5526e+00, -2.1942e+00],\n        [ 1.3150e+00,  7.2630e-01],\n        [-1.6555e-01,  1.0041e+00],\n        [ 2.1359e+00, -8.7109e-01],\n        [ 2.0043e+00, -7.7110e-01],\n        [ 1.4789e+00, -1.3004e-01],\n        [ 1.6429e+00, -1.3270e+00],\n        [ 3.1186e+00, -1.1103e+00],\n        [ 1.9761e+00, -9.8991e-01],\n        [ 2.3914e+00, -2.7372e-01],\n        [ 4.3736e+00,  7.4278e-01],\n        [ 1.8854e+00, -1.4722e+00],\n        [ 9.3488e-01,  1.6436e-01],\n        [ 3.2802e+00, -4.7274e-01],\n        [ 4.4121e+00,  1.1208e+00],\n        [ 2.9587e+00, -1.7206e+00],\n        [ 3.3471e+00, -1.6893e+00],\n        [ 3.7478e+00, -9.4949e-01],\n        [ 3.3900e+00, -7.7844e-01],\n        [ 2.5054e+00,  8.0795e-01],\n        [ 3.5268e+00, -8.6234e-01],\n        [ 2.9209e+00, -1.1964e-01],\n        [ 3.1103e+00,  8.8759e-01],\n        [ 2.2071e+00,  5.0589e-01],\n        [ 3.8879e+00, -2.7422e-02],\n        [ 2.5083e+00, -2.7225e-02],\n        [ 2.9061e+00,  3.8858e-01],\n        [ 7.4180e-01, -8.9862e-01],\n        [ 1.5805e+00, -1.3521e-01],\n        [ 2.6518e+00, -3.8531e-01],\n        [ 2.3262e+00, -1.1400e+00],\n        [ 2.9372e+00, -1.8845e-01],\n        [ 2.3535e+00,  1.7045e+00],\n        [ 2.3385e+00,  2.3848e-01],\n        [ 4.2441e+00, -1.0279e+00],\n        [ 9.4209e-01, -4.6633e-01],\n        [ 4.6228e+00, -1.1867e+00],\n        [ 3.2838e+00, -7.7649e-01],\n        [ 2.5579e+00, -1.4860e+00],\n        [ 7.8685e-01,  1.1450e+00],\n        [ 2.7267e+00,  1.0688e+00],\n        [ 2.3411e+00,  7.0225e-01],\n        [ 2.0113e+00,  8.0777e-01],\n        [ 1.8941e+00,  7.0274e-01],\n        [ 3.7039e+00,  5.7771e-01],\n        [ 4.9262e+00, -6.0117e-01],\n        [ 1.8229e+00, -4.9001e-01],\n        [ 3.6899e+00, -4.1721e-01],\n        [ 1.9879e+00,  7.1605e-01],\n        [ 3.2777e+00, -1.5564e+00],\n        [ 2.4747e+00,  1.8786e-01],\n        [ 4.5654e+00,  3.0488e-01],\n        [ 2.1635e+00, -7.3464e-01],\n        [ 1.2236e+00,  8.4081e-01],\n        [ 4.0938e-01,  4.9532e-01],\n        [ 2.3172e+00, -1.1511e-01],\n        [ 3.5170e+00, -5.8678e-01],\n        [ 2.3317e+00, -2.2710e-01],\n        [ 2.2659e+00,  1.4232e-01],\n        [ 3.2532e+00, -1.9552e+00],\n        [ 3.0555e+00, -5.7729e-01],\n        [ 1.3117e+00,  3.6015e-01],\n        [ 3.0235e+00,  2.5600e-01],\n        [ 6.5353e-01, -3.8660e-01],\n        [ 2.3127e+00, -1.1180e+00],\n        [ 1.6358e+00, -1.5857e-01],\n        [ 3.0956e+00,  1.2732e+00],\n        [ 3.5637e+00,  1.3591e+00],\n        [ 2.3682e+00, -4.2592e-01],\n        [ 2.7814e+00,  3.7535e-01],\n        [ 2.1721e+00, -3.4882e-01],\n        [ 3.1090e+00, -7.4556e-01],\n        [ 3.6816e+00,  5.7636e-01],\n        [ 3.6511e+00,  3.5716e-01],\n        [ 2.1390e+00,  1.8337e-01],\n        [ 3.0226e+00,  1.6705e-01],\n        [ 2.0262e+00,  6.4598e-01],\n        [ 2.6662e+00,  1.2563e+00],\n        [ 2.5057e+00,  1.5154e+00],\n        [ 3.3075e+00, -1.5425e+00],\n        [ 1.4473e+00, -1.1209e+00],\n        [ 1.7946e+00, -8.6565e-01],\n        [ 3.0389e+00, -1.2577e+00],\n        [ 1.3303e+00, -1.4957e+00],\n        [ 1.5725e+00, -2.6543e-01],\n        [ 3.0611e+00, -2.3702e+00],\n        [ 4.3959e-01, -1.4403e+00],\n        [ 4.6203e+00, -7.6325e-01],\n        [ 2.1819e+00,  2.7190e+00],\n        [ 2.5415e+00, -9.7832e-01],\n        [ 3.2936e+00,  9.2364e-02],\n        [ 3.0293e+00, -8.8562e-01],\n        [ 3.1986e+00,  7.4631e-01],\n        [ 2.6827e+00,  8.7890e-01],\n        [ 4.1502e+00,  1.3476e+00],\n        [ 2.7265e+00,  1.2929e+00],\n        [ 2.1370e-01, -4.4642e-01],\n        [ 4.9454e-01, -1.7895e+00],\n        [ 2.7057e+00,  4.3336e-01],\n        [ 1.6980e+00, -8.7312e-01],\n        [ 3.0874e+00, -6.4791e-01],\n        [ 2.2573e+00, -1.0846e+00],\n        [ 1.6072e+00, -7.3014e-01],\n        [ 3.5225e+00,  7.7718e-01],\n        [ 7.8773e-01,  8.3977e-01],\n        [ 4.8706e+00, -4.7442e-01],\n        [ 2.6128e+00, -1.1130e+00],\n        [ 4.5964e+00, -1.2843e+00],\n        [ 2.0572e+00, -8.2181e-01],\n        [ 2.0402e+00,  9.9764e-01],\n        [ 1.3664e+00,  2.2737e-01],\n        [ 2.3786e+00, -1.8102e+00],\n        [ 1.8929e+00, -2.7582e-01],\n        [ 5.6295e-01, -1.2771e+00],\n        [ 1.8638e+00, -4.2227e-01],\n        [ 1.3571e+00,  5.8736e-01],\n        [ 3.0178e+00,  9.1251e-02],\n        [ 3.2916e+00, -1.2415e+00],\n        [ 4.7610e+00, -3.8410e-01],\n        [ 1.6995e+00, -1.8484e-01],\n        [ 1.2946e+00,  1.4677e-01],\n        [ 9.0460e-01, -7.2740e-01],\n        [ 1.4520e+00,  3.0846e-01],\n        [ 3.3374e+00,  9.7952e-01],\n        [ 3.6296e+00, -1.1146e+00],\n        [ 5.4718e-01,  1.3537e+00],\n        [ 2.6670e+00, -5.9918e-01],\n        [ 2.3537e+00, -1.4082e+00],\n        [ 1.4029e+00,  1.3615e+00],\n        [ 8.7805e-01, -5.0400e-01],\n        [ 2.5482e+00, -1.0446e+00],\n        [ 1.3341e+00,  3.2078e-01],\n        [ 7.3490e-01, -3.5387e-01],\n        [ 1.0606e+00,  5.6427e-01],\n        [ 1.5931e+00, -7.7991e-01],\n        [ 2.9272e+00, -1.0953e+00],\n        [ 1.2880e+00, -1.8394e+00],\n        [ 1.0287e+00, -1.3109e+00],\n        [ 2.2995e+00, -1.9631e+00],\n        [ 2.0633e+00, -4.7448e-01],\n        [ 2.3455e+00, -1.4261e+00],\n        [ 1.7902e+00, -5.1366e-01],\n        [ 3.5421e+00,  1.2081e+00],\n        [ 2.7757e+00, -5.1091e-01],\n        [ 4.1116e+00, -9.1917e-01],\n        [ 1.9187e+00, -1.3897e+00],\n        [ 2.1228e+00,  1.9428e+00],\n        [ 2.1683e+00, -1.2765e+00],\n        [ 4.1917e+00,  4.0950e-03],\n        [ 2.5675e+00,  1.7181e+00],\n        [ 2.2657e+00,  1.5759e+00],\n        [ 2.6938e+00, -1.3721e-01],\n        [ 2.7775e+00,  1.7776e-02],\n        [ 3.0087e+00,  1.5985e+00],\n        [ 1.7294e+00,  9.6804e-01],\n        [ 2.0422e+00, -7.6454e-01],\n        [ 2.1457e+00,  3.3076e-02],\n        [ 2.6636e+00, -1.2711e-02],\n        [ 2.1625e+00,  1.0429e+00],\n        [ 3.3539e+00, -6.7054e-01],\n        [ 1.0617e+00, -1.4600e+00],\n        [ 3.1899e+00, -3.1582e-01],\n        [ 5.9011e-01,  4.1462e-01],\n        [ 1.0768e+00,  1.1856e-01],\n        [ 1.7789e+00, -5.4115e-02],\n        [ 2.8178e+00,  1.2112e+00],\n        [ 3.3423e+00,  3.6996e-01],\n        [ 3.0827e+00,  1.6320e+00],\n        [ 2.9788e+00, -6.3303e-01],\n        [ 2.6096e+00, -6.3097e-01],\n        [ 2.2826e+00, -4.1838e-01],\n        [ 4.6516e+00, -3.7813e-01],\n        [ 3.7392e+00, -1.4341e+00],\n        [ 2.2137e+00,  2.9286e+00],\n        [ 2.1454e+00,  1.9618e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.7963e-03, 1.0522e-02, 2.1283e-02,  ..., 9.5995e-01, 9.6202e-01,\n         1.0000e+00],\n        [2.8651e-03, 7.3671e-02, 9.1382e-02,  ..., 9.9935e-01, 9.9935e-01,\n         1.0000e+00],\n        [5.4243e-02, 6.3254e-02, 6.8370e-02,  ..., 9.6109e-01, 9.9206e-01,\n         1.0000e+00],\n        ...,\n        [7.2084e-03, 6.0351e-02, 6.0351e-02,  ..., 9.9800e-01, 9.9870e-01,\n         1.0000e+00],\n        [5.4912e-06, 2.4694e-01, 2.4694e-01,  ..., 9.9999e-01, 1.0000e+00,\n         1.0000e+00],\n        [1.3790e-02, 3.6968e-02, 3.8154e-02,  ..., 9.1670e-01, 9.1695e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.9257],\n        [0.4203],\n        [0.5223],\n        [0.7590],\n        [0.0318],\n        [0.4400],\n        [0.7571],\n        [0.2636],\n        [0.0862],\n        [0.8832],\n        [0.5741],\n        [0.1334],\n        [0.6567],\n        [0.5190],\n        [0.7153],\n        [0.6211],\n        [0.4909],\n        [0.5955],\n        [0.7305],\n        [0.0425],\n        [0.6114],\n        [0.4095],\n        [0.4002],\n        [0.1760],\n        [0.4213],\n        [0.7328],\n        [0.7397],\n        [0.1580],\n        [0.0545],\n        [0.9880],\n        [0.1660],\n        [0.3612],\n        [0.4537],\n        [0.8705],\n        [0.6760],\n        [0.9638],\n        [0.9384],\n        [0.6019],\n        [0.8451],\n        [0.4677],\n        [0.0258],\n        [0.2333],\n        [0.1110],\n        [0.7989],\n        [0.3178],\n        [0.7704],\n        [0.2658],\n        [0.0302],\n        [0.9776],\n        [0.8067],\n        [0.4893],\n        [0.1712],\n        [0.4375],\n        [0.6929],\n        [0.9429],\n        [0.7136],\n        [0.3567],\n        [0.0033],\n        [0.8715],\n        [0.0099],\n        [0.6838],\n        [0.6083],\n        [0.8043],\n        [0.6660],\n        [0.2751],\n        [0.2872],\n        [0.9701],\n        [0.1285],\n        [0.6366],\n        [0.4567],\n        [0.6544],\n        [0.4086],\n        [0.2576],\n        [0.2614],\n        [0.3593],\n        [0.0084],\n        [0.7249],\n        [0.6799],\n        [0.5166],\n        [0.8823],\n        [0.2755],\n        [0.4391],\n        [0.9225],\n        [0.1525],\n        [0.9144],\n        [0.8025],\n        [0.0906],\n        [0.0784],\n        [0.1386],\n        [0.8957],\n        [0.8894],\n        [0.1725],\n        [0.8689],\n        [0.4087],\n        [0.0080],\n        [0.0159],\n        [0.4757],\n        [0.8717],\n        [0.6175],\n        [0.0327],\n        [0.3806],\n        [0.1995],\n        [0.8244],\n        [0.2170],\n        [0.9172],\n        [0.1764],\n        [0.0076],\n        [0.2061],\n        [0.4910],\n        [0.4976],\n        [0.2645],\n        [0.8341],\n        [0.5509],\n        [0.1207],\n        [0.4144],\n        [0.4326],\n        [0.4268],\n        [0.6630],\n        [0.0776],\n        [0.3734],\n        [0.1534],\n        [0.0282],\n        [0.9712],\n        [0.9332],\n        [0.1426],\n        [0.1477],\n        [0.2691],\n        [0.4474],\n        [0.7103],\n        [0.8744],\n        [0.7015],\n        [0.1529],\n        [0.0584],\n        [0.0967],\n        [0.4512],\n        [0.4961],\n        [0.8307],\n        [0.9750],\n        [0.7541],\n        [0.1877],\n        [0.3458],\n        [0.9707],\n        [0.0496],\n        [0.9545],\n        [0.7118],\n        [0.7404],\n        [0.3549],\n        [0.6208],\n        [0.2095],\n        [0.3556],\n        [0.0516],\n        [0.8335],\n        [0.5872],\n        [0.0569],\n        [0.4868],\n        [0.2736],\n        [0.9445],\n        [0.6392],\n        [0.7600],\n        [0.4944],\n        [0.7611],\n        [0.1398],\n        [0.2461],\n        [0.8983],\n        [0.3745],\n        [0.9296],\n        [0.4728],\n        [0.9100],\n        [0.0491],\n        [0.1933],\n        [0.9258],\n        [0.3841],\n        [0.2365],\n        [0.0373],\n        [0.9299],\n        [0.9078],\n        [0.2827],\n        [0.3159],\n        [0.4762],\n        [0.0059],\n        [0.6308],\n        [0.1682],\n        [0.3536],\n        [0.4521],\n        [0.9433],\n        [0.6725],\n        [0.3287],\n        [0.7434],\n        [0.0786],\n        [0.9875],\n        [0.0170],\n        [0.3107],\n        [0.1204],\n        [0.3942],\n        [0.6673],\n        [0.8753],\n        [0.7528],\n        [0.5277],\n        [0.0356],\n        [0.7346],\n        [0.0800],\n        [0.5274],\n        [0.8035],\n        [0.8719],\n        [0.3753],\n        [0.7057],\n        [0.0060],\n        [0.8595],\n        [0.6685],\n        [0.8247],\n        [0.3644],\n        [0.8515],\n        [0.3399],\n        [0.7845],\n        [0.4467],\n        [0.0446],\n        [0.6434],\n        [0.0299],\n        [0.7347],\n        [0.6737],\n        [0.5897],\n        [0.9848],\n        [0.8787],\n        [0.8922],\n        [0.5290],\n        [0.1744],\n        [0.7804],\n        [0.4117],\n        [0.9415],\n        [0.7301],\n        [0.9537],\n        [0.7363],\n        [0.2074],\n        [0.0587],\n        [0.5470],\n        [0.3587],\n        [0.9355],\n        [0.8603],\n        [0.6043],\n        [0.9364],\n        [0.4484],\n        [0.4643],\n        [0.5653],\n        [0.1295],\n        [0.2336],\n        [0.7516],\n        [0.7529],\n        [0.1917],\n        [0.9096],\n        [0.7734],\n        [0.5940],\n        [0.0023],\n        [0.2675],\n        [0.5020],\n        [0.5336],\n        [0.4672],\n        [0.8204],\n        [0.9598],\n        [0.1081],\n        [0.9155],\n        [0.2060],\n        [0.7798],\n        [0.4855],\n        [0.3665],\n        [0.6423],\n        [0.6660],\n        [0.8354],\n        [0.7027],\n        [0.3923],\n        [0.4725],\n        [0.2232],\n        [0.0066],\n        [0.5629],\n        [0.5596],\n        [0.8045],\n        [0.3455],\n        [0.6670],\n        [0.9283],\n        [0.9063],\n        [0.2359],\n        [0.1200],\n        [0.8993],\n        [0.2556],\n        [0.1482],\n        [0.3586],\n        [0.1690],\n        [0.7954],\n        [0.8541],\n        [0.8722],\n        [0.9706],\n        [0.8079],\n        [0.3705],\n        [0.7679],\n        [0.9757],\n        [0.1406],\n        [0.2061],\n        [0.7459],\n        [0.9258],\n        [0.3443],\n        [0.9764],\n        [0.4673],\n        [0.4567],\n        [0.6760],\n        [0.4874],\n        [0.7033],\n        [0.5605],\n        [0.1725],\n        [0.5735],\n        [0.2036],\n        [0.4222],\n        [0.0808],\n        [0.6861]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False,  True, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-0.4477, -0.3674],\n        [ 1.3477,  1.3639],\n        [ 0.5148,  0.0374],\n        ...,\n        [ 3.3531,  0.9324],\n        [-0.6945,  0.5364],\n        [ 2.4059,  0.4548]]) torch.Size([9984, 2])\nsamples tensor([[ 2.7215e+00, -1.2577e-01],\n        [ 1.5943e+00,  1.0800e+00],\n        [ 3.1751e+00, -4.3291e-01],\n        [ 3.1867e+00, -1.7806e+00],\n        [ 1.8649e+00,  8.2541e-01],\n        [ 3.6881e+00,  4.1212e-01],\n        [ 1.5400e+00,  2.1182e-01],\n        [ 2.8723e+00,  1.8407e-02],\n        [ 1.5569e+00,  5.0573e-01],\n        [ 2.5848e+00,  7.8773e-02],\n        [ 4.7573e+00, -3.4734e-01],\n        [ 4.0076e+00, -2.1601e+00],\n        [ 9.3856e-01, -1.2234e+00],\n        [ 2.4052e+00, -1.5204e+00],\n        [ 2.3117e+00,  5.5783e-01],\n        [ 2.1146e+00, -8.2766e-01],\n        [ 3.0305e+00, -1.1465e+00],\n        [ 4.1469e+00,  7.8587e-01],\n        [ 2.7988e+00,  3.2885e-01],\n        [ 2.0837e+00,  1.1550e+00],\n        [ 3.5224e+00,  5.6770e-01],\n        [ 1.3158e+00,  5.1392e-01],\n        [ 2.3827e+00,  8.0769e-01],\n        [ 4.1543e+00,  5.6804e-01],\n        [ 2.1656e+00,  1.6171e+00],\n        [ 3.5246e+00, -2.1874e+00],\n        [ 2.5801e+00,  1.5062e+00],\n        [ 3.3777e+00, -8.8362e-01],\n        [ 4.0287e+00, -8.5042e-01],\n        [ 2.2197e+00,  6.3755e-01],\n        [ 3.5449e+00, -1.3115e+00],\n        [ 2.5224e+00, -3.7189e-01],\n        [ 1.3456e+00, -2.2765e+00],\n        [ 3.1927e+00,  2.6187e-01],\n        [ 1.7362e+00, -1.7509e+00],\n        [ 1.6095e+00, -1.4097e+00],\n        [ 3.9496e+00, -2.9749e-01],\n        [ 3.8747e+00, -2.4943e+00],\n        [ 1.6024e+00, -8.6191e-01],\n        [ 2.2976e+00,  1.7270e+00],\n        [ 1.7960e+00, -7.6349e-01],\n        [ 3.5895e+00,  4.9423e-01],\n        [ 2.2252e+00,  6.6526e-01],\n        [ 2.7706e+00, -4.5932e-01],\n        [ 2.5426e+00, -3.2246e-01],\n        [ 1.3984e+00, -1.8607e-01],\n        [ 2.6678e+00,  1.2975e+00],\n        [ 2.7466e+00, -4.5186e-01],\n        [ 2.9620e+00,  5.8089e-01],\n        [ 3.3531e+00, -4.2884e-01],\n        [ 8.2482e-01, -2.2580e-03],\n        [ 3.1831e+00, -5.1772e-01],\n        [ 1.7756e+00,  2.3211e-02],\n        [ 3.6375e+00,  4.9651e-01],\n        [ 8.3189e-01,  2.9571e-01],\n        [ 1.9227e+00, -1.3315e-01],\n        [ 2.6919e+00, -5.1231e-01],\n        [ 1.0257e-01, -3.6566e-01],\n        [ 4.6874e+00,  5.5249e-01],\n        [ 4.7906e+00,  9.1723e-01],\n        [ 3.5748e+00, -1.0780e-01],\n        [ 1.0658e+00, -4.7929e-02],\n        [ 1.4611e+00,  2.1296e+00],\n        [ 1.9722e+00,  1.7878e-01],\n        [ 3.3454e+00, -2.7840e-01],\n        [ 2.8987e+00, -4.5721e-01],\n        [ 2.5226e+00,  2.7384e-01],\n        [ 3.3462e+00, -1.5960e+00],\n        [ 2.8871e+00, -7.6185e-01],\n        [ 4.0303e+00,  1.4283e+00],\n        [ 2.6595e+00,  2.5560e-01],\n        [ 1.1406e+00,  3.0592e-01],\n        [ 3.5396e+00, -3.4249e-01],\n        [ 2.2405e+00,  5.9475e-01],\n        [ 2.7970e+00,  3.7089e-01],\n        [ 1.8196e+00,  5.0647e-01],\n        [ 2.3845e+00,  4.1506e-01],\n        [ 2.7805e+00, -7.2476e-02],\n        [ 3.4606e+00, -1.1361e+00],\n        [ 2.7076e+00, -1.1438e+00],\n        [ 1.2267e+00, -3.2807e-01],\n        [ 3.5933e+00, -1.4926e+00],\n        [ 1.5938e+00,  1.5245e+00],\n        [ 1.6830e+00,  6.2150e-01],\n        [ 1.1157e+00, -1.5530e+00],\n        [ 1.8377e+00, -3.7068e-01],\n        [ 2.0011e+00,  1.3997e+00],\n        [ 8.8802e-01,  6.4626e-01],\n        [ 4.3395e+00,  4.4975e-01],\n        [ 2.0234e-01,  1.0297e+00],\n        [ 1.2143e+00, -3.1136e-01],\n        [ 7.6806e-01, -4.3680e-01],\n        [ 1.1996e+00, -2.0023e-01],\n        [ 2.1796e+00, -1.2906e+00],\n        [ 3.5214e+00,  3.6504e-01],\n        [ 6.1522e-01,  7.7306e-01],\n        [ 1.5548e+00,  1.4680e-01],\n        [ 3.3327e+00,  5.9070e-01],\n        [ 7.7554e-01, -8.8744e-01],\n        [ 2.3545e+00,  1.6216e+00],\n        [ 2.7694e+00, -1.7947e+00],\n        [ 2.3739e+00,  8.5411e-01],\n        [-7.9337e-03, -1.5831e+00],\n        [ 2.4772e-01,  1.1742e+00],\n        [ 2.8618e+00, -5.6091e-01],\n        [ 1.5715e+00, -1.2052e+00],\n        [ 3.9344e+00,  1.5632e+00],\n        [ 1.3342e+00,  4.1818e-01],\n        [ 3.1765e+00, -1.9910e+00],\n        [ 2.2447e+00,  7.8366e-01],\n        [ 2.7933e+00, -1.3193e+00],\n        [ 2.5936e+00, -6.9330e-01],\n        [ 2.1356e+00,  1.6395e+00],\n        [ 1.7368e+00, -1.8556e-01],\n        [ 2.7773e+00, -1.4099e-01],\n        [ 1.9149e+00, -3.0785e+00],\n        [ 3.1226e+00,  7.5754e-02],\n        [ 3.8038e+00, -1.4835e+00],\n        [ 3.0239e+00,  2.1521e-01],\n        [ 1.0219e+00,  3.8196e-01],\n        [ 2.3707e+00,  1.4084e+00],\n        [ 2.2168e+00,  1.1930e-01],\n        [ 1.4535e+00,  2.1510e+00],\n        [ 1.5243e-01,  8.4456e-02],\n        [ 2.8918e+00,  2.3242e+00],\n        [ 2.9060e+00, -9.3352e-01],\n        [ 3.9243e+00,  4.6936e-01],\n        [ 2.9234e+00, -1.0442e+00],\n        [ 2.6566e+00, -2.6414e-01],\n        [ 3.7990e+00,  1.8842e+00],\n        [ 1.0686e+00, -1.8850e-01],\n        [ 2.4633e+00, -1.0872e+00],\n        [ 2.4421e+00, -4.2961e-02],\n        [ 3.8386e+00, -4.5354e-01],\n        [ 3.4129e+00,  2.2247e+00],\n        [ 1.5053e+00, -8.1677e-01],\n        [ 1.3727e+00, -1.6470e-01],\n        [ 2.9263e+00,  1.4342e+00],\n        [ 2.0484e+00, -4.3081e-01],\n        [ 1.0228e+00,  8.2831e-01],\n        [ 2.2508e+00, -1.3111e+00],\n        [ 1.6708e+00,  5.2151e-02],\n        [ 3.3316e+00, -2.8949e-01],\n        [ 5.0840e-01, -2.2015e-01],\n        [ 2.5331e+00,  3.0157e-01],\n        [ 2.1055e+00,  1.9219e-01],\n        [ 2.3456e+00,  2.9315e-01],\n        [ 1.5590e+00,  1.4686e+00],\n        [ 2.3168e+00,  1.1793e-01],\n        [ 1.4477e+00,  1.2674e+00],\n        [ 2.0870e+00, -1.8488e+00],\n        [ 1.8923e+00, -1.2878e+00],\n        [ 9.6730e-01,  1.2587e-01],\n        [ 1.2311e+00, -2.9348e-01],\n        [ 1.6322e+00,  8.3465e-01],\n        [ 4.0469e+00,  2.5035e-01],\n        [ 4.1992e+00,  1.1662e+00],\n        [ 2.8990e+00, -1.0367e-01],\n        [ 1.7038e+00, -2.6095e-01],\n        [ 3.6581e+00, -4.9485e-01],\n        [ 1.3060e+00,  6.1179e-01],\n        [ 3.1186e+00,  1.9248e-01],\n        [ 2.1574e+00, -4.8878e-01],\n        [ 1.4977e+00, -1.4062e-01],\n        [ 3.2128e+00, -1.0364e+00],\n        [ 1.9558e+00, -1.3255e-01],\n        [ 1.5157e+00, -1.6310e+00],\n        [ 3.4009e+00, -1.8500e+00],\n        [ 3.1966e+00, -1.6587e+00],\n        [ 3.1192e+00, -1.1161e+00],\n        [ 1.2937e+00,  9.2648e-01],\n        [ 1.9410e+00, -2.1440e+00],\n        [ 1.8234e+00,  1.4348e+00],\n        [ 4.7273e+00,  8.4111e-01],\n        [ 5.4670e-01, -3.6654e-02],\n        [ 3.1831e+00, -1.1601e-01],\n        [ 2.4652e+00, -5.3227e-02],\n        [ 1.9162e+00,  1.5824e+00],\n        [ 3.4748e+00, -1.0362e+00],\n        [ 2.8732e+00,  1.6690e+00],\n        [ 1.2439e+00, -6.5118e-01],\n        [ 3.5851e+00,  6.1965e-01],\n        [ 1.6577e+00,  7.8812e-01],\n        [ 2.4021e+00,  9.9529e-01],\n        [ 9.6035e-01,  1.4586e-01],\n        [ 3.0988e+00,  2.0523e-01],\n        [ 3.6688e+00, -5.5109e-01],\n        [ 3.2440e-01, -1.3341e+00],\n        [ 2.1280e+00, -9.6322e-01],\n        [ 7.5412e-01, -5.6391e-01],\n        [ 1.5654e+00,  8.8418e-01],\n        [ 3.4712e+00, -6.3376e-02],\n        [ 1.2270e+00,  7.9490e-01],\n        [ 3.8134e+00,  4.9484e-01],\n        [ 3.2463e+00, -5.6404e-01],\n        [ 3.7743e+00,  8.5685e-01],\n        [ 2.5730e+00,  6.1610e-01],\n        [ 3.1040e+00, -1.3785e+00],\n        [ 2.2390e+00,  1.1859e+00],\n        [ 3.2169e+00, -1.5975e+00],\n        [ 1.2823e+00, -4.5043e-01],\n        [ 3.4467e+00, -4.1555e-01],\n        [ 3.4352e+00,  6.7942e-03],\n        [ 2.9096e+00, -8.6035e-02],\n        [ 3.6182e+00, -1.4725e+00],\n        [ 2.1004e+00,  8.5502e-01],\n        [ 1.7327e+00,  8.4537e-01],\n        [ 3.0275e+00,  5.0852e-01],\n        [ 7.5907e-01, -1.1464e+00],\n        [ 1.0776e+00, -2.3277e+00],\n        [ 1.8420e+00, -5.0442e-01],\n        [ 2.7565e+00, -1.1959e+00],\n        [ 2.9451e+00,  5.9262e-01],\n        [ 2.7784e+00, -1.0290e+00],\n        [ 1.8137e+00,  1.7259e+00],\n        [ 1.4616e+00,  2.6020e+00],\n        [ 1.2629e+00,  2.6906e-01],\n        [ 2.7539e+00, -3.5441e-01],\n        [ 4.8035e+00, -1.5040e+00],\n        [ 2.6078e+00, -4.5499e-01],\n        [ 2.1881e+00, -6.6610e-01],\n        [ 1.2795e+00, -2.8988e-01],\n        [ 3.6260e+00, -4.1558e-01],\n        [ 3.9588e+00, -3.6157e-01],\n        [ 3.4234e+00,  8.8083e-01],\n        [ 1.6045e+00, -9.4308e-01],\n        [ 2.7498e+00, -2.1745e-01],\n        [ 1.9773e+00, -2.4230e-01],\n        [ 1.5254e+00, -1.2729e+00],\n        [ 1.5897e+00, -1.1535e-01],\n        [ 1.6692e+00,  8.7677e-01],\n        [ 1.9285e+00, -1.5508e+00],\n        [ 1.0391e+00, -1.7053e+00],\n        [ 7.0641e-01, -4.4402e-01],\n        [ 3.4658e+00, -6.9393e-01],\n        [ 2.1945e+00, -1.2796e+00],\n        [ 6.4028e-01, -6.7304e-01],\n        [ 2.7105e+00, -3.8011e-01],\n        [ 1.3736e+00, -6.9892e-01],\n        [ 2.9437e+00, -9.8246e-01],\n        [ 2.4759e+00,  1.9234e-01],\n        [ 2.2235e+00,  7.7299e-02],\n        [ 2.2237e+00, -4.5264e-01],\n        [ 1.6575e+00, -2.2951e+00],\n        [ 2.4750e+00, -1.8328e-01],\n        [ 1.4297e+00,  7.5167e-01],\n        [ 1.1783e+00, -2.2256e+00],\n        [ 3.2804e+00,  7.6666e-01],\n        [ 1.6189e+00, -7.6835e-01],\n        [ 2.7807e+00,  1.0803e-02],\n        [ 1.6588e+00,  8.4008e-01],\n        [-2.4437e-01, -1.1846e+00],\n        [ 2.3785e+00,  7.4154e-01],\n        [ 3.4482e+00, -5.1103e-01],\n        [ 3.2046e+00,  9.6890e-01],\n        [ 3.5000e+00, -1.5477e+00],\n        [ 1.3797e+00,  1.3496e+00],\n        [ 2.8197e+00,  8.6030e-01],\n        [ 2.9137e+00, -1.8024e+00],\n        [ 2.3433e+00, -1.5117e+00],\n        [ 1.6235e+00, -1.3011e+00],\n        [ 3.5380e+00,  9.0784e-01],\n        [ 2.0232e+00, -6.0931e-01],\n        [ 2.3198e+00, -1.7340e+00],\n        [ 1.6904e+00, -4.6983e-01],\n        [ 3.2097e+00, -1.8532e-01],\n        [ 1.0260e+00,  2.2299e-01],\n        [ 3.4906e-01, -7.1265e-01],\n        [ 2.4928e+00, -1.1798e+00],\n        [ 6.8037e-01, -9.8680e-02],\n        [ 4.1496e+00, -9.0676e-01],\n        [ 1.2005e+00,  7.1898e-01],\n        [ 1.6687e+00,  4.7766e-01],\n        [ 4.7244e-01,  7.5954e-01],\n        [ 1.7568e+00, -6.6991e-02],\n        [ 2.9431e+00, -1.9741e+00],\n        [ 2.7501e+00,  1.0940e+00],\n        [ 7.2520e-01, -5.0069e-01],\n        [ 2.3246e+00,  8.4579e-01],\n        [ 2.9917e+00, -2.5316e-01],\n        [ 3.4417e+00, -6.2426e-01],\n        [ 2.1233e-01,  1.3957e+00],\n        [ 3.2275e+00,  8.8202e-01],\n        [ 3.7894e+00,  1.5333e+00],\n        [ 2.2501e+00,  3.3099e-01],\n        [ 2.0778e+00, -2.4984e+00],\n        [ 2.3976e+00, -1.3215e+00],\n        [ 1.8443e+00, -1.7198e+00],\n        [ 3.7181e+00, -5.0552e-01],\n        [ 2.2198e+00,  2.3673e-01],\n        [ 1.4100e+00, -3.9822e-01],\n        [ 6.3302e-01, -1.2209e+00],\n        [ 3.8584e+00, -2.6730e+00],\n        [ 1.2826e+00, -8.0602e-01],\n        [ 2.9659e+00, -1.0088e-02],\n        [ 1.2501e+00,  1.0935e+00],\n        [ 1.6906e+00,  8.6036e-01],\n        [ 1.0108e+00, -8.6651e-01],\n        [ 4.4807e+00, -1.1466e-02],\n        [ 1.8760e+00,  8.9078e-01],\n        [ 2.6381e+00, -1.3572e+00],\n        [ 2.6019e+00, -6.3016e-02],\n        [ 2.9000e+00, -6.7196e-01],\n        [ 2.7161e+00,  6.9158e-02],\n        [ 3.4699e+00, -6.1124e-01],\n        [-4.8603e-01, -8.6333e-01],\n        [ 1.5765e+00, -3.2122e-01],\n        [ 1.9889e+00,  1.0030e-01],\n        [ 2.8594e+00, -7.7936e-01],\n        [ 2.6349e+00, -5.9992e-02],\n        [ 2.4083e+00, -2.9546e+00],\n        [ 2.0450e+00, -5.7169e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[4.8697e-10, 1.1030e-01, 1.1030e-01,  ..., 9.9541e-01, 9.9552e-01,\n         1.0000e+00],\n        [1.5312e-01, 1.5316e-01, 2.0265e-01,  ..., 9.9627e-01, 9.9667e-01,\n         1.0000e+00],\n        [5.1596e-03, 1.9103e-02, 1.9103e-02,  ..., 9.9714e-01, 9.9728e-01,\n         1.0000e+00],\n        ...,\n        [5.0824e-06, 6.2010e-03, 2.2358e-02,  ..., 6.8734e-01, 7.7324e-01,\n         1.0000e+00],\n        [4.0256e-03, 1.0892e-02, 1.4011e-02,  ..., 8.2634e-01, 8.2634e-01,\n         1.0000e+00],\n        [1.5676e-02, 6.5597e-02, 7.1789e-02,  ..., 9.9950e-01, 9.9998e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.0957],\n        [0.0592],\n        [0.1456],\n        [0.6692],\n        [0.6175],\n        [0.3609],\n        [0.1008],\n        [0.4906],\n        [0.9877],\n        [0.6593],\n        [0.9677],\n        [0.4237],\n        [0.7858],\n        [0.6959],\n        [0.7331],\n        [0.4523],\n        [0.3777],\n        [0.9570],\n        [0.9198],\n        [0.7025],\n        [0.6490],\n        [0.8782],\n        [0.0847],\n        [0.0436],\n        [0.3458],\n        [0.8110],\n        [0.6279],\n        [0.8933],\n        [0.9138],\n        [0.4408],\n        [0.9536],\n        [0.0711],\n        [0.2472],\n        [0.6690],\n        [0.9300],\n        [0.4894],\n        [0.7697],\n        [0.8985],\n        [0.4935],\n        [0.1529],\n        [0.4722],\n        [0.2434],\n        [0.5772],\n        [0.3814],\n        [0.5461],\n        [0.0645],\n        [0.6058],\n        [0.7546],\n        [0.4673],\n        [0.8192],\n        [0.0516],\n        [0.3840],\n        [0.0278],\n        [0.5638],\n        [0.5837],\n        [0.6805],\n        [0.6877],\n        [0.5954],\n        [0.3541],\n        [0.3478],\n        [0.5502],\n        [0.3916],\n        [0.6880],\n        [0.7336],\n        [0.6893],\n        [0.2252],\n        [0.2173],\n        [0.5513],\n        [0.4143],\n        [0.9585],\n        [0.9273],\n        [0.0367],\n        [0.1860],\n        [0.6774],\n        [0.8229],\n        [0.3028],\n        [0.0538],\n        [0.7460],\n        [0.2319],\n        [0.1476],\n        [0.7407],\n        [0.1946],\n        [0.5159],\n        [0.7975],\n        [0.4218],\n        [0.6981],\n        [0.1599],\n        [0.5973],\n        [0.2873],\n        [0.9437],\n        [0.0461],\n        [0.8628],\n        [0.8258],\n        [0.5976],\n        [0.7008],\n        [0.0579],\n        [0.4342],\n        [0.9582],\n        [0.1971],\n        [0.8754],\n        [0.6450],\n        [0.7895],\n        [0.0852],\n        [0.3064],\n        [0.9404],\n        [0.6729],\n        [0.3398],\n        [0.4396],\n        [0.7528],\n        [0.4390],\n        [0.1010],\n        [0.8536],\n        [0.3708],\n        [0.1305],\n        [0.3219],\n        [0.0075],\n        [0.9319],\n        [0.6298],\n        [0.8294],\n        [0.9247],\n        [0.2627],\n        [0.9050],\n        [0.8221],\n        [0.5333],\n        [0.9887],\n        [0.8880],\n        [0.9953],\n        [0.6721],\n        [0.1442],\n        [0.3283],\n        [0.1368],\n        [0.4618],\n        [0.2907],\n        [0.4003],\n        [0.6482],\n        [0.5261],\n        [0.0668],\n        [0.1231],\n        [0.9629],\n        [0.0779],\n        [0.6134],\n        [0.5222],\n        [0.4685],\n        [0.5500],\n        [0.8981],\n        [0.2025],\n        [0.6713],\n        [0.9856],\n        [0.3248],\n        [0.6525],\n        [0.8378],\n        [0.6230],\n        [0.9923],\n        [0.6455],\n        [0.5147],\n        [0.5217],\n        [0.5841],\n        [0.6436],\n        [0.7372],\n        [0.0585],\n        [0.1025],\n        [0.9484],\n        [0.8144],\n        [0.0602],\n        [0.0915],\n        [0.9703],\n        [0.1517],\n        [0.7086],\n        [0.4877],\n        [0.4609],\n        [0.4563],\n        [0.9689],\n        [0.8836],\n        [0.0274],\n        [0.7991],\n        [0.4802],\n        [0.8841],\n        [0.0890],\n        [0.8510],\n        [0.1472],\n        [0.8368],\n        [0.2457],\n        [0.3683],\n        [0.0620],\n        [0.8941],\n        [0.8996],\n        [0.6546],\n        [0.5591],\n        [0.0803],\n        [0.9781],\n        [0.0313],\n        [0.5101],\n        [0.1100],\n        [0.3611],\n        [0.7537],\n        [0.7493],\n        [0.2313],\n        [0.7495],\n        [0.3213],\n        [0.3314],\n        [0.0950],\n        [0.1800],\n        [0.6535],\n        [0.3602],\n        [0.4162],\n        [0.5679],\n        [0.7662],\n        [0.9812],\n        [0.2226],\n        [0.4087],\n        [0.8783],\n        [0.1614],\n        [0.7575],\n        [0.7992],\n        [0.2627],\n        [0.1583],\n        [0.4485],\n        [0.0609],\n        [0.9059],\n        [0.5573],\n        [0.6520],\n        [0.1534],\n        [0.5245],\n        [0.6482],\n        [0.3982],\n        [0.7089],\n        [0.6688],\n        [0.0296],\n        [0.2409],\n        [0.3207],\n        [0.6956],\n        [0.9262],\n        [0.4947],\n        [0.3219],\n        [0.0969],\n        [0.0413],\n        [0.2574],\n        [0.0160],\n        [0.6900],\n        [0.6859],\n        [0.0115],\n        [0.4763],\n        [0.1090],\n        [0.0684],\n        [0.9602],\n        [0.0471],\n        [0.4444],\n        [0.6184],\n        [0.1890],\n        [0.2413],\n        [0.9161],\n        [0.1365],\n        [0.8109],\n        [0.3382],\n        [0.3531],\n        [0.5515],\n        [0.6156],\n        [0.8453],\n        [0.3814],\n        [0.9668],\n        [0.2756],\n        [0.7389],\n        [0.5725],\n        [0.6968],\n        [0.1650],\n        [0.9593],\n        [0.5670],\n        [0.4476],\n        [0.3750],\n        [0.4813],\n        [0.9928],\n        [0.0885],\n        [0.2401],\n        [0.6219],\n        [0.1965],\n        [0.1863],\n        [0.3609],\n        [0.1280],\n        [0.4744],\n        [0.8843],\n        [0.5025],\n        [0.8608],\n        [0.2119],\n        [0.3121],\n        [0.8722],\n        [0.3853],\n        [0.0387],\n        [0.6378],\n        [0.7772],\n        [0.1267],\n        [0.4834],\n        [0.4319],\n        [0.4253],\n        [0.9284],\n        [0.3641],\n        [0.3981],\n        [0.7266],\n        [0.5048],\n        [0.9735],\n        [0.7169],\n        [0.1289],\n        [0.8358],\n        [0.3072],\n        [0.2543],\n        [0.7767],\n        [0.6305],\n        [0.8202],\n        [0.3255],\n        [0.7525],\n        [0.9706],\n        [0.5142],\n        [0.9353]]) torch.Size([312, 1])\nmask tensor([[False,  True, False,  ..., False, False, False],\n        [ True, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False,  True],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-3.9570,  0.5638],\n        [ 2.1706,  0.3302],\n        [-3.1424, -0.1032],\n        ...,\n        [-0.7391,  0.6996],\n        [-0.5700,  0.7222],\n        [-1.3413,  1.9923]]) torch.Size([9984, 2])\nsamples tensor([[ 2.1706e+00,  3.3024e-01],\n        [ 8.6827e-01, -3.2541e-01],\n        [ 2.8028e-01,  3.2546e-01],\n        [ 3.5658e+00,  5.8616e-01],\n        [ 2.6509e+00,  1.9419e+00],\n        [ 2.8807e+00, -2.0194e-01],\n        [ 7.3097e-01, -2.0313e-02],\n        [ 1.7425e+00,  4.1770e-01],\n        [ 1.9336e+00, -1.2109e+00],\n        [ 3.1044e+00, -1.1602e+00],\n        [ 1.2334e+00, -7.4907e-01],\n        [ 7.1167e-01, -7.9428e-01],\n        [ 2.5903e+00, -6.8262e-01],\n        [ 1.1553e+00, -1.3188e-01],\n        [ 3.4712e+00, -2.3074e-02],\n        [ 1.9162e+00,  2.0603e+00],\n        [ 1.0618e+00,  1.8707e+00],\n        [ 8.7627e-01, -6.3667e-01],\n        [ 1.7197e+00,  6.7641e-01],\n        [ 1.6558e+00,  9.3212e-02],\n        [ 3.5118e+00,  5.7338e-01],\n        [ 2.2922e+00,  1.2673e+00],\n        [ 4.0110e+00,  1.1414e+00],\n        [ 3.8313e+00,  4.7715e-01],\n        [ 1.6863e+00,  5.3758e-02],\n        [ 3.0866e+00, -3.2560e-01],\n        [ 1.2718e+00, -4.2383e-01],\n        [ 4.3584e+00, -4.7719e-03],\n        [ 2.3239e+00,  8.9633e-01],\n        [ 1.9236e+00, -5.3643e-02],\n        [ 2.6293e+00,  1.2107e+00],\n        [ 3.2702e+00,  1.6867e-01],\n        [ 2.8802e+00, -7.6651e-01],\n        [ 2.8409e+00,  3.8137e-01],\n        [ 1.3450e+00,  1.8567e-01],\n        [ 1.6776e+00, -8.3852e-01],\n        [ 2.4619e+00, -1.0379e+00],\n        [ 1.0234e+00, -5.1605e-01],\n        [ 2.5571e+00, -1.3294e+00],\n        [ 2.4030e+00, -1.2831e+00],\n        [ 1.8711e+00,  3.2809e-01],\n        [ 2.7081e+00,  3.3391e-01],\n        [ 2.6232e+00, -1.9516e+00],\n        [ 1.2735e+00,  4.6883e-01],\n        [ 1.9013e+00,  2.3808e-01],\n        [ 2.4839e+00, -1.0221e+00],\n        [ 2.7734e+00, -7.2522e-01],\n        [-5.1392e-02, -9.9498e-02],\n        [ 5.8791e-01, -1.0994e+00],\n        [ 2.2190e+00,  8.0072e-01],\n        [ 2.7472e+00, -1.9173e+00],\n        [ 2.5501e+00,  1.6703e-01],\n        [ 1.6000e+00, -3.2189e-01],\n        [ 1.4914e+00, -1.4585e-01],\n        [ 2.2666e+00,  4.3157e-01],\n        [ 1.5296e+00,  9.4335e-01],\n        [ 3.6584e-01, -7.1547e-01],\n        [ 2.1855e+00,  5.9190e-01],\n        [ 3.8301e+00, -1.2692e-01],\n        [ 2.2522e+00, -5.2950e-01],\n        [ 2.2945e+00,  1.2439e-01],\n        [ 3.0804e+00, -2.4934e+00],\n        [ 2.0161e+00,  1.2481e+00],\n        [ 2.2451e+00,  5.1507e-01],\n        [ 3.6544e+00, -7.3409e-01],\n        [ 2.8349e+00, -2.1499e+00],\n        [ 2.9032e+00,  9.5554e-01],\n        [ 1.1398e+00, -1.3052e+00],\n        [ 3.2961e+00, -4.3884e-01],\n        [ 1.5541e+00,  2.4833e+00],\n        [ 9.0615e-01, -1.1892e+00],\n        [-3.3472e-03,  7.7642e-01],\n        [ 2.0827e+00,  1.0171e+00],\n        [ 2.7380e+00,  3.4685e-01],\n        [ 4.1788e+00, -9.4802e-01],\n        [ 1.6932e+00,  1.3162e+00],\n        [ 1.4392e+00,  2.3716e-01],\n        [ 2.9392e+00, -2.0765e-01],\n        [ 1.8934e+00, -3.4515e+00],\n        [ 2.5769e+00,  1.5653e-01],\n        [ 1.7695e+00,  9.3207e-01],\n        [ 1.6033e+00,  2.0254e-01],\n        [ 1.6787e+00, -3.7402e-01],\n        [ 2.5868e+00, -1.1903e-01],\n        [ 2.5451e+00, -8.1473e-01],\n        [ 3.5407e+00, -9.1833e-01],\n        [ 3.5691e+00,  7.3210e-01],\n        [ 3.4024e+00, -9.6670e-01],\n        [ 2.5169e+00, -1.5297e+00],\n        [ 2.6463e+00, -7.7160e-01],\n        [ 2.7520e+00, -2.4343e-01],\n        [ 1.6608e+00, -2.1786e-01],\n        [ 4.2804e+00, -1.7322e+00],\n        [ 3.6016e+00,  1.0173e+00],\n        [ 2.8167e+00,  1.6624e+00],\n        [ 4.6136e-01, -6.5147e-01],\n        [ 8.5286e-01, -1.0863e+00],\n        [ 4.8499e+00, -3.4655e-01],\n        [ 7.2064e-01,  7.6285e-01],\n        [ 3.6610e+00,  5.6265e-01],\n        [ 2.3014e+00, -7.7413e-01],\n        [ 2.5176e+00, -2.0612e+00],\n        [ 4.2988e-01,  4.7434e-02],\n        [ 2.4615e+00,  6.8029e-01],\n        [ 2.4783e+00, -9.9786e-02],\n        [ 2.6936e+00, -1.0994e+00],\n        [ 2.4468e+00, -1.3160e+00],\n        [ 3.5369e+00, -1.0785e-01],\n        [ 2.6964e+00,  4.2760e-01],\n        [ 2.0618e+00, -6.8761e-01],\n        [ 2.7555e+00,  9.0818e-01],\n        [ 9.1356e-01,  1.0957e+00],\n        [ 1.6133e+00, -1.0867e+00],\n        [ 2.1269e+00, -7.6124e-01],\n        [ 8.2481e-01,  1.1410e-01],\n        [ 1.4673e+00,  9.6949e-01],\n        [ 2.4267e-02, -1.0089e+00],\n        [ 2.3285e+00,  6.7420e-02],\n        [ 1.4094e+00,  1.7487e+00],\n        [ 9.1648e-01, -2.2597e-01],\n        [ 1.5257e+00, -1.1017e+00],\n        [ 2.7157e+00,  6.3555e-01],\n        [ 4.0124e+00, -3.8843e-02],\n        [ 3.2248e+00, -1.2083e-02],\n        [ 1.8491e+00, -1.0294e+00],\n        [ 3.3608e+00,  9.2172e-01],\n        [ 1.7002e+00,  1.0766e+00],\n        [ 1.5854e+00,  1.6314e+00],\n        [ 4.1428e-01, -1.2333e+00],\n        [ 1.9389e+00,  1.8193e-01],\n        [ 1.5119e+00, -3.1598e-01],\n        [ 1.8953e+00, -1.4103e+00],\n        [ 3.7814e+00,  1.8048e-01],\n        [ 2.7851e+00, -1.3955e+00],\n        [ 1.4979e+00,  8.4386e-01],\n        [ 2.9919e+00, -1.0030e+00],\n        [ 2.2396e+00, -1.5120e+00],\n        [ 1.7412e+00, -1.1323e+00],\n        [ 1.8606e+00,  3.2443e-01],\n        [ 3.2054e+00,  1.7504e+00],\n        [ 2.7435e+00, -8.1115e-01],\n        [ 2.5042e+00, -9.7049e-01],\n        [ 3.9291e+00, -1.1785e+00],\n        [ 5.5081e-01,  2.6865e-01],\n        [ 6.2783e-01, -6.6760e-01],\n        [ 2.5014e+00, -2.2928e+00],\n        [ 1.8028e+00, -4.6278e-01],\n        [ 1.0145e+00,  2.8780e-01],\n        [ 2.2138e+00,  6.6920e-01],\n        [ 2.2489e+00,  5.6657e-01],\n        [ 1.4528e+00,  5.4648e-01],\n        [ 2.2896e+00, -1.1851e+00],\n        [ 3.3334e+00,  1.1142e+00],\n        [-2.1296e-01,  5.7018e-01],\n        [ 9.2390e-01,  7.7841e-01],\n        [ 1.9093e+00, -6.3845e-01],\n        [ 1.6881e+00,  9.5183e-01],\n        [ 4.5758e+00,  1.6052e-01],\n        [ 4.2324e+00, -4.3141e-01],\n        [ 2.0049e+00, -2.6648e-01],\n        [ 7.2026e-01, -9.3335e-01],\n        [ 1.6310e+00, -1.3332e+00],\n        [ 3.2387e+00, -5.3923e-01],\n        [ 2.8878e+00, -4.5715e-01],\n        [ 2.5941e+00, -5.5942e-01],\n        [-7.4066e-01,  4.5139e-01],\n        [ 9.6622e-01, -1.3850e+00],\n        [ 1.4360e+00,  3.5685e-01],\n        [ 2.7054e+00, -3.3783e-01],\n        [ 3.4072e+00,  1.8609e-01],\n        [ 2.8055e+00, -7.9888e-02],\n        [ 3.2368e+00, -6.1742e-01],\n        [ 2.7373e+00,  1.2748e-01],\n        [ 4.9762e-01,  9.2233e-01],\n        [ 3.3541e+00, -2.8530e+00],\n        [ 2.8005e+00, -6.7667e-01],\n        [ 2.0465e+00,  2.3111e-02],\n        [ 2.4674e+00,  7.9413e-01],\n        [ 8.3781e-01, -1.1219e+00],\n        [ 2.4917e+00, -1.3906e+00],\n        [-4.9813e-01, -7.5751e-01],\n        [ 2.1806e+00,  4.7364e-01],\n        [ 2.0534e+00,  1.5659e-01],\n        [ 1.2604e+00,  1.6466e-01],\n        [ 1.8756e+00, -7.5287e-01],\n        [ 2.0236e+00, -6.1422e-01],\n        [ 2.3553e+00,  1.3290e-01],\n        [ 2.4860e+00, -1.6462e+00],\n        [ 3.9439e+00, -2.2054e+00],\n        [ 1.2433e+00,  6.6501e-02],\n        [ 1.8725e+00, -1.3201e+00],\n        [ 3.0114e+00, -2.1163e+00],\n        [ 1.4950e+00, -5.0903e-01],\n        [ 1.5216e+00,  8.3362e-01],\n        [ 2.4279e+00, -2.2110e-01],\n        [ 2.9629e+00, -4.0149e-03],\n        [ 3.5264e+00, -7.2028e-01],\n        [ 2.2243e+00, -1.4727e+00],\n        [ 2.3681e+00,  4.0314e-01],\n        [ 1.4251e-01, -5.3407e-01],\n        [ 2.7662e+00, -5.8689e-01],\n        [ 2.5596e+00, -8.3557e-01],\n        [ 2.9983e+00, -2.1106e+00],\n        [ 1.8356e+00, -3.1375e+00],\n        [ 1.6866e+00, -1.6645e+00],\n        [-5.9034e-01, -1.7110e+00],\n        [ 2.0048e+00,  6.7962e-01],\n        [ 2.2268e+00,  3.8658e-01],\n        [ 3.1325e+00, -2.0848e+00],\n        [ 3.2303e+00,  2.9354e-02],\n        [ 1.3162e+00,  8.3086e-02],\n        [ 1.3813e+00,  1.7942e-01],\n        [ 3.1656e+00,  6.1439e-01],\n        [ 2.1035e+00, -3.6832e-02],\n        [ 3.2464e+00, -9.0755e-01],\n        [ 1.7563e+00,  4.1609e-01],\n        [ 2.0472e+00,  1.1622e+00],\n        [ 2.2612e+00,  7.4338e-01],\n        [ 9.5619e-01, -8.0490e-01],\n        [ 3.0966e+00, -9.8595e-01],\n        [ 1.8015e+00,  8.8719e-01],\n        [ 4.0656e+00, -1.5566e+00],\n        [ 4.3635e+00, -1.4167e+00],\n        [ 2.8637e+00, -1.2978e+00],\n        [ 1.4826e+00, -1.7088e+00],\n        [ 2.3817e+00, -1.4911e+00],\n        [ 4.8307e+00, -2.1091e+00],\n        [ 1.4343e+00,  5.8341e-01],\n        [ 3.1095e+00,  6.0261e-01],\n        [ 1.4551e+00,  3.6913e-01],\n        [ 3.0812e+00, -1.2832e+00],\n        [ 1.5788e+00, -2.1225e-01],\n        [ 4.5026e+00, -9.6699e-01],\n        [ 3.9645e+00, -1.7288e+00],\n        [ 2.6988e+00, -7.1599e-01],\n        [ 2.7696e+00,  3.1019e-01],\n        [ 4.5897e+00,  1.3984e+00],\n        [ 2.6881e+00,  1.3333e+00],\n        [ 3.3567e+00, -2.1652e+00],\n        [ 1.1335e-01,  6.8781e-01],\n        [ 1.4698e+00,  2.0780e+00],\n        [ 1.5355e+00,  4.4000e-01],\n        [ 1.7547e+00, -2.6757e-01],\n        [ 2.7309e+00, -1.6455e+00],\n        [ 2.9199e+00,  1.0346e+00],\n        [ 3.5634e+00,  1.5108e+00],\n        [ 2.5114e+00, -1.7040e-01],\n        [ 2.6690e+00,  1.1138e-01],\n        [ 2.4165e+00, -8.9524e-01],\n        [ 1.6398e+00, -1.1999e+00],\n        [ 2.8882e+00, -1.2943e+00],\n        [ 1.5137e+00, -1.2083e+00],\n        [ 2.3865e+00, -9.9495e-02],\n        [ 2.5024e+00,  4.4783e-01],\n        [ 2.6121e+00, -2.5030e-01],\n        [ 3.1519e+00,  1.2993e-01],\n        [ 4.2179e+00, -7.6896e-01],\n        [ 2.7984e+00, -9.9075e-01],\n        [-1.2582e-01, -1.6000e-01],\n        [ 1.4546e+00,  1.2234e+00],\n        [ 2.2849e+00, -1.6365e+00],\n        [ 4.3523e+00, -4.3914e-01],\n        [ 2.2598e+00, -1.7868e+00],\n        [ 2.6172e+00, -4.2485e-01],\n        [ 2.7352e+00,  3.6780e-01],\n        [ 2.1998e+00, -2.1051e+00],\n        [ 2.4515e+00,  1.0476e+00],\n        [ 7.7484e-01, -5.5715e-01],\n        [ 6.5890e-01, -3.7322e-01],\n        [ 2.9675e+00,  7.7556e-01],\n        [ 3.6039e+00,  1.9174e+00],\n        [ 2.3632e+00, -1.5124e+00],\n        [ 2.1704e+00, -1.0991e+00],\n        [ 3.3689e+00,  3.2714e-01],\n        [ 1.1669e+00, -1.3294e+00],\n        [ 2.8948e+00, -2.6265e+00],\n        [ 3.2053e+00, -9.3257e-01],\n        [ 2.5398e+00,  3.5889e-01],\n        [ 3.7054e+00,  1.7825e-01],\n        [ 1.7395e+00,  7.9529e-01],\n        [ 1.0312e+00, -3.4378e-01],\n        [ 2.4039e+00,  4.7677e-01],\n        [ 1.9282e+00, -9.6549e-01],\n        [ 2.3701e+00,  2.8337e-01],\n        [ 1.4451e+00,  7.3883e-01],\n        [ 1.9661e+00, -1.5563e+00],\n        [ 1.1230e+00,  9.4418e-01],\n        [ 2.0729e+00,  1.4535e-01],\n        [ 4.8274e+00, -1.3301e-01],\n        [ 1.9275e+00, -4.3711e-01],\n        [ 3.2012e+00,  2.0447e-01],\n        [ 3.7162e+00, -1.9465e+00],\n        [ 2.8642e+00, -8.1534e-01],\n        [ 5.4058e-01,  1.5426e+00],\n        [ 1.6115e+00, -3.9451e-01],\n        [ 3.8085e+00, -1.5447e+00],\n        [ 1.6971e+00, -7.9006e-01],\n        [ 1.3144e+00, -7.0849e-02],\n        [ 3.1520e+00,  7.5012e-01],\n        [ 3.2292e+00,  3.4395e-01],\n        [ 3.0863e+00, -1.0594e+00],\n        [ 2.9864e+00, -6.7226e-01],\n        [ 4.8342e+00, -2.0027e+00],\n        [ 3.6047e+00,  1.1614e+00],\n        [ 2.6310e+00, -7.1911e-01],\n        [ 2.3727e+00,  3.0741e-01],\n        [ 2.3498e+00, -1.7455e+00],\n        [ 2.9368e+00,  2.8360e-01],\n        [ 3.2640e+00, -2.3037e+00],\n        [ 3.7305e+00, -1.0116e+00],\n        [ 2.6713e+00, -1.4032e-01],\n        [ 2.8254e+00,  8.1196e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[2.7790e-04, 1.9367e-01, 1.9720e-01,  ..., 8.3951e-01, 8.6727e-01,\n         1.0000e+00],\n        [2.8802e-01, 2.8807e-01, 2.8813e-01,  ..., 9.4759e-01, 9.4759e-01,\n         1.0000e+00],\n        [1.3037e-04, 1.4870e-01, 1.5661e-01,  ..., 9.9689e-01, 1.0000e+00,\n         1.0000e+00],\n        ...,\n        [3.5453e-11, 1.9182e-10, 4.2105e-03,  ..., 9.7987e-01, 9.7998e-01,\n         1.0000e+00],\n        [2.8263e-03, 2.1912e-01, 2.1916e-01,  ..., 9.9210e-01, 9.9998e-01,\n         1.0000e+00],\n        [1.6592e-03, 5.8600e-03, 5.8609e-03,  ..., 9.7873e-01, 9.8116e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[5.2945e-01],\n        [5.5700e-01],\n        [3.8864e-01],\n        [9.1752e-01],\n        [3.4719e-01],\n        [8.6546e-01],\n        [3.3501e-01],\n        [5.8002e-01],\n        [1.8345e-01],\n        [3.5166e-02],\n        [2.3272e-01],\n        [4.5047e-01],\n        [7.6294e-01],\n        [1.4596e-02],\n        [9.8779e-01],\n        [1.5786e-01],\n        [5.9804e-01],\n        [6.8836e-01],\n        [7.5247e-01],\n        [1.4254e-01],\n        [9.2984e-01],\n        [9.0804e-01],\n        [2.2226e-01],\n        [5.1185e-01],\n        [3.8315e-01],\n        [1.7535e-01],\n        [6.0765e-01],\n        [1.1578e-01],\n        [9.0069e-01],\n        [4.8644e-01],\n        [5.4738e-01],\n        [2.8435e-01],\n        [7.9661e-02],\n        [6.1198e-01],\n        [7.5087e-01],\n        [4.5884e-01],\n        [5.3443e-01],\n        [3.9924e-01],\n        [7.5872e-01],\n        [8.3369e-01],\n        [7.6697e-01],\n        [3.1959e-01],\n        [3.8807e-01],\n        [8.2821e-01],\n        [8.1509e-01],\n        [6.9264e-01],\n        [4.7600e-01],\n        [9.2479e-01],\n        [8.8245e-01],\n        [2.2135e-01],\n        [9.5181e-01],\n        [4.6475e-01],\n        [5.7093e-02],\n        [9.1719e-01],\n        [3.2723e-01],\n        [1.8774e-01],\n        [9.4767e-01],\n        [5.5352e-01],\n        [7.9706e-01],\n        [2.2111e-01],\n        [1.6534e-01],\n        [6.9671e-01],\n        [5.1297e-01],\n        [8.7848e-01],\n        [3.5976e-01],\n        [5.9135e-01],\n        [2.7975e-01],\n        [5.0139e-01],\n        [2.3794e-01],\n        [3.5636e-01],\n        [7.7748e-02],\n        [2.7393e-01],\n        [1.6121e-01],\n        [7.8615e-01],\n        [5.6716e-01],\n        [8.4643e-01],\n        [4.9701e-01],\n        [4.8041e-01],\n        [5.4866e-01],\n        [8.9481e-01],\n        [4.8106e-01],\n        [2.3347e-01],\n        [5.9669e-01],\n        [9.3100e-01],\n        [9.6244e-01],\n        [3.7238e-01],\n        [2.0473e-01],\n        [3.0574e-01],\n        [2.2673e-01],\n        [6.5915e-01],\n        [1.7206e-01],\n        [5.6497e-01],\n        [3.3540e-01],\n        [5.5015e-01],\n        [7.9595e-01],\n        [2.4310e-01],\n        [7.7179e-01],\n        [9.8376e-01],\n        [4.0675e-02],\n        [5.8460e-01],\n        [6.8361e-01],\n        [2.6063e-01],\n        [5.8279e-01],\n        [2.4347e-01],\n        [1.8578e-01],\n        [5.8121e-01],\n        [4.7413e-01],\n        [3.9720e-01],\n        [1.1475e-01],\n        [4.2210e-01],\n        [5.0544e-01],\n        [9.1903e-02],\n        [8.8103e-01],\n        [5.5297e-01],\n        [9.0720e-01],\n        [6.7415e-01],\n        [8.4855e-01],\n        [3.6163e-01],\n        [6.6809e-01],\n        [2.9239e-01],\n        [2.1315e-02],\n        [6.5504e-01],\n        [7.2682e-02],\n        [3.0164e-01],\n        [1.6413e-01],\n        [3.5769e-01],\n        [8.3556e-01],\n        [9.3947e-01],\n        [5.3758e-01],\n        [2.5750e-01],\n        [5.5807e-01],\n        [5.0780e-01],\n        [1.8678e-01],\n        [3.1271e-01],\n        [4.8273e-01],\n        [7.6194e-01],\n        [4.1586e-01],\n        [4.5026e-01],\n        [3.0085e-01],\n        [5.1144e-02],\n        [6.4515e-01],\n        [2.7724e-01],\n        [1.2880e-02],\n        [4.9754e-01],\n        [3.3532e-01],\n        [4.5779e-01],\n        [2.1787e-01],\n        [5.4249e-01],\n        [1.0700e-01],\n        [2.5349e-01],\n        [6.4162e-01],\n        [8.3329e-01],\n        [7.8025e-01],\n        [9.6639e-01],\n        [1.6662e-01],\n        [7.9430e-01],\n        [6.9541e-02],\n        [5.9915e-01],\n        [7.7135e-01],\n        [3.0905e-01],\n        [1.1419e-01],\n        [3.1567e-01],\n        [1.5005e-01],\n        [5.8502e-01],\n        [1.6620e-01],\n        [6.3248e-01],\n        [1.1671e-01],\n        [1.6307e-01],\n        [4.6353e-01],\n        [9.4332e-01],\n        [7.7124e-01],\n        [1.1285e-01],\n        [9.2250e-01],\n        [2.2226e-01],\n        [3.8538e-01],\n        [2.7449e-01],\n        [7.2721e-01],\n        [8.7416e-01],\n        [9.7013e-04],\n        [6.4427e-01],\n        [8.3908e-01],\n        [9.4184e-01],\n        [1.7573e-01],\n        [1.6524e-01],\n        [1.5275e-01],\n        [1.4585e-01],\n        [8.4151e-01],\n        [5.4326e-01],\n        [4.5613e-01],\n        [4.5283e-01],\n        [3.4721e-02],\n        [3.5816e-01],\n        [8.6069e-01],\n        [7.7178e-02],\n        [1.7933e-01],\n        [8.6796e-01],\n        [6.1880e-01],\n        [7.4964e-01],\n        [6.7492e-01],\n        [8.2714e-01],\n        [6.5973e-01],\n        [2.2623e-01],\n        [6.1421e-01],\n        [9.5276e-01],\n        [9.5576e-04],\n        [9.6602e-01],\n        [7.4269e-01],\n        [2.4308e-01],\n        [1.1218e-02],\n        [9.3002e-01],\n        [1.0460e-01],\n        [7.5780e-01],\n        [1.4189e-01],\n        [7.5018e-01],\n        [3.4476e-02],\n        [4.7321e-01],\n        [4.4708e-01],\n        [4.6550e-01],\n        [1.1555e-01],\n        [1.3573e-02],\n        [5.8689e-01],\n        [1.8775e-01],\n        [9.9551e-01],\n        [5.8861e-01],\n        [4.3288e-01],\n        [2.9000e-01],\n        [8.5529e-01],\n        [4.6044e-01],\n        [9.4744e-01],\n        [7.9746e-01],\n        [8.8870e-01],\n        [3.8238e-01],\n        [3.3099e-01],\n        [4.9407e-01],\n        [2.0076e-01],\n        [1.4031e-01],\n        [5.1926e-01],\n        [8.3839e-01],\n        [2.1713e-01],\n        [8.1373e-01],\n        [8.9346e-01],\n        [3.5796e-01],\n        [7.9700e-01],\n        [8.9596e-01],\n        [2.3986e-01],\n        [1.2200e-01],\n        [4.9712e-01],\n        [7.1128e-01],\n        [1.4363e-01],\n        [3.7395e-01],\n        [1.1263e-01],\n        [1.1606e-01],\n        [6.5529e-01],\n        [5.2133e-01],\n        [2.4975e-02],\n        [2.4519e-02],\n        [2.8184e-02],\n        [4.3865e-01],\n        [8.7977e-01],\n        [9.6179e-01],\n        [7.6777e-01],\n        [8.1458e-01],\n        [1.7568e-01],\n        [5.8704e-01],\n        [7.7990e-01],\n        [4.2556e-01],\n        [7.6428e-01],\n        [1.6376e-01],\n        [7.2709e-03],\n        [5.1577e-01],\n        [9.4812e-01],\n        [2.3374e-01],\n        [3.2044e-01],\n        [8.3028e-01],\n        [5.9045e-01],\n        [4.6785e-02],\n        [9.8284e-01],\n        [7.1235e-01],\n        [5.6591e-01],\n        [3.2022e-01],\n        [2.9573e-01],\n        [2.8484e-01],\n        [8.9730e-01],\n        [3.9700e-01],\n        [3.0713e-02],\n        [5.1198e-01],\n        [5.5838e-01],\n        [8.0745e-01],\n        [4.5395e-01],\n        [1.4033e-01],\n        [6.2896e-01],\n        [4.6523e-01],\n        [7.3571e-01],\n        [7.7303e-01],\n        [2.8687e-02],\n        [1.1762e-01],\n        [3.1535e-01],\n        [2.2797e-01],\n        [9.2967e-01],\n        [2.6666e-01],\n        [7.5389e-01],\n        [5.7736e-01],\n        [1.4499e-01],\n        [2.2241e-01],\n        [6.5510e-01],\n        [6.6548e-01],\n        [7.4908e-01],\n        [2.7839e-01],\n        [7.7782e-01],\n        [7.0855e-01],\n        [2.6633e-02],\n        [9.3247e-01]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False,  True, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-7.5792e-01, -5.1349e-02],\n        [ 1.7094e+00, -1.3095e+00],\n        [ 1.2086e+00,  1.6483e+00],\n        ...,\n        [ 1.8988e+00,  2.4336e+00],\n        [-3.1414e-01,  2.5183e-02],\n        [ 5.0468e-01,  1.2901e-03]]) torch.Size([9984, 2])\nsamples tensor([[ 2.6244, -0.3288],\n        [ 2.6215,  0.0834],\n        [ 3.0015, -1.1636],\n        [ 2.3345, -0.6898],\n        [ 3.0893, -1.3194],\n        [ 2.2924,  0.3317],\n        [ 1.6904,  0.7515],\n        [ 2.9050,  1.5006],\n        [ 2.2060,  1.9952],\n        [ 2.3812,  1.1296],\n        [ 2.5630, -1.2837],\n        [ 3.1094, -1.3993],\n        [ 2.8459,  0.9929],\n        [ 2.8355, -2.5551],\n        [ 1.7477, -0.9152],\n        [ 1.8615,  1.2307],\n        [ 1.6504, -0.8877],\n        [ 1.5587,  0.9355],\n        [ 3.1436, -0.7061],\n        [ 2.5305,  0.7692],\n        [ 3.7260,  0.8412],\n        [ 3.4054,  2.2914],\n        [ 2.9565,  0.1065],\n        [ 0.1636,  0.9765],\n        [ 2.4272,  0.0365],\n        [ 2.5837, -0.1358],\n        [ 2.3624,  1.4647],\n        [ 2.8812,  0.4898],\n        [ 2.3546, -0.3726],\n        [ 2.4910,  0.4543],\n        [ 2.2792,  0.2395],\n        [ 2.4662, -0.3755],\n        [ 1.9304,  0.5026],\n        [ 0.6442, -0.8468],\n        [ 2.3520, -1.2063],\n        [ 0.5533,  0.8670],\n        [ 2.6557, -0.8702],\n        [ 2.0578,  0.4575],\n        [ 2.9433, -1.9493],\n        [ 2.7326, -0.4452],\n        [ 1.5105, -0.0892],\n        [ 2.2684, -0.4553],\n        [ 2.0248,  1.3430],\n        [ 2.4341, -1.6035],\n        [ 0.6964,  0.8452],\n        [ 2.9455, -2.2956],\n        [ 3.6037,  1.1101],\n        [ 0.5787, -0.4493],\n        [ 3.1442,  0.1193],\n        [ 3.1134, -0.5298],\n        [ 2.1969, -0.2385],\n        [ 2.8462, -0.5554],\n        [ 3.2873,  0.7411],\n        [ 2.0746, -1.7199],\n        [ 2.8583,  0.0611],\n        [ 2.5884,  0.0666],\n        [ 1.9347, -0.0292],\n        [ 1.1888, -0.2613],\n        [ 2.0256, -0.6170],\n        [ 4.8283,  1.0856],\n        [ 3.6765, -2.9052],\n        [ 1.1497,  0.7668],\n        [ 4.8633, -1.9201],\n        [ 3.4232,  2.3419],\n        [ 2.7205,  0.1990],\n        [ 1.7529,  0.5969],\n        [ 4.4043,  0.0457],\n        [ 1.9908, -1.7982],\n        [ 1.5544,  2.4851],\n        [ 2.3882, -0.9858],\n        [ 2.1554,  0.0453],\n        [ 4.2462,  0.7233],\n        [ 1.5002, -0.0292],\n        [ 1.2977, -0.8058],\n        [ 4.4940,  0.6371],\n        [ 0.7005,  0.1223],\n        [ 2.6939, -2.1155],\n        [ 3.1548, -0.7917],\n        [ 3.1229, -1.0001],\n        [ 3.6202, -1.7224],\n        [ 3.9401,  0.6294],\n        [ 2.9534, -0.0248],\n        [ 1.5743,  0.1176],\n        [ 4.1724, -2.1193],\n        [ 2.6474, -0.2700],\n        [ 2.5668, -0.0977],\n        [ 0.8788, -0.6864],\n        [ 2.2165, -0.6289],\n        [ 0.4556, -0.3931],\n        [ 3.2017,  0.4186],\n        [ 0.7230,  0.9981],\n        [ 0.8471, -1.3426],\n        [ 4.9597, -0.1260],\n        [ 4.1341,  0.5660],\n        [ 3.3719,  0.5731],\n        [ 1.3302, -0.8325],\n        [ 3.3693, -0.6177],\n        [ 1.5349,  0.6479],\n        [ 1.7567, -0.5239],\n        [ 2.7130,  0.3369],\n        [ 2.6940,  1.5323],\n        [ 1.2238,  1.7985],\n        [ 2.7974,  0.3021],\n        [ 1.7710,  0.0369],\n        [ 2.3820,  0.1366],\n        [ 1.7972, -0.9406],\n        [ 1.9333, -1.4536],\n        [ 2.9467, -1.1636],\n        [ 1.7185, -1.0746],\n        [ 2.8106,  0.5777],\n        [ 4.6065, -0.2790],\n        [ 0.7955,  0.3411],\n        [ 2.6291,  0.2485],\n        [ 3.6599, -0.7119],\n        [ 2.7577, -1.3997],\n        [ 2.4435, -1.0996],\n        [ 2.5989, -0.7433],\n        [ 3.2719,  0.1868],\n        [ 3.3828, -0.8474],\n        [ 1.5353,  0.2982],\n        [ 1.1995,  1.9301],\n        [ 1.6617,  0.1257],\n        [ 1.2556,  0.2876],\n        [ 2.6965,  0.8500],\n        [ 2.9107, -0.8254],\n        [ 1.9079, -0.8780],\n        [ 1.7555, -0.1270],\n        [ 1.3834, -0.3926],\n        [ 3.4999,  1.3381],\n        [ 2.2422, -1.9295],\n        [ 3.0679,  0.9161],\n        [ 1.2148, -0.6026],\n        [ 1.3963,  0.3751],\n        [ 3.7920, -0.3870],\n        [ 1.6055, -0.0922],\n        [ 1.7618, -0.8052],\n        [ 2.2420, -0.4612],\n        [ 1.8494,  0.3084],\n        [ 3.2105, -1.3082],\n        [ 1.6481, -1.4745],\n        [ 2.1910,  0.2963],\n        [ 2.2114, -1.4254],\n        [ 2.4628,  1.1549],\n        [ 2.9677,  0.3165],\n        [ 3.9344,  0.6629],\n        [ 2.2703,  2.1393],\n        [ 3.5052, -1.2984],\n        [ 3.7670,  1.4939],\n        [ 0.5026, -0.5686],\n        [ 1.9112, -0.2657],\n        [ 3.5043, -1.0678],\n        [ 0.4926, -0.9717],\n        [ 1.6437,  0.2915],\n        [ 2.2440,  1.1889],\n        [ 3.1797, -0.0487],\n        [ 4.3698,  0.5370],\n        [ 2.7466,  0.2428],\n        [ 2.4907,  1.0408],\n        [ 2.4980, -0.1350],\n        [ 3.4787, -1.5973],\n        [ 0.4909,  0.1161],\n        [ 2.6007, -0.1283],\n        [ 1.9166,  0.3276],\n        [ 2.2250, -1.6983],\n        [ 1.6219,  0.3647],\n        [ 1.1795, -0.6531],\n        [ 2.0528,  0.5354],\n        [ 1.5912, -0.0294],\n        [ 2.7887,  1.0018],\n        [ 1.0796,  1.9422],\n        [ 1.1283, -0.5767],\n        [ 2.9611,  0.6708],\n        [ 2.8607,  0.8251],\n        [ 0.8136,  0.2848],\n        [ 3.3453,  0.1379],\n        [ 2.8770, -0.5077],\n        [ 1.4249, -1.0677],\n        [ 2.9279, -0.7061],\n        [ 0.0746,  0.3061],\n        [ 3.1964, -0.4163],\n        [ 2.9241, -0.7003],\n        [ 1.8156, -0.5198],\n        [ 3.5045, -0.3338],\n        [ 1.6922,  0.6545],\n        [ 2.1869,  1.2373],\n        [ 0.5131, -0.7678],\n        [ 3.7500, -0.2050],\n        [ 3.4122,  0.6464],\n        [ 1.9317, -1.2514],\n        [ 1.3714,  0.1213],\n        [ 3.7302, -0.0636],\n        [ 3.7306,  1.6574],\n        [ 2.6093,  0.2783],\n        [ 2.5355,  0.5534],\n        [ 0.5298, -1.0852],\n        [ 0.1246,  1.2028],\n        [ 2.4090,  0.5212],\n        [ 2.0905, -0.8672],\n        [ 4.0939, -0.5831],\n        [ 2.7034, -0.3527],\n        [ 3.0973,  0.1125],\n        [ 4.8178, -1.1743],\n        [ 0.7303, -0.0102],\n        [ 1.9818,  0.5056],\n        [ 0.7785, -1.5641],\n        [ 1.1012,  0.4566],\n        [ 3.5893, -1.0330],\n        [ 0.5526, -1.0072],\n        [ 2.2215,  0.6035],\n        [ 3.0888,  0.2955],\n        [ 0.4024, -1.2384],\n        [ 2.3504, -0.0578],\n        [ 3.3119, -0.2744],\n        [ 2.8001,  0.1132],\n        [ 3.2312, -0.3888],\n        [ 1.6617, -0.4542],\n        [ 0.6520, -0.7312],\n        [ 1.6538, -0.1383],\n        [ 1.7125,  1.1078],\n        [ 1.4980,  0.0430],\n        [ 2.8298,  0.9793],\n        [ 2.8828,  0.3369],\n        [ 2.3051, -0.7706],\n        [ 2.9177, -1.2126],\n        [ 1.7588, -0.3656],\n        [ 1.7631, -0.0087],\n        [ 2.8767, -0.5598],\n        [ 1.8122, -0.1965],\n        [ 4.2621,  1.3942],\n        [ 2.2660,  0.8253],\n        [ 0.5232, -0.4328],\n        [ 3.4196,  0.0216],\n        [ 3.8410, -0.0154],\n        [ 4.2107, -0.3875],\n        [ 1.4873,  0.0574],\n        [ 1.1107,  0.9791],\n        [ 1.9645,  3.1685],\n        [ 1.1741,  1.2121],\n        [ 1.9083, -0.4084],\n        [ 2.5323, -1.9186],\n        [ 0.8118,  0.5451],\n        [ 2.5165, -2.7097],\n        [ 2.7617, -0.6457],\n        [ 3.2740,  1.0664],\n        [ 3.2092,  0.3496],\n        [ 2.5888,  0.3336],\n        [ 1.9853,  0.6459],\n        [ 2.4946, -0.9225],\n        [ 1.5763,  1.2276],\n        [ 1.7317,  0.4268],\n        [ 3.4813,  1.5459],\n        [ 2.0360,  1.4217],\n        [ 2.7228, -1.0310],\n        [ 1.5120, -0.4344],\n        [ 3.0062,  0.2651],\n        [ 0.9669, -0.2559],\n        [ 2.6816,  0.5191],\n        [ 2.7137, -0.8402],\n        [ 4.5388,  0.9568],\n        [ 4.0340, -1.2852],\n        [ 2.9174, -1.6606],\n        [ 1.0493, -1.7543],\n        [ 3.4387, -0.1752],\n        [ 2.5004, -0.2528],\n        [ 3.5841, -0.4782],\n        [ 3.0805,  0.2704],\n        [ 3.6908, -0.3465],\n        [ 1.7357, -0.6531],\n        [ 1.1304,  1.0511],\n        [ 2.0278,  1.1902],\n        [ 2.0311, -0.1334],\n        [ 1.0174,  0.2082],\n        [ 1.1264, -0.0055],\n        [ 1.9017,  1.1222],\n        [ 3.2584,  0.8658],\n        [ 3.1714, -0.2919],\n        [ 2.3522, -0.0791],\n        [ 3.4949, -0.6085],\n        [ 2.6296,  0.0824],\n        [ 1.4230, -0.8876],\n        [ 2.6834, -0.1674],\n        [ 3.0817, -0.4959],\n        [ 3.6312,  0.1024],\n        [ 2.8045, -1.1796],\n        [ 2.4271,  1.1803],\n        [ 2.3565,  0.4907],\n        [ 2.7732,  1.6901],\n        [ 3.2449,  1.3248],\n        [ 3.3747, -1.4106],\n        [ 2.3018, -0.2575],\n        [ 4.8601, -0.0752],\n        [ 2.2021, -0.2381],\n        [ 1.5928, -1.6901],\n        [ 2.2230, -1.0506],\n        [ 1.0552, -0.0633],\n        [ 0.6259, -1.3470],\n        [ 1.6363,  1.3658],\n        [ 1.8032,  1.0080],\n        [ 1.8228,  0.6077],\n        [ 2.0471, -0.9110],\n        [ 3.0080,  0.7606],\n        [ 2.0180,  0.0314],\n        [ 0.4107, -0.4807],\n        [ 2.9456,  0.1167],\n        [ 3.3514, -0.5265],\n        [ 3.1863,  0.3291],\n        [ 2.0597, -1.0002],\n        [ 2.8538, -0.7554],\n        [ 1.7703,  0.2603],\n        [ 2.8149, -1.6894],\n        [ 2.6309, -0.1498],\n        [ 1.8466,  0.5894]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[4.7564e-07, 4.1120e-02, 4.1122e-02,  ..., 9.9843e-01, 9.9849e-01,\n         1.0000e+00],\n        [9.1987e-09, 9.6166e-02, 2.0773e-01,  ..., 9.7749e-01, 9.7749e-01,\n         1.0000e+00],\n        [1.7509e-06, 1.4680e-02, 7.1291e-02,  ..., 9.9977e-01, 9.9979e-01,\n         1.0000e+00],\n        ...,\n        [1.3277e-04, 4.4634e-04, 1.6181e-03,  ..., 9.4977e-01, 9.9313e-01,\n         1.0000e+00],\n        [2.4817e-02, 2.7570e-02, 5.0513e-02,  ..., 9.9958e-01, 1.0000e+00,\n         1.0000e+00],\n        [4.1671e-05, 7.7781e-04, 7.7781e-04,  ..., 8.2205e-01, 8.2241e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.3995],\n        [0.7139],\n        [0.4784],\n        [0.1683],\n        [0.3519],\n        [0.8154],\n        [0.1262],\n        [0.7861],\n        [0.9926],\n        [0.1345],\n        [0.5857],\n        [0.7353],\n        [0.7837],\n        [0.5723],\n        [0.3182],\n        [0.9082],\n        [0.8983],\n        [0.9452],\n        [0.0198],\n        [0.4332],\n        [0.4553],\n        [0.3248],\n        [0.1246],\n        [0.0668],\n        [0.7260],\n        [0.3396],\n        [0.6583],\n        [0.2914],\n        [0.3922],\n        [0.6736],\n        [0.2230],\n        [0.8154],\n        [0.6022],\n        [0.8255],\n        [0.6662],\n        [0.1857],\n        [0.8080],\n        [0.2983],\n        [0.6129],\n        [0.9161],\n        [0.7317],\n        [0.5274],\n        [0.7097],\n        [0.3452],\n        [0.4782],\n        [0.1202],\n        [0.7429],\n        [0.4925],\n        [0.1828],\n        [0.9566],\n        [0.1458],\n        [0.9355],\n        [0.1640],\n        [0.8692],\n        [0.5522],\n        [0.0784],\n        [0.4162],\n        [0.8915],\n        [0.6236],\n        [0.9529],\n        [0.7917],\n        [0.2538],\n        [0.3151],\n        [0.7349],\n        [0.2893],\n        [0.8861],\n        [0.9809],\n        [0.4319],\n        [0.8034],\n        [0.7803],\n        [0.7169],\n        [0.0194],\n        [0.7444],\n        [0.2203],\n        [0.8498],\n        [0.4580],\n        [0.3625],\n        [0.9488],\n        [0.2095],\n        [0.0725],\n        [0.9354],\n        [0.5198],\n        [0.4584],\n        [0.9506],\n        [0.7497],\n        [0.0395],\n        [0.4461],\n        [0.0680],\n        [0.5384],\n        [0.6778],\n        [0.0207],\n        [0.5163],\n        [0.9613],\n        [0.3856],\n        [0.8797],\n        [0.5328],\n        [0.5877],\n        [0.2807],\n        [0.3097],\n        [0.2031],\n        [0.8128],\n        [0.2725],\n        [0.2202],\n        [0.6298],\n        [0.8538],\n        [0.7034],\n        [0.3727],\n        [0.1527],\n        [0.1729],\n        [0.7613],\n        [0.2615],\n        [0.8022],\n        [0.3580],\n        [0.8919],\n        [0.4729],\n        [0.7169],\n        [0.4043],\n        [0.1709],\n        [0.1566],\n        [0.4382],\n        [0.8278],\n        [0.5769],\n        [0.4260],\n        [0.7932],\n        [0.4212],\n        [0.5894],\n        [0.9049],\n        [0.3766],\n        [0.4566],\n        [0.5473],\n        [0.3681],\n        [0.1530],\n        [0.2094],\n        [0.7050],\n        [0.9028],\n        [0.2304],\n        [0.2781],\n        [0.4256],\n        [0.9504],\n        [0.8976],\n        [0.3871],\n        [0.8014],\n        [0.0751],\n        [0.2229],\n        [0.0479],\n        [0.9514],\n        [0.1420],\n        [0.2913],\n        [0.1030],\n        [0.9848],\n        [0.2809],\n        [0.5917],\n        [0.9421],\n        [0.7804],\n        [0.3705],\n        [0.0411],\n        [0.5737],\n        [0.3948],\n        [0.9107],\n        [0.4233],\n        [0.3372],\n        [0.5704],\n        [0.2590],\n        [0.5105],\n        [0.3902],\n        [0.6490],\n        [0.4917],\n        [0.5434],\n        [0.1105],\n        [0.2037],\n        [0.5513],\n        [0.8413],\n        [0.4051],\n        [0.4847],\n        [0.2791],\n        [0.4766],\n        [0.3264],\n        [0.6912],\n        [0.9796],\n        [0.1566],\n        [0.3531],\n        [0.7744],\n        [0.6499],\n        [0.1721],\n        [0.5236],\n        [0.8970],\n        [0.1657],\n        [0.8192],\n        [0.6300],\n        [0.2570],\n        [0.4132],\n        [0.9918],\n        [0.1270],\n        [0.4492],\n        [0.8922],\n        [0.9631],\n        [0.7749],\n        [0.5604],\n        [0.7326],\n        [0.4081],\n        [0.5644],\n        [0.2597],\n        [0.7598],\n        [0.3504],\n        [0.9756],\n        [0.9598],\n        [0.7576],\n        [0.2225],\n        [0.7908],\n        [0.4717],\n        [0.1266],\n        [0.7380],\n        [0.0054],\n        [0.5038],\n        [0.5274],\n        [0.3798],\n        [0.1150],\n        [0.8359],\n        [0.5651],\n        [0.7024],\n        [0.7752],\n        [0.6127],\n        [0.0934],\n        [0.7318],\n        [0.7576],\n        [0.3349],\n        [0.9460],\n        [0.1352],\n        [0.9616],\n        [0.3909],\n        [0.1643],\n        [0.8751],\n        [0.5166],\n        [0.4706],\n        [0.5817],\n        [0.5772],\n        [0.6572],\n        [0.8559],\n        [0.1561],\n        [0.0682],\n        [0.1968],\n        [0.3130],\n        [0.1775],\n        [0.3810],\n        [0.3988],\n        [0.0109],\n        [0.1315],\n        [0.3809],\n        [0.0815],\n        [0.8831],\n        [0.4913],\n        [0.6626],\n        [0.9265],\n        [0.1939],\n        [0.4418],\n        [0.0816],\n        [0.9523],\n        [0.8094],\n        [0.2785],\n        [0.8296],\n        [0.4363],\n        [0.2711],\n        [0.1025],\n        [0.1375],\n        [0.1915],\n        [0.8627],\n        [0.0532],\n        [0.8086],\n        [0.4231],\n        [0.0865],\n        [0.7354],\n        [0.8910],\n        [0.5720],\n        [0.3986],\n        [0.8231],\n        [0.2062],\n        [0.1800],\n        [0.8053],\n        [0.5766],\n        [0.6790],\n        [0.0681],\n        [0.8656],\n        [0.9486],\n        [0.7466],\n        [0.1538],\n        [0.8688],\n        [0.8649],\n        [0.1797],\n        [0.7794],\n        [0.7958],\n        [0.0387],\n        [0.4948],\n        [0.2969],\n        [0.0718],\n        [0.4412],\n        [0.2526],\n        [0.2991],\n        [0.8017],\n        [0.3973],\n        [0.5735],\n        [0.4913],\n        [0.7137],\n        [0.8575],\n        [0.4417],\n        [0.0797],\n        [0.9853],\n        [0.5260],\n        [0.4204],\n        [0.3028],\n        [0.6382],\n        [0.4757],\n        [0.9788]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False,  True]]) torch.Size([312, 32])\nthetas tensor([[-2.5398,  0.2915],\n        [ 2.4416,  1.1723],\n        [-1.9394,  1.5867],\n        ...,\n        [ 2.7818,  3.5648],\n        [-0.4469,  1.6847],\n        [ 1.1386, -0.7713]]) torch.Size([9984, 2])\nsamples tensor([[ 2.0442e+00, -1.8015e+00],\n        [ 3.3275e+00,  1.3190e+00],\n        [ 1.4329e+00,  5.2462e-01],\n        [ 7.6452e-01, -6.4446e-01],\n        [ 2.5028e+00, -3.1982e+00],\n        [ 3.9521e+00,  7.3146e-02],\n        [ 2.1743e+00,  2.0659e+00],\n        [ 3.7649e+00, -3.1203e-01],\n        [ 2.3731e+00,  2.2001e+00],\n        [ 2.2483e+00, -1.2906e+00],\n        [ 1.5788e+00,  1.1464e+00],\n        [ 3.1313e+00,  5.5520e-02],\n        [ 3.3035e+00,  8.8973e-01],\n        [ 4.1469e+00, -2.4737e+00],\n        [ 2.0599e+00,  8.1088e-02],\n        [ 1.5476e+00,  1.1020e+00],\n        [ 3.2587e+00, -1.8838e-01],\n        [ 1.4750e+00,  1.8272e+00],\n        [ 3.7849e+00,  6.1147e-01],\n        [ 3.8490e+00,  3.2905e+00],\n        [ 1.5715e+00, -1.5261e+00],\n        [ 2.9411e+00,  1.8404e-01],\n        [ 1.1650e+00, -7.5123e-01],\n        [ 1.0074e+00,  8.1893e-01],\n        [ 2.9538e+00, -2.2228e+00],\n        [ 3.1534e+00, -1.2699e+00],\n        [ 2.1111e+00,  2.9807e-02],\n        [ 3.9705e+00, -1.0218e+00],\n        [ 3.5351e+00, -4.1580e-01],\n        [ 1.8730e+00, -1.8280e+00],\n        [ 3.4835e+00, -3.5071e-01],\n        [ 2.1630e+00,  4.7703e-02],\n        [ 3.2970e+00, -4.5404e-01],\n        [ 1.2534e+00, -4.3327e-01],\n        [ 2.3149e+00,  5.5498e-01],\n        [ 2.4703e+00,  6.3411e-01],\n        [ 1.7158e+00,  1.4922e-01],\n        [ 2.2166e+00, -8.4624e-01],\n        [ 2.7628e+00,  7.7494e-01],\n        [ 1.2229e+00,  1.9598e-01],\n        [ 3.6286e+00,  9.9191e-02],\n        [ 4.0516e+00,  6.0545e-01],\n        [ 2.5287e+00,  1.9384e-01],\n        [ 1.2157e+00,  5.1640e-01],\n        [ 1.3136e+00, -1.8201e+00],\n        [ 1.9021e+00,  7.8224e-01],\n        [ 2.2863e+00,  2.1798e-01],\n        [ 1.3787e+00,  8.2447e-01],\n        [ 2.4349e+00, -8.2046e-01],\n        [ 3.8329e+00,  6.2425e-01],\n        [ 1.8893e+00,  9.0311e-01],\n        [ 3.8410e+00,  7.0309e-01],\n        [ 2.7095e+00,  1.0675e-01],\n        [ 3.0320e+00, -4.5291e-01],\n        [ 1.2621e+00,  2.2868e-01],\n        [ 2.3007e+00,  5.8601e-01],\n        [ 3.4907e+00, -3.4522e-01],\n        [ 6.6371e-01,  9.3289e-02],\n        [ 3.1216e+00, -6.4652e-01],\n        [ 8.9516e-01, -4.7498e-01],\n        [ 4.6435e+00, -1.4310e+00],\n        [ 2.5977e+00,  2.9988e-01],\n        [ 8.8792e-01,  8.4720e-01],\n        [ 1.7364e+00, -3.6484e-01],\n        [ 3.5663e+00,  2.5830e-01],\n        [ 3.0826e+00, -3.1332e+00],\n        [ 1.8741e+00,  3.0242e-02],\n        [ 1.8281e+00, -2.0434e+00],\n        [ 6.9745e-01, -1.5821e+00],\n        [ 1.8567e+00, -1.7819e-01],\n        [ 1.3980e+00, -5.8737e-01],\n        [ 2.7860e+00,  1.5515e+00],\n        [ 2.4318e+00, -1.1472e+00],\n        [ 1.8546e+00,  1.6105e-01],\n        [ 2.2743e+00,  2.8318e-01],\n        [ 2.9555e+00, -1.2945e+00],\n        [ 3.6951e+00,  3.0641e-01],\n        [ 1.1839e+00,  1.0857e-01],\n        [ 2.0254e+00, -8.2972e-01],\n        [ 9.6309e-01,  7.8545e-01],\n        [ 1.8510e+00,  1.0053e+00],\n        [ 2.4314e+00,  7.4874e-01],\n        [ 3.0321e+00, -6.6507e-01],\n        [ 2.9790e+00, -2.9558e+00],\n        [ 2.7249e+00, -4.3747e-01],\n        [ 3.4281e+00, -1.6022e+00],\n        [ 3.1614e+00, -2.5542e-01],\n        [ 2.6505e+00,  1.0736e+00],\n        [ 3.1044e+00, -1.1235e+00],\n        [ 3.5394e+00,  1.2207e+00],\n        [ 1.9147e+00,  2.2944e-02],\n        [ 5.7615e-01,  5.3484e-01],\n        [ 3.6026e+00, -5.7964e-01],\n        [ 9.0297e-01, -1.4353e+00],\n        [ 3.7014e+00, -7.2870e-01],\n        [ 7.8439e-01,  2.1323e+00],\n        [ 3.5711e+00, -3.2512e-01],\n        [ 2.1817e+00,  3.5723e-01],\n        [ 2.3644e+00, -1.3986e+00],\n        [ 2.5358e+00, -1.9386e+00],\n        [ 4.2527e+00, -2.2126e+00],\n        [ 1.1626e+00,  5.7163e-01],\n        [ 7.0410e-01, -1.1741e+00],\n        [ 1.5114e+00,  3.4251e-01],\n        [ 2.9275e+00, -3.6862e-01],\n        [ 3.3159e+00,  1.6353e+00],\n        [ 1.4744e+00, -1.7689e+00],\n        [ 1.1478e+00, -5.5023e-01],\n        [ 9.1922e-01,  1.6841e+00],\n        [ 1.9393e+00,  1.4087e+00],\n        [ 2.5324e+00,  6.4188e-01],\n        [ 3.7546e+00,  8.6849e-01],\n        [ 2.4788e+00, -2.2879e+00],\n        [ 2.7072e+00, -1.2770e+00],\n        [ 2.9120e+00, -1.7857e+00],\n        [ 2.8497e+00,  5.6554e-01],\n        [ 1.9331e+00,  1.0651e+00],\n        [ 2.6093e+00,  1.1965e+00],\n        [ 3.3499e+00,  1.4349e+00],\n        [ 4.5457e+00, -1.3388e+00],\n        [ 1.7383e+00,  1.2181e-01],\n        [ 2.5087e+00, -4.3156e-01],\n        [ 2.7600e+00, -3.4173e-01],\n        [ 4.0551e+00, -7.8325e-02],\n        [ 3.0418e+00, -2.3037e-02],\n        [ 4.6334e+00, -1.5260e-03],\n        [ 2.3545e+00, -1.4323e+00],\n        [ 3.1886e-01, -1.7699e+00],\n        [ 2.7316e+00,  6.9067e-01],\n        [ 1.9692e+00, -4.2702e-01],\n        [ 1.0203e+00,  3.1840e-01],\n        [ 2.5211e+00, -2.3378e+00],\n        [ 2.9020e+00,  1.0941e+00],\n        [ 3.8337e+00, -1.0181e+00],\n        [ 1.4368e+00,  1.1013e+00],\n        [ 3.7871e+00,  4.7799e-01],\n        [ 2.5625e+00,  1.8124e+00],\n        [ 3.7365e+00,  5.3056e-01],\n        [ 3.2318e+00,  7.6441e-01],\n        [ 3.4084e+00,  7.5899e-01],\n        [ 3.4012e+00, -1.5882e+00],\n        [ 1.8196e+00,  1.7598e-01],\n        [ 3.1183e+00,  3.6478e-01],\n        [ 1.7116e+00, -5.0630e-01],\n        [ 2.5592e+00, -4.8156e-01],\n        [ 2.3718e+00,  3.9056e-01],\n        [ 1.9917e+00, -6.4939e-01],\n        [ 1.7483e+00, -8.0113e-01],\n        [ 1.0751e+00, -6.9625e-01],\n        [ 3.6416e+00,  9.3511e-01],\n        [ 7.9531e-01, -1.0621e+00],\n        [ 3.8205e+00,  1.5141e+00],\n        [ 3.0158e+00,  2.3770e-02],\n        [ 2.4771e+00,  8.4656e-01],\n        [ 2.6343e+00, -4.4134e-01],\n        [ 9.0845e-01, -1.7198e-01],\n        [ 8.2095e-01,  1.1707e-01],\n        [ 2.5072e+00, -1.0935e-01],\n        [ 3.0162e+00,  9.2582e-01],\n        [ 2.5225e+00, -1.7382e+00],\n        [ 1.7326e+00, -1.2727e+00],\n        [ 3.8088e+00, -2.5129e-01],\n        [ 2.5858e+00,  5.3149e-01],\n        [ 3.4133e+00,  7.2479e-01],\n        [ 2.5844e+00,  3.7058e-01],\n        [ 1.6420e+00, -7.7769e-01],\n        [ 3.0526e+00,  4.6862e-01],\n        [ 2.3417e+00,  7.0636e-01],\n        [ 3.2641e+00, -1.9903e+00],\n        [ 3.0601e+00, -2.3339e+00],\n        [ 3.7688e+00, -1.5865e+00],\n        [ 2.0277e+00,  1.3229e+00],\n        [ 7.4006e-01, -1.2161e+00],\n        [ 2.0656e+00,  6.9583e-01],\n        [ 2.6907e+00,  7.2051e-02],\n        [ 4.1225e+00, -1.3390e+00],\n        [ 2.4399e+00, -3.5703e-01],\n        [ 1.5481e+00,  5.4865e-02],\n        [ 2.0713e+00,  7.5136e-02],\n        [ 1.9390e+00, -3.9716e-01],\n        [ 1.2266e+00,  2.5383e+00],\n        [ 2.1503e+00, -1.4420e-02],\n        [ 1.2764e+00,  6.6675e-02],\n        [ 1.9429e+00, -1.1010e+00],\n        [ 3.1059e+00, -1.4022e+00],\n        [ 2.3986e+00,  3.3464e-01],\n        [ 3.5000e+00, -9.2510e-02],\n        [ 3.3116e+00,  2.5203e+00],\n        [ 1.7550e+00,  9.7508e-01],\n        [ 2.2934e+00, -1.3579e+00],\n        [ 3.6553e+00, -1.1397e-01],\n        [ 1.5978e+00, -1.8536e+00],\n        [ 1.9828e+00, -2.2436e-01],\n        [ 3.3289e+00,  1.5732e-01],\n        [ 1.7117e+00, -7.6058e-01],\n        [-1.0503e-01,  9.7033e-01],\n        [ 2.2474e+00,  4.9867e-01],\n        [ 2.7475e+00,  2.6160e+00],\n        [ 2.1665e+00, -4.8483e-01],\n        [ 1.4530e+00, -2.0256e+00],\n        [ 3.1002e+00, -2.3294e-01],\n        [ 1.0904e+00,  3.5012e-01],\n        [ 2.6098e+00, -1.1548e+00],\n        [ 1.6602e+00, -3.2037e-01],\n        [ 1.7509e+00, -1.0416e+00],\n        [ 1.4588e+00,  1.5448e-01],\n        [ 2.3926e+00,  9.2322e-01],\n        [ 2.2031e+00, -9.8607e-01],\n        [ 1.4656e+00,  1.5518e+00],\n        [ 3.7606e+00, -9.5345e-02],\n        [ 1.5297e+00,  1.8026e-01],\n        [ 3.3280e+00, -5.0962e-01],\n        [ 3.5030e+00,  4.6334e-01],\n        [ 1.0226e+00, -1.4727e-02],\n        [ 3.0901e+00,  6.7555e-01],\n        [ 3.3732e+00, -1.7804e+00],\n        [ 1.7063e+00,  3.4190e-01],\n        [ 2.5325e+00,  1.4186e+00],\n        [ 3.5023e+00, -6.6156e-01],\n        [ 3.2497e-01,  1.1110e-01],\n        [ 1.9858e+00,  2.7929e-01],\n        [ 2.0423e-01, -1.8222e+00],\n        [ 8.2539e-01, -1.1886e+00],\n        [ 1.3334e+00,  2.5559e-01],\n        [ 3.0346e+00,  9.3262e-01],\n        [ 3.4041e+00, -1.3935e+00],\n        [ 1.4556e+00,  1.5492e-01],\n        [ 4.1462e+00,  7.0608e-01],\n        [ 1.2823e+00,  9.5047e-01],\n        [ 9.9667e-01, -3.9833e-01],\n        [ 2.2579e+00,  6.9469e-01],\n        [ 2.9116e+00,  1.9904e-01],\n        [ 2.7953e+00, -1.6746e-02],\n        [ 3.4405e+00, -1.2108e+00],\n        [ 3.2384e+00,  5.2609e-01],\n        [ 1.9290e+00, -1.0835e+00],\n        [ 3.4119e+00, -1.6368e+00],\n        [ 3.7516e+00,  1.0754e-01],\n        [ 1.9398e+00,  2.9119e-01],\n        [ 1.6267e+00,  4.7013e-01],\n        [ 1.6991e+00, -6.0203e-01],\n        [ 2.3552e+00, -5.8156e-01],\n        [ 8.4641e-01, -2.7308e-01],\n        [ 2.5411e+00, -1.4995e+00],\n        [ 2.5967e+00, -1.2945e+00],\n        [ 3.0599e+00, -2.9050e-02],\n        [ 3.3233e+00, -6.5657e-02],\n        [ 2.8155e+00, -1.9006e-01],\n        [ 1.5985e+00,  3.2574e-01],\n        [ 2.3080e+00,  1.0972e-01],\n        [ 2.4896e+00, -8.2059e-01],\n        [ 2.0235e+00,  7.4246e-01],\n        [ 3.3325e+00,  6.7046e-02],\n        [ 1.1755e+00,  3.6991e-01],\n        [ 3.1568e+00,  1.6271e-02],\n        [ 1.1727e+00,  6.5585e-02],\n        [ 2.3328e+00,  9.5344e-01],\n        [ 2.5953e+00, -1.7207e-01],\n        [ 2.5602e+00, -1.0632e+00],\n        [ 2.1039e+00, -2.2345e-01],\n        [ 3.5559e+00, -2.1393e-01],\n        [ 2.0119e+00, -1.1436e+00],\n        [ 3.0073e+00, -6.5598e-01],\n        [ 3.0269e+00, -9.6309e-01],\n        [ 2.1582e+00,  7.8537e-01],\n        [ 3.5453e+00,  1.6847e-01],\n        [ 7.6600e-01, -2.5132e-01],\n        [ 2.4796e+00,  1.7883e+00],\n        [ 2.3220e+00,  6.9371e-01],\n        [ 1.9585e+00, -5.8415e-02],\n        [ 2.3002e+00, -4.5445e-01],\n        [ 9.5605e-01, -2.4200e-01],\n        [ 1.7047e+00,  5.4767e-01],\n        [ 1.6142e+00,  1.2299e+00],\n        [ 3.6089e+00, -9.5670e-01],\n        [ 7.8435e-01, -2.6425e-01],\n        [ 2.7598e+00,  9.6211e-01],\n        [ 3.2350e+00, -6.1711e-01],\n        [ 2.0825e+00, -9.1135e-01],\n        [ 3.0734e+00,  9.8844e-01],\n        [ 2.0632e+00,  2.0277e-01],\n        [ 1.8580e+00,  6.3679e-01],\n        [ 1.4765e+00,  9.1317e-01],\n        [ 2.9539e+00, -1.3172e-01],\n        [ 2.8326e+00, -7.4192e-01],\n        [ 4.3916e+00, -6.3190e-02],\n        [ 1.3604e+00, -4.8072e-01],\n        [ 4.0457e+00, -4.1175e-01],\n        [ 2.6237e+00, -1.7122e+00],\n        [ 3.1464e+00,  1.8409e+00],\n        [ 2.1667e+00,  7.9232e-01],\n        [ 2.8714e+00, -1.7959e+00],\n        [ 2.6961e+00, -2.0349e+00],\n        [ 1.6884e+00,  1.1167e+00],\n        [ 2.2123e+00, -3.5500e-01],\n        [ 2.5134e+00, -1.0955e+00],\n        [ 2.3172e+00,  1.5414e+00],\n        [ 3.3363e+00,  5.8595e-01],\n        [ 2.4468e+00, -7.0034e-01],\n        [ 1.6565e+00, -7.0683e-01],\n        [ 1.7215e+00,  9.2773e-01],\n        [ 2.3616e+00, -8.7738e-01],\n        [ 7.0451e-01, -1.4777e+00],\n        [ 2.4648e+00, -1.2669e+00],\n        [ 1.5220e+00, -1.2606e+00],\n        [ 2.1526e+00,  1.2105e+00],\n        [ 4.4789e+00,  6.3674e-01],\n        [ 3.5238e+00, -2.2523e+00],\n        [ 3.8436e-01,  9.1575e-01],\n        [ 2.8066e+00, -4.7648e-01],\n        [ 3.1536e+00, -4.6874e-01],\n        [ 1.1386e+00, -7.7129e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.5402e-06, 1.2609e-01, 1.2609e-01,  ..., 9.8936e-01, 9.8936e-01,\n         1.0000e+00],\n        [1.3356e-03, 3.8154e-03, 1.0073e-02,  ..., 9.9470e-01, 9.9613e-01,\n         1.0000e+00],\n        [1.5126e-02, 7.1660e-02, 8.3672e-02,  ..., 4.3341e-01, 4.3341e-01,\n         1.0000e+00],\n        ...,\n        [1.6093e-04, 3.1387e-03, 3.9273e-03,  ..., 7.9909e-01, 8.1274e-01,\n         1.0000e+00],\n        [1.8032e-03, 3.7066e-03, 3.7081e-03,  ..., 9.4648e-01, 9.4655e-01,\n         1.0000e+00],\n        [2.3584e-03, 1.3908e-01, 3.0676e-01,  ..., 9.9105e-01, 1.0000e+00,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.5509],\n        [0.7408],\n        [0.0017],\n        [0.3619],\n        [0.7298],\n        [0.7946],\n        [0.9191],\n        [0.1020],\n        [0.3436],\n        [0.7338],\n        [0.3208],\n        [0.9311],\n        [0.9341],\n        [0.6590],\n        [0.8880],\n        [0.3167],\n        [0.2315],\n        [0.0849],\n        [0.4959],\n        [0.5750],\n        [0.0806],\n        [0.8500],\n        [0.5325],\n        [0.5900],\n        [0.1571],\n        [0.6991],\n        [0.9254],\n        [0.1297],\n        [0.5519],\n        [0.1363],\n        [0.8513],\n        [0.7898],\n        [0.1486],\n        [0.0211],\n        [0.9601],\n        [0.9017],\n        [0.7513],\n        [0.8493],\n        [0.0838],\n        [0.1129],\n        [0.1651],\n        [0.2722],\n        [0.6790],\n        [0.0655],\n        [0.3913],\n        [0.6574],\n        [0.4118],\n        [0.5305],\n        [0.7143],\n        [0.8413],\n        [0.0641],\n        [0.7156],\n        [0.0934],\n        [0.6202],\n        [0.0788],\n        [0.0758],\n        [0.6040],\n        [0.2191],\n        [0.2649],\n        [0.7213],\n        [0.0289],\n        [0.5631],\n        [0.5597],\n        [0.1722],\n        [0.2713],\n        [0.2872],\n        [0.1155],\n        [0.4589],\n        [0.8017],\n        [0.3609],\n        [0.4279],\n        [0.5827],\n        [0.2161],\n        [0.2685],\n        [0.1345],\n        [0.6947],\n        [0.4466],\n        [0.9919],\n        [0.2190],\n        [0.1345],\n        [0.3930],\n        [0.6733],\n        [0.3378],\n        [0.4727],\n        [0.0792],\n        [0.2871],\n        [0.8075],\n        [0.5557],\n        [0.6357],\n        [0.0911],\n        [0.4996],\n        [0.9235],\n        [0.4351],\n        [0.4842],\n        [0.1502],\n        [0.6340],\n        [0.9773],\n        [0.2986],\n        [0.2161],\n        [0.7945],\n        [0.6661],\n        [0.1482],\n        [0.9196],\n        [0.4192],\n        [0.0832],\n        [0.0733],\n        [0.1868],\n        [0.3679],\n        [0.3907],\n        [0.7724],\n        [0.7803],\n        [0.7495],\n        [0.2761],\n        [0.2980],\n        [0.3904],\n        [0.7524],\n        [0.9093],\n        [0.5965],\n        [0.0621],\n        [0.5419],\n        [0.0160],\n        [0.8988],\n        [0.3874],\n        [0.0464],\n        [0.2400],\n        [0.7722],\n        [0.8527],\n        [0.0677],\n        [0.8313],\n        [0.1996],\n        [0.9876],\n        [0.4227],\n        [0.9732],\n        [0.2700],\n        [0.7542],\n        [0.4701],\n        [0.7943],\n        [0.7676],\n        [0.1499],\n        [0.7131],\n        [0.7146],\n        [0.7487],\n        [0.6752],\n        [0.7958],\n        [0.1393],\n        [0.0808],\n        [0.2124],\n        [0.1533],\n        [0.1557],\n        [0.3589],\n        [0.0577],\n        [0.4948],\n        [0.8197],\n        [0.4825],\n        [0.8729],\n        [0.7488],\n        [0.6378],\n        [0.0546],\n        [0.4245],\n        [0.4965],\n        [0.2795],\n        [0.5918],\n        [0.0122],\n        [0.7633],\n        [0.7739],\n        [0.5756],\n        [0.6863],\n        [0.9947],\n        [0.2027],\n        [0.5087],\n        [0.9518],\n        [0.5894],\n        [0.5630],\n        [0.5547],\n        [0.8477],\n        [0.0132],\n        [0.0640],\n        [0.9310],\n        [0.6981],\n        [0.6532],\n        [0.9860],\n        [0.9405],\n        [0.5961],\n        [0.0268],\n        [0.2905],\n        [0.7147],\n        [0.6893],\n        [0.3412],\n        [0.2448],\n        [0.9113],\n        [0.7781],\n        [0.4102],\n        [0.0871],\n        [0.8233],\n        [0.4522],\n        [0.2593],\n        [0.8127],\n        [0.0927],\n        [0.8844],\n        [0.6595],\n        [0.9858],\n        [0.3903],\n        [0.3237],\n        [0.3255],\n        [0.2407],\n        [0.0734],\n        [0.7581],\n        [0.9357],\n        [0.6791],\n        [0.2792],\n        [0.5280],\n        [0.8186],\n        [0.6442],\n        [0.8728],\n        [0.5245],\n        [0.1263],\n        [0.7017],\n        [0.9256],\n        [0.5125],\n        [0.3881],\n        [0.7490],\n        [0.9959],\n        [0.8724],\n        [0.6729],\n        [0.1871],\n        [0.7258],\n        [0.4786],\n        [0.4537],\n        [0.0260],\n        [0.4344],\n        [0.9098],\n        [0.2805],\n        [0.9272],\n        [0.3794],\n        [0.1083],\n        [0.7774],\n        [0.1417],\n        [0.1514],\n        [0.9041],\n        [0.5237],\n        [0.1139],\n        [0.8758],\n        [0.1783],\n        [0.3726],\n        [0.5208],\n        [0.8998],\n        [0.7259],\n        [0.5947],\n        [0.8568],\n        [0.8800],\n        [0.5497],\n        [0.0768],\n        [0.6876],\n        [0.2388],\n        [0.2927],\n        [0.6101],\n        [0.4291],\n        [0.7261],\n        [0.1185],\n        [0.0068],\n        [0.5548],\n        [0.3107],\n        [0.5929],\n        [0.1547],\n        [0.6245],\n        [0.0184],\n        [0.8540],\n        [0.6355],\n        [0.9924],\n        [0.3134],\n        [0.5525],\n        [0.1939],\n        [0.9685],\n        [0.4942],\n        [0.6352],\n        [0.6565],\n        [0.4030],\n        [0.2299],\n        [0.6001],\n        [0.1852],\n        [0.1188],\n        [0.5840],\n        [0.8607],\n        [0.9929],\n        [0.1857],\n        [0.6132],\n        [0.5419],\n        [0.7746],\n        [0.1796],\n        [0.8987],\n        [0.5839],\n        [0.8511],\n        [0.9490],\n        [0.4679],\n        [0.3495],\n        [0.3030],\n        [0.3931],\n        [0.3773],\n        [0.9350],\n        [0.7302],\n        [0.6399],\n        [0.8410],\n        [0.7915],\n        [0.6825],\n        [0.2180],\n        [0.1452],\n        [0.2960],\n        [0.8574],\n        [0.1357],\n        [0.7358],\n        [0.6426],\n        [0.7445]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [ True, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-2.5569, -0.2481],\n        [ 1.6768,  0.4394],\n        [-4.6025, -0.0825],\n        ...,\n        [ 3.5634, -1.3398],\n        [ 1.4263,  1.0908],\n        [-2.5801, -0.1989]]) torch.Size([9984, 2])\nsamples tensor([[ 9.4465e-01, -2.3446e-01],\n        [ 1.2090e+00,  4.1963e-01],\n        [ 2.8659e+00,  2.3457e+00],\n        [ 2.8349e+00,  5.1580e-01],\n        [ 3.3740e+00,  6.6353e-01],\n        [ 2.4144e+00,  5.0690e-01],\n        [ 3.4470e+00, -5.1491e-01],\n        [ 2.6888e+00,  6.1983e-01],\n        [ 2.5981e+00,  2.0505e-01],\n        [ 2.3029e+00, -1.6079e+00],\n        [ 3.4919e+00, -2.2508e+00],\n        [ 3.6943e+00,  1.3212e+00],\n        [ 2.3896e+00,  1.1987e+00],\n        [ 1.8135e+00, -7.9221e-01],\n        [ 2.0047e+00,  2.2434e-01],\n        [ 2.4507e+00, -1.8632e+00],\n        [ 2.7427e+00, -2.1023e+00],\n        [ 2.2545e+00,  1.2306e+00],\n        [ 2.5404e+00, -6.3376e-01],\n        [ 4.2982e+00, -1.5958e+00],\n        [ 1.9394e+00, -1.6240e-01],\n        [ 1.6967e+00,  4.4156e-01],\n        [ 1.9940e+00, -1.9722e+00],\n        [ 1.6035e+00,  1.6048e+00],\n        [ 1.5810e+00,  9.3689e-01],\n        [ 2.2430e+00, -1.4905e+00],\n        [ 2.5439e+00,  7.1784e-01],\n        [ 3.4905e+00, -6.9947e-01],\n        [ 2.3973e+00, -9.7164e-01],\n        [ 1.8408e+00, -1.0460e+00],\n        [ 3.1992e+00, -1.9067e+00],\n        [ 3.9850e+00,  1.6787e+00],\n        [ 5.8794e-01, -5.4670e-01],\n        [ 1.3360e+00,  1.2902e-01],\n        [ 1.1938e+00,  7.1569e-01],\n        [ 3.3918e+00,  3.0509e-01],\n        [ 3.8295e+00, -1.4595e+00],\n        [ 2.0892e+00,  1.1834e+00],\n        [ 1.5495e+00, -4.2162e-01],\n        [ 2.6268e+00, -2.0679e+00],\n        [ 1.7829e+00,  5.3221e-01],\n        [ 2.5595e+00, -6.6197e-02],\n        [ 3.3679e+00,  1.8406e+00],\n        [ 2.5139e+00, -2.6857e-01],\n        [ 2.1528e+00,  2.7227e-01],\n        [ 2.8845e+00,  2.5011e+00],\n        [ 2.3612e+00, -6.4195e-01],\n        [ 3.0329e+00,  1.5136e+00],\n        [ 1.4892e+00,  2.7439e-01],\n        [ 1.6965e+00, -3.9155e-01],\n        [ 2.5996e+00, -7.4970e-01],\n        [ 4.0458e+00,  8.1510e-01],\n        [ 1.4367e+00, -6.1931e-02],\n        [ 2.0353e+00, -1.3979e+00],\n        [ 1.3183e+00,  4.6620e-01],\n        [ 6.0255e-01,  3.1459e-01],\n        [ 3.0666e+00, -1.3641e+00],\n        [ 3.7882e+00, -1.3989e-01],\n        [ 4.5480e+00,  4.1013e-01],\n        [ 1.7335e+00,  1.4482e+00],\n        [ 6.8403e-01,  7.3779e-01],\n        [ 3.3810e+00, -4.4934e-01],\n        [ 1.5260e+00,  9.5011e-01],\n        [ 2.5885e+00, -2.9189e-01],\n        [ 3.0991e+00,  8.1090e-01],\n        [ 1.4316e+00, -1.8673e+00],\n        [ 2.5955e+00,  1.1176e+00],\n        [ 8.0518e-01,  1.0024e+00],\n        [ 1.5613e+00,  4.6266e-01],\n        [ 7.9492e-01,  2.2200e+00],\n        [ 3.4295e+00,  6.7635e-01],\n        [ 2.5825e+00, -6.2900e-01],\n        [ 1.8801e+00,  1.0365e+00],\n        [ 3.0008e+00,  1.3378e-01],\n        [ 1.0483e+00,  1.3678e+00],\n        [ 1.9868e+00,  8.7625e-01],\n        [ 2.0117e+00,  1.3045e-01],\n        [ 1.1057e+00,  1.2163e+00],\n        [ 1.4802e+00, -8.7190e-01],\n        [ 2.4069e+00,  1.6149e+00],\n        [ 1.3365e+00, -9.1076e-01],\n        [ 1.9360e+00, -4.0805e-01],\n        [ 2.1678e+00, -1.3632e+00],\n        [ 3.3636e+00,  9.6959e-02],\n        [ 4.9013e+00,  1.5847e-01],\n        [ 2.9988e+00, -3.4680e+00],\n        [ 1.4783e+00, -1.8584e-01],\n        [ 2.5403e+00,  6.8921e-01],\n        [ 2.2983e+00, -2.9616e-02],\n        [-7.7548e-02, -7.6729e-01],\n        [ 2.4797e+00,  1.3540e+00],\n        [ 2.6372e+00,  5.0600e-01],\n        [ 2.2004e+00,  5.7186e-01],\n        [ 1.7097e+00, -1.4503e+00],\n        [ 3.6435e+00, -8.1459e-01],\n        [ 3.8503e+00, -1.5902e-01],\n        [ 3.1039e+00,  1.7156e-01],\n        [ 2.0362e+00,  1.3449e+00],\n        [ 3.0264e+00, -1.2703e+00],\n        [ 2.6763e+00, -2.1001e-01],\n        [ 3.5552e+00,  6.3673e-01],\n        [ 3.4315e+00,  1.2186e+00],\n        [ 2.5109e+00, -4.3098e-01],\n        [ 9.5793e-01,  1.5122e+00],\n        [ 2.1115e+00, -5.9865e-01],\n        [ 3.1604e-01,  5.9103e-01],\n        [ 3.3603e+00, -5.2078e-01],\n        [ 3.2158e+00, -2.9346e-01],\n        [ 3.7283e+00, -1.4784e+00],\n        [ 2.2775e+00,  1.0680e+00],\n        [ 3.1680e+00,  6.4705e-01],\n        [ 3.5470e+00,  8.0580e-01],\n        [ 2.0336e+00, -4.6831e-01],\n        [ 1.8693e+00, -2.0002e+00],\n        [ 2.1384e+00,  1.2370e-01],\n        [ 6.7199e-01,  6.1076e-01],\n        [ 2.6266e+00,  6.7841e-01],\n        [ 3.1607e+00,  9.9457e-01],\n        [ 1.0979e+00, -5.5348e-01],\n        [ 3.4486e+00,  4.4863e-01],\n        [ 5.3888e-01, -2.3906e-01],\n        [ 2.5151e+00, -1.1135e-01],\n        [ 2.0725e+00,  6.4742e-01],\n        [ 2.8938e+00,  1.4065e+00],\n        [ 2.1013e+00,  4.4395e-01],\n        [ 4.9675e+00, -5.0754e-01],\n        [ 2.6856e+00, -1.0557e-01],\n        [ 2.1044e+00, -1.3740e-01],\n        [ 1.9182e+00, -4.5195e-01],\n        [ 2.6522e+00,  5.0992e-01],\n        [ 3.6006e+00,  2.1739e-01],\n        [ 1.8640e+00, -4.5732e-01],\n        [ 1.3802e+00,  1.3187e+00],\n        [ 2.4586e+00, -1.8125e-01],\n        [ 2.6318e+00,  1.3631e-01],\n        [ 5.7959e-01, -5.2738e-01],\n        [ 1.3770e+00,  1.3234e+00],\n        [ 1.5677e+00, -9.4991e-01],\n        [ 2.3089e+00, -2.6085e-02],\n        [ 1.2195e+00,  5.0537e-01],\n        [ 1.6732e+00, -7.8322e-01],\n        [ 4.0654e+00,  3.8137e-01],\n        [ 1.9644e+00, -1.5988e+00],\n        [ 2.7831e+00,  9.3959e-02],\n        [ 2.2181e+00,  4.6304e-01],\n        [ 5.5457e-01,  4.5027e-01],\n        [ 1.7443e+00, -7.6013e-01],\n        [ 1.2149e+00, -7.6907e-01],\n        [ 1.5011e+00, -4.7869e-01],\n        [ 1.3237e+00, -2.4036e+00],\n        [ 3.0128e+00, -1.9333e+00],\n        [ 2.9964e+00, -1.2087e+00],\n        [ 2.6081e+00,  8.3839e-01],\n        [ 3.0818e+00,  4.1936e-01],\n        [ 2.9306e+00, -4.7032e-01],\n        [ 2.5931e+00, -1.2149e+00],\n        [ 2.1189e+00, -8.3079e-02],\n        [ 1.7015e+00,  1.7802e+00],\n        [ 3.1228e+00, -1.9647e+00],\n        [ 2.2682e+00,  5.8294e-01],\n        [ 2.8767e+00, -1.3449e+00],\n        [ 1.1040e+00,  9.3961e-01],\n        [ 4.4850e-01, -6.1503e-01],\n        [ 4.6398e+00,  1.9965e+00],\n        [ 4.5203e+00, -1.6092e-01],\n        [ 2.3531e+00,  5.1516e-01],\n        [ 1.8958e+00,  5.6769e-01],\n        [ 9.3344e-01,  8.8539e-01],\n        [ 3.4472e+00, -9.1380e-01],\n        [ 2.7846e+00, -5.3224e-01],\n        [ 3.8380e+00,  2.0410e-01],\n        [ 3.2883e+00,  5.9560e-01],\n        [ 3.2205e+00,  1.2581e+00],\n        [ 1.6359e+00,  2.9902e-01],\n        [ 2.5347e+00,  6.3461e-02],\n        [ 8.8694e-01,  4.5189e-01],\n        [ 1.9279e+00,  6.7393e-01],\n        [ 2.9567e+00,  4.1244e-01],\n        [ 4.3998e+00, -1.7002e+00],\n        [ 2.1083e+00, -1.8864e-01],\n        [ 1.7792e+00,  1.5109e+00],\n        [ 1.3816e+00, -1.1702e+00],\n        [ 2.6524e+00, -1.5869e+00],\n        [ 1.8234e+00,  3.8121e-01],\n        [ 2.0838e+00, -2.7524e-01],\n        [ 2.2850e+00, -5.9603e-01],\n        [ 2.2240e+00,  1.0310e+00],\n        [ 2.8653e+00,  8.2505e-01],\n        [ 3.9020e+00,  2.2765e-01],\n        [ 3.2037e+00,  5.5588e-01],\n        [ 2.7587e+00, -3.2760e-01],\n        [ 4.1195e+00,  4.6594e-01],\n        [ 2.2710e+00,  1.0582e+00],\n        [ 2.3805e+00, -1.7134e+00],\n        [ 2.0196e+00,  1.1387e-02],\n        [ 2.0368e+00,  4.2945e-01],\n        [ 2.0048e+00, -1.4544e+00],\n        [ 2.3931e+00, -2.6238e-01],\n        [ 2.0768e+00,  1.7638e-01],\n        [ 1.8870e+00,  3.8584e-01],\n        [ 2.5267e+00, -4.7560e-01],\n        [ 2.1647e+00, -6.7389e-02],\n        [ 4.6353e+00, -6.9686e-01],\n        [ 2.9317e+00,  1.4731e-02],\n        [ 3.5791e+00, -4.0773e-01],\n        [ 2.0688e+00, -9.1721e-01],\n        [ 2.6170e+00, -1.2054e+00],\n        [ 2.1372e+00,  1.8650e+00],\n        [ 1.6575e+00, -4.1580e-01],\n        [ 2.7901e+00, -7.1693e-01],\n        [ 2.1866e+00, -1.2893e+00],\n        [ 2.7719e+00,  4.0585e-01],\n        [ 3.5481e+00,  1.0470e+00],\n        [ 3.7275e+00,  8.1090e-01],\n        [ 2.1364e+00, -3.7648e-01],\n        [ 2.0460e+00, -3.2741e-01],\n        [ 1.7094e+00,  1.2552e+00],\n        [ 7.1128e-01, -8.8622e-01],\n        [ 1.3733e+00,  5.3501e-01],\n        [ 3.5017e+00,  5.5906e-04],\n        [ 1.9199e+00, -4.7160e-01],\n        [ 2.4642e+00,  2.3755e-01],\n        [ 3.8386e-01,  4.1791e-01],\n        [ 4.6303e-01,  2.6272e-02],\n        [ 2.3078e+00, -1.3559e+00],\n        [ 2.3489e+00, -2.4939e+00],\n        [ 2.0253e+00, -1.1873e+00],\n        [ 3.0367e+00, -1.8201e+00],\n        [ 2.7381e+00,  1.0830e-01],\n        [ 2.7699e+00,  1.1936e+00],\n        [ 6.9991e-01, -1.9942e-01],\n        [ 3.4647e+00,  4.4621e-01],\n        [ 2.2718e+00,  4.2421e-01],\n        [ 3.5592e+00,  2.4017e-01],\n        [ 1.7488e+00,  1.4463e+00],\n        [ 2.3791e+00, -7.2610e-01],\n        [ 1.2879e+00, -2.0272e-01],\n        [ 1.1311e+00, -4.9749e-01],\n        [ 2.6643e+00,  3.0323e-01],\n        [ 5.6670e-01,  6.4869e-01],\n        [ 3.0807e+00, -8.1691e-01],\n        [ 2.7298e+00, -1.6239e-01],\n        [ 3.2106e+00,  2.0176e+00],\n        [ 3.0646e+00, -1.2538e-01],\n        [ 3.5736e+00,  1.2502e+00],\n        [ 1.9513e+00,  4.7108e-03],\n        [ 2.6571e+00, -1.7541e+00],\n        [ 1.0473e+00, -1.8007e+00],\n        [ 1.0623e+00, -1.5290e-01],\n        [ 2.9806e+00,  3.3996e-01],\n        [ 9.7874e-01,  2.2696e-01],\n        [ 2.6513e+00, -3.4294e-01],\n        [ 2.0948e+00,  1.2867e+00],\n        [ 1.3257e+00, -4.7430e-01],\n        [ 3.9575e+00,  1.0578e+00],\n        [ 1.9520e+00, -1.0342e+00],\n        [ 2.3234e+00,  1.6824e+00],\n        [ 4.5130e+00,  1.2733e+00],\n        [ 3.1121e+00, -5.7663e-01],\n        [ 2.0815e+00,  7.1261e-01],\n        [ 3.2639e+00, -4.0921e-01],\n        [ 3.4401e+00,  1.0147e+00],\n        [ 1.4381e+00, -5.1068e-01],\n        [ 2.0222e+00,  3.7102e-01],\n        [ 9.8607e-01,  5.9622e-01],\n        [ 4.6237e+00,  2.7829e-02],\n        [ 2.4984e+00, -6.0574e-01],\n        [ 2.4896e+00, -1.1569e+00],\n        [ 2.3530e+00,  1.8874e+00],\n        [ 1.7697e+00,  7.7348e-01],\n        [ 3.2493e+00,  1.4120e+00],\n        [ 4.0260e+00, -2.0153e+00],\n        [ 3.0970e+00, -1.0247e+00],\n        [ 4.8188e-01,  7.8215e-01],\n        [ 2.6187e+00, -1.0867e+00],\n        [ 1.5385e+00, -1.3442e+00],\n        [ 3.6760e+00, -2.1728e-01],\n        [ 2.2615e+00, -2.8150e-01],\n        [ 1.6357e+00,  4.6396e-01],\n        [ 1.6725e+00, -3.5619e-01],\n        [ 3.0172e+00, -7.9699e-01],\n        [ 2.3309e+00, -1.3958e+00],\n        [ 2.3978e+00,  1.5676e+00],\n        [ 1.2853e+00,  1.3336e+00],\n        [ 1.2547e+00, -3.0152e-01],\n        [ 1.2817e+00, -5.3834e-01],\n        [ 2.8158e+00,  8.8797e-02],\n        [ 2.7121e+00,  2.5483e-01],\n        [ 2.8557e+00, -4.7001e-01],\n        [ 4.0173e+00,  4.6909e-01],\n        [ 1.9979e+00, -4.6792e-01],\n        [ 4.2862e+00, -1.5789e-01],\n        [ 3.0256e+00, -1.4461e+00],\n        [ 2.6919e+00, -5.5203e-01],\n        [ 2.9096e+00,  2.1037e+00],\n        [ 1.9606e+00,  1.1717e+00],\n        [ 4.4930e+00, -8.5447e-01],\n        [ 3.1394e+00, -6.1570e-01],\n        [ 1.1991e+00,  3.2431e-01],\n        [ 3.1769e+00, -1.0275e+00],\n        [ 2.1122e+00,  1.0869e+00],\n        [ 1.5272e+00, -3.5797e-01],\n        [ 1.2261e+00, -1.3426e+00],\n        [ 2.8860e+00,  3.7251e-01],\n        [ 2.2553e-01, -6.0205e-01],\n        [ 3.6654e+00,  3.4749e-01],\n        [ 1.9571e+00,  5.7163e-01],\n        [ 3.0612e+00, -1.2848e+00],\n        [ 4.8547e+00, -1.3816e-01],\n        [ 2.6414e+00, -4.8991e-01],\n        [ 2.0949e+00,  2.9282e-01],\n        [ 2.6356e+00, -1.4066e+00]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[1.7882e-03, 2.1099e-02, 4.4053e-02,  ..., 9.9999e-01, 1.0000e+00,\n         1.0000e+00],\n        [1.2660e-02, 1.3937e-02, 2.2551e-02,  ..., 9.8825e-01, 9.8841e-01,\n         1.0000e+00],\n        [4.6043e-04, 2.5078e-01, 2.9011e-01,  ..., 8.4691e-01, 8.4692e-01,\n         1.0000e+00],\n        ...,\n        [1.8127e-10, 2.1064e-01, 2.1067e-01,  ..., 9.9999e-01, 9.9999e-01,\n         1.0000e+00],\n        [6.4549e-03, 6.4611e-03, 7.9589e-03,  ..., 9.9806e-01, 9.9971e-01,\n         1.0000e+00],\n        [1.0792e-02, 1.3065e-02, 1.3496e-02,  ..., 9.6661e-01, 9.9752e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.0757],\n        [0.3916],\n        [0.6416],\n        [0.4547],\n        [0.5854],\n        [0.0798],\n        [0.7822],\n        [0.7302],\n        [0.3376],\n        [0.8744],\n        [0.9476],\n        [0.3477],\n        [0.9516],\n        [0.8770],\n        [0.1432],\n        [0.6987],\n        [0.2824],\n        [0.6219],\n        [0.7012],\n        [0.2802],\n        [0.7194],\n        [0.9598],\n        [0.3065],\n        [0.5219],\n        [0.3276],\n        [0.3450],\n        [0.3576],\n        [0.9737],\n        [0.9681],\n        [0.6198],\n        [0.8215],\n        [0.7792],\n        [0.4650],\n        [0.8190],\n        [0.1125],\n        [0.3709],\n        [0.6695],\n        [0.9017],\n        [0.9106],\n        [0.4421],\n        [0.0473],\n        [0.4480],\n        [0.3344],\n        [0.3950],\n        [0.1529],\n        [0.2866],\n        [0.9508],\n        [0.8101],\n        [0.7708],\n        [0.5943],\n        [0.0569],\n        [0.3846],\n        [0.3409],\n        [0.4594],\n        [0.1666],\n        [0.9566],\n        [0.1472],\n        [0.7229],\n        [0.1622],\n        [0.1258],\n        [0.6381],\n        [0.3982],\n        [0.0927],\n        [0.1655],\n        [0.5280],\n        [0.1708],\n        [0.4642],\n        [0.5233],\n        [0.0167],\n        [0.6844],\n        [0.0186],\n        [0.9707],\n        [0.5550],\n        [0.2992],\n        [0.5020],\n        [0.2857],\n        [0.6674],\n        [0.7013],\n        [0.0758],\n        [0.3462],\n        [0.0781],\n        [0.4204],\n        [0.3928],\n        [0.0015],\n        [0.4487],\n        [0.9273],\n        [0.1282],\n        [0.1345],\n        [0.1797],\n        [0.4857],\n        [0.9437],\n        [0.5002],\n        [0.9667],\n        [0.3070],\n        [0.0234],\n        [0.4717],\n        [0.1912],\n        [0.3652],\n        [0.7834],\n        [0.1881],\n        [0.8992],\n        [0.2890],\n        [0.5798],\n        [0.6198],\n        [0.8974],\n        [0.1439],\n        [0.2453],\n        [0.7806],\n        [0.1384],\n        [0.4448],\n        [0.6019],\n        [0.0197],\n        [0.4480],\n        [0.4555],\n        [0.7431],\n        [0.4192],\n        [0.2760],\n        [0.8531],\n        [0.8643],\n        [0.9893],\n        [0.7554],\n        [0.8792],\n        [0.0592],\n        [0.1637],\n        [0.5494],\n        [0.4462],\n        [0.1062],\n        [0.1791],\n        [0.1230],\n        [0.4317],\n        [0.0428],\n        [0.1663],\n        [0.6335],\n        [0.7421],\n        [0.1949],\n        [0.4120],\n        [0.3806],\n        [0.2991],\n        [0.0401],\n        [0.1919],\n        [0.7387],\n        [0.4435],\n        [0.4141],\n        [0.7185],\n        [0.6022],\n        [0.5185],\n        [0.7759],\n        [0.9616],\n        [0.7373],\n        [0.4671],\n        [0.4691],\n        [0.3762],\n        [0.2731],\n        [0.7440],\n        [0.1166],\n        [0.5791],\n        [0.4492],\n        [0.5418],\n        [0.9928],\n        [0.8442],\n        [0.7326],\n        [0.4196],\n        [0.0893],\n        [0.3789],\n        [0.8345],\n        [0.7966],\n        [0.7337],\n        [0.5587],\n        [0.1243],\n        [0.4673],\n        [0.9022],\n        [0.7814],\n        [0.0544],\n        [0.8960],\n        [0.6265],\n        [0.3738],\n        [0.0105],\n        [0.5394],\n        [0.2965],\n        [0.7045],\n        [0.6250],\n        [0.4906],\n        [0.4528],\n        [0.5629],\n        [0.5089],\n        [0.3910],\n        [0.8300],\n        [0.5809],\n        [0.0934],\n        [0.2101],\n        [0.1711],\n        [0.6041],\n        [0.3842],\n        [0.9518],\n        [0.6800],\n        [0.9563],\n        [0.2613],\n        [0.5013],\n        [0.6101],\n        [0.9231],\n        [0.4637],\n        [0.6387],\n        [0.1900],\n        [0.4496],\n        [0.3672],\n        [0.4143],\n        [0.1759],\n        [0.2801],\n        [0.0163],\n        [0.5276],\n        [0.9596],\n        [0.2049],\n        [0.2800],\n        [0.8972],\n        [0.3283],\n        [0.6170],\n        [0.9573],\n        [0.9999],\n        [0.4707],\n        [0.7931],\n        [0.4476],\n        [0.8267],\n        [0.6722],\n        [0.5954],\n        [0.2604],\n        [0.7544],\n        [0.9113],\n        [0.1753],\n        [0.0039],\n        [0.8374],\n        [0.2888],\n        [0.1811],\n        [0.7138],\n        [0.7232],\n        [0.1475],\n        [0.6237],\n        [0.0553],\n        [0.1054],\n        [0.0714],\n        [0.0058],\n        [0.4417],\n        [0.3966],\n        [0.7569],\n        [0.9961],\n        [0.7481],\n        [0.2735],\n        [0.9212],\n        [0.1026],\n        [0.6359],\n        [0.3645],\n        [0.5020],\n        [0.9778],\n        [0.0852],\n        [0.4823],\n        [0.5121],\n        [0.7376],\n        [0.9758],\n        [0.3936],\n        [0.6038],\n        [0.3034],\n        [0.8922],\n        [0.1101],\n        [0.0816],\n        [0.5724],\n        [0.3276],\n        [0.7480],\n        [0.0776],\n        [0.1360],\n        [0.3203],\n        [0.6545],\n        [0.0333],\n        [0.7610],\n        [0.5937],\n        [0.3421],\n        [0.2936],\n        [0.7624],\n        [0.4995],\n        [0.8118],\n        [0.6577],\n        [0.1584],\n        [0.4628],\n        [0.0880],\n        [0.3155],\n        [0.9738],\n        [0.6466],\n        [0.0637],\n        [0.6723],\n        [0.6665],\n        [0.2690],\n        [0.4775],\n        [0.1199],\n        [0.5163],\n        [0.3720],\n        [0.9865],\n        [0.2600],\n        [0.0852],\n        [0.8539],\n        [0.8713],\n        [0.7697],\n        [0.9806],\n        [0.3280],\n        [0.8770],\n        [0.9221],\n        [0.2711],\n        [0.0139],\n        [0.9982],\n        [0.7911],\n        [0.4998],\n        [0.4678],\n        [0.6947],\n        [0.7828],\n        [0.0647]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-0.2002,  0.7387],\n        [ 1.9408,  1.9504],\n        [ 2.0480,  1.8723],\n        ...,\n        [-2.7503, -0.3761],\n        [ 1.8729,  0.8758],\n        [ 0.4581,  1.1236]]) torch.Size([9984, 2])\nsamples tensor([[ 2.6878e+00,  1.3261e+00],\n        [ 3.4066e+00,  2.4546e-02],\n        [ 1.6464e+00,  7.8408e-01],\n        [ 2.0820e+00, -7.2677e-01],\n        [ 1.7768e+00, -9.4129e-01],\n        [ 3.3076e+00,  1.2447e+00],\n        [ 2.4347e+00, -2.0747e-01],\n        [ 2.8258e+00, -2.6620e-01],\n        [ 1.9410e+00, -5.3959e-01],\n        [ 7.4208e-01, -2.6276e-01],\n        [ 1.0669e+00, -1.6217e-01],\n        [ 2.1667e+00, -1.5013e+00],\n        [ 2.7867e+00,  6.9394e-01],\n        [ 2.5353e+00, -1.0237e+00],\n        [ 1.7565e+00, -8.5177e-01],\n        [ 3.4714e+00, -1.8448e+00],\n        [ 6.1685e-01,  3.2175e-01],\n        [ 2.0142e+00, -1.6190e+00],\n        [ 2.1603e+00, -1.0059e-01],\n        [ 2.1912e+00, -5.4398e-01],\n        [ 3.9526e+00, -7.3560e-01],\n        [-2.4392e-01,  1.7425e+00],\n        [ 1.1925e+00, -1.9766e-01],\n        [ 3.1322e+00, -1.6643e-01],\n        [ 9.8356e-01, -1.1432e+00],\n        [ 2.5932e-01, -1.2623e+00],\n        [ 3.8550e+00,  5.1838e-01],\n        [ 1.6067e+00,  7.4941e-01],\n        [ 3.2844e+00, -2.5760e-01],\n        [ 2.4460e+00, -2.0402e+00],\n        [ 2.2626e+00, -8.3186e-02],\n        [ 1.5476e+00, -1.4055e+00],\n        [ 2.3228e+00,  6.9896e-01],\n        [ 4.6723e-02,  1.8764e+00],\n        [ 8.5435e-01, -6.2414e-01],\n        [ 3.2188e+00,  3.9321e-01],\n        [ 2.3713e+00, -7.9448e-01],\n        [ 3.6205e+00, -1.1241e+00],\n        [ 2.9857e+00, -5.2453e-01],\n        [ 2.3255e+00,  1.0067e+00],\n        [ 1.1471e+00, -6.6896e-01],\n        [ 2.0624e+00, -5.7811e-01],\n        [ 2.1823e+00, -1.4513e+00],\n        [ 3.9268e+00,  1.9144e+00],\n        [ 3.0661e+00, -3.6011e-01],\n        [ 2.9586e+00,  2.7681e-01],\n        [ 2.7726e+00,  1.1502e+00],\n        [ 1.9252e+00,  1.8858e-01],\n        [ 1.2235e+00, -1.1125e+00],\n        [ 1.9282e+00, -4.8844e-02],\n        [ 1.4432e+00,  2.9659e-01],\n        [ 1.7867e+00,  1.1400e+00],\n        [ 2.5154e+00, -4.4715e-01],\n        [ 2.1010e+00, -1.1909e+00],\n        [ 4.0603e+00, -6.7751e-01],\n        [ 1.6985e+00,  8.4455e-01],\n        [ 1.4308e+00,  2.1292e+00],\n        [ 3.0124e+00,  1.2955e+00],\n        [ 2.5081e+00,  1.7288e-01],\n        [ 4.4726e+00, -2.9458e-01],\n        [ 2.2575e+00,  3.8661e-01],\n        [ 4.0061e+00, -1.1956e+00],\n        [ 2.7027e+00,  1.9238e-01],\n        [ 1.9009e+00, -1.1804e+00],\n        [ 1.9088e+00, -9.2929e-01],\n        [ 3.4974e+00, -2.0804e+00],\n        [ 2.9034e+00,  7.8345e-02],\n        [ 2.6149e-01, -1.8164e+00],\n        [ 1.9589e+00, -9.3450e-01],\n        [ 8.0747e-01,  1.4433e+00],\n        [ 2.1571e+00, -2.5132e-01],\n        [ 1.9350e+00, -3.8128e-01],\n        [ 1.5993e+00,  5.4769e-01],\n        [ 3.4589e+00, -2.2046e+00],\n        [ 1.4582e+00, -7.4516e-02],\n        [ 1.6090e+00, -2.7187e-01],\n        [-8.9162e-03,  1.5055e+00],\n        [ 2.9699e+00, -4.6591e-01],\n        [ 1.6396e+00,  4.0576e-01],\n        [ 1.0744e+00, -8.6388e-01],\n        [ 2.8351e+00, -1.2444e+00],\n        [ 3.0166e+00, -6.9813e-01],\n        [ 2.7597e+00, -1.8137e+00],\n        [ 6.0038e-01,  3.8648e-01],\n        [ 3.2005e+00,  1.1144e+00],\n        [ 1.7540e+00,  4.3864e-01],\n        [ 2.2413e+00, -3.8608e-01],\n        [ 2.9750e+00, -2.9513e-01],\n        [ 1.7227e+00, -4.2181e-01],\n        [ 2.7041e+00, -8.0724e-02],\n        [ 2.3135e+00,  1.2081e+00],\n        [ 1.4985e+00,  1.2080e-01],\n        [ 3.3391e+00,  1.2814e-02],\n        [ 1.5818e+00, -1.1508e+00],\n        [ 2.9206e+00,  1.3897e+00],\n        [ 3.9947e-01,  2.7718e-01],\n        [ 1.6121e+00,  1.6381e+00],\n        [ 2.3019e+00,  7.8125e-01],\n        [ 1.8557e+00, -8.4357e-01],\n        [ 1.9962e+00, -8.6626e-01],\n        [ 2.7862e+00, -1.4272e-02],\n        [ 8.4725e-01,  8.8015e-01],\n        [ 1.7180e+00, -8.1290e-01],\n        [ 1.3603e+00,  8.1625e-02],\n        [ 2.3960e+00,  7.8330e-01],\n        [ 2.5899e+00,  7.9896e-02],\n        [ 1.5570e+00, -8.0747e-02],\n        [ 2.8744e+00, -4.0237e-01],\n        [ 2.0693e+00,  3.4737e-01],\n        [ 4.8879e+00,  3.9138e-02],\n        [ 1.1642e+00,  1.0078e+00],\n        [ 3.5324e+00,  7.4355e-01],\n        [ 1.9196e+00, -5.5366e-01],\n        [ 2.3702e+00,  1.5174e-02],\n        [ 1.4986e+00, -1.2053e+00],\n        [ 3.4773e+00, -1.8924e+00],\n        [ 1.8055e+00,  1.8814e-01],\n        [ 3.9956e+00, -9.2819e-01],\n        [ 3.7818e+00,  6.6893e-01],\n        [ 3.8633e+00,  1.4014e+00],\n        [ 1.0175e+00,  7.9493e-01],\n        [ 1.0300e+00,  9.3625e-01],\n        [ 4.7396e+00,  7.3240e-01],\n        [ 1.4284e+00,  4.4080e-01],\n        [ 2.8136e+00, -1.6386e+00],\n        [ 2.2827e+00,  1.0416e+00],\n        [ 2.4800e+00,  1.2476e+00],\n        [ 2.3192e+00, -1.3841e-01],\n        [ 2.8275e+00, -5.5313e-01],\n        [ 2.2464e+00, -2.1473e+00],\n        [ 1.7604e+00,  3.1733e-01],\n        [ 2.9167e+00,  1.0792e+00],\n        [ 2.3779e+00, -1.9215e+00],\n        [ 1.2790e+00, -6.9525e-01],\n        [ 6.3659e-01,  7.7261e-01],\n        [ 2.4412e+00,  8.7796e-02],\n        [ 1.2991e+00, -8.0705e-01],\n        [ 3.4555e+00,  1.9318e-02],\n        [ 1.4849e+00, -7.5628e-02],\n        [ 2.6874e+00, -3.2686e-01],\n        [ 1.4486e+00,  9.5522e-01],\n        [ 3.6548e+00,  1.9590e-02],\n        [ 1.6864e+00,  4.4919e-02],\n        [ 3.2609e+00,  5.6915e-01],\n        [ 2.2237e+00, -1.5155e+00],\n        [ 3.3667e+00, -1.4203e+00],\n        [ 2.5233e+00,  1.4459e+00],\n        [ 3.5280e+00,  9.7744e-01],\n        [ 1.5254e+00, -8.2393e-01],\n        [ 3.4114e+00,  1.1877e+00],\n        [ 3.4409e+00, -5.9070e-02],\n        [ 3.5504e+00, -2.3453e-01],\n        [ 1.0062e+00,  8.4509e-02],\n        [ 1.6195e+00,  1.0167e+00],\n        [ 3.4667e+00, -7.8671e-02],\n        [ 3.4679e+00, -4.5063e-01],\n        [ 3.5283e+00, -1.3215e+00],\n        [ 3.5237e+00,  1.8265e-01],\n        [ 6.0988e-01,  7.6545e-02],\n        [ 4.5750e-01, -1.1002e+00],\n        [ 1.7065e+00, -1.0079e-01],\n        [ 2.7975e+00, -4.5446e-01],\n        [ 8.9249e-01,  7.7221e-02],\n        [ 3.0623e+00, -3.4215e-01],\n        [ 1.6324e+00, -2.7539e-01],\n        [ 2.9944e+00, -9.8567e-01],\n        [ 1.8898e+00, -5.6851e-01],\n        [ 3.1553e+00, -1.8841e-01],\n        [ 2.4355e+00, -5.0765e-01],\n        [ 1.6719e+00, -3.6262e-01],\n        [ 1.8999e+00,  2.7782e-01],\n        [ 2.7331e+00, -8.7162e-02],\n        [ 1.2820e+00,  1.7853e-01],\n        [ 1.7429e+00,  3.5802e-01],\n        [ 2.5248e+00, -9.0166e-01],\n        [ 1.9012e+00,  1.5675e+00],\n        [ 3.0172e+00,  1.8867e+00],\n        [ 3.0612e+00, -9.4568e-02],\n        [ 1.7977e+00, -9.4829e-01],\n        [ 2.0191e+00,  1.5521e+00],\n        [ 2.6713e+00, -1.1962e+00],\n        [ 2.2907e+00,  1.4169e+00],\n        [ 1.8160e+00, -1.8481e+00],\n        [ 2.6845e+00, -4.2901e-01],\n        [ 2.0774e+00,  4.9769e-03],\n        [ 2.6298e+00, -2.5692e-01],\n        [ 2.2470e+00, -2.8095e+00],\n        [ 2.5502e+00,  1.2866e+00],\n        [ 3.1793e+00, -6.4597e-01],\n        [ 2.2475e+00, -4.6107e-01],\n        [ 1.7950e+00, -6.2458e-01],\n        [ 2.4558e+00, -7.6357e-01],\n        [ 1.0543e+00,  4.0339e-02],\n        [ 2.6044e+00, -2.6806e-01],\n        [ 2.5857e+00,  8.6033e-01],\n        [ 2.2232e+00,  1.4098e-01],\n        [ 2.1324e+00, -5.8753e-02],\n        [ 1.5573e+00, -2.6463e-01],\n        [ 2.3764e+00,  5.5077e-01],\n        [ 2.1636e+00,  1.3291e+00],\n        [ 2.1473e+00,  2.0066e+00],\n        [ 2.3944e+00, -9.2114e-01],\n        [ 1.9504e+00, -8.5567e-01],\n        [ 2.7166e+00,  1.9699e-02],\n        [ 1.6569e+00,  1.0500e+00],\n        [ 1.2478e+00,  1.5389e-01],\n        [ 2.8387e+00, -6.4378e-01],\n        [-1.0130e-01, -8.0157e-01],\n        [ 2.0982e+00,  5.2139e-02],\n        [ 2.0562e+00, -1.2179e+00],\n        [-3.6495e-01, -1.0504e-01],\n        [ 1.3131e+00, -3.4325e-01],\n        [ 1.8047e+00, -8.1844e-01],\n        [ 3.3223e+00,  2.0677e+00],\n        [ 2.3990e+00, -1.2857e-01],\n        [ 2.1962e+00,  5.2815e-01],\n        [ 2.0757e+00,  6.8862e-01],\n        [ 8.8375e-01, -1.7087e+00],\n        [ 3.7532e+00,  2.6638e-01],\n        [ 1.6686e+00, -9.4663e-01],\n        [ 2.3351e+00, -5.4017e-01],\n        [ 2.5688e+00, -5.5408e-01],\n        [ 1.1013e+00,  6.2495e-01],\n        [ 3.0570e+00,  8.0116e-01],\n        [ 2.6931e+00, -2.6623e-01],\n        [ 1.8846e+00, -1.8641e+00],\n        [ 3.7070e+00, -4.5570e-01],\n        [ 3.3984e+00,  3.4068e-01],\n        [ 6.4406e-01,  8.2295e-01],\n        [ 4.2481e+00, -1.2610e-01],\n        [ 2.3224e+00, -1.5339e+00],\n        [ 3.1369e+00, -1.2942e+00],\n        [ 3.3862e+00, -7.4611e-01],\n        [ 2.6424e+00,  7.6303e-01],\n        [ 3.9883e+00, -1.3030e+00],\n        [ 2.2557e+00,  4.7624e-01],\n        [ 1.8903e+00, -2.1560e+00],\n        [ 2.1575e+00, -4.3295e-01],\n        [ 2.9744e+00, -1.9825e-01],\n        [ 1.8519e+00,  1.1859e+00],\n        [ 3.5756e+00,  5.4069e-01],\n        [ 3.1907e+00,  4.0109e-01],\n        [ 3.6339e+00,  1.8688e+00],\n        [ 5.9347e-01,  1.5406e+00],\n        [ 3.3304e+00, -9.7760e-01],\n        [ 1.5609e+00, -4.4374e-01],\n        [ 2.2528e+00, -5.7379e-01],\n        [ 1.3959e+00, -8.0085e-01],\n        [ 3.8300e+00, -9.6793e-01],\n        [ 4.7364e+00, -7.8476e-01],\n        [ 2.5962e+00, -2.1219e-01],\n        [ 3.9448e+00,  4.6900e-01],\n        [ 1.4506e+00, -1.4624e+00],\n        [ 2.8009e+00, -8.7561e-01],\n        [ 2.2680e+00,  8.9251e-01],\n        [ 3.5774e+00,  6.0678e-01],\n        [ 2.2953e+00, -2.7355e-02],\n        [ 2.0714e+00, -5.8906e-01],\n        [ 1.8334e+00,  1.6085e+00],\n        [ 1.3932e+00, -6.2916e-01],\n        [ 8.3012e-01,  8.6009e-01],\n        [ 2.1750e+00, -4.4540e-01],\n        [ 1.7275e+00, -8.2325e-01],\n        [ 2.7457e+00, -1.1134e-01],\n        [ 1.0036e+00,  1.5544e+00],\n        [ 3.1633e+00, -8.2018e-01],\n        [ 9.5580e-01,  1.8172e+00],\n        [ 1.3434e+00, -6.0353e-01],\n        [ 2.3545e+00,  1.2932e+00],\n        [ 2.9332e+00, -1.7078e-01],\n        [ 1.9335e+00,  2.1000e-01],\n        [ 3.3027e+00, -4.2126e-01],\n        [ 2.6774e+00,  8.0991e-01],\n        [ 6.6251e-01, -1.3784e+00],\n        [ 3.3880e+00,  1.2715e-01],\n        [ 4.3586e+00,  9.8414e-01],\n        [ 2.0537e+00,  1.2898e-01],\n        [ 2.1991e+00, -2.5088e-01],\n        [ 1.1815e+00, -2.2245e+00],\n        [ 2.4974e+00,  1.0115e-01],\n        [ 2.3619e+00, -3.6319e-01],\n        [ 2.5889e+00,  1.3487e-01],\n        [ 3.3677e+00, -3.2632e-01],\n        [ 3.4680e+00,  1.7959e+00],\n        [ 4.3601e+00,  1.6514e+00],\n        [ 4.4541e+00,  5.5863e-01],\n        [ 1.5675e+00,  2.3797e-01],\n        [ 8.2438e-01, -5.6783e-01],\n        [ 4.1256e+00,  2.0598e-01],\n        [ 3.1017e+00, -2.9499e-01],\n        [ 2.3390e+00, -1.0853e+00],\n        [ 4.1148e+00,  6.0248e-02],\n        [ 3.1932e+00, -1.1714e+00],\n        [ 1.2865e+00,  3.0225e-01],\n        [ 2.5645e+00, -2.6348e-01],\n        [ 2.0610e+00,  9.7587e-02],\n        [ 2.6176e+00, -4.3935e-03],\n        [ 2.6581e+00,  4.2214e-01],\n        [ 2.7542e+00, -1.1728e+00],\n        [ 2.7539e+00, -1.0289e+00],\n        [ 2.3026e+00, -5.5336e-01],\n        [ 2.0919e+00,  3.0511e-02],\n        [ 2.7587e+00,  2.6906e-01],\n        [ 1.5962e+00, -1.0187e+00],\n        [ 5.1412e-01, -3.2023e-01],\n        [ 3.4952e+00,  2.6118e+00],\n        [ 2.8961e+00, -1.4559e+00],\n        [ 9.7166e-01, -2.7498e-01],\n        [ 2.7794e+00,  9.4963e-01],\n        [ 2.0812e+00, -1.8063e+00],\n        [ 2.9030e+00, -1.3045e+00],\n        [ 1.8204e+00,  3.7433e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[5.7962e-03, 6.6508e-03, 6.7763e-03,  ..., 9.6238e-01, 9.6238e-01,\n         1.0000e+00],\n        [2.1966e-01, 2.1966e-01, 2.2099e-01,  ..., 9.8478e-01, 9.9993e-01,\n         1.0000e+00],\n        [1.3039e-03, 3.0179e-03, 4.1648e-01,  ..., 9.9649e-01, 9.9655e-01,\n         1.0000e+00],\n        ...,\n        [5.5921e-02, 5.7329e-02, 9.5090e-02,  ..., 9.9171e-01, 1.0000e+00,\n         1.0000e+00],\n        [2.5833e-01, 2.6448e-01, 2.6735e-01,  ..., 9.0462e-01, 9.5580e-01,\n         1.0000e+00],\n        [4.0626e-07, 9.0282e-03, 9.0282e-03,  ..., 9.9198e-01, 9.9974e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.3023],\n        [0.5719],\n        [0.9171],\n        [0.0722],\n        [0.3520],\n        [0.8808],\n        [0.8152],\n        [0.7893],\n        [0.2332],\n        [0.1073],\n        [0.2491],\n        [0.2337],\n        [0.4914],\n        [0.9221],\n        [0.1685],\n        [0.8910],\n        [0.9689],\n        [0.9405],\n        [0.0536],\n        [0.2878],\n        [0.8784],\n        [0.9417],\n        [0.3875],\n        [0.2644],\n        [0.7686],\n        [0.2026],\n        [0.9093],\n        [0.7564],\n        [0.3739],\n        [0.1234],\n        [0.6330],\n        [0.4843],\n        [0.1741],\n        [0.2623],\n        [0.5914],\n        [0.3575],\n        [0.2878],\n        [0.3035],\n        [0.0648],\n        [0.2442],\n        [0.2995],\n        [0.1560],\n        [0.6693],\n        [0.9786],\n        [0.0478],\n        [0.4791],\n        [0.2423],\n        [0.2384],\n        [0.0691],\n        [0.4432],\n        [0.7426],\n        [0.3096],\n        [0.6073],\n        [0.5866],\n        [0.7109],\n        [0.2303],\n        [0.3868],\n        [0.0178],\n        [0.2958],\n        [0.1589],\n        [0.2714],\n        [0.2786],\n        [0.8562],\n        [0.4201],\n        [0.5542],\n        [0.7950],\n        [0.9415],\n        [0.8734],\n        [0.8241],\n        [0.0570],\n        [0.2539],\n        [0.6971],\n        [0.3598],\n        [0.7156],\n        [0.8073],\n        [0.5756],\n        [0.6210],\n        [0.2109],\n        [0.7971],\n        [0.0552],\n        [0.4289],\n        [0.9356],\n        [0.7990],\n        [0.1670],\n        [0.0936],\n        [0.7437],\n        [0.4702],\n        [0.5881],\n        [0.4328],\n        [0.5792],\n        [0.8358],\n        [0.1174],\n        [0.7464],\n        [0.8412],\n        [0.3203],\n        [0.2751],\n        [0.3556],\n        [0.3523],\n        [0.4002],\n        [0.1249],\n        [0.6295],\n        [0.5510],\n        [0.2519],\n        [0.7150],\n        [0.7489],\n        [0.0874],\n        [0.9225],\n        [0.8305],\n        [0.3392],\n        [0.7611],\n        [0.2277],\n        [0.6147],\n        [0.0234],\n        [0.5301],\n        [0.4746],\n        [0.5133],\n        [0.9712],\n        [0.6127],\n        [0.5941],\n        [0.0787],\n        [0.4801],\n        [0.5771],\n        [0.2610],\n        [0.6265],\n        [0.5729],\n        [0.1941],\n        [0.3973],\n        [0.9734],\n        [0.8022],\n        [0.9598],\n        [0.3793],\n        [0.4324],\n        [0.4142],\n        [0.9651],\n        [0.3737],\n        [0.7982],\n        [0.0856],\n        [0.8175],\n        [0.2891],\n        [0.8845],\n        [0.4875],\n        [0.5224],\n        [0.4601],\n        [0.9826],\n        [0.7755],\n        [0.9151],\n        [0.1596],\n        [0.8449],\n        [0.5058],\n        [0.9517],\n        [0.0407],\n        [0.8523],\n        [0.3920],\n        [0.2100],\n        [0.8905],\n        [0.3426],\n        [0.4790],\n        [0.4692],\n        [0.0655],\n        [0.2790],\n        [0.2729],\n        [0.3488],\n        [0.3514],\n        [0.5686],\n        [0.0099],\n        [0.1895],\n        [0.3060],\n        [0.1157],\n        [0.0801],\n        [0.9929],\n        [0.1507],\n        [0.1293],\n        [0.1682],\n        [0.9213],\n        [0.5855],\n        [0.7434],\n        [0.6196],\n        [0.3709],\n        [0.5143],\n        [0.3924],\n        [0.2195],\n        [0.4374],\n        [0.9785],\n        [0.2498],\n        [0.3812],\n        [0.1748],\n        [0.7145],\n        [0.7234],\n        [0.7447],\n        [0.8233],\n        [0.3471],\n        [0.9850],\n        [0.6017],\n        [0.9684],\n        [0.8602],\n        [0.9465],\n        [0.2727],\n        [0.8867],\n        [0.5682],\n        [0.5332],\n        [0.1266],\n        [0.4703],\n        [0.1245],\n        [0.8590],\n        [0.6239],\n        [0.6151],\n        [0.0151],\n        [0.8225],\n        [0.6913],\n        [0.3778],\n        [0.2527],\n        [0.8630],\n        [0.8937],\n        [0.1865],\n        [0.8629],\n        [0.5327],\n        [0.3188],\n        [0.0960],\n        [0.5734],\n        [0.2198],\n        [0.0745],\n        [0.8787],\n        [0.4613],\n        [0.1851],\n        [0.8338],\n        [0.1797],\n        [0.5730],\n        [0.0533],\n        [0.8659],\n        [0.1094],\n        [0.0217],\n        [0.6862],\n        [0.2327],\n        [0.9676],\n        [0.9365],\n        [0.3742],\n        [0.7727],\n        [0.9535],\n        [0.0061],\n        [0.2056],\n        [0.2758],\n        [0.5981],\n        [0.8907],\n        [0.6646],\n        [0.8515],\n        [0.4101],\n        [0.2019],\n        [0.2478],\n        [0.3816],\n        [0.2909],\n        [0.8810],\n        [0.2173],\n        [0.1516],\n        [0.2690],\n        [0.2671],\n        [0.9984],\n        [0.0165],\n        [0.9823],\n        [0.0111],\n        [0.8836],\n        [0.7690],\n        [0.0332],\n        [0.6717],\n        [0.2766],\n        [0.0464],\n        [0.9169],\n        [0.4158],\n        [0.0063],\n        [0.2933],\n        [0.2771],\n        [0.7295],\n        [0.5187],\n        [0.9414],\n        [0.4068],\n        [0.1046],\n        [0.7311],\n        [0.0756],\n        [0.3112],\n        [0.8343],\n        [0.5892],\n        [0.2551],\n        [0.6839],\n        [0.7455],\n        [0.1166],\n        [0.2534],\n        [0.6648],\n        [0.7178],\n        [0.2760],\n        [0.9878],\n        [0.1901],\n        [0.8980],\n        [0.0662],\n        [0.6040],\n        [0.2721],\n        [0.5207],\n        [0.9246],\n        [0.3753],\n        [0.4801],\n        [0.9522],\n        [0.4429],\n        [0.5665],\n        [0.9047],\n        [0.6870],\n        [0.4307],\n        [0.2906],\n        [0.5541],\n        [0.2558],\n        [0.3985],\n        [0.9816],\n        [0.2753],\n        [0.7242],\n        [0.7320]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 1.6399,  1.9841],\n        [ 0.0443,  1.1860],\n        [-0.9360,  0.4089],\n        ...,\n        [-1.2342,  0.5024],\n        [ 2.6426,  2.7001],\n        [-0.8548,  0.6405]]) torch.Size([9984, 2])\nsamples tensor([[ 2.0921e+00,  1.1336e-01],\n        [ 2.3189e+00, -2.6874e+00],\n        [ 2.8213e+00,  1.0002e+00],\n        [ 2.6197e+00,  1.3097e+00],\n        [ 8.4618e-01, -4.6152e-01],\n        [ 3.1899e+00, -1.4695e+00],\n        [ 2.8705e+00, -1.5489e+00],\n        [ 2.5218e+00,  9.4725e-01],\n        [ 3.4699e+00,  3.3402e-01],\n        [ 3.2641e+00,  4.0111e-01],\n        [ 1.1444e+00, -1.5945e+00],\n        [ 1.4345e+00,  2.5731e-01],\n        [ 1.3562e+00, -1.7973e+00],\n        [ 3.0473e+00,  4.0386e-01],\n        [ 2.8298e+00, -4.5061e-01],\n        [ 2.8322e+00,  1.0248e+00],\n        [ 3.1937e+00, -2.0928e+00],\n        [ 2.5072e+00,  1.5605e-01],\n        [ 1.1409e+00,  3.5191e-02],\n        [ 1.7949e+00, -2.3404e+00],\n        [ 2.0685e+00, -9.3806e-01],\n        [ 2.9261e+00, -1.6898e+00],\n        [ 2.1663e+00, -5.9147e-03],\n        [ 2.9299e+00, -1.4375e+00],\n        [ 1.9115e+00, -1.5680e-01],\n        [ 2.6080e+00,  8.5434e-01],\n        [ 3.4162e+00, -5.5505e-01],\n        [ 3.4278e+00, -4.7701e-01],\n        [ 1.4594e+00, -2.9936e-01],\n        [ 4.4578e+00, -4.0341e-01],\n        [ 2.5280e+00, -5.3349e-01],\n        [ 3.4116e+00,  1.0276e+00],\n        [ 3.4204e+00, -7.7865e-01],\n        [ 2.7328e+00, -1.5497e-01],\n        [ 2.6823e+00,  1.3368e-01],\n        [ 7.2527e-01, -2.2068e-01],\n        [ 2.3302e+00, -2.0908e+00],\n        [ 3.1677e+00, -1.1628e+00],\n        [ 1.6038e+00,  2.5999e-01],\n        [ 2.4130e+00, -7.6493e-03],\n        [ 8.8710e-01,  1.8843e+00],\n        [ 1.4432e+00, -1.7679e+00],\n        [ 1.2737e+00,  1.0065e+00],\n        [ 1.6387e+00,  2.1490e+00],\n        [ 3.5047e+00, -5.2477e-01],\n        [ 2.3471e+00, -3.5997e-01],\n        [ 3.2827e+00,  1.3483e-01],\n        [ 3.0214e+00, -8.2068e-01],\n        [ 2.9358e+00,  8.7585e-02],\n        [ 2.1290e+00, -9.5063e-01],\n        [ 2.4930e+00, -3.3205e-01],\n        [ 7.9079e-01,  1.1120e+00],\n        [ 2.5863e+00, -1.3159e+00],\n        [ 3.3513e+00,  1.5681e-01],\n        [ 1.6307e+00,  1.0187e+00],\n        [ 1.9638e+00, -3.9675e-01],\n        [ 2.1079e+00, -1.0627e+00],\n        [ 1.1139e+00,  1.1179e-01],\n        [ 3.0741e+00,  5.8512e-01],\n        [ 2.3747e+00,  3.8623e-01],\n        [ 2.6202e+00, -2.2677e+00],\n        [ 4.5011e+00,  1.2365e+00],\n        [ 3.0209e+00, -1.5234e+00],\n        [ 1.5550e+00, -1.2734e+00],\n        [ 4.1268e+00, -4.5198e-01],\n        [ 1.8020e+00,  1.1840e+00],\n        [ 2.4856e+00, -2.0189e+00],\n        [ 2.5102e+00,  1.0633e+00],\n        [ 3.4521e+00,  4.4238e-01],\n        [ 4.2447e+00,  9.2631e-01],\n        [ 1.2469e+00, -1.2295e+00],\n        [ 2.3727e+00, -8.3751e-01],\n        [ 1.3674e+00,  8.1073e-01],\n        [ 1.8236e+00,  1.0059e+00],\n        [ 1.5266e+00, -8.9695e-01],\n        [ 2.1283e+00,  3.8843e-01],\n        [ 2.6483e+00,  7.5883e-01],\n        [ 3.2689e+00, -1.4149e+00],\n        [ 2.1359e+00,  7.7648e-02],\n        [ 1.4587e+00, -1.5701e+00],\n        [ 1.1133e+00, -1.6432e+00],\n        [ 4.5675e+00, -1.1994e+00],\n        [ 3.0519e+00,  1.0663e+00],\n        [ 3.3875e+00, -4.6298e-01],\n        [ 2.7195e+00,  5.6895e-01],\n        [ 3.3058e+00, -7.6730e-01],\n        [ 2.2625e+00,  1.9521e+00],\n        [ 1.5847e+00, -1.4362e-01],\n        [ 3.2060e+00, -1.0038e+00],\n        [ 1.3808e+00, -1.1469e+00],\n        [ 2.5661e+00, -1.5370e-01],\n        [ 5.4125e-01,  2.7291e-01],\n        [ 2.9926e+00, -4.8484e-03],\n        [ 2.2798e+00, -6.5299e-01],\n        [ 1.9967e+00, -1.8288e+00],\n        [ 2.4542e+00, -4.3406e-01],\n        [ 2.2578e+00,  6.3102e-01],\n        [ 2.4546e+00, -1.1834e-01],\n        [ 4.6776e+00,  1.4228e+00],\n        [ 3.8151e+00,  6.6488e-01],\n        [ 4.1385e+00,  1.7034e+00],\n        [ 1.8312e+00, -1.2162e+00],\n        [ 2.9410e+00, -6.5166e-01],\n        [ 2.5304e+00, -2.4478e-01],\n        [ 3.0041e+00,  4.8253e-01],\n        [ 2.4499e+00,  4.0277e-01],\n        [-1.3664e+00,  8.4263e-01],\n        [ 2.8091e+00, -4.0561e-01],\n        [ 2.4326e+00,  5.5998e-01],\n        [-3.5114e-01, -6.9891e-01],\n        [ 1.7625e+00,  6.4574e-01],\n        [ 3.3742e+00, -1.9017e+00],\n        [ 1.8197e+00, -7.7953e-01],\n        [ 1.7541e+00, -1.7019e-01],\n        [ 3.3475e+00, -1.4426e+00],\n        [ 2.7828e+00,  2.6509e-01],\n        [ 6.4606e-01, -4.5342e-01],\n        [ 3.2767e+00,  2.5385e-01],\n        [ 5.4744e-01, -5.2708e-01],\n        [ 2.2005e+00,  8.8529e-01],\n        [ 2.3794e+00,  8.9038e-01],\n        [ 1.7324e+00, -1.2120e-01],\n        [ 2.6456e+00, -6.4392e-01],\n        [ 8.8625e-01, -1.6794e+00],\n        [ 2.4615e+00, -1.1531e+00],\n        [ 4.0709e+00,  9.9660e-01],\n        [ 2.3014e+00, -6.0704e-01],\n        [ 1.3229e+00, -4.9400e-01],\n        [ 3.3823e+00,  1.3747e+00],\n        [ 1.4089e+00,  7.9027e-01],\n        [ 2.4450e+00, -1.5711e+00],\n        [ 1.4872e+00,  8.0475e-01],\n        [ 2.4112e+00,  2.2399e-01],\n        [ 2.8563e+00, -3.4759e-01],\n        [ 1.0339e+00, -2.0595e+00],\n        [ 1.6824e+00,  1.0543e+00],\n        [ 1.2640e+00,  2.6417e-02],\n        [ 3.1977e+00, -5.5014e-01],\n        [ 2.4496e+00,  4.5905e-01],\n        [ 2.1717e+00, -1.5571e+00],\n        [ 2.5954e+00,  1.0098e-01],\n        [ 2.3588e+00, -2.4590e+00],\n        [ 2.4680e+00, -6.1285e-01],\n        [ 2.9456e+00,  1.4588e+00],\n        [ 3.3673e+00,  1.4659e+00],\n        [ 1.8335e+00, -5.3828e-02],\n        [ 2.6424e+00, -1.4021e+00],\n        [-1.8284e-01,  1.8266e+00],\n        [ 4.0436e+00, -1.0578e+00],\n        [ 2.4445e+00,  7.0409e-01],\n        [ 3.1622e+00, -7.1656e-01],\n        [ 2.1662e+00,  4.5490e-01],\n        [ 4.5741e+00, -1.8879e+00],\n        [ 1.5155e+00,  3.0418e-01],\n        [ 2.9737e+00, -8.2648e-01],\n        [ 2.5856e+00, -2.3951e-01],\n        [ 2.4250e+00, -2.0067e+00],\n        [ 1.5947e+00, -7.9497e-01],\n        [ 9.1572e-01,  2.4331e-01],\n        [ 3.6403e+00, -1.1155e-01],\n        [ 1.5862e+00,  3.2151e-02],\n        [ 2.9504e+00, -1.3607e+00],\n        [ 3.3495e+00,  1.5454e+00],\n        [ 2.6509e+00, -9.9583e-01],\n        [ 1.1726e+00, -7.0437e-01],\n        [ 1.9212e+00, -9.2240e-01],\n        [ 1.8182e+00,  1.9418e+00],\n        [ 2.5632e+00,  6.6089e-01],\n        [ 2.6257e+00,  8.1812e-01],\n        [-3.1994e-01, -4.5791e-02],\n        [ 2.5658e+00, -9.9638e-01],\n        [ 1.6247e+00,  6.2449e-01],\n        [ 2.7269e+00,  1.4325e+00],\n        [ 3.0446e+00, -1.2857e+00],\n        [ 3.9312e+00,  1.0919e+00],\n        [ 3.1111e+00,  1.3781e+00],\n        [ 4.2150e-01, -7.4769e-01],\n        [ 1.9770e+00, -4.7559e-01],\n        [ 1.7330e+00, -3.6410e-01],\n        [ 1.5442e+00,  9.8756e-01],\n        [ 1.2024e+00,  6.7252e-01],\n        [ 1.3505e+00, -2.9587e-01],\n        [ 1.6391e+00,  1.5117e+00],\n        [ 3.8175e+00, -5.9647e-01],\n        [ 2.3884e+00,  5.5280e-01],\n        [ 3.7531e+00, -1.1775e+00],\n        [ 1.8683e+00, -1.4181e+00],\n        [ 8.8547e-01, -4.1974e-02],\n        [ 1.9201e+00, -7.8260e-01],\n        [ 1.9020e+00,  2.2897e-01],\n        [ 2.5885e+00, -2.8233e-01],\n        [ 2.6468e+00, -7.5825e-02],\n        [ 3.1827e+00, -1.8759e+00],\n        [ 1.7480e+00, -1.5312e-02],\n        [ 4.2303e+00, -1.3233e+00],\n        [ 1.3908e+00,  1.5055e+00],\n        [ 3.1955e+00, -1.5535e+00],\n        [ 2.3309e+00,  4.8679e-02],\n        [ 2.2942e+00,  1.7286e-01],\n        [ 3.0833e+00, -1.3693e+00],\n        [ 2.6219e+00, -1.1443e+00],\n        [ 1.4137e+00, -3.7731e-01],\n        [ 3.2666e+00, -1.3300e+00],\n        [ 2.8677e+00, -7.5906e-01],\n        [ 1.6070e+00,  1.2541e-01],\n        [ 2.3347e+00, -1.3006e+00],\n        [ 4.2224e+00, -2.3366e+00],\n        [ 2.2290e+00, -6.8987e-01],\n        [ 1.9578e+00, -1.2759e-01],\n        [ 4.2771e+00,  4.2909e-01],\n        [ 2.6418e+00,  2.2309e-01],\n        [ 1.3715e+00, -3.6785e-01],\n        [ 6.6579e-01, -7.5887e-01],\n        [ 2.7677e+00, -1.0224e+00],\n        [ 2.9014e+00, -1.1897e+00],\n        [ 2.6068e+00, -1.0305e-01],\n        [ 2.4243e+00, -5.8833e-01],\n        [ 1.1658e+00,  2.4858e-01],\n        [ 1.6773e+00,  8.1571e-04],\n        [ 2.7083e+00,  8.0004e-02],\n        [ 2.8453e+00,  9.2290e-01],\n        [ 1.4282e+00,  1.2766e+00],\n        [ 1.3969e+00,  6.7363e-01],\n        [ 1.6161e+00, -8.2697e-01],\n        [ 2.7431e+00, -1.4020e+00],\n        [ 1.6902e+00, -9.3589e-01],\n        [ 1.7900e+00,  1.4653e+00],\n        [ 1.8598e+00, -1.2923e+00],\n        [ 2.6656e+00, -4.6726e-01],\n        [ 1.2943e+00, -1.0031e+00],\n        [ 1.2874e+00, -8.1706e-01],\n        [ 3.3846e+00,  6.8810e-01],\n        [ 2.8140e+00, -1.7043e-01],\n        [ 2.4506e+00,  3.2851e-01],\n        [ 2.4556e+00,  4.5544e-01],\n        [ 3.2918e+00,  7.2145e-01],\n        [ 4.0779e+00, -3.5280e-01],\n        [ 1.5335e+00, -3.7930e-01],\n        [ 3.5706e+00,  1.9710e+00],\n        [ 1.5707e+00,  1.4045e-01],\n        [ 2.3671e+00, -2.8092e+00],\n        [ 2.6496e+00,  1.9412e-01],\n        [ 1.8955e+00,  2.9165e-01],\n        [ 4.1354e+00, -1.7446e+00],\n        [ 2.7200e+00, -1.9020e+00],\n        [ 1.9514e+00, -7.9268e-01],\n        [ 3.1943e+00, -8.8337e-01],\n        [ 2.6491e+00, -1.7857e+00],\n        [ 2.6398e+00, -1.1575e+00],\n        [ 3.4657e+00, -2.6416e-01],\n        [ 2.2027e+00, -4.1071e-01],\n        [ 3.3740e+00,  1.3572e+00],\n        [ 2.9681e+00, -1.3192e+00],\n        [ 3.4708e+00, -1.0625e+00],\n        [ 2.0587e+00, -8.1314e-01],\n        [ 3.5756e+00,  2.1120e+00],\n        [ 4.0426e+00,  9.3503e-01],\n        [ 1.0409e+00, -1.1056e+00],\n        [ 2.2782e+00,  7.5086e-01],\n        [ 1.9722e+00,  1.0474e+00],\n        [ 1.7055e+00,  6.7447e-01],\n        [ 1.9670e+00, -3.3415e-01],\n        [ 2.9348e+00, -1.2902e+00],\n        [ 2.9590e+00, -1.7205e+00],\n        [ 1.3886e+00,  6.5429e-01],\n        [ 3.4103e+00, -1.2554e+00],\n        [ 7.7820e-01, -6.8497e-01],\n        [ 1.3326e+00, -1.7576e-01],\n        [ 3.3678e+00, -1.5290e+00],\n        [ 2.3697e+00, -8.9167e-01],\n        [ 2.3338e+00,  2.7321e-01],\n        [ 1.8569e+00,  9.6358e-01],\n        [ 2.3074e+00, -3.9121e-01],\n        [ 2.9554e+00, -1.4218e-01],\n        [ 1.5621e+00, -8.3039e-01],\n        [ 1.1215e+00,  6.3199e-01],\n        [ 6.8011e-01, -5.6141e-01],\n        [ 2.1801e+00, -1.5151e+00],\n        [ 1.2295e+00, -9.9278e-01],\n        [ 1.4567e+00, -1.4441e+00],\n        [ 3.6096e+00, -6.9180e-01],\n        [ 1.5977e+00,  6.1756e-01],\n        [ 2.3531e+00,  1.0868e+00],\n        [ 4.4708e+00, -6.3480e-01],\n        [ 2.6088e+00, -7.7801e-01],\n        [ 2.4713e+00, -1.4048e-01],\n        [ 3.2934e+00,  3.1315e-01],\n        [ 1.1298e+00, -9.2207e-01],\n        [ 1.4568e+00,  1.2534e+00],\n        [ 2.8043e+00,  9.1853e-02],\n        [ 2.3558e+00,  4.2052e-01],\n        [ 8.3723e-02,  3.1727e-01],\n        [ 1.0260e+00,  2.2052e-01],\n        [ 2.9444e+00, -9.1403e-01],\n        [ 3.3412e+00, -3.5398e-01],\n        [ 2.3222e+00, -1.5201e-01],\n        [ 3.3281e+00, -2.2541e+00],\n        [ 4.3589e+00, -1.3047e+00],\n        [ 2.0106e+00,  7.3364e-01],\n        [ 1.3451e+00,  1.0964e+00],\n        [ 2.7485e+00,  1.2877e+00],\n        [ 2.7411e+00, -2.2450e+00],\n        [ 1.9381e+00, -1.7156e-01],\n        [ 3.5577e+00,  1.1759e-01],\n        [ 2.6510e+00,  2.5863e+00],\n        [ 1.7934e+00, -2.0156e+00],\n        [ 6.1406e-01, -1.5241e-01],\n        [ 2.3820e+00, -2.6636e-01],\n        [ 2.7224e+00, -6.1521e-01],\n        [ 3.0403e+00, -4.3566e-01],\n        [ 1.5337e+00,  4.4730e-01],\n        [ 4.2510e+00, -5.7308e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[5.7150e-03, 1.6778e-01, 1.8372e-01,  ..., 8.4374e-01, 1.0000e+00,\n         1.0000e+00],\n        [2.0360e-03, 2.1357e-03, 6.8483e-02,  ..., 8.4654e-01, 8.5069e-01,\n         1.0000e+00],\n        [8.9537e-02, 9.0614e-02, 9.0664e-02,  ..., 9.5597e-01, 9.8502e-01,\n         1.0000e+00],\n        ...,\n        [1.6530e-03, 1.6686e-03, 1.6432e-02,  ..., 9.2979e-01, 9.7345e-01,\n         1.0000e+00],\n        [4.7645e-11, 3.5923e-02, 5.4727e-02,  ..., 9.5745e-01, 9.9884e-01,\n         1.0000e+00],\n        [5.3925e-12, 6.8887e-02, 9.8586e-02,  ..., 7.9129e-01, 7.9129e-01,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.8206],\n        [0.8313],\n        [0.1477],\n        [0.8523],\n        [0.3726],\n        [0.0124],\n        [0.6178],\n        [0.3591],\n        [0.3697],\n        [0.0672],\n        [0.2132],\n        [0.7549],\n        [0.9436],\n        [0.9663],\n        [0.2385],\n        [0.5329],\n        [0.7304],\n        [0.2564],\n        [0.7182],\n        [0.9232],\n        [0.5079],\n        [0.4617],\n        [0.2063],\n        [0.9711],\n        [0.2756],\n        [0.1634],\n        [0.8469],\n        [0.5056],\n        [0.8430],\n        [0.3440],\n        [0.2782],\n        [0.5780],\n        [0.8380],\n        [0.8619],\n        [0.0382],\n        [0.9205],\n        [0.8347],\n        [0.9757],\n        [0.6857],\n        [0.4887],\n        [0.4626],\n        [0.3809],\n        [0.0152],\n        [0.9314],\n        [0.0982],\n        [0.9962],\n        [0.3019],\n        [0.8115],\n        [0.2045],\n        [0.7836],\n        [0.3466],\n        [0.4427],\n        [0.7379],\n        [0.0407],\n        [0.5874],\n        [0.1826],\n        [0.2554],\n        [0.9301],\n        [0.5517],\n        [0.5624],\n        [0.9428],\n        [0.5735],\n        [0.8303],\n        [0.3169],\n        [0.5901],\n        [0.4957],\n        [0.3849],\n        [0.9392],\n        [0.3185],\n        [0.5936],\n        [0.6036],\n        [0.3243],\n        [0.1529],\n        [0.4782],\n        [0.6336],\n        [0.7241],\n        [0.5519],\n        [0.8894],\n        [0.8016],\n        [0.6102],\n        [0.5442],\n        [0.3048],\n        [0.5981],\n        [0.0254],\n        [0.4310],\n        [0.0579],\n        [0.1947],\n        [0.6515],\n        [0.7909],\n        [0.1921],\n        [0.0286],\n        [0.3676],\n        [0.7988],\n        [0.2273],\n        [0.3721],\n        [0.6610],\n        [0.3649],\n        [0.3934],\n        [0.3475],\n        [0.7029],\n        [0.6209],\n        [0.4631],\n        [0.3875],\n        [0.4541],\n        [0.4915],\n        [0.5846],\n        [0.0235],\n        [0.0559],\n        [0.4597],\n        [0.3130],\n        [0.7754],\n        [0.6995],\n        [0.6179],\n        [0.9466],\n        [0.0206],\n        [0.8393],\n        [0.3675],\n        [0.8923],\n        [0.3254],\n        [0.9752],\n        [0.3801],\n        [0.2263],\n        [0.8655],\n        [0.5563],\n        [0.0033],\n        [0.5282],\n        [0.6324],\n        [0.0028],\n        [0.4943],\n        [0.9314],\n        [0.9722],\n        [0.1257],\n        [0.6275],\n        [0.4136],\n        [0.2257],\n        [0.8557],\n        [0.2978],\n        [0.7032],\n        [0.7966],\n        [0.7030],\n        [0.2843],\n        [0.0388],\n        [0.8684],\n        [0.9128],\n        [0.4454],\n        [0.8438],\n        [0.9171],\n        [0.7310],\n        [0.0406],\n        [0.4741],\n        [0.3994],\n        [0.6145],\n        [0.5732],\n        [0.8740],\n        [0.4095],\n        [0.9950],\n        [0.2159],\n        [0.0688],\n        [0.8896],\n        [0.1415],\n        [0.1868],\n        [0.0762],\n        [0.2401],\n        [0.3344],\n        [0.3861],\n        [0.3622],\n        [0.4869],\n        [0.7840],\n        [0.3673],\n        [0.9312],\n        [0.7315],\n        [0.5801],\n        [0.7553],\n        [0.1069],\n        [0.9683],\n        [0.2600],\n        [0.4340],\n        [0.7367],\n        [0.9801],\n        [0.6703],\n        [0.4402],\n        [0.5004],\n        [0.3599],\n        [0.0865],\n        [0.8051],\n        [0.7965],\n        [0.1461],\n        [0.3128],\n        [0.3742],\n        [0.5914],\n        [0.4210],\n        [0.8145],\n        [0.8228],\n        [0.2446],\n        [0.8489],\n        [0.4149],\n        [0.1139],\n        [0.8176],\n        [0.6173],\n        [0.2136],\n        [0.8288],\n        [0.9675],\n        [0.3272],\n        [0.9590],\n        [0.6885],\n        [0.7695],\n        [0.1531],\n        [0.4956],\n        [0.6708],\n        [0.8430],\n        [0.3334],\n        [0.6703],\n        [0.2352],\n        [0.4256],\n        [0.4463],\n        [0.0814],\n        [0.8805],\n        [0.9521],\n        [0.7857],\n        [0.4264],\n        [0.6855],\n        [0.4585],\n        [0.7011],\n        [0.6680],\n        [0.8294],\n        [0.8790],\n        [0.5145],\n        [0.2573],\n        [0.7639],\n        [0.2773],\n        [0.0320],\n        [0.1208],\n        [0.7325],\n        [0.3382],\n        [0.6595],\n        [0.4092],\n        [0.1526],\n        [0.1838],\n        [0.6882],\n        [0.0032],\n        [0.5258],\n        [0.4429],\n        [0.4255],\n        [0.8973],\n        [0.5471],\n        [0.2364],\n        [0.4665],\n        [0.8951],\n        [0.7909],\n        [0.0112],\n        [0.4048],\n        [0.5868],\n        [0.3203],\n        [0.1914],\n        [0.3378],\n        [0.0030],\n        [0.6735],\n        [0.8897],\n        [0.3514],\n        [0.0180],\n        [0.5593],\n        [0.1716],\n        [0.4714],\n        [0.1400],\n        [0.0168],\n        [0.9169],\n        [0.1154],\n        [0.4277],\n        [0.6520],\n        [0.1754],\n        [0.8923],\n        [0.8382],\n        [0.9197],\n        [0.8806],\n        [0.2264],\n        [0.5247],\n        [0.7057],\n        [0.9991],\n        [0.7896],\n        [0.6878],\n        [0.6067],\n        [0.3128],\n        [0.8979],\n        [0.1501],\n        [0.5171],\n        [0.3511],\n        [0.8918],\n        [0.4294],\n        [0.2471],\n        [0.3364],\n        [0.9362],\n        [0.3925],\n        [0.0507],\n        [0.8488],\n        [0.6102],\n        [0.4152],\n        [0.8029],\n        [0.1256],\n        [0.3710],\n        [0.0329],\n        [0.6489],\n        [0.7002],\n        [0.3672],\n        [0.3053],\n        [0.0774],\n        [0.0808],\n        [0.4287],\n        [0.0800],\n        [0.9139],\n        [0.8710],\n        [0.8152],\n        [0.0231]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False,  True, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[ 2.7414,  2.6252],\n        [ 2.9542, -2.5611],\n        [ 1.1647,  0.7752],\n        ...,\n        [ 1.7704,  1.5614],\n        [-2.2214, -1.3501],\n        [ 2.4014, -0.8556]]) torch.Size([9984, 2])\nsamples tensor([[ 1.5748e+00,  6.6512e-01],\n        [ 3.4105e+00,  9.0575e-02],\n        [ 2.1486e+00,  5.9562e-01],\n        [ 2.2568e+00,  1.0473e+00],\n        [ 2.0805e+00, -4.6587e-01],\n        [ 1.6730e+00, -5.6834e-01],\n        [ 2.7604e+00, -8.4460e-01],\n        [ 3.7157e+00,  1.0217e+00],\n        [ 2.2078e+00, -1.3644e-01],\n        [ 2.4460e+00,  1.2522e+00],\n        [ 3.0342e+00,  1.2227e-01],\n        [ 3.1267e+00,  6.3216e-01],\n        [ 1.5781e+00,  9.4531e-01],\n        [ 1.8287e+00,  1.5691e+00],\n        [ 3.6890e+00, -1.3886e+00],\n        [ 2.5703e+00, -3.2474e-01],\n        [ 2.2964e+00, -3.2635e-01],\n        [ 4.3828e+00,  1.2993e-01],\n        [ 1.9591e+00, -6.5018e-01],\n        [ 3.8127e+00, -7.2323e-01],\n        [ 1.6107e+00, -5.0670e-01],\n        [ 2.4975e+00, -1.0125e-01],\n        [ 9.9444e-01, -4.0007e-02],\n        [ 1.3631e+00,  3.4366e-01],\n        [ 1.2079e+00, -6.0965e-02],\n        [ 2.7442e+00, -8.3562e-01],\n        [ 1.8483e+00, -3.9191e-01],\n        [ 9.1969e-01, -3.3091e-03],\n        [ 8.6873e-02,  1.9148e-01],\n        [ 2.6468e+00, -1.4790e+00],\n        [ 3.8396e+00,  2.0040e-01],\n        [ 3.3712e+00,  4.9789e-01],\n        [ 3.1275e+00, -1.0385e-01],\n        [ 2.6797e+00,  2.1621e-01],\n        [ 1.9263e+00,  3.8638e-01],\n        [ 1.0089e+00,  2.1496e-01],\n        [ 1.4286e+00, -1.4019e+00],\n        [ 1.7258e+00,  2.5620e-01],\n        [ 1.0494e+00, -6.1695e-01],\n        [ 2.2265e+00,  1.1928e-02],\n        [ 3.6089e+00, -1.2920e-01],\n        [ 2.1842e+00, -9.7688e-01],\n        [ 1.5445e+00,  2.6647e-01],\n        [ 1.1217e+00,  7.8656e-01],\n        [ 8.0464e-01,  1.0300e+00],\n        [ 9.2446e-01, -9.6784e-01],\n        [ 2.0051e+00, -2.7640e-01],\n        [ 3.0523e+00,  9.8662e-02],\n        [ 2.4417e+00, -6.6065e-01],\n        [ 1.0419e+00, -1.0579e+00],\n        [ 2.1341e+00,  1.7951e-01],\n        [ 2.3598e+00, -1.4332e+00],\n        [ 3.1338e+00,  5.5310e-01],\n        [ 2.9310e+00,  1.7670e+00],\n        [ 2.8569e+00,  8.3477e-01],\n        [ 3.0123e+00,  1.1206e+00],\n        [ 3.3999e+00,  2.5279e-01],\n        [ 3.2925e+00,  1.0309e+00],\n        [ 2.4806e+00,  1.0925e-01],\n        [ 2.3898e+00,  2.2159e-01],\n        [ 1.9798e+00,  1.8062e-02],\n        [ 3.4373e+00,  1.5569e+00],\n        [ 2.0820e+00, -1.0003e-01],\n        [ 1.9807e+00, -3.6933e-01],\n        [ 2.7806e+00, -4.2916e-01],\n        [ 2.7729e+00,  6.4874e-03],\n        [ 3.0889e+00, -1.6793e+00],\n        [ 3.7497e+00,  1.0249e+00],\n        [ 3.7664e-01,  8.1884e-01],\n        [ 1.8038e+00, -7.0758e-01],\n        [ 2.4307e+00,  4.8615e-01],\n        [ 2.5157e+00, -1.5855e+00],\n        [ 1.3001e+00, -1.0787e+00],\n        [ 2.8562e+00, -1.9724e+00],\n        [ 2.4179e+00, -5.6943e-01],\n        [ 3.3046e+00, -7.5637e-01],\n        [ 2.8097e+00,  1.0434e+00],\n        [ 8.1664e-01, -1.6793e+00],\n        [ 2.5419e+00,  7.9944e-01],\n        [ 3.5572e+00,  9.6421e-01],\n        [ 3.2722e+00, -1.1267e-01],\n        [ 5.8175e-01, -3.6740e-01],\n        [ 3.5263e+00, -2.5219e+00],\n        [ 3.2021e+00, -3.5320e+00],\n        [ 1.6266e+00,  1.6670e+00],\n        [ 1.7788e+00, -1.2426e-01],\n        [ 2.3352e+00, -5.6810e-01],\n        [ 1.3176e+00, -3.5788e-01],\n        [ 4.1981e+00, -2.7975e-01],\n        [ 2.4365e+00,  9.4064e-01],\n        [ 1.5989e+00,  4.6658e-01],\n        [ 3.3647e+00,  7.3233e-01],\n        [ 2.6730e+00, -9.5622e-01],\n        [ 1.4379e+00,  1.5231e+00],\n        [ 3.8664e+00, -3.5268e-01],\n        [ 2.1148e+00,  6.3734e-01],\n        [ 3.1867e+00,  4.7593e-01],\n        [ 1.3584e+00, -8.8159e-01],\n        [ 2.4514e+00, -1.3794e+00],\n        [ 2.4167e-01, -3.5623e-02],\n        [ 2.5869e+00,  9.9771e-02],\n        [ 9.7546e-01, -7.7420e-01],\n        [ 2.2681e+00, -1.5003e-01],\n        [ 2.7860e+00,  5.4610e-01],\n        [ 1.2811e+00, -5.2863e-01],\n        [ 1.1741e+00, -7.7412e-01],\n        [ 2.0475e+00,  8.7262e-01],\n        [ 1.3814e+00, -2.5992e-01],\n        [ 2.3340e+00,  4.2307e-01],\n        [ 2.1765e+00, -3.6507e+00],\n        [ 1.9064e+00, -1.2500e+00],\n        [ 1.9588e+00, -1.8533e+00],\n        [ 2.5940e+00, -2.6674e+00],\n        [ 1.7831e+00, -2.3465e-01],\n        [ 8.5712e-01,  7.6099e-01],\n        [ 1.9731e+00, -2.0773e+00],\n        [ 2.7665e+00, -3.0731e-01],\n        [ 1.9092e+00, -3.4881e-01],\n        [ 2.3178e+00,  4.0802e-01],\n        [ 3.7020e+00,  1.1379e+00],\n        [ 4.4457e+00,  1.5074e+00],\n        [ 3.5678e+00, -6.3903e-01],\n        [ 1.4905e+00, -2.9208e-01],\n        [ 3.3827e+00, -5.9204e-01],\n        [ 1.0686e+00,  5.5403e-01],\n        [ 2.6358e+00,  7.0673e-01],\n        [ 1.5353e+00, -2.0214e+00],\n        [ 1.7765e+00,  6.0426e-01],\n        [ 1.3874e+00, -1.8536e+00],\n        [ 9.6774e-01, -1.2498e+00],\n        [ 3.8445e+00,  1.1901e+00],\n        [ 9.4109e-01,  2.2539e-01],\n        [ 2.6236e+00,  4.6520e-01],\n        [ 2.3852e+00, -2.6751e-01],\n        [ 1.4971e+00,  2.1733e-01],\n        [ 3.9300e+00,  4.4390e-01],\n        [ 5.1244e-01, -1.1545e+00],\n        [ 1.5678e+00, -2.1512e-01],\n        [ 2.7575e+00, -1.5440e-01],\n        [ 8.7355e-01, -1.3103e+00],\n        [ 3.3879e+00, -1.8219e+00],\n        [ 7.3339e-01,  3.6509e-01],\n        [ 1.6715e+00, -1.4153e+00],\n        [ 1.9352e+00, -1.0310e+00],\n        [ 1.3457e+00,  5.2418e-01],\n        [ 4.3327e+00,  2.8645e-01],\n        [ 2.1878e+00,  1.1513e+00],\n        [ 1.6639e-01, -2.3741e-01],\n        [ 1.9361e+00,  1.0690e+00],\n        [ 2.2145e+00,  1.0612e-01],\n        [ 2.7228e+00, -1.3539e+00],\n        [ 4.3568e+00, -7.9163e-01],\n        [ 1.1363e+00,  1.0268e+00],\n        [ 2.5515e+00,  1.4839e-01],\n        [ 3.0407e+00, -1.5143e-01],\n        [ 2.2079e+00,  2.0493e+00],\n        [ 1.7186e+00, -6.5234e-01],\n        [ 4.0696e+00, -1.2897e-01],\n        [ 2.1222e+00, -8.7459e-01],\n        [ 3.8548e+00,  6.6901e-02],\n        [ 3.1870e+00, -1.1487e+00],\n        [ 1.4502e+00, -5.2199e-01],\n        [ 3.1476e+00, -9.3661e-01],\n        [ 1.9827e+00,  7.5676e-01],\n        [ 2.0761e+00,  1.5396e-01],\n        [ 3.4532e+00, -6.9147e-01],\n        [ 3.0510e+00, -3.0474e-01],\n        [ 2.7760e+00, -9.2511e-01],\n        [ 3.8867e+00,  1.2015e+00],\n        [ 1.6503e+00, -4.8225e-01],\n        [ 2.2865e+00, -1.7556e+00],\n        [ 2.6456e+00,  1.2926e+00],\n        [ 3.2832e+00, -1.2562e+00],\n        [ 1.3711e+00, -2.0696e-01],\n        [ 2.8757e+00,  4.3073e-01],\n        [ 2.0565e+00, -1.5891e+00],\n        [ 1.4201e+00, -3.0848e+00],\n        [ 2.1610e+00, -1.7824e-01],\n        [ 1.6914e+00,  9.9076e-01],\n        [ 3.2876e+00, -1.1048e-01],\n        [ 4.2089e+00,  1.6168e-01],\n        [ 3.5110e+00, -1.5417e+00],\n        [ 3.9453e+00,  4.4619e-01],\n        [ 1.7849e+00, -1.2073e-01],\n        [ 2.7077e+00,  6.8029e-01],\n        [ 3.5666e+00,  1.1820e+00],\n        [ 2.0694e+00, -2.3544e-01],\n        [ 2.4211e+00,  6.8303e-01],\n        [ 2.6554e+00,  1.9850e+00],\n        [ 4.1362e+00, -1.5750e+00],\n        [ 8.9999e-01, -3.7501e-01],\n        [ 3.5907e+00, -1.7254e+00],\n        [ 2.3740e+00,  1.3220e-01],\n        [ 3.7913e+00,  3.3673e-01],\n        [ 1.5338e+00, -2.0915e+00],\n        [ 3.2009e+00, -3.5942e-01],\n        [ 2.4948e+00,  5.7002e-01],\n        [ 1.4258e+00,  9.1420e-01],\n        [ 2.3722e+00, -1.0930e+00],\n        [ 4.1028e-01, -9.5599e-01],\n        [ 2.2194e+00,  5.7977e-01],\n        [ 2.0176e+00,  5.5117e-02],\n        [ 2.8746e+00, -1.9925e-02],\n        [ 2.0687e+00, -1.3426e+00],\n        [ 2.6407e+00, -1.0257e+00],\n        [ 2.8297e+00,  1.9408e-01],\n        [ 1.9181e+00, -2.8919e-01],\n        [ 4.1799e+00, -1.9759e+00],\n        [ 4.9784e+00,  1.2740e+00],\n        [ 2.1161e+00,  8.4444e-01],\n        [ 8.1144e-01,  2.3324e-01],\n        [ 3.5527e+00,  1.1761e-01],\n        [ 1.7949e+00,  1.3814e+00],\n        [ 4.6125e+00, -3.5963e-01],\n        [ 2.3271e+00, -6.7322e-01],\n        [ 3.7467e+00,  6.0503e-01],\n        [ 2.1632e+00, -5.4999e-01],\n        [ 3.5003e+00, -1.6124e-02],\n        [ 3.1372e+00, -9.1571e-02],\n        [ 2.2233e+00,  2.5333e-01],\n        [ 1.8863e+00, -1.7194e+00],\n        [ 3.2183e+00, -1.0608e+00],\n        [ 3.3923e+00,  2.6118e-01],\n        [ 2.8736e+00,  1.3572e+00],\n        [ 2.2161e+00, -1.9054e+00],\n        [ 2.0092e+00,  7.4460e-01],\n        [ 2.5358e+00, -2.9410e-01],\n        [ 3.2031e+00, -7.3816e-01],\n        [ 2.6028e+00,  3.2083e-01],\n        [ 4.0622e+00, -1.9339e-01],\n        [ 2.4544e+00, -1.4535e+00],\n        [ 1.7172e+00, -3.3482e-01],\n        [ 4.5646e-01,  6.2350e-01],\n        [ 4.2548e+00, -1.4756e+00],\n        [ 3.2005e+00, -1.2267e+00],\n        [ 2.9794e+00, -1.2444e+00],\n        [ 6.1245e-01,  1.1618e-01],\n        [ 1.1730e+00,  1.0875e+00],\n        [ 2.2598e+00, -1.8569e-01],\n        [ 2.8959e+00,  2.4153e+00],\n        [ 2.9680e+00, -1.1573e+00],\n        [ 2.1879e+00, -8.9113e-01],\n        [ 2.1098e+00, -3.7810e-01],\n        [ 3.5607e+00,  6.8827e-01],\n        [ 2.1624e+00,  9.5269e-01],\n        [ 2.7314e+00,  3.3669e-01],\n        [ 2.0010e+00, -1.1585e+00],\n        [ 1.1064e+00, -4.4861e-02],\n        [ 1.5018e+00,  1.3407e-02],\n        [ 2.8909e+00,  1.7605e+00],\n        [ 2.3967e+00, -1.1652e+00],\n        [ 2.9649e+00,  2.1494e-01],\n        [ 3.0249e+00, -1.6350e+00],\n        [ 2.9723e+00,  2.3841e+00],\n        [ 2.7132e+00, -4.1278e-01],\n        [ 9.0224e-01,  3.0541e+00],\n        [ 1.8516e+00, -1.2011e+00],\n        [ 1.6183e+00, -1.4854e-01],\n        [ 4.9019e-01,  9.6905e-01],\n        [ 3.0528e+00,  2.3730e+00],\n        [ 2.1911e+00, -3.1204e-01],\n        [ 3.2803e+00,  1.5786e+00],\n        [ 1.6463e+00,  5.2284e-01],\n        [ 2.9364e+00, -5.4513e-01],\n        [ 2.2500e+00,  1.9084e+00],\n        [ 1.9489e+00, -7.0188e-01],\n        [ 3.7724e-01, -9.8398e-01],\n        [ 2.7826e+00,  1.0828e+00],\n        [ 2.2076e+00,  4.3566e-01],\n        [ 2.7867e+00, -5.5517e-01],\n        [ 3.0548e+00, -1.5787e+00],\n        [ 3.1076e+00,  1.0897e+00],\n        [ 8.1708e-01,  6.8789e-01],\n        [ 1.8437e+00,  7.6104e-01],\n        [ 2.2378e+00,  7.9765e-01],\n        [ 2.5399e+00,  9.9374e-01],\n        [ 2.7259e+00,  3.1306e-02],\n        [ 3.8711e+00, -1.8700e+00],\n        [ 3.0073e+00, -2.0422e+00],\n        [ 1.8535e+00, -6.6463e-02],\n        [ 2.2565e+00,  1.9162e+00],\n        [ 1.9708e+00,  4.7272e-01],\n        [ 2.8176e+00,  2.1174e+00],\n        [ 2.0731e+00, -6.3278e-01],\n        [ 2.5507e+00,  7.3524e-01],\n        [ 1.7688e+00, -8.9459e-01],\n        [ 2.3946e+00,  1.9587e+00],\n        [ 3.4376e-01,  5.6878e-01],\n        [ 2.6411e+00,  2.9128e-01],\n        [ 3.4041e+00, -3.3682e-01],\n        [ 1.0747e+00,  1.7319e+00],\n        [ 3.4745e+00,  1.0979e+00],\n        [ 2.3752e+00,  2.1483e-01],\n        [ 1.6822e+00, -1.6372e+00],\n        [ 2.0474e+00,  1.2184e+00],\n        [ 3.9363e+00, -3.2945e+00],\n        [ 2.2731e+00, -1.7057e+00],\n        [ 3.0323e+00,  4.2794e-01],\n        [ 3.2685e-01, -5.4029e-01],\n        [ 2.2787e+00,  8.0107e-01],\n        [ 1.7773e+00,  2.5732e+00],\n        [ 2.5272e+00, -1.9369e+00],\n        [ 3.7335e+00, -2.5846e+00],\n        [ 2.1096e+00,  1.0141e+00],\n        [ 2.9123e+00,  1.2164e+00],\n        [ 3.6629e+00, -2.8382e-01],\n        [ 2.0374e+00, -3.3719e-01],\n        [ 3.2954e+00,  1.1134e+00],\n        [ 2.8126e+00, -4.7277e-01],\n        [ 8.7315e-01, -6.8058e-01],\n        [ 4.9065e+00, -3.7545e-01],\n        [ 2.3058e+00,  3.8376e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\nbatch_size 312\nweights tensor([[2.2378e-04, 2.2382e-04, 4.0735e-04,  ..., 9.5972e-01, 9.5972e-01,\n         1.0000e+00],\n        [1.6763e-02, 1.6936e-02, 2.5870e-02,  ..., 9.8845e-01, 9.9217e-01,\n         1.0000e+00],\n        [6.2434e-02, 6.2438e-02, 6.2858e-02,  ..., 7.2481e-01, 7.7349e-01,\n         1.0000e+00],\n        ...,\n        [1.0455e-04, 1.7796e-02, 2.0527e-02,  ..., 6.8730e-01, 8.2094e-01,\n         1.0000e+00],\n        [1.1285e-02, 1.1940e-01, 1.1941e-01,  ..., 9.6500e-01, 9.7798e-01,\n         1.0000e+00],\n        [1.1140e-02, 5.9760e-02, 7.2801e-02,  ..., 8.6188e-01, 1.0000e+00,\n         1.0000e+00]]) torch.Size([312, 32])\nuniform_decision tensor([[0.1674],\n        [0.7188],\n        [0.0245],\n        [0.4920],\n        [0.2697],\n        [0.3584],\n        [0.4400],\n        [0.2117],\n        [0.1262],\n        [0.3596],\n        [0.2191],\n        [0.8166],\n        [0.5127],\n        [0.8835],\n        [0.7096],\n        [0.6857],\n        [0.1644],\n        [0.3477],\n        [0.9732],\n        [0.1356],\n        [0.7517],\n        [0.6777],\n        [0.8052],\n        [0.3554],\n        [0.1854],\n        [0.6233],\n        [0.8289],\n        [0.0501],\n        [0.4435],\n        [0.0811],\n        [0.5240],\n        [0.5856],\n        [0.3775],\n        [0.9253],\n        [0.8189],\n        [0.9028],\n        [0.0962],\n        [0.6072],\n        [0.3299],\n        [0.7449],\n        [0.3825],\n        [0.5911],\n        [0.6724],\n        [0.5207],\n        [0.9632],\n        [0.9169],\n        [0.7288],\n        [0.2593],\n        [0.4107],\n        [0.8629],\n        [0.4118],\n        [0.6922],\n        [0.7690],\n        [0.6571],\n        [0.8241],\n        [0.5773],\n        [0.3467],\n        [0.1615],\n        [0.2494],\n        [0.0731],\n        [0.7821],\n        [0.7652],\n        [0.3801],\n        [0.7114],\n        [0.0848],\n        [0.8962],\n        [0.1074],\n        [0.4326],\n        [0.6061],\n        [0.4355],\n        [0.8781],\n        [0.2483],\n        [0.1354],\n        [0.4075],\n        [0.2897],\n        [0.1809],\n        [0.7299],\n        [0.9577],\n        [0.8804],\n        [0.8429],\n        [0.1474],\n        [0.8715],\n        [0.0967],\n        [0.2660],\n        [0.7014],\n        [0.1321],\n        [0.9451],\n        [0.9840],\n        [0.8185],\n        [0.1219],\n        [0.4172],\n        [0.8620],\n        [0.8511],\n        [0.7066],\n        [0.1622],\n        [0.5141],\n        [0.5511],\n        [0.1177],\n        [0.1980],\n        [0.3664],\n        [0.1437],\n        [0.3117],\n        [0.7830],\n        [0.1647],\n        [0.0847],\n        [0.3948],\n        [0.2538],\n        [0.9781],\n        [0.9202],\n        [0.3541],\n        [0.2936],\n        [0.3752],\n        [0.0087],\n        [0.0989],\n        [0.6840],\n        [0.7077],\n        [0.8762],\n        [0.0871],\n        [0.9755],\n        [0.6200],\n        [0.8982],\n        [0.4847],\n        [0.9410],\n        [0.2292],\n        [0.0455],\n        [0.7776],\n        [0.2621],\n        [0.7873],\n        [0.3883],\n        [0.2744],\n        [0.3896],\n        [0.7369],\n        [0.5316],\n        [0.5297],\n        [0.5587],\n        [0.6461],\n        [0.9411],\n        [0.8517],\n        [0.3086],\n        [0.0361],\n        [0.3189],\n        [0.3334],\n        [0.9025],\n        [0.0356],\n        [0.8485],\n        [0.6198],\n        [0.2524],\n        [0.2943],\n        [0.9302],\n        [0.9764],\n        [0.4106],\n        [0.9880],\n        [0.2071],\n        [0.6378],\n        [0.2856],\n        [0.7937],\n        [0.6132],\n        [0.9761],\n        [0.2929],\n        [0.4963],\n        [0.1570],\n        [0.6032],\n        [0.0394],\n        [0.6941],\n        [0.2509],\n        [0.2903],\n        [0.5267],\n        [0.5166],\n        [0.7326],\n        [0.4189],\n        [0.6656],\n        [0.1574],\n        [0.2965],\n        [0.5947],\n        [0.5134],\n        [0.8260],\n        [0.3934],\n        [0.6670],\n        [0.6432],\n        [0.9951],\n        [0.2939],\n        [0.5823],\n        [0.6044],\n        [0.9228],\n        [0.1621],\n        [0.1296],\n        [0.9659],\n        [0.8733],\n        [0.7260],\n        [0.9358],\n        [0.2761],\n        [0.2267],\n        [0.0228],\n        [0.5355],\n        [0.1155],\n        [0.0985],\n        [0.5066],\n        [0.8887],\n        [0.9133],\n        [0.5043],\n        [0.8422],\n        [0.0808],\n        [0.2079],\n        [0.4488],\n        [0.4730],\n        [0.9598],\n        [0.6867],\n        [0.3951],\n        [0.4248],\n        [0.4462],\n        [0.8272],\n        [0.7956],\n        [0.1956],\n        [0.3158],\n        [0.0618],\n        [0.5687],\n        [0.5242],\n        [0.5379],\n        [0.5888],\n        [0.2179],\n        [0.9658],\n        [0.6571],\n        [0.4635],\n        [0.1319],\n        [0.8225],\n        [0.9845],\n        [0.2999],\n        [0.6915],\n        [0.2786],\n        [0.0899],\n        [0.4351],\n        [0.3574],\n        [0.9057],\n        [0.4036],\n        [0.4407],\n        [0.8050],\n        [0.1386],\n        [0.2048],\n        [0.4658],\n        [0.4106],\n        [0.3676],\n        [0.6689],\n        [0.1007],\n        [0.2981],\n        [0.5558],\n        [0.7584],\n        [0.9684],\n        [0.0066],\n        [0.3945],\n        [0.3823],\n        [0.7166],\n        [0.7930],\n        [0.3469],\n        [0.2134],\n        [0.9138],\n        [0.5855],\n        [0.8910],\n        [0.9430],\n        [0.5766],\n        [0.9267],\n        [0.1210],\n        [0.5129],\n        [0.4725],\n        [0.2813],\n        [0.6343],\n        [0.2631],\n        [0.4356],\n        [0.9252],\n        [0.0715],\n        [0.6764],\n        [0.0536],\n        [0.1878],\n        [0.3163],\n        [0.6788],\n        [0.5866],\n        [0.8075],\n        [0.4291],\n        [0.8776],\n        [0.2763],\n        [0.3929],\n        [0.0639],\n        [0.5026],\n        [0.4577],\n        [0.9654],\n        [0.0898],\n        [0.9318],\n        [0.1721],\n        [0.6255],\n        [0.9588],\n        [0.0238],\n        [0.5816],\n        [0.7527],\n        [0.8935],\n        [0.0978],\n        [0.5396],\n        [0.6795],\n        [0.1594],\n        [0.8476],\n        [0.2262],\n        [0.9429],\n        [0.9110],\n        [0.7937],\n        [0.8653],\n        [0.8008],\n        [0.5564],\n        [0.7972],\n        [0.1050],\n        [0.6682],\n        [0.5952],\n        [0.5492],\n        [0.1329],\n        [0.4652]]) torch.Size([312, 1])\nmask tensor([[False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [ True, False, False,  ..., False, False, False],\n        ...,\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False],\n        [False, False, False,  ..., False, False, False]]) torch.Size([312, 32])\nthetas tensor([[-0.6964,  0.8339],\n        [-2.9023,  1.4815],\n        [-1.2076, -0.4469],\n        ...,\n        [ 0.4430,  0.1385],\n        [ 2.7782,  0.1458],\n        [-4.0567,  0.5252]]) torch.Size([9984, 2])\nsamples tensor([[ 2.0566e+00, -1.0804e+00],\n        [ 2.6606e+00, -7.7666e-01],\n        [ 1.8765e+00,  4.2155e-01],\n        [ 2.1586e+00, -2.7307e+00],\n        [ 2.1851e+00,  1.8609e+00],\n        [ 2.5785e+00, -1.6698e+00],\n        [ 8.6696e-01, -1.9129e+00],\n        [ 2.4005e+00,  1.1971e+00],\n        [ 1.2874e+00,  8.7262e-02],\n        [ 4.1687e+00, -1.5779e+00],\n        [ 3.3556e+00, -2.1102e-01],\n        [ 2.0069e+00, -1.5278e+00],\n        [ 2.5493e+00, -2.7697e+00],\n        [ 2.3087e+00, -3.1843e-02],\n        [ 3.0342e+00, -1.4025e+00],\n        [ 2.6558e+00, -5.6993e-01],\n        [ 2.5493e+00,  4.1488e-01],\n        [ 2.0536e+00, -1.5803e-01],\n        [ 1.6271e+00,  3.9703e-01],\n        [ 2.2534e+00,  5.7948e-01],\n        [ 3.4507e+00, -3.2207e-01],\n        [ 3.1050e+00,  2.6483e+00],\n        [ 3.8426e+00, -1.7396e-01],\n        [ 2.5750e+00,  1.0184e+00],\n        [ 1.0063e+00,  8.2336e-01],\n        [ 3.1102e+00, -1.2728e+00],\n        [ 3.0450e+00,  2.3619e-01],\n        [ 3.6385e+00, -9.7288e-02],\n        [ 1.6703e+00, -1.8690e-01],\n        [ 3.2929e+00, -3.9270e+00],\n        [ 3.4524e+00, -3.4106e-01],\n        [ 3.9942e+00,  3.7075e-01],\n        [ 1.1734e+00, -2.2467e-01],\n        [ 1.8875e+00,  1.1406e+00],\n        [ 3.5675e+00,  1.4710e+00],\n        [ 2.9736e+00, -4.0362e-02],\n        [ 1.9795e+00, -9.1918e-03],\n        [ 1.2020e+00,  5.0370e-01],\n        [ 2.4677e-01,  4.9973e-01],\n        [ 3.3603e+00,  6.3148e-01],\n        [ 1.0239e+00, -7.0265e-02],\n        [ 1.7998e+00,  1.0963e-01],\n        [ 1.8498e+00, -6.0503e-01],\n        [ 2.3365e+00, -1.1755e+00],\n        [ 6.7820e-01, -4.8300e-01],\n        [ 3.2108e+00, -1.5101e+00],\n        [ 2.9752e+00, -5.3494e-01],\n        [ 3.5361e+00, -4.0761e-01],\n        [ 1.6218e+00, -1.1678e+00],\n        [ 1.5396e+00,  4.4086e-02],\n        [ 1.4943e+00, -1.3593e+00],\n        [ 3.2501e+00, -8.6062e-01],\n        [ 2.0382e+00, -6.0504e-01],\n        [ 3.6453e+00,  6.9754e-01],\n        [ 3.2859e+00,  1.2723e+00],\n        [ 3.9847e+00,  2.8398e-01],\n        [ 1.1675e+00, -2.1553e+00],\n        [ 2.5196e+00, -4.1727e-01],\n        [ 2.3019e+00, -6.2441e-01],\n        [ 2.7073e+00, -5.9011e-01],\n        [ 3.2821e+00,  5.6452e-01],\n        [ 3.2788e+00,  1.1373e+00],\n        [ 5.4238e-01,  1.0050e+00],\n        [ 2.4965e+00, -1.9136e+00],\n        [ 3.6016e+00, -1.3356e+00],\n        [ 2.6992e+00,  6.3325e-01],\n        [ 2.9273e+00,  1.9093e-01],\n        [ 3.2826e+00,  1.8166e+00],\n        [ 2.6880e+00,  9.1477e-01],\n        [ 1.1138e+00, -1.4490e+00],\n        [ 1.6660e+00, -5.5003e-01],\n        [ 2.3374e+00, -1.0072e+00],\n        [ 2.3261e+00,  7.1190e-01],\n        [ 7.0046e-01, -4.8016e-01],\n        [ 1.9348e+00, -8.5276e-01],\n        [ 4.3826e+00,  1.6593e-01],\n        [ 2.9130e+00, -1.0641e+00],\n        [ 2.0206e+00,  1.6084e+00],\n        [ 9.3813e-01, -5.6144e-01],\n        [ 1.9276e+00, -5.6635e-02],\n        [ 3.6676e+00, -1.5827e+00],\n        [ 3.8730e+00, -7.1781e-02],\n        [ 1.8116e+00, -1.7837e+00],\n        [ 3.5064e+00, -2.7130e-02],\n        [ 3.2539e+00, -5.3110e-01],\n        [ 2.8272e+00, -2.1387e+00],\n        [ 2.2522e+00,  1.3467e+00],\n        [ 1.8588e+00,  1.7203e+00],\n        [ 1.1556e+00, -3.6235e-01],\n        [ 3.5351e+00,  9.6270e-01],\n        [ 3.4138e+00, -5.6474e-01],\n        [ 1.8334e+00, -7.5833e-01],\n        [ 4.0229e+00, -1.0130e+00],\n        [ 1.6705e+00, -1.2351e+00],\n        [ 3.4130e+00,  6.3482e-01],\n        [ 3.1420e+00,  6.4155e-01],\n        [ 2.9443e+00,  6.4665e-01],\n        [ 3.1650e+00, -2.3953e+00],\n        [ 8.4095e-01,  1.4381e+00],\n        [ 3.6020e+00,  1.3672e+00],\n        [ 1.3287e+00, -9.2148e-01],\n        [ 1.6847e+00,  1.1752e-01],\n        [ 4.3315e+00, -1.7723e+00],\n        [ 1.8795e+00,  8.4262e-01],\n        [ 2.4912e+00, -1.5800e+00],\n        [ 2.1012e+00,  1.4856e+00],\n        [ 2.4881e+00, -2.4093e-01],\n        [ 1.6490e+00,  3.8678e-01],\n        [ 2.3247e+00,  2.0098e-02],\n        [ 2.4916e+00, -1.9438e+00],\n        [ 2.0090e+00,  7.6900e-01],\n        [ 2.5689e+00, -1.2883e+00],\n        [ 3.9550e+00,  1.0238e+00],\n        [ 1.4942e+00, -1.6763e+00],\n        [ 2.1616e+00, -4.0942e-01],\n        [ 2.4347e+00,  1.2918e+00],\n        [ 8.3926e-01,  1.3678e+00],\n        [ 1.8419e+00,  2.9456e-01],\n        [ 1.0932e+00,  8.6534e-01],\n        [ 5.3859e-01,  9.1471e-01],\n        [ 4.1917e+00, -2.1975e+00],\n        [ 2.4666e+00,  9.0527e-01],\n        [ 2.9460e+00,  1.4987e+00],\n        [ 2.4349e+00,  1.7878e+00],\n        [ 1.2953e+00, -2.1804e-01],\n        [ 1.2239e+00, -1.0794e-01],\n        [ 8.7889e-01,  1.7305e+00],\n        [ 2.9263e+00,  9.7232e-02],\n        [ 3.6923e+00, -7.8224e-01],\n        [ 1.8978e+00, -8.1462e-01],\n        [ 3.3519e+00, -1.0483e+00],\n        [ 3.5255e+00, -1.4183e+00],\n        [ 2.2218e+00, -1.0676e-01],\n        [ 2.8887e+00,  3.5724e-01],\n        [ 9.2938e-01, -6.0108e-01],\n        [ 1.6468e+00, -1.0592e+00],\n        [ 1.6210e+00,  8.0369e-01],\n        [ 3.0489e+00, -6.2138e-01],\n        [ 1.6197e+00,  6.9900e-01],\n        [ 7.1443e-01, -1.1651e+00],\n        [ 3.5887e+00, -4.8372e-01],\n        [ 2.7805e+00,  6.5564e-01],\n        [ 2.8514e+00,  4.1654e-01],\n        [ 2.4881e+00,  1.0498e+00],\n        [ 4.3188e+00,  3.1348e-01],\n        [ 2.3655e+00, -6.2196e-02],\n        [ 3.6382e+00,  2.5450e-01],\n        [ 1.6756e+00,  2.3375e+00],\n        [ 1.4139e+00,  4.3297e-02],\n        [ 1.9015e+00,  3.3836e-01],\n        [ 2.2681e+00,  2.4282e+00],\n        [ 2.2877e+00, -9.3849e-01],\n        [ 2.6506e+00,  4.0390e-01],\n        [ 3.5866e+00,  1.6369e-02],\n        [ 1.9845e+00, -2.8849e-01],\n        [ 3.6425e+00, -7.3408e-01],\n        [ 1.4594e+00, -5.1622e-01],\n        [ 8.4448e-01, -7.3670e-01],\n        [ 3.5946e+00, -5.8416e-01],\n        [ 4.1853e+00, -7.9674e-01],\n        [ 4.9938e-01, -3.0381e-01],\n        [ 2.8534e+00,  5.2522e-01],\n        [ 9.7611e-01,  7.7014e-02],\n        [ 2.0031e+00, -3.0281e-01],\n        [ 4.4462e+00, -8.3250e-01],\n        [ 3.6532e+00, -5.4660e-01],\n        [-1.5241e-01, -9.9735e-01],\n        [ 4.9138e+00,  7.0061e-01],\n        [ 3.5144e+00,  7.5198e-01],\n        [ 2.9349e+00,  2.2416e-01],\n        [ 2.1572e+00,  1.3727e-01],\n        [ 1.0554e+00, -9.3947e-01],\n        [ 3.1914e+00, -2.2539e-01],\n        [ 2.2300e+00,  1.3671e+00],\n        [ 2.1570e+00, -7.1066e-01],\n        [ 1.8880e+00, -6.1749e-01],\n        [ 1.5768e+00, -2.0948e+00],\n        [ 2.9752e+00, -2.5194e-01],\n        [ 2.7881e+00,  2.6110e-01],\n        [ 3.4370e+00,  4.5078e-01],\n        [ 2.1835e+00, -1.2171e+00],\n        [ 4.2030e+00,  1.3325e-01],\n        [ 3.1739e+00, -4.5697e-01],\n        [ 1.1574e+00, -3.8246e-01],\n        [ 2.9750e+00, -1.2944e+00],\n        [ 2.5414e+00,  8.4456e-01],\n        [ 3.3010e+00, -2.0808e+00],\n        [ 9.1341e-01, -1.7572e+00],\n        [ 3.7604e+00,  4.7261e-03],\n        [ 2.8181e+00, -6.4527e-01],\n        [ 3.4286e+00,  4.0296e-01],\n        [ 3.5025e+00,  8.0682e-01],\n        [ 5.0521e-01,  4.3012e-02],\n        [ 3.5908e+00, -1.1092e-01],\n        [ 2.8813e+00,  1.4549e+00],\n        [ 1.7159e+00, -3.1263e-01],\n        [ 3.2260e+00,  3.5095e+00],\n        [ 2.1134e+00,  9.2662e-01],\n        [ 1.2954e+00, -3.7381e-01],\n        [ 1.8390e+00,  1.1772e-01],\n        [ 3.2809e+00,  1.6234e-01],\n        [ 9.3195e-01,  1.7716e+00],\n        [ 2.7372e+00, -7.5285e-01],\n        [ 2.0238e+00, -1.5727e+00],\n        [ 5.5781e-01, -9.0900e-01],\n        [ 8.8994e-01, -6.7544e-01],\n        [ 3.4152e+00, -5.4750e-01],\n        [ 1.8302e+00, -7.1042e-01],\n        [ 2.1596e+00, -2.0176e+00],\n        [ 2.4246e+00, -8.3360e-01],\n        [ 2.0937e+00,  2.0887e-01],\n        [ 2.3347e+00, -1.0783e+00],\n        [ 2.1942e+00, -9.7597e-01],\n        [ 2.4834e+00,  9.9830e-02],\n        [ 1.5122e+00,  6.5871e-01],\n        [ 2.3860e+00, -4.9633e-01],\n        [ 3.0673e+00, -1.5074e+00],\n        [ 1.9988e+00, -1.8192e+00],\n        [ 2.4722e+00, -6.9229e-01],\n        [ 3.0360e+00, -8.2838e-01],\n        [ 1.1576e+00,  4.5205e-01],\n        [ 4.0644e+00, -1.2388e+00],\n        [ 2.8641e+00,  6.8281e-01],\n        [ 2.5796e+00, -5.5502e-01],\n        [ 3.9656e+00, -1.3982e+00],\n        [ 2.5314e+00,  1.7613e+00],\n        [ 3.2430e+00,  1.4881e-01],\n        [ 1.4885e+00, -2.6818e-01],\n        [ 3.3928e+00,  1.6708e+00],\n        [ 2.1793e+00,  2.3319e-01],\n        [ 2.7979e+00, -4.7182e-01],\n        [ 2.1340e+00,  3.8084e-01],\n        [ 1.7631e+00,  4.0900e-02],\n        [ 3.3934e+00,  2.0149e-01],\n        [ 2.2117e+00, -1.0862e-01],\n        [ 1.4286e+00, -1.4204e+00],\n        [ 2.3244e+00, -9.6825e-01],\n        [ 6.1312e-01, -1.3216e-01],\n        [ 2.8229e+00, -9.6281e-01],\n        [ 1.5815e+00, -4.5157e-01],\n        [ 3.0163e+00,  2.4756e+00],\n        [ 2.2036e+00, -1.8917e+00],\n        [ 2.4449e+00,  2.7157e-01],\n        [ 2.2126e+00, -1.5493e+00],\n        [ 4.5114e-01,  7.0364e-01],\n        [ 2.1031e+00,  1.1230e+00],\n        [ 1.6165e+00,  2.0425e-01],\n        [ 1.5534e+00,  1.5273e+00],\n        [ 2.3761e+00,  3.8975e-01],\n        [ 2.8353e+00,  1.2485e-01],\n        [ 3.1413e+00,  2.0645e-01],\n        [ 2.4109e-01, -7.6570e-01],\n        [ 1.8500e+00, -1.3881e+00],\n        [ 2.5769e+00,  4.6391e-01],\n        [ 3.1729e+00,  3.4357e-02],\n        [ 2.2448e+00,  6.7691e-01],\n        [ 2.4331e+00,  8.8931e-01],\n        [ 2.4986e+00,  1.1141e+00],\n        [ 7.1723e-01, -9.2739e-01],\n        [ 2.1557e+00, -3.3531e-01],\n        [ 1.0756e+00, -5.3108e-01],\n        [ 2.7291e+00, -1.6756e-01],\n        [ 1.8847e+00, -1.5585e+00],\n        [ 1.3452e+00, -4.6690e-02],\n        [ 2.9718e+00, -1.7957e+00],\n        [ 2.4603e+00,  2.9114e-01],\n        [ 1.9149e+00, -1.2619e-01],\n        [ 1.5205e+00,  1.7448e+00],\n        [ 2.6990e+00, -1.9306e-01],\n        [ 3.3108e+00, -4.0385e-01],\n        [ 4.0314e+00, -1.0534e+00],\n        [ 1.3800e+00, -5.8088e-01],\n        [ 4.5294e+00, -7.3603e-01],\n        [ 1.6385e-01, -4.4486e-01],\n        [ 2.7958e+00, -6.8028e-02],\n        [ 2.9866e+00, -1.5906e+00],\n        [ 4.3250e+00,  1.9174e-01],\n        [ 2.4392e-01, -5.3338e-01],\n        [ 1.4583e+00,  1.5405e-01],\n        [ 2.4005e+00, -3.0276e+00],\n        [ 1.2512e+00, -1.7534e+00],\n        [ 2.1771e+00, -7.4014e-01],\n        [ 1.3409e+00, -8.6556e-01],\n        [ 1.9360e+00,  4.5482e-01],\n        [ 2.8951e+00, -1.7568e+00],\n        [ 2.7601e+00,  6.6906e-01],\n        [ 3.2467e+00,  3.0874e-01],\n        [ 2.0186e+00, -1.8200e+00],\n        [ 3.2549e+00,  1.2921e+00],\n        [ 1.1949e+00,  7.5689e-01],\n        [ 3.0175e+00, -1.2808e+00],\n        [ 1.8830e+00, -5.8291e-01],\n        [ 4.3109e+00,  1.9025e+00],\n        [ 2.0676e+00, -3.6236e-01],\n        [ 3.1168e+00, -1.6357e+00],\n        [ 2.4126e+00,  4.6843e-02],\n        [ 2.1164e+00,  3.3750e-01],\n        [ 3.3635e+00, -1.1217e+00],\n        [ 4.3504e+00,  5.8908e-01],\n        [ 1.3861e+00,  1.3800e-01],\n        [ 2.2707e+00,  5.6894e-01],\n        [ 3.5025e+00, -3.7722e-01],\n        [ 3.3911e+00,  1.2212e+00],\n        [ 2.4421e+00, -8.3540e-01],\n        [ 4.3475e+00, -1.0266e+00],\n        [ 2.7242e+00, -8.6750e-01],\n        [ 2.6827e+00,  5.0487e-01],\n        [ 3.4966e+00, -9.7933e-01],\n        [ 3.1738e+00, -5.2980e-01],\n        [ 3.1731e+00, -1.0725e+00],\n        [ 1.9618e+00,  5.5628e-01],\n        [ 2.4399e+00, -2.3726e-01]]) torch.Size([312, 2])\n\n\n\nDrawing 512 posterior samples:   0%|          | 0/512 [00:00&lt;?, ?it/s]\n\n\nbatch_size 16\nweights tensor([[8.1593e-04, 1.0512e-03, 8.5689e-02, 8.5696e-02, 1.2302e-01, 1.4705e-01,\n         1.4707e-01, 1.4707e-01, 2.1163e-01, 2.2965e-01, 2.3050e-01, 2.3055e-01,\n         2.3111e-01, 2.3112e-01, 2.3147e-01, 2.3165e-01, 2.7941e-01, 2.7968e-01,\n         2.7976e-01, 2.7976e-01, 4.9991e-01, 5.0009e-01, 6.4366e-01, 6.4366e-01,\n         6.4366e-01, 6.5187e-01, 6.5395e-01, 9.5090e-01, 9.9995e-01, 9.9999e-01,\n         9.9999e-01, 1.0000e+00],\n        [4.1718e-04, 3.8413e-03, 3.8413e-03, 3.8418e-03, 2.0433e-02, 3.7250e-01,\n         3.7250e-01, 3.7250e-01, 3.8290e-01, 3.8348e-01, 3.8383e-01, 4.0635e-01,\n         4.0635e-01, 4.0635e-01, 4.1645e-01, 4.9673e-01, 4.9689e-01, 4.9730e-01,\n         4.9730e-01, 4.9731e-01, 8.0501e-01, 8.0534e-01, 8.0534e-01, 8.1814e-01,\n         8.2504e-01, 8.2518e-01, 8.4641e-01, 8.5256e-01, 8.8789e-01, 8.8814e-01,\n         9.9265e-01, 1.0000e+00],\n        [4.7591e-03, 4.7593e-03, 7.8859e-03, 2.1814e-02, 2.2332e-02, 4.7094e-02,\n         5.4670e-02, 2.0013e-01, 2.0013e-01, 2.0013e-01, 2.8241e-01, 3.6080e-01,\n         5.6699e-01, 5.6743e-01, 5.6765e-01, 5.6872e-01, 6.7286e-01, 6.7293e-01,\n         6.7449e-01, 6.7449e-01, 6.7458e-01, 6.7458e-01, 7.1303e-01, 7.1326e-01,\n         7.1799e-01, 7.2272e-01, 7.2273e-01, 7.2303e-01, 7.3078e-01, 7.3110e-01,\n         7.3457e-01, 1.0000e+00],\n        [1.0211e-05, 4.9363e-04, 4.8813e-02, 4.9466e-02, 8.8238e-02, 8.8246e-02,\n         8.8289e-02, 9.1148e-02, 9.1149e-02, 9.1161e-02, 9.1366e-02, 1.5871e-01,\n         7.9373e-01, 7.9422e-01, 7.9778e-01, 8.0717e-01, 8.0717e-01, 8.2142e-01,\n         8.2147e-01, 8.2147e-01, 8.2148e-01, 8.2149e-01, 8.2150e-01, 8.2283e-01,\n         8.2307e-01, 8.9861e-01, 9.8483e-01, 9.9271e-01, 9.9271e-01, 9.9311e-01,\n         1.0000e+00, 1.0000e+00],\n        [3.9965e-03, 4.1940e-03, 4.1940e-03, 2.3957e-02, 9.6023e-02, 9.6033e-02,\n         1.1017e-01, 1.1017e-01, 1.1148e-01, 1.1148e-01, 1.1964e-01, 1.1964e-01,\n         1.3281e-01, 1.3281e-01, 1.3310e-01, 3.3698e-01, 3.3708e-01, 3.3708e-01,\n         3.3750e-01, 5.7621e-01, 5.7630e-01, 5.7996e-01, 5.8005e-01, 5.9884e-01,\n         6.0100e-01, 6.0100e-01, 7.9541e-01, 7.9541e-01, 9.8564e-01, 9.8752e-01,\n         9.9799e-01, 1.0000e+00],\n        [2.3669e-01, 2.7951e-01, 3.3153e-01, 3.3212e-01, 3.5797e-01, 3.5849e-01,\n         3.5850e-01, 4.2687e-01, 4.2720e-01, 5.4782e-01, 5.4782e-01, 5.5888e-01,\n         5.5889e-01, 5.7365e-01, 5.8065e-01, 5.8081e-01, 6.8634e-01, 6.8634e-01,\n         6.8647e-01, 6.8730e-01, 6.8731e-01, 7.3431e-01, 7.4327e-01, 7.7185e-01,\n         8.8045e-01, 8.8348e-01, 8.8468e-01, 8.8469e-01, 8.8510e-01, 9.1985e-01,\n         9.7219e-01, 1.0000e+00],\n        [9.8879e-08, 4.4151e-03, 1.0149e-02, 1.0609e-02, 1.4244e-01, 1.4255e-01,\n         1.4299e-01, 3.9106e-01, 4.0020e-01, 4.4709e-01, 4.9264e-01, 4.9401e-01,\n         4.9480e-01, 4.9480e-01, 5.0188e-01, 5.0188e-01, 5.4702e-01, 5.4702e-01,\n         5.8991e-01, 5.8991e-01, 8.8818e-01, 8.8819e-01, 8.9644e-01, 9.1021e-01,\n         9.1477e-01, 9.1477e-01, 9.1641e-01, 9.1721e-01, 9.1736e-01, 9.2303e-01,\n         1.0000e+00, 1.0000e+00],\n        [4.3773e-06, 1.0665e-04, 1.0957e-04, 4.4341e-02, 4.4343e-02, 4.4346e-02,\n         5.3596e-02, 5.3597e-02, 7.1110e-02, 7.1393e-02, 7.1511e-02, 1.0530e-01,\n         1.1129e-01, 1.1142e-01, 1.5739e-01, 1.5742e-01, 1.5919e-01, 4.9656e-01,\n         4.9656e-01, 5.0749e-01, 5.0758e-01, 5.0760e-01, 5.1548e-01, 5.1953e-01,\n         5.2947e-01, 8.3456e-01, 8.4866e-01, 8.4866e-01, 8.4887e-01, 9.9619e-01,\n         9.9995e-01, 1.0000e+00],\n        [2.6152e-04, 2.6161e-04, 2.9293e-04, 3.1188e-04, 4.4316e-02, 4.4331e-02,\n         4.4331e-02, 1.3094e-01, 1.5450e-01, 1.8813e-01, 1.9995e-01, 2.0397e-01,\n         2.0712e-01, 2.7111e-01, 2.7112e-01, 3.1899e-01, 3.1899e-01, 5.5622e-01,\n         7.3306e-01, 7.3508e-01, 7.6858e-01, 7.6858e-01, 9.0500e-01, 9.0500e-01,\n         9.0501e-01, 9.0509e-01, 9.0511e-01, 9.6894e-01, 9.9292e-01, 9.9543e-01,\n         9.9547e-01, 1.0000e+00],\n        [1.1028e-01, 1.2440e-01, 1.2769e-01, 1.3043e-01, 5.5492e-01, 5.5493e-01,\n         5.5501e-01, 5.5502e-01, 5.5588e-01, 6.3590e-01, 6.4036e-01, 6.4181e-01,\n         6.4181e-01, 6.6782e-01, 6.8371e-01, 6.9509e-01, 7.0305e-01, 7.0770e-01,\n         7.5587e-01, 7.6236e-01, 7.7006e-01, 7.7010e-01, 7.7013e-01, 9.7006e-01,\n         9.9619e-01, 9.9621e-01, 9.9626e-01, 9.9626e-01, 9.9931e-01, 9.9978e-01,\n         1.0000e+00, 1.0000e+00],\n        [2.4095e-05, 2.4103e-05, 2.4496e-05, 2.7235e-01, 2.7259e-01, 2.7360e-01,\n         2.7364e-01, 2.8041e-01, 2.8041e-01, 2.9622e-01, 3.4743e-01, 3.5580e-01,\n         3.5862e-01, 4.4911e-01, 4.4911e-01, 4.4912e-01, 8.0514e-01, 8.1364e-01,\n         8.2935e-01, 8.2936e-01, 8.4256e-01, 8.4490e-01, 8.4526e-01, 8.5067e-01,\n         8.5083e-01, 8.5135e-01, 9.1890e-01, 9.1890e-01, 9.1890e-01, 9.1894e-01,\n         9.1994e-01, 1.0000e+00],\n        [4.0196e-10, 1.4963e-04, 9.7903e-02, 1.6644e-01, 1.6644e-01, 1.6913e-01,\n         3.1075e-01, 3.4377e-01, 3.4378e-01, 3.4378e-01, 3.5580e-01, 3.5599e-01,\n         3.5804e-01, 3.6616e-01, 3.6630e-01, 4.5079e-01, 4.5081e-01, 6.0488e-01,\n         6.6700e-01, 6.6795e-01, 6.7111e-01, 6.7120e-01, 6.7978e-01, 6.7978e-01,\n         7.5660e-01, 8.0333e-01, 8.0333e-01, 8.0333e-01, 8.2043e-01, 9.9823e-01,\n         9.9825e-01, 1.0000e+00],\n        [1.3210e-08, 3.6135e-07, 1.4009e-03, 1.4719e-03, 2.0179e-01, 3.5828e-01,\n         3.7389e-01, 3.7546e-01, 3.7675e-01, 3.9316e-01, 5.5827e-01, 5.8126e-01,\n         5.8128e-01, 5.8128e-01, 5.8315e-01, 5.8914e-01, 5.9698e-01, 7.3467e-01,\n         7.3599e-01, 7.4093e-01, 7.5730e-01, 7.5745e-01, 7.8266e-01, 9.9701e-01,\n         9.9701e-01, 9.9878e-01, 9.9878e-01, 9.9892e-01, 9.9898e-01, 9.9916e-01,\n         9.9917e-01, 1.0000e+00],\n        [3.1885e-02, 3.2166e-02, 3.2333e-02, 3.2333e-02, 3.3780e-02, 3.7313e-02,\n         3.7317e-02, 3.7771e-02, 4.5534e-01, 4.6860e-01, 4.6860e-01, 4.6878e-01,\n         4.6891e-01, 4.6894e-01, 4.7511e-01, 4.7513e-01, 4.7513e-01, 8.1233e-01,\n         8.2838e-01, 8.8444e-01, 9.5531e-01, 9.5631e-01, 9.5632e-01, 9.6106e-01,\n         9.6701e-01, 9.7317e-01, 9.7317e-01, 9.7622e-01, 9.7625e-01, 9.7626e-01,\n         9.7628e-01, 1.0000e+00],\n        [1.4748e-02, 1.6440e-02, 3.1843e-02, 3.1843e-02, 3.1847e-02, 3.1877e-02,\n         3.4775e-02, 3.4797e-02, 3.6236e-02, 3.9044e-01, 3.9044e-01, 4.1089e-01,\n         4.1258e-01, 4.4425e-01, 4.4425e-01, 4.4431e-01, 4.4626e-01, 4.4626e-01,\n         6.9312e-01, 6.9315e-01, 6.9338e-01, 7.4038e-01, 7.4060e-01, 9.5267e-01,\n         9.5547e-01, 9.5926e-01, 9.6786e-01, 9.7806e-01, 9.9948e-01, 9.9948e-01,\n         9.9951e-01, 1.0000e+00],\n        [4.8540e-02, 5.1860e-02, 3.9096e-01, 3.9096e-01, 3.9096e-01, 3.9337e-01,\n         3.9412e-01, 3.9459e-01, 4.0521e-01, 4.0525e-01, 4.0561e-01, 4.1855e-01,\n         4.1897e-01, 4.2104e-01, 4.2130e-01, 4.2155e-01, 4.2368e-01, 4.2389e-01,\n         8.5300e-01, 8.5303e-01, 8.5304e-01, 8.6363e-01, 8.6665e-01, 8.7962e-01,\n         8.7962e-01, 8.7962e-01, 8.7965e-01, 8.7965e-01, 8.8267e-01, 9.9738e-01,\n         9.9738e-01, 1.0000e+00]]) torch.Size([16, 32])\nuniform_decision tensor([[0.8520],\n        [0.0456],\n        [0.1339],\n        [0.1599],\n        [0.2245],\n        [0.7124],\n        [0.1367],\n        [0.9855],\n        [0.2702],\n        [0.4623],\n        [0.5226],\n        [0.6647],\n        [0.8427],\n        [0.9547],\n        [0.7779],\n        [0.8923]]) torch.Size([16, 1])\nmask tensor([[False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False,  True, False, False,\n         False, False],\n        [False, False, False, False, False,  True, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False,  True, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False,  True, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False,  True, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False,  True, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False,  True, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False,  True,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False,  True, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False,  True, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False,  True, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False,  True, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False,  True, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n          True, False, False, False, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False,  True, False, False, False, False, False, False,\n         False, False],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False,  True,\n         False, False]]) torch.Size([16, 32])\nthetas tensor([[-0.2931,  0.5207],\n        [-0.7866,  0.3730],\n        [ 4.9572,  0.1860],\n        ...,\n        [ 1.4958, -0.0133],\n        [-3.2456,  0.5106],\n        [ 0.0861,  0.9045]]) torch.Size([512, 2])\nsamples tensor([[ 2.7641, -1.0670],\n        [ 2.3962, -0.1905],\n        [ 1.3104, -0.4666],\n        [ 1.7428, -2.6051],\n        [ 2.5024, -0.6990],\n        [ 4.1916,  1.6638],\n        [ 2.0104,  0.0144],\n        [ 2.1081,  0.2179],\n        [ 2.0445,  0.4226],\n        [ 1.5249, -1.4755],\n        [ 2.3338, -1.5420],\n        [ 2.2381,  0.6313],\n        [ 3.3044, -1.0268],\n        [ 1.9700,  0.6789],\n        [ 1.7458, -0.4808],\n        [ 1.4958, -0.0133]]) torch.Size([16, 2])\n</code></pre> <pre><code>theta_inferred_sir_2.shape\n</code></pre> <pre><code>torch.Size([10000, 2])\n</code></pre> <pre><code>fig, ax = marginal_plot(\n    [theta_inferred_sir_2, theta_inferred_sir_32, gt_samples],\n    limits=[[-5, 5], [-5, 5]],\n    figsize=(5, 1.5),\n    diag=\"kde\",  # smooth histogram\n)\nax[0][1].legend([\"NPE\", \"NPE-IS\", \"Groud Truth\"], loc=\"upper right\", bbox_to_anchor=[1.8, 1.0, 0.0, 0.0])\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x2abeadba0&gt;\n</code></pre> <p></p> <pre><code>fig, ax = marginal_plot(\n    [gt_samples, theta_inferred],\n    limits=[[-5, 5], [-5, 5]],\n    weights=[None, w],\n    figsize=(5, 1.5),\n    diag=\"kde\",  # smooth histogram\n)\nax[0][1].legend([\"NPE\", \"Corrected\"], loc=\"upper right\", bbox_to_anchor=[1.8, 1.0, 0.0, 0.0])\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x2bbd8efb0&gt;\n</code></pre> <p></p> <pre><code>    corrected_posterior = ImportanceSamplingPosterior(\n        potential_fn=log_prob_fn,\n        proposal=posterior.set_default_x(observation),\n        method=\"sir\",\n    )\n    theta = corrected_posterior.sample((len(theta_inferred),), oversampling_factor=oversampling_factor)\n    corrected_samples_for_all_observations.append(corrected_samples)\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\nCell In[90], line 6\n      1 corrected_posterior = ImportanceSamplingPosterior(\n      2     potential_fn=log_prob_fn,\n      3     proposal=posterior.set_default_x(observation),\n      4     method=\"sir\",\n      5 )\n----&gt; 6 corrected_samples = corrected_posterior.sample((len(theta_inferred),), oversampling_factor=oversampling_factor)\n      7 corrected_samples_for_all_observations.append(corrected_samples)\n\n\nNameError: name 'oversampling_factor' is not defined\n</code></pre> <pre><code># gt samples\ngt_samples = MultivariateNormal(observation, eye(2)).sample((len(theta_inferred) * 5,))\ngt_samples = gt_samples[prior.support.check(gt_samples)][:len(theta_inferred)]\n\nkwargs = dict(density=True, bins=50)\nfor idx_param in range(2):\n    plt.hist(gt_samples[:, idx_param], **kwargs)\n    plt.show()\n</code></pre> <p></p> <p></p> <pre><code>_ = torch.manual_seed(3)\ntheta = prior.sample((50,))\nx = sim.sample(theta)\n\n_ = torch.manual_seed(4)\ninference = SNPE(prior=prior)\n_ = inference.append_simulations(theta, x).train()\nposterior = inference.build_posterior()\n\n_ = torch.manual_seed(2)\ntheta_gt = prior.sample((5,))\nobservations = sim.sample(theta_gt)\nprint(\"observations.shape\", observations.shape)\n\n\noversampling_factor = 128  # higher will be slower but more accurate\nn_samples = 5000\n\nnon_corrected_samples_for_all_observations = []\ncorrected_samples_for_all_observations = []\ntrue_samples = []\nfor obs in observations:\n    non_corrected_samples_for_all_observations.append(posterior.set_default_x(obs).sample((n_samples,)))\n    corrected_posterior = ImportanceSamplingPosterior(\n        potential_fn=Potential(prior=None, x_o=obs),\n        proposal=posterior.set_default_x(obs),\n        method=\"sir\",\n    )\n    corrected_samples = corrected_posterior.sample((n_samples,), oversampling_factor=oversampling_factor)\n    corrected_samples_for_all_observations.append(corrected_samples)\n\n    gt_samples = MultivariateNormal(obs, eye(2)).sample((n_samples * 5,))\n    gt_samples = gt_samples[prior.support.check(gt_samples)][:n_samples]\n    true_samples.append(gt_samples)\n\n\nfor i in range(len(observations)):\n    fig, ax = marginal_plot(\n        [non_corrected_samples_for_all_observations[i], corrected_samples_for_all_observations[i], true_samples[i]],\n        limits=[[-5, 5], [-5, 5]],\n        points=theta_gt[i],\n        figsize=(5, 1.5),\n        diag=\"kde\",  # smooth histogram\n    )\n    ax[0][1].legend([\"NPE\", \"Corrected\", \"Ground truth\"], loc=\"upper right\", bbox_to_anchor=[1.8, 1.0, 0.0, 0.0])\n</code></pre> <p>_ = torch.manual_seed(3) theta = prior.sample((50,)) x = sim.sample(theta)</p> <p>_ = torch.manual_seed(4) inference = SNPE(prior=prior) _ = inference.append_simulations(theta, x).train() posterior = inference.build_posterior()</p> <p>_ = torch.manual_seed(2) theta_gt = prior.sample((5,)) observations = sim.sample(theta_gt) print(\u201cobservations.shape\u201d, observations.shape)</p> <p>oversampling_factor = 128  # higher will be slower but more accurate n_samples = 5000</p> <p>non_corrected_samples_for_all_observations = [] corrected_samples_for_all_observations = [] true_samples = [] for obs in observations:     non_corrected_samples_for_all_observations.append(posterior.set_default_x(obs).sample((n_samples,)))     corrected_posterior = ImportanceSamplingPosterior(         potential_fn=Potential(prior=None, x_o=obs),         proposal=posterior.set_default_x(obs),         method=\u201dsir\u201d,     )     corrected_samples = corrected_posterior.sample((n_samples,), oversampling_factor=oversampling_factor)     corrected_samples_for_all_observations.append(corrected_samples)</p> <pre><code>gt_samples = MultivariateNormal(obs, eye(2)).sample((n_samples * 5,))\ngt_samples = gt_samples[prior.support.check(gt_samples)][:n_samples]\ntrue_samples.append(gt_samples)\n</code></pre> <p>for i in range(len(observations)):     fig, ax = marginal_plot(         [non_corrected_samples_for_all_observations[i], corrected_samples_for_all_observations[i], true_samples[i]],          limits=[[-5, 5], [-5, 5]],          points=theta_gt[i],          figsize=(5, 1.5),         diag=\u201dkde\u201d,  # smooth histogram     )     ax0.legend([\u201cNPE\u201d, \u201cCorrected\u201d, \u201cGround truth\u201d], loc=\u201dupper right\u201d, bbox_to_anchor=[1.8, 1.0, 0.0, 0.0])</p> <pre><code>log_prob = sim.log_prob(theta, x, myprior)\n</code></pre> <pre><code>log_prob\n</code></pre> <pre><code>tensor([ -7.0203,  -6.7757,  -8.7409, -11.0604,  -6.8828,  -6.8849,  -6.7167,\n         -6.5307,  -7.6620,  -7.1050,  -6.9813,  -7.7949,  -6.4848,  -8.8385,\n         -6.5047,  -7.5428,  -6.5311,  -7.8525,  -7.6094,  -6.8969,  -6.5591,\n         -8.4800,  -9.2732,  -7.5526,  -6.8612,  -6.9509,  -6.6061,  -6.9288,\n         -8.6525,  -6.8885,  -9.0233,  -6.6701,  -6.9285, -11.2049,  -6.5632,\n         -6.6593,  -7.2530,  -7.5786, -11.1936,  -6.6386,  -6.6733,  -7.0817,\n         -6.5013, -10.9662,  -6.8552,  -7.1537,  -7.4354,  -8.6405,  -7.6694,\n         -6.6940])\n</code></pre> <pre><code>from torch import ones, eye\nimport torch\nfrom torch.distributions import MultivariateNormal\n\nfrom sbi.inference import SNPE, ImportanceSamplingPosterior\nfrom sbi.utils import BoxUniform\nfrom sbi.inference.potentials.base_potential import BasePotential\nfrom sbi.analysis import pairplot, marginal_plot\n\n\nclass Simulator:\n    def __init__(self):\n        pass\n\n    def log_prob(self, theta, x):\n        return MultivariateNormal(theta, eye(2)).log_prob(x) + prior.log_prob(theta)\n\n    def sample(self, theta):\n        return theta + torch.randn((theta.shape))\n\n\nclass Potential(BasePotential):\n    allow_iid_x = False\n\n    def __init__(self, prior, x_o, **kwargs):\n        super().__init__(prior, x_o, **kwargs)\n\n    def __call__(self, theta, **kwargs):\n        return sim.log_prob(theta, self.x_o)\n\n\nprior = BoxUniform(-5 * ones((2,)), 5 * ones((2,)))\nsim = Simulator()\n\n_ = torch.manual_seed(3)\ntheta = prior.sample((50,))\nx = sim.sample(theta)\n\n_ = torch.manual_seed(4)\ninference = SNPE(prior=prior)\n_ = inference.append_simulations(theta, x).train()\nposterior = inference.build_posterior()\n\n_ = torch.manual_seed(2)\ntheta_gt = prior.sample((5,))\nobservations = sim.sample(theta_gt)\nprint(\"observations.shape\", observations.shape)\n\n\noversampling_factor = 128  # higher will be slower but more accurate\nn_samples = 5000\n\nnon_corrected_samples_for_all_observations = []\ncorrected_samples_for_all_observations = []\ntrue_samples = []\nfor obs in observations:\n    non_corrected_samples_for_all_observations.append(posterior.set_default_x(obs).sample((n_samples,)))\n    corrected_posterior = ImportanceSamplingPosterior(\n        potential_fn=Potential(prior=None, x_o=obs),\n        proposal=posterior.set_default_x(obs),\n        method=\"sir\",\n    )\n    corrected_samples = corrected_posterior.sample((n_samples,), oversampling_factor=oversampling_factor)\n    corrected_samples_for_all_observations.append(corrected_samples)\n\n    gt_samples = MultivariateNormal(obs, eye(2)).sample((n_samples * 5,))\n    gt_samples = gt_samples[prior.support.check(gt_samples)][:n_samples]\n    true_samples.append(gt_samples)\n\n\nfor i in range(len(observations)):\n    fig, ax = marginal_plot(\n        [non_corrected_samples_for_all_observations[i], corrected_samples_for_all_observations[i], true_samples[i]],\n        limits=[[-5, 5], [-5, 5]],\n        points=theta_gt[i],\n        figsize=(5, 1.5),\n        diag=\"kde\",  # smooth histogram\n    )\n    ax[0][1].legend([\"NPE\", \"Corrected\", \"Ground truth\"], loc=\"upper right\", bbox_to_anchor=[1.8, 1.0, 0.0, 0.0])\n</code></pre> <pre><code> Neural network successfully converged after 93 epochs.observations.shape torch.Size([5, 2])\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 256 posterior samples:   0%|          | 0/256 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 256 posterior samples:   0%|          | 0/256 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 256 posterior samples:   0%|          | 0/256 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 256 posterior samples:   0%|          | 0/256 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 9984 posterior samples:   0%|          | 0/9984 [00:00&lt;?, ?it/s]\n\n\n\nDrawing 256 posterior samples:   0%|          | 0/256 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <pre><code>potential_logprobs = potential_fn(samples)\nproposal_logprobs = proposal.log_prob(samples)\nlog_importance_weights = potential_logprobs - proposal_logprobs\n</code></pre> <pre><code>corrected_posterior.method\n</code></pre> <pre><code>'sir'\n</code></pre> <pre><code>observations\n</code></pre> <pre><code>tensor([[ 0.6470, -1.2714],\n        [ 1.2079,  1.2723],\n        [ 1.7336,  1.2876],\n        [-1.1429, -5.3115],\n        [ 1.7205, -5.9448]])\n</code></pre> <pre><code>posterior.set_default_x(obs).sample((n_samples,))\n</code></pre> <pre><code>Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\n\n\ntensor([[ 0.5436, -4.4635],\n        [ 0.9399, -4.8755],\n        [ 0.7478, -3.8601],\n        ...,\n        [ 3.1949, -3.7732],\n        [ 2.4367, -3.1603],\n        [ 0.2160, -4.4788]])\n</code></pre> <pre><code>corrected_posterior = ImportanceSamplingPosterior(\n    potential_fn=Potential(prior=None, x_o=obs),\n    proposal=posterior.set_default_x(obs),\n    # method=\"sir\",\n    method=\"importance\",\n)\n</code></pre> <pre><code>oversampling_factor\n</code></pre> <pre><code>128\n</code></pre> <pre><code>corrected_samples, weights = corrected_posterior.sample((n_samples,), oversampling_factor=oversampling_factor)\n</code></pre> <pre><code>Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>torch.sum(torch.exp(weights))**2 / torch.sum(torch.exp(weights)**2)\n</code></pre> <pre><code>tensor(2356.5281)\n</code></pre> <pre><code>torch.mean(torch.exp(weights))\n</code></pre> <pre><code>tensor(0.0033)\n</code></pre> <pre><code>corrected_samples\n</code></pre> <pre><code>(tensor([[ 0.7254, -4.4200],\n         [ 0.0597, -4.9532],\n         [ 1.5032, -4.4630],\n         ...,\n         [ 0.2389, -4.5077],\n         [ 0.1377, -4.4233],\n         [ 1.6937, -4.9443]]),\n tensor([-5.6697, -5.8994, -5.0362,  ..., -6.1128, -6.3228, -4.3970]))\n</code></pre> <pre><code>non_corrected_samples_for_all_observations.append(posterior.set_default_x(obs).sample((n_samples,)))\ncorrected_posterior = ImportanceSamplingPosterior(\n    potential_fn=Potential(prior=None, x_o=obs),\n    proposal=posterior.set_default_x(obs),\n    method=\"sir\",\n)\ncorrected_samples = corrected_posterior.sample((n_samples,), oversampling_factor=oversampling_factor)\ncorrected_samples_for_all_observations.append(corrected_samples)\n\ngt_samples = MultivariateNormal(obs, eye(2)).sample((n_samples * 5,))\ngt_samples = gt_samples[prior.support.check(gt_samples)][:n_samples]\ntrue_samples.append(gt_samples)\n</code></pre>"},{"location":"tutorials/18_diagnostics_lc2st/","title":"Local Classifier Two-Sample Tests (\\(\\ell\\)-C2ST)","text":"<p>After a density estimator has been trained with simulated data to obtain a posterior, the estimator should be made subject to several diagnostic tests. This diagnostic should be performed before the posterior is used for inference given the actual observed data. </p> <p>Posterior Predictive Checks (see previous tutorial) provide one way to \u201ccritique\u201d a trained estimator via its predictive performance. </p> <p>Another approach is Simulation-Based Calibration (SBC, see previous tutorial). SBC evaluates whether the estimated posterior is balanced, i.e., neither over-confident nor under-confident. These checks are performed in expectation (on average) over the observation space, i.e. they are performed on a set of \\((\\theta,x)\\) pairs sampled from the joint distribution over simulator parameters \\(\\theta\\) and corresponding observations \\(x\\). As such, SBC is a global validation method that can be viewed as a necessary condition (but not sufficient) for a valid inference algorithm: If SBC checks fail, this tells you that your inference is invalid. If SBC checks pass, this is no guarantee that the posterior estimation is working.</p> <p>Local Classifier Two-Sample Tests (\\(\\ell\\)-C2ST) as developed by Linhart et al, 2023 present a new local validation method that allows to evaluate the correctness of the posterior estimator at a fixed observation, i.e. they work on a single \\((\\theta,x)\\) pair. They provide necessary and sufficient conditions for the validity of the SBI algorithm, as well as easy-to-interpret qualitative and quantitative diagnostics. </p> <p>If global checks (like SBC) fail, \\(\\ell\\)-C2ST allows to further investigate where (for which observation) and why (bias, overdispersion, etc.) the posterior estimator fails. If global validation checks pass, \\(\\ell\\)-C2ST allows to guarantee whether the inference is correct for a specific observation.</p>"},{"location":"tutorials/18_diagnostics_lc2st/#in-a-nutshell","title":"In a nutshell","text":"<p>Suppose you have an \u201camortized\u201d posterior estimator \\(q_\\phi(\\theta\\mid x)\\), meaning that we can quickly get samples for any new observation \\(x\\). The goal is to test the local consistency of our estimator at a fixed observation \\(x_\\mathrm{o}\\), i.e. whether the following null hypothesis holds about \\(q_\\phi(\\theta\\mid x)\\) and the true posterior \\(p(\\theta\\mid x)\\):</p> \\[\\mathcal{H}_0(x_\\mathrm{o}) := q_\\phi(\\theta\\mid x_\\mathrm{o}) = p(\\theta \\mid x_\\mathrm{o}), \\quad \\forall \\theta \\in \\mathbb{R}^m\\] <p>To run \\(\\ell\\)-C2ST, </p> <ol> <li>we sample new parameters from the prior of the problem at hand: \\(\\Theta_i \\sim p(\\theta)\\)</li> <li>we simulate corresponding \u201cobservations\u201d: \\(X_i = \\mathrm{Simulator}(\\Theta_i) \\sim p(x\\mid \\Theta_i)\\)</li> <li>we sample the estimated posterior at each observation: \\(Q_i \\sim q_\\phi(\\theta \\mid X_i)\\)</li> </ol> <p>This creates a calibration dataset of samples from the \u201cestimated\u201d and true joint distributions on which we train a binary classifier \\(d(\\theta, x)\\) to distinguish between the estimated joint \\(q(\\theta \\mid x)p(x)\\) (class \\(C=0\\)) and the true joint distribution \\(p(\\theta)p(x\\mid\\theta)\\) (class \\(C=1\\)):</p> \\[\\mathcal{D}_\\mathrm{cal} = \\left \\{\\underbrace{(Q_i, X_i)}_{(C=0)} \\cup \\underbrace{(\\Theta_i, X_i)}_{(C=1)} \\right \\}_{i=1}^{N_\\mathrm{cal}}\\] <p>Note: \\(D_\\mathrm{cal}\\) contains data from the joint distribution (over prior and simulator) that have to be different from the data used to train the posterior estimator. \\(N_\\mathrm{cal}\\) is typically smaller than \\(N_\\mathrm{train}\\), the number of training samples for the posterior estimator, but has to be sufficiently large to allow the convergence of the classifier. For a fixed simulation budget, a rule of thumb is to use \\(90\\%\\) for the posterior estimation and \\(10\\%\\) for the calibration.</p> <p>Once the classifier is trained, we evaluate it for a given observation \\(x^\\star\\) and multiple samples \\(Q^\\star_i \\sim q_\\phi(\\theta \\mid x^\\star)\\). This gives us a set of predicted probabilities \\(\\left\\{d(Q^\\star_i, x^\\star)\\right\\}_{i=1}^{N_\\mathrm{eval}}\\) that are then used to compute the different diagnostics. This proceedure can be repeated for several different observations, without having to retrain the classifiers, which allows to perform an efficient and thorough analysis of the failure modes of the posterior estimator.</p> <p>Note: The number of evaluation samples can be arbitrarily large (typically we use \\(N_\\mathrm{eval} = 10\\,000\\)), because we suppose our posterior estimator to be amortized. </p>"},{"location":"tutorials/18_diagnostics_lc2st/#key-ideas-behind-ell-c2st","title":"Key ideas behind \\(\\ell\\)-C2ST","text":"<p>\\(\\ell\\)-C2ST allows to evaluate the correctness your posterior estimator without requiring access to samples from the true posterior. It is built on the following two key ideas:</p> <ol> <li> <p>Train the classifier on the joint: this allows to implicitly learn the distance between the true and estimated posterior for any observation (we could call this step \u201camortized\u201d C2ST training). </p> </li> <li> <p>Local evaluation on data from one class only: we use a metric that, as opposed to the accuracy (used in C2ST) does not require samples from the true posterior, only the estimator. It consists in the Mean Squared Error (MSE) between the predicted probabilities for samples from the estimator evaluated at the given observation and one half.</p> </li> </ol> <p>Note: A predicted probability of one half corresponds to the chance level or total uncertainty of the classifier, that is unable to distinguish between the two data classes.</p> <p>The MSE metric is used as a test statistic for a hypothesis test that gives us theoretical guarantees on the correctness of the posterior estimator (at the considered observation), as well as easy-to-interpret diagnostics that allow to investigate its failure modes.</p> <p>Quick reminder on hypothesis tests. Additionaly to the observed test statistic \\(T^\\star\\), evaluating the test requires to 1. compute the test statistics \\(T_h\\) under the null hyposthesis (H0) of equal (true and estimated) distributions over multiple trials \\(h\\). 2. compute the p-value \\(p_v = \\frac{1}{H}\\sum_{h=1}^H \\mathbb{I}(T_h &gt; T^\\star)\\): \u201cHow many times is the observed test statistic \u201cbetter\u201d (i.e. below) the test statistic computed under H0?\u201d 3. choose a significance level \\(\\alpha\\) (typically \\(0.05\\)) that defines the rejection threshold and evaluate the test: - quantitatively: a p-value below this level indicates the rejection of the null hypothesis, meaning the detection of significant differences between the true and the estimated posterior.  - qualitatively: P-P plots: visually check whether the distribution of \\(T^\\star\\) falls into the \\(1-\\alpha\\) confidence region, computed by taking the corresponding quantiles of \\((T_1,\\dots, T_H)\\).</p>"},{"location":"tutorials/18_diagnostics_lc2st/#what-can-ell-c2st-diagnose","title":"What can \\(\\ell\\)-C2ST diagnose?","text":"<ul> <li> <p>Quantitatively: the MSE metric (or test statistic) gives us a distance measure between the estimated and true posterior that can be quickly evaluated for any new observation \\(x^\\star\\). Comparing it to the values of the null-distribution gives us the p-values that are used to check how significant their differences are. If the check passes (no significant differences), this tells us that we can be confident about the correctness of the estimator, but only upto to a certain confidence level (typically \\(95\\%\\)). </p> </li> <li> <p>Qualitatively: we can choose to look at the predicted probabilities used to compute the MSE metric. P-P plots allow to evaluate a general trend of over or under confidence, by comparing theire distribution to the confidence region (obtained for probabilities predicted under H0). We can go further and map these predicted probabilities to a pairplot of the samples they were evaluated on, shows us the regions of over and underconfidence of the estimator. This allows us to investigate the nature of the inconsistencies, such as positive/negative bias or under/over dispersion.</p> </li> </ul> <p>Note: High (resp. low) predicted probability indicates that the classifier is confident about the fact that the sample belongs to the estimated posterior (resp. to the true posterior). This means that the estimator associates too much (resp. not enough) mass to this sample. In other words it is \u201cover-confident\u201d (resp. \u201cunder-confident\u201d). </p> <p>To summarize \\(\\ell\\)-C2ST can:</p> <ul> <li>tell you whether your posterior estimator is valid for a given observation (with a guaranteed confidence)</li> <li>show you where (for which observation) and why (bais, overdispersion, etc.) it fails </li> </ul>"},{"location":"tutorials/18_diagnostics_lc2st/#illustration-on-a-benchmark-sbi-example","title":"Illustration on a benchmark SBI example","text":"<p>We consider the Gaussian Mixture SBI task from Lueckmann et al, 2021. It consists of inferring the common mean of a mixture of two 2D Gaussian distributions, one with much broader covariance than the other: - Prior: \\(p(\\theta) = \\mathcal{U}(-10,10)\\) - Simulator: \\(p(x|\\theta) = 0.5 \\mathcal{N}(\\theta, \\mathbf{I}_2)+ 0.5 \\mathcal{N}(\\theta, 0.1 \\times \\mathbf{I}_2)\\) - Dimensionality: \\(\\theta \\in \\mathbb{R}^2\\), \\(x \\in \\mathbb{R}^2\\)</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n</code></pre>"},{"location":"tutorials/18_diagnostics_lc2st/#sbi-task","title":"SBI Task","text":"<pre><code>from sbi.simulators.gaussian_mixture import (\n    gaussian_mixture,\n    uniform_prior_gaussian_mixture,\n)\n\n# SBI task: prior and simualtor\ndim = 2\nprior = uniform_prior_gaussian_mixture(dim=dim)\nsimulator = gaussian_mixture\n\n# Number of samples for training, calibration and evaluation\nNUM_TRAIN = 10_000\nNUM_CAL = int(0.1 * NUM_TRAIN) # 10% of the training data\nNUM_EVAL = 10_000\n</code></pre>"},{"location":"tutorials/18_diagnostics_lc2st/#posterior-inference","title":"Posterior Inference","text":"<p>We use neural posterior estimation as our SBI-algorithm with a MAF as underlying density estimator. </p> <p>Note: Here you could use any other SBI algorithm of your own choosing (e.g. NRE, NLE, etc.). IMPORTANT: make sure it is amortized (which corresponds to sequential methods with a signle round), so sampling the posterior can be performed quickly.</p> <p>We train the estimator on a small training set (<code>small_num_train=1000</code>) over a small number of epochs (<code>max_num_epochs=10</code>), which means that it doesn\u2019t converge. Therefore the diagnostics should detect major differences between the estimated and the true posterior, i.e. the null hypothesis is rejected.</p> <p>Note: You can play with the number of training samples or epochs to see whether this influences the quality of the posterior estimator and how it is reflected in the diagnostics.</p> <pre><code>from sbi.inference import SNPE\n\ntorch.manual_seed(42) # seed for reproducibility\n\n# Sample training data for the density estimator\nsmall_num_train = 1000\ntheta_train = prior.sample((NUM_TRAIN,))[:small_num_train]\nx_train = simulator(theta_train)[:small_num_train]\n\n# Train the neural posterior estimators\ntorch.manual_seed(42) # seed for reproducibility\ninference = SNPE(prior, density_estimator='maf', device='cpu')\ninference = inference.append_simulations(theta=theta_train, x=x_train)\nnpe = inference.train(training_batch_size=256, max_num_epochs=10)\n</code></pre> <pre><code> Training neural network. Epochs trained: 11\n</code></pre>"},{"location":"tutorials/18_diagnostics_lc2st/#evaluate-the-posterior-estimator","title":"Evaluate the posterior estimator","text":"<p>We choose to evaluate the posterior estimator at three different observations, simulated from parameters independently sampled from the prior:  \\(\\(\\theta^\\star_i \\sim p(\\theta) \\quad \\rightarrow \\quad x^\\star_i \\sim p(x\\mid \\theta_i), \\quad i=1,2,3~.\\)\\)</p> <pre><code>from sbi.simulators.gaussian_mixture import (\n    samples_true_posterior_gaussian_mixture_uniform_prior,\n)\n\n# get reference observations\ntorch.manual_seed(0) # seed for reproducibility\nthetas_star = prior.sample((3,))\nxs_star = simulator(thetas_star)\n\n# Sample from the true and estimated posterior\npost_samples_star = {}\nref_samples_star = {}\nfor i,x in enumerate(xs_star):\n    post_samples_star[i] = npe.sample(\n        (NUM_EVAL,), condition=x[None,:]\n    ).reshape(-1, thetas_star.shape[-1]).detach()\n    ref_samples_star[i] = samples_true_posterior_gaussian_mixture_uniform_prior(\n        x_o=x[None,:],\n        num_samples=1000,\n    )\n</code></pre>"},{"location":"tutorials/18_diagnostics_lc2st/#set-up-ell-c2st","title":"Set-up \\(\\ell\\)-C2ST","text":"<p>To setup the hypothesis test, we train the classifiers on the calibration dataset in two settings: - <code>train_under_null_hypothesis</code>: uses the permutation method to train the classifiers under the nulll hypothesis over several trials - <code>train_on_observed_data</code>: train the the classifier once on the observed data.</p> <p>For any new observation <code>x_o</code>, this allows to quickly compute (without having to retrain the classifiers) the test statistics <code>T_null</code> under the null hypothesis and <code>T_data</code> on the observed data. They will be used to compute the diagnostics (p-value or P-P plots).</p> <p>Note: we choose the default configuration with a MLP classifier (<code>classifier='mlp'</code>). You can also choose to use the default Random Forest classifier (<code>classifier='random_forest'</code>) or use your own custom <code>sklearn</code> classifier by specifying <code>clf_class</code> and <code>clf_kwargs</code> during the initialization of the <code>LC2ST</code> class. You can also use an ensemble classifier by setting <code>num_ensemble</code> &gt; 1 for more stable classifier predictions (see the <code>EnsembleClassifier</code> class in <code>sbi/diagnostics/lc2st.py</code>).</p> <pre><code>from sbi.diagnostics.lc2st import LC2ST\n\ntorch.manual_seed(42) # seed for reproducibility\n\n# sample calibration data\ntheta_cal = prior.sample((NUM_CAL,))\nx_cal = simulator(theta_cal)\npost_samples_cal = npe.sample((1,), x_cal).reshape(-1, theta_cal.shape[-1]).detach()\n\n# set up the LC2ST: train the classifiers\nlc2st = LC2ST(\n    thetas=theta_cal,\n    xs=x_cal,\n    posterior_samples=post_samples_cal,\n    classifier=\"mlp\",\n    num_ensemble=1, # number of classifiers for the ensemble\n)\n_ = lc2st.train_under_null_hypothesis() # over 100 trials under (H0)\n_ = lc2st.train_on_observed_data() # on observed data\n</code></pre> <pre><code>Training the classifiers under H0, permutation = True: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:24&lt;00:00,  4.06it/s]\n</code></pre> <pre><code># Define significance level for diagnostics\nconf_alpha = 0.05\n</code></pre>"},{"location":"tutorials/18_diagnostics_lc2st/#quantitative-diagnostics","title":"Quantitative diagnostics","text":"<p>We here compute the test statistics and p-values for three different observations <code>x_o</code> (as mentioned above, this is done in an amortized way without having to retrain the classifiers). </p> <p>Note: The p-value associated to the test corresponds to the proportion of times the L-C2ST statistic under the null hypothesis \\(\\{T_h\\}_{h=1}^H\\) is greater than the L-C2ST statistic \\(T_\\mathrm{o}\\) at the observation <code>x_o</code>. It is computed by taking the empirical mean over statistics computed on several trials under the null hypothesis: \\(\\(\\text{p-value}(x_\\mathrm{o}) = \\frac{1}{H} \\sum_{h=1}^{H} I(T_h &lt; T_o)~.\\)\\)</p> <pre><code>fig, axes = plt.subplots(1,len(thetas_star), figsize=(12,3))\nfor i in range(len(thetas_star)):\n    probs, scores = lc2st.get_scores(\n        theta_o=post_samples_star[i],\n        x_o=xs_star[i],\n        return_probs=True,\n        trained_clfs=lc2st.trained_clfs\n    )\n    T_data = lc2st.get_statistic_on_observed_data(\n        theta_o=post_samples_star[i],\n        x_o=xs_star[i]\n    )\n    T_null = lc2st.get_statistics_under_null_hypothesis(\n        theta_o=post_samples_star[i],\n        x_o=xs_star[i]\n    )\n    p_value = lc2st.p_value(post_samples_star[i], xs_star[i])\n    reject = lc2st.reject_test(post_samples_star[i], xs_star[i], alpha=conf_alpha)\n\n    # plot 95% confidence interval\n    quantiles = np.quantile(T_null, [0, 1-conf_alpha])\n    axes[i].hist(T_null, bins=50, density=True, alpha=0.5, label=\"Null\")\n    axes[i].axvline(T_data, color=\"red\", label=\"Observed\")\n    axes[i].axvline(quantiles[0], color=\"black\", linestyle=\"--\", label=\"95% CI\")\n    axes[i].axvline(quantiles[1], color=\"black\", linestyle=\"--\")\n    axes[i].set_xlabel(\"Test statistic\")\n    axes[i].set_ylabel(\"Density\")\n    axes[i].set_xlim(-0.01,0.25)\n    axes[i].set_title(\n        f\"observation {i+1} \\n p-value = {p_value:.3f}, reject = {reject}\"\n    )\naxes[-1].legend(bbox_to_anchor=(1.1, .5), loc='center left')\nplt.show()\n</code></pre> <p></p> <p>Results: the plots show the test statistics under the null hypothesis <code>T_null</code> (in blue) defining the \\(95\\%\\) (<code>1 - conf_alpha</code>) confidence region (black dotted lines). The test statistic correponding to the observed data <code>T_data</code> (red) is outside of the confidence region, indicating the rejection of the null hypothesis and therefore a \u201cbad\u201d posterior estimator.</p>"},{"location":"tutorials/18_diagnostics_lc2st/#qualitative-diagnostics","title":"Qualitative diagnostics","text":""},{"location":"tutorials/18_diagnostics_lc2st/#p-p-plots","title":"P-P plots","text":"<p>P-P plots allow to evaluate a general trend of over- or under- confidence, by comparing the predicted probabilities of belonging to the estimated posterior (class 0). If the red curve is not fully contained in the gray confidence region, this means that the test rejects the null hypothesis and that a significant discrepancy from the true posterior is detected. Here two scenarios are possible: - over-confidence: the red curve is mostly on the right side of the gray CR (high probabilities are predominant) - under-confidence: the red curve is mostly on the left side of the gray CR (low probabilities are predominant)</p> <p>Note: The predominance of high (resp. low) probabilities indicates a classifier that is mostly confident about predicting the class corresponding to the estimated (resp. true) posterior. This in turn means that the estimator associates too much (resp. not enough) mass to the evaluation space, i.e. is overall over confident (resp. under confident).</p> <pre><code># P-P plots\nfrom sbi.analysis.plot import pp_plot_lc2st\n\nfig, axes = plt.subplots(1,len(thetas_star), figsize=(12,3))\nfor i in range(len(thetas_star)):\n    probs_data, _ = lc2st.get_scores(\n        theta_o=post_samples_star[i],\n        x_o=xs_star[i],\n        return_probs=True,\n        trained_clfs=lc2st.trained_clfs\n    )\n    probs_null, _ = lc2st.get_statistics_under_null_hypothesis(\n        theta_o=post_samples_star[i],\n        x_o=xs_star[i],\n        return_probs=True\n    )\n\n    pp_plot_lc2st(\n        probs=[probs_data],\n        probs_null=probs_null,\n        conf_alpha=conf_alpha,\n        labels=[\"Classifier probabilities \\n on observed data\"],\n        colors=[\"red\"],\n        ax=axes[i],\n    )\n    axes[i].set_title(f\"PP-plot for observation {i+1}\")\naxes[-1].legend(bbox_to_anchor=(1.1, .5), loc='center left')\nplt.show()\n</code></pre> <p></p> <p>Results: the plots below show a general trend of overconfident behavior (red curves on the right side of the black dots).</p>"},{"location":"tutorials/18_diagnostics_lc2st/#pairplot-with-heatmap-of-classifier-probabilities","title":"Pairplot with heatmap of classifier probabilities","text":"<p>We can go further and map these predicted probabilities to a pairplot of the samples they were evaluated on, which shows us the regions of over and underconfidence of the estimator. This allows us to investigate the nature of the inconsistencies, such as positive/negative bias or under/over dispersion.</p> <p>Note: High (resp. low) predicted probability indicates that the classifier is confident about the fact that the sample belongs to the estimated posterior (resp. to the true posterior). This means that the estimator associates too much (resp. not enough) mass to this sample. In other words it is \u201cover-confident\u201d (resp. \u201cunder-confident\u201d). </p> <pre><code>from sbi.analysis.plot import marginal_plot_with_probs_intensity\nfrom sbi.utils.analysis_utils import get_probs_per_marginal\n\nlabel = \"Probabilities (class 0)\"\n# label = r\"$\\hat{p}(\\Theta\\sim q_{\\phi}(\\theta \\mid x_0) \\mid x_0)$\"\n\nfig, axes = plt.subplots(len(thetas_star), 3, figsize=(9,6), constrained_layout=True)\nfor i in range(len(thetas_star)):\n    probs_data, _ = lc2st.get_scores(\n        theta_o=post_samples_star[i][:1000],\n        x_o=xs_star[i],\n        return_probs=True,\n        trained_clfs=lc2st.trained_clfs\n    )\n    dict_probs_marginals = get_probs_per_marginal(\n        probs_data[0],\n        post_samples_star[i][:1000].numpy()\n    )\n    # 2d histogram\n    marginal_plot_with_probs_intensity(\n        dict_probs_marginals['0_1'],\n        marginal_dim=2,\n        ax=axes[i][0],\n        n_bins=50,\n        label=label\n    )\n    axes[i][0].scatter(\n        ref_samples_star[i][:,0],\n        ref_samples_star[i][:,1],\n        alpha=0.2,\n        color=\"gray\",\n        label=\"True posterior\"\n    )\n\n    # marginal 1\n    marginal_plot_with_probs_intensity(\n        dict_probs_marginals['0'],\n        marginal_dim=1,\n        ax=axes[i][1],\n        n_bins=50,\n        label=label,\n    )\n    axes[i][1].hist(\n        ref_samples_star[i][:,0],\n        density=True,\n        bins=10,\n        alpha=0.5,\n        label=\"True Posterior\",\n        color=\"gray\"\n    )\n\n    # marginal 2\n    marginal_plot_with_probs_intensity(\n        dict_probs_marginals['1'],\n        marginal_dim=1,\n        ax=axes[i][2],\n        n_bins=50,\n        label=label,\n    )\n    axes[i][2].hist(\n        ref_samples_star[i][:,1],\n        density=True,\n        bins=10,\n        alpha=0.5,\n        label=\"True posterior\",\n        color=\"gray\"\n    )\n\naxes[0][1].set_title(\"marginal 1\")\naxes[0][2].set_title(\"marginal 2\")\n\nfor j in range(3):\n    axes[j][0].set_ylabel(f\"observation {j + 1}\")\naxes[0][2].legend()\nplt.show()\n</code></pre> <p></p> <p>Results: the plots below indicate over dispersion of our estimator at all three considered observations. Indeed, the 2D histograms display a small blue-green region at the center where the estimator is \u201cunderconfident\u201d, surrounded by a yellow region of \u201cequal probability\u201d, and the rest of the estimated posterior samlpes correspond to the red regions of \u201coverconfidence\u201d. </p> <p>Validation of the diagnostic tool: we verify the statement of over dispersion by plotting the true posterior samples (in grey) and are happy to see that they fall into the underconfident region of the estimator. </p>"},{"location":"tutorials/18_diagnostics_lc2st/#classifier-choice-and-calibration-data-size-how-to-ensure-meaningful-test-results","title":"Classifier choice and calibration data size: how to ensure meaningful test results","text":""},{"location":"tutorials/18_diagnostics_lc2st/#choice-of-the-classifier","title":"Choice of the classifier","text":"<p>If you are not sure about which classifier architecture is best for your data, you can do a quick check by looking at the variance of the results over different random state initializations of the classifier: For <code>i=1,2,...</code>  1. train the ith classifier: run <code>lc2st.train_on_observed_data(seed=i)</code>  2. compute the corresponding test statistic for a dataset <code>theta_o, x_o</code>: <code>T_i = lc2st.get_statistic_on_observed_data(theta_o, x_o)</code></p> <p>For different classifier architectures, you should choose the one with the smallest variance. </p>"},{"location":"tutorials/18_diagnostics_lc2st/#number-of-calibration-samples","title":"Number of calibration samples","text":"<p>A similar check can also be performed via cross-validation: set the <code>num_folds</code> parameter of your <code>LC2ST</code> object, train on observed data and call <code>lc2st.get_scores(theta_o, x_o, lc2st.trained_clfs)</code>. This outputs the test statistics obtained for each cv-fold. You should choose the smallest calibration set size that gives you a small enough variance over the test statistics. </p> <p>Note: Ideally, these checks should be performed in a separable data setting, i.e. for a dataset <code>theta_o, x_o</code> coming from a sub-optimal estimator: the classifier is supposed to be able to discriminate between the two classes; the test is supposed to be rejected; the variance is supposed to be small. In other words, we are ensuring a high statistical power (our true positive rate) of our test. If you want to be really rigurous, you should also check the type I error (or false positive rate), that should be controlled by the significance level of your test (cf. Figure 2 in [Linhart et al., 2023]).</p>"},{"location":"tutorials/18_diagnostics_lc2st/#reducing-the-variance-of-the-test-results","title":"Reducing the variance of the test results","text":"<p>To ensure more stable results, you can play with the following <code>LC2ST</code> parameters: - <code>num_ensemble</code>: number of classifiers used for ensembling. An ensemble classifier is a set of classifiers initialized with different <code>random_state</code>s and whose predicted class probalility is the mean probability over all classifiers. It reduces the variance coming from the classifier itself. - <code>num_folds</code>: number of folds used for cross-validation. It reduces the variance coming from the data.</p> <p>As these numbers increase the results become more stable (less variance) and the test becomes more disciminative (smaller confidence region). Both can be combined (i.e. you can perform cross-validation on an ensemble classifier). </p> <p>Note: Be careful, you don\u2019t want your test to be too discriminative!</p>"},{"location":"tutorials/18_diagnostics_lc2st/#the-case-of-normalizing-flows-ell-c2st-nf","title":"The case of Normalizing Flows (\\(\\ell\\)-C2ST-NF)","text":"<p>\\(\\ell\\)-C2ST can also be specialized for normalizing flows,leading to improved test performance. The idea is to train and evaluate the classifiers in the space of the base distribution of the normalizing flow, instead of the parameter space that can be highly structured.  Following Theorem 4 of [Linhart et al., 2023], the null hypothesis \\(\\mathcal{H}_0(x_\\mathrm{o}) := q_\\phi(\\theta\\mid x_\\mathrm{o}) = p(\\theta \\mid x_\\mathrm{o})\\) of local consistency holds if, and only if, the inverse flow transformation applied to the target distribution recovers the base distribution. This gives us the following new null hypothesis for posterior estimators based on normalizing flows (cf. Eq. 17 in [Linhart et al., 2023]):</p> \\[\\mathcal{H}_0(x_\\mathrm{o}) := p(T_\\phi^{-1}(\\theta ; x_\\mathrm{o}) \\mid x_\\mathrm{o}) = \\mathcal{N}(0, \\mathbf{I}_m), \\quad \\forall \\theta \\in \\mathbb{R}^m~,\\] <p>which leads to a new binary classification framework to discriminate between the joint distributions \\(\\mathcal{N}(0, \\mathbf{I}_m)p(x)\\) (class \\(C=0\\)) and \\(p(T_\\phi^{-1}(\\theta ; x_\\mathrm{o}), x_\\mathrm{o})\\) (class \\(C=1\\)).</p> <p>This results in two main advantages leading to a statistically more performant and flexible test:  - easier classification task: it is easier to discriminate samples w.r.t. a simple Gaussian than a complex (e.g. multimodal) posterior.  - an analytically known null distribution: it consists of the base distribution of the flow, which is independant of \\(x\\) and the posterior estimator. This also allows to pre-compute the null distribution and re-use it for any new posterior estimator you whish to evaluate. </p> <p>Remember that the original \\(\\ell\\)-C2ST relies on a permutation method to approximate the null distribution.</p> <p>The new method is implemented within the <code>LC2ST_NF</code> class, built on the <code>LC2ST</code> class with following major changes: - no evaluation samples <code>theta_o</code> have to be passed to the evaluation methods (e.g. <code>get_scores_on_observed_data</code>, <code>get_statistic_on_observed_data</code>, <code>p_value</code>, etc.) - the precomputed <code>trained_clfs_null</code> can be passed at initialization - no permutation method is used inside <code>train_under_null_hypothesis</code></p> <p>Note: Quick reminder on Normalizing Flows. We consider a conditional Normalizing Flow \\(q_{\\phi}(\\theta \\mid x)\\) with base distribution \\(p(z) = \\mathcal{N}(0,\\mathbf{1}_m)\\) and bijective transormation \\(T_{\\phi}(.; x)\\) defined on \\(\\mathbb{R}^2\\) and for all \\(x \\in \\mathbb{R}^2\\) for our example problem in 2D. Sampling from the normalizing flow consists of applying the forward transformation \\(T_\\phi\\): \\(\\(\\theta = T_{\\phi}(z; x) \\sim q_{\\phi}(\\theta \\mid x), \\quad z\\sim p(z)~.\\)\\) Characterization of the null hypothesis. Comparing the estimated and true posterior distributions is equivalent to comparing the base distribution to the inversely transformed prior samples:  $$ p(\\theta \\mid x) = q_{\\phi}(\\theta \\mid x) \\iff p(T_{\\phi}^{-1}(\\theta; x)\\mid x) = p(T_{\\phi}^{-1}(T_{\\phi}(z; x); x)) = p(z) = \\mathcal{N}(0,\\mathbf{1}_m)$$</p>"},{"location":"tutorials/18_diagnostics_lc2st/#set-up-ell-c2st-nf","title":"Set up \\(\\ell\\)-C2ST-NF","text":"<p>The setup of the NF version is the same as for the original \\(\\ell\\)-C2ST, but the trained classifiers can be used to compute test results and diagnostics for any new observation and new posterior estimator.</p> <pre><code>from sbi.diagnostics.lc2st import LC2ST_NF\n\nflow_inverse_transform = lambda theta, x: npe.net._transform(theta, context=x)[0]\nflow_base_dist = torch.distributions.MultivariateNormal(\n    torch.zeros(2), torch.eye(2)\n) # same as npe.net._distribution\n\nlc2st_nf = LC2ST_NF(\n    thetas=theta_cal,\n    xs=x_cal,\n    posterior_samples=post_samples_cal,\n    flow_inverse_transform=flow_inverse_transform,\n    flow_base_dist=flow_base_dist,\n    num_ensemble=1,\n)\n_ = lc2st_nf.train_under_null_hypothesis()\n_ = lc2st_nf.train_on_observed_data()\n</code></pre> <pre><code>Training the classifiers under H0, permutation = False:   0%|          | 0/100 [00:00&lt;?, ?it/s]\n\nTraining the classifiers under H0, permutation = False: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:19&lt;00:00,  5.11it/s]\n</code></pre> <pre><code># Define significance level\nconf_alpha = 0.05\n</code></pre>"},{"location":"tutorials/18_diagnostics_lc2st/#quantitative-diagnostics_1","title":"Quantitative diagnostics","text":"<p>Same as before: we compute test statistics, confidence regions and p-values.</p> <pre><code>fig, axes = plt.subplots(1,len(thetas_star), figsize=(12,3))\nfor i in range(len(thetas_star)):\n    probs, scores = lc2st_nf.get_scores(\n        x_o=xs_star[i],\n        return_probs=True,\n        trained_clfs=lc2st_nf.trained_clfs\n    )\n    T_data = lc2st_nf.get_statistic_on_observed_data(x_o=xs_star[i])\n    T_null = lc2st_nf.get_statistics_under_null_hypothesis(x_o=xs_star[i])\n    p_value = lc2st_nf.p_value(xs_star[i])\n    reject = lc2st_nf.reject_test(xs_star[i], alpha=conf_alpha)\n\n    # plot 95% confidence interval\n    quantiles = np.quantile(T_null, [0, 1-conf_alpha])\n    axes[i].hist(T_null, bins=50, density=True, alpha=0.5, label=\"Null\")\n    axes[i].axvline(T_data, color=\"red\", label=\"Observed\")\n    axes[i].axvline(quantiles[0], color=\"black\", linestyle=\"--\", label=\"95% CI\")\n    axes[i].axvline(quantiles[1], color=\"black\", linestyle=\"--\")\n    axes[i].set_xlabel(\"Test statistic\")\n    axes[i].set_ylabel(\"Density\")\n    axes[i].set_xlim(-0.01,0.25)\n    axes[i].set_title(\n        f\"observation {i+1} \\n p-value = {p_value:.3f}, reject = {reject}\"\n    )\naxes[-1].legend(bbox_to_anchor=(1.1, .5), loc='center left')\nplt.show()\n</code></pre> <p></p> <p>Results: Again the test hypothesis is rejected for all three observations.</p>"},{"location":"tutorials/18_diagnostics_lc2st/#qualitative-diagnostics_1","title":"Qualitative diagnostics","text":""},{"location":"tutorials/18_diagnostics_lc2st/#p-p-plots_1","title":"P-P plots","text":"<p>Results: As before, the plots below show a general trend of overconfident behavior (red curves on the right side of the black dots).</p> <pre><code># P-P plots\nfrom sbi.analysis.plot import pp_plot_lc2st\n\nfig, axes = plt.subplots(1,len(thetas_star), figsize=(12,3))\nfor i in range(len(thetas_star)):\n    probs_data, _ = lc2st_nf.get_scores(\n        x_o=xs_star[i],\n        return_probs=True,\n        trained_clfs=lc2st_nf.trained_clfs\n    )\n    probs_null, _ = lc2st_nf.get_statistics_under_null_hypothesis(\n        x_o=xs_star[i],\n        return_probs=True\n    )\n\n    pp_plot_lc2st(\n        probs=[probs_data],\n        probs_null=probs_null,\n        conf_alpha=conf_alpha,\n        labels=[\"Classifier probabilities \\n on observed data\"],\n        colors=[\"red\"],\n        ax=axes[i],\n    )\n    axes[i].set_title(f\"PP-plot for observation {i+1}\")\naxes[-1].legend(bbox_to_anchor=(1.1, .5), loc='center left')\nplt.show()\n</code></pre> <p></p>"},{"location":"tutorials/18_diagnostics_lc2st/#heatmap-of-classifier-probabilities","title":"Heatmap of classifier probabilities","text":"<p>For the NF case and as displayed in the plots below, we can choose to plot the heatmap of predicted classifier probabilities in the base distribution space, instead of the parameter space, which can be easier to interpret if the posterior space is highly structured.</p> <pre><code>from sbi.analysis.plot import marginal_plot_with_probs_intensity\nfrom sbi.utils.analysis_utils import get_probs_per_marginal\n\nlabel = \"Probabilities (class 0)\"\n# label = r\"$\\hat{p}(Z\\sim\\mathcal{N}(0,1)\\mid x_0)$\"\n\nfig, axes = plt.subplots(len(thetas_star), 3, figsize=(9,6), constrained_layout=True)\nfor i in range(len(thetas_star)):\n    inv_ref_samples = lc2st_nf.flow_inverse_transform(\n        ref_samples_star[i], xs_star[i]\n    ).detach()\n    probs_data, _ = lc2st_nf.get_scores(\n        x_o=xs_star[i],\n        return_probs=True,\n        trained_clfs=lc2st_nf.trained_clfs\n    )\n    marginal_probs = get_probs_per_marginal(\n        probs_data[0],\n        lc2st_nf.theta_o.numpy()\n    )\n    # 2d histogram\n    marginal_plot_with_probs_intensity(\n        marginal_probs['0_1'],\n        marginal_dim=2,\n        ax=axes[i][0],\n        n_bins=50,\n        label=label\n    )\n    axes[i][0].scatter(\n        inv_ref_samples[:,0],\n        inv_ref_samples[:,1],\n        alpha=0.2, color=\"gray\",\n        label=\"True posterior\"\n    )\n\n    # marginal 1\n    marginal_plot_with_probs_intensity(\n        marginal_probs['0'],\n        marginal_dim=1,\n        ax=axes[i][1],\n        n_bins=50,\n        label=label\n    )\n    axes[i][1].hist(\n        inv_ref_samples[:,0],\n        density=True,\n        bins=10,\n        alpha=0.5,\n        label=\"True Posterior\",\n        color=\"gray\"\n    )\n\n    # marginal 2\n    marginal_plot_with_probs_intensity(\n        marginal_probs['1'],\n        marginal_dim=1,\n        ax=axes[i][2],\n        n_bins=50,\n        label=label\n    )\n    axes[i][2].hist(\n        inv_ref_samples[:,1],\n        density=True,\n        bins=10,\n        alpha=0.5,\n        label=\"True posterior\",\n        color=\"gray\"\n    )\n\naxes[0][1].set_title(\"marginal 1\")\naxes[0][2].set_title(\"marginal 2\")\n\nfor j in range(3):\n    axes[j][0].set_ylabel(f\"observation {j + 1}\")\naxes[0][2].legend()\nplt.show()\n</code></pre> <p></p> <p>Results: Again, the plots below confirm that the true posterior samples (in grey) correspond to regions of \u201cunderconfidence\u201d (blue-green) or \u201cequal probability\u201d (yellow), indicating over dispersion of our posterior esimator.</p>"},{"location":"tutorials/19_plotting_functionality/","title":"19 plotting functionality","text":"<pre><code>import torch\n\nfrom sbi.analysis.plot import marginal_plot, pairplot\n</code></pre> <pre><code>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n</code></pre>"},{"location":"tutorials/19_plotting_functionality/#plotting-functionality","title":"Plotting functionality","text":"<p>Here we will have a look at the different options for finetuning <code>pairplots</code> and <code>marginal_plots</code>.</p> <p>Lets first draw some samples from the posterior used in a previous tutorial.</p> <pre><code>from toy_posterior_for_07_cc import ExamplePosterior\n\nposterior = ExamplePosterior()\nposterior_samples = posterior.sample((100,))\n</code></pre> <p>We will start with the default plot and gradually make it prettier</p> <pre><code>_ = pairplot(\n    posterior_samples,\n)\n</code></pre> <p></p>"},{"location":"tutorials/19_plotting_functionality/#customisation","title":"Customisation","text":"<p>The pairplots are split into three regions, the diagonal (<code>diag</code>) and the upper and lower off-diagonal regions(<code>upper</code> and <code>lower</code>). We can pass separate arguments (e.g. <code>hist</code>, <code>kde</code>, <code>scatter</code>) for each region, as well as corresponding style keywords in a dictionary (by using e.g. <code>upper_kwargs</code>). For overall figure stylisation one can use <code>fig_kwargs</code>.</p> <p>To get a closer look at the potential options, have a look at the <code>_get_default_fig_kwargs</code>, <code>_get_default_diag_kwargs</code> and <code>_get_default_offdiag_kwargs</code> functions in analysis/plot.py.</p> <p>As illustrated below, we can directly use any <code>matplotlib</code> keywords (such as <code>cmap</code> for images) by passing them in the <code>mpl_kwargs</code> entry of <code>upper_kwargs</code> or <code>diag_kwargs</code>.</p> <p>Let\u2019s now make a scatter plot for the upper diagonal, a histogram for the diagonal, and pass keyword dictionaries for both.</p> <pre><code>_ = pairplot(\n    posterior_samples,\n    limits=[[-3, 3] * 3],\n    figsize=(5, 5),\n    diag=\"hist\",\n    upper=\"scatter\",\n    diag_kwargs={\n        \"mpl_kwargs\": {\n            \"bins\": 10,\n            \"color\": 'tab:blue',\n            \"edgecolor\": 'white',\n            \"linewidth\": 1,\n            \"alpha\": 0.6,\n            \"histtype\": \"bar\",\n            \"fill\": True,\n        }\n    },\n    upper_kwargs={\"mpl_kwargs\": {\"color\": 'tab:blue', \"s\": 20, \"alpha\": 0.8}},\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n)\n</code></pre> <p></p>"},{"location":"tutorials/19_plotting_functionality/#compare-two-sets-of-samples","title":"Compare two sets of samples","text":"<p>By passing a list of samples, we can plot two sets of samples on top of each other.</p> <pre><code># draw two different subsets of samples to plot\nposterior_samples1 = posterior.sample((20,))\nposterior_samples2 = posterior.sample((20,))\n\n_ = pairplot(\n    [posterior_samples1, posterior_samples2],\n    limits=[[-3, 3] * 3],\n    figsize=(5, 5),\n    diag=[\"hist\", \"hist\"],\n    upper=[\"scatter\", \"scatter\"],\n    diag_kwargs={\n        \"mpl_kwargs\": {\n            \"bins\": 10,\n            \"edgecolor\": 'white',\n            \"linewidth\": 1,\n            \"alpha\": 0.6,\n            \"histtype\": \"bar\",\n            \"fill\": True,\n        }\n    },\n    upper_kwargs={\"mpl_kwargs\": {\"s\": 50, \"alpha\": 0.8}},\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n)\n</code></pre> <p></p>"},{"location":"tutorials/19_plotting_functionality/#multi-layered-plots","title":"Multi-layered plots","text":"<p>We can use the same functionality to make a multi-layered plot using the same set of samples, e.g. a kernel-density estimate on top of a scatter plot.</p> <pre><code>_ = pairplot(\n    [posterior_samples, posterior_samples],\n    limits=[[-3, 3] * 3],\n    figsize=(5, 5),\n    diag=[\"hist\", None],\n    upper=[\"scatter\", \"contour\"],\n    diag_kwargs={\n        \"mpl_kwargs\": {\n            \"bins\": 10,\n            \"color\": 'tab:blue',\n            \"edgecolor\": 'white',\n            \"linewidth\": 1,\n            \"alpha\": 0.6,\n            \"histtype\": \"bar\",\n            \"fill\": True,\n        },\n    },\n    upper_kwargs=[\n        {\n            \"mpl_kwargs\": {\"color\": 'tab:blue', \"s\": 20, \"alpha\": 0.8},\n        },\n        {\"mpl_kwargs\": {\"cmap\": 'Blues_r', \"alpha\": 0.8, \"colors\": None}},\n    ],\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n    fig_kwargs={\"despine\": {\"offset\": 0}},\n)\n</code></pre> <p></p>"},{"location":"tutorials/19_plotting_functionality/#lower-diagonal","title":"Lower diagonal","text":"<p>We can add something in the lower off-diagonal as well.</p> <pre><code>_ = pairplot(\n    [posterior_samples, posterior_samples],\n    limits=[[-3, 3] * 3],\n    figsize=(5, 5),\n    diag=[\"hist\", None],\n    upper=[\"scatter\", \"contour\"],\n    lower=[\"kde\", None],\n    diag_kwargs={\n        \"mpl_kwargs\": {\n            \"bins\": 10,\n            \"color\": 'tab:blue',\n            \"edgecolor\": 'white',\n            \"linewidth\": 1,\n            \"alpha\": 0.6,\n            \"histtype\": \"bar\",\n            \"fill\": True,\n        }\n    },\n    upper_kwargs=[\n        {\"mpl_kwargs\": {\"color\": 'tab:blue', \"s\": 20, \"alpha\": 0.8}},\n        {\"mpl_kwargs\": {\"cmap\": 'Blues_r', \"alpha\": 0.8, \"colors\": None}},\n    ],\n    lower_kwargs={\"mpl_kwargs\": {\"cmap\": \"Blues_r\"}},\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n)\n</code></pre> <p></p>"},{"location":"tutorials/19_plotting_functionality/#adding-observed-data","title":"Adding observed data","text":"<p>We can also add points, e.g., our observed data \\(\\theta_o\\) to the plot.</p> <pre><code># fake observed data:\ntheta_o = torch.ones(1, 3)\n\n_ = pairplot(\n    [posterior_samples, posterior_samples],\n    limits=[[-3, 3] * 3],\n    figsize=(5, 5),\n    diag=[\"hist\", None],\n    upper=[\"scatter\", \"contour\"],\n    diag_kwargs={\n        \"mpl_kwargs\": {\n            \"bins\": 10,\n            \"color\": 'tab:blue',\n            \"edgecolor\": 'white',\n            \"linewidth\": 1,\n            \"alpha\": 0.6,\n            \"histtype\": \"bar\",\n            \"fill\": True,\n        }\n    },\n    upper_kwargs=[\n        {\"mpl_kwargs\": {\"color\": 'tab:blue', \"s\": 20, \"alpha\": 0.8}},\n        {\"mpl_kwargs\": {\"cmap\": 'Blues_r', \"alpha\": 0.8, \"colors\": None}},\n    ],\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n    points=theta_o,\n    fig_kwargs={\n        \"points_labels\": [r\"$\\theta_o$\"],\n        \"legend\": True,\n        \"points_colors\": [\"purple\"],\n        \"points_offdiag\": {\"marker\": \"+\", \"markersize\": 20},\n        \"despine\": {\"offset\": 0},\n    },\n)\n</code></pre> <p></p>"},{"location":"tutorials/19_plotting_functionality/#subsetting-the-plot","title":"Subsetting the plot","text":"<p>For high-dimensional posteriors, we might only want to visualise a subset of the marginals. This can be done by passing a list of entries to plot to the <code>subset</code> argument of the <code>pairplot</code> function.</p> <pre><code>_ = pairplot(\n    [posterior_samples, posterior_samples],\n    limits=[[-3, 3] * 3],\n    figsize=(5, 5),\n    subset=[0, 2],\n    diag=[\"hist\", None],\n    upper=[\"scatter\", \"contour\"],\n    diag_kwargs={\n        \"mpl_kwargs\": {\n            \"bins\": 10,\n            \"color\": 'tab:blue',\n            \"edgecolor\": 'white',\n            \"linewidth\": 1,\n            \"alpha\": 0.6,\n            \"histtype\": \"bar\",\n            \"fill\": True,\n        }\n    },\n    upper_kwargs=[\n        {\"mpl_kwargs\": {\"color\": 'tab:blue', \"s\": 20, \"alpha\": 0.8}},\n        {\"mpl_kwargs\": {\"cmap\": 'Blues_r', \"alpha\": 0.8, \"colors\": None}},\n    ],\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n    points=theta_o,\n    fig_kwargs={\n        \"points_labels\": [r\"$\\theta_o$\"],\n        \"legend\": True,\n        \"points_colors\": [\"purple\"],\n        \"points_offdiag\": {\"marker\": \"+\", \"markersize\": 20},\n        \"despine\": {\"offset\": 0},\n    },\n)\n</code></pre> <p></p>"},{"location":"tutorials/19_plotting_functionality/#plot-just-the-marginals","title":"Plot just the marginals","text":"<p>1D Marginals can also be visualised using the <code>marginal_plot</code> function</p> <pre><code># plot posterior samples\n_ = marginal_plot(\n    [posterior_samples, posterior_samples],\n    limits=[[-3, 3] * 3],\n    subset=[0, 1],\n    diag=[\"hist\", None],\n    diag_kwargs={\n        \"mpl_kwargs\": {\n            \"bins\": 10,\n            \"color\": 'tab:blue',\n            \"edgecolor\": 'white',\n            \"linewidth\": 1,\n            \"alpha\": 0.6,\n            \"histtype\": \"bar\",\n            \"fill\": True,\n        },\n    },\n    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"],\n    points=[torch.ones(1, 3)],\n    figsize=(4, 2),\n    fig_kwargs={\n        \"points_labels\": [r\"$\\theta_o$\"],\n        \"legend\": True,\n        \"points_colors\": [\"purple\"],\n        \"points_offdiag\": {\"marker\": \"+\", \"markersize\": 20},\n        \"despine\": {\"offset\": 0},\n    },\n)\n</code></pre> <p></p>"}]}