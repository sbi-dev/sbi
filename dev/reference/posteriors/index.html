
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://sbi-dev.github.io/sbi/dev/reference/posteriors/">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>Posteriors - sbi</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
  
  <style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.75 1.75 0 0 1 1 7.775m1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2"/></svg>');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../static/global.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#posteriors" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="sbi" class="md-header__button md-logo" aria-label="sbi" data-md-component="logo">
      
  <img src="../../static/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            sbi
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Posteriors
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="http://github.com/sbi-dev/sbi" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    sbi-dev/sbi
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="sbi" class="md-nav__button md-logo" aria-label="sbi" data-md-component="logo">
      
  <img src="../../static/logo.svg" alt="logo">

    </a>
    sbi
  </label>
  
    <div class="md-nav__source">
      <a href="http://github.com/sbi-dev/sbi" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    sbi-dev/sbi
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tutorials and Examples
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Contributing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How to contribute
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code of Conduct
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../citation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Citation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../credits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Credits
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;DirectPosterior
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" DirectPosterior">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;leakage_correction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_prob
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob_batched" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_prob_batched
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.map" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;map
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.sample" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.sample_batched" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample_batched
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;ImportanceSamplingPosterior
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" ImportanceSamplingPosterior">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.estimate_normalization_constant" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;estimate_normalization_constant
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_prob
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.map" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;map
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.sample" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MCMCPosterior
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" MCMCPosterior">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.mcmc_method" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;mcmc_method
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.posterior_sampler" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;posterior_sampler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__getstate__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__getstate__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.get_arviz_inference_data" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_arviz_inference_data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_prob
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.map" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;map
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample_batched" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample_batched
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_mcmc_method" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;set_mcmc_method
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;RejectionPosterior
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" RejectionPosterior">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_prob
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.map" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;map
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.sample" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.score_posterior.ScorePosterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;ScorePosterior
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" ScorePosterior">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_prob
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.map" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;map
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.sample" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.sample_via_zuko" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample_via_zuko
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;VIPosterior
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" VIPosterior">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.q" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;q
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.vi_method" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;vi_method
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__deepcopy__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__deepcopy__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__getstate__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__getstate__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__setstate__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__setstate__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.evaluate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;evaluate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.log_prob" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_prob
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.map" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;map
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.sample" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_q" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;set_q
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_vi_method" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;set_vi_method
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="posteriors">Posteriors<a class="headerlink" href="#posteriors" title="Permanent link">&para;</a></h1>


<div class="doc doc-object doc-class">



<h2 id="sbi.inference.posteriors.direct_posterior.DirectPosterior" class="doc doc-heading">
            <code>DirectPosterior</code>


<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>


        <p>Posterior <span class="arithmatex">\(p(\theta|x_o)\)</span> with <code>log_prob()</code> and <code>sample()</code> methods, only
applicable to SNPE.<br/><br/>
SNPE trains a neural network to directly approximate the posterior distribution.
However, for bounded priors, the neural network can have leakage: it puts non-zero
mass in regions where the prior is zero. The <code>DirectPosterior</code> class wraps the
trained network to deal with these cases.<br/><br/>
Specifically, this class offers the following functionality:<br/>
- correct the calculation of the log probability such that it compensates for the
  leakage.<br/>
- reject samples that lie outside of the prior bounds.<br/><br/>
This class can not be used in combination with SNLE or SNRE.</p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DirectPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Posterior $p(\theta|x_o)$ with `log_prob()` and `sample()` methods, only</span>
<span class="sd">    applicable to SNPE.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNPE trains a neural network to directly approximate the posterior distribution.</span>
<span class="sd">    However, for bounded priors, the neural network can have leakage: it puts non-zero</span>
<span class="sd">    mass in regions where the prior is zero. The `DirectPosterior` class wraps the</span>
<span class="sd">    trained network to deal with these cases.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    Specifically, this class offers the following functionality:&lt;br/&gt;</span>
<span class="sd">    - correct the calculation of the log probability such that it compensates for the</span>
<span class="sd">      leakage.&lt;br/&gt;</span>
<span class="sd">    - reject samples that lie outside of the prior bounds.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    This class can not be used in combination with SNLE or SNRE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">posterior_estimator</span><span class="p">:</span> <span class="n">ConditionalDensityEstimator</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">            posterior_estimator: The trained neural posterior.</span>
<span class="sd">            max_sampling_batch_size: Batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Deprecated, should not be passed.</span>
<span class="sd">            enable_transform: Whether to transform parameters to unconstrained space</span>
<span class="sd">                during MAP optimization. When False, an identity transform will be</span>
<span class="sd">                returned for `theta_transform`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Because `DirectPosterior` does not take the `potential_fn` as input, it</span>
        <span class="c1"># builds it itself. The `potential_fn` and `theta_transform` are used only for</span>
        <span class="c1"># obtaining the MAP.</span>
        <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
        <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">posterior_estimator_based_potential</span><span class="p">(</span>
            <span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">prior</span><span class="p">,</span>
            <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">posterior_estimator</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;It samples the posterior network and rejects samples that</span>
<span class="s2">            lie outside of the prior bounds.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;.sample() supports only `batchsize == 1`. If you intend &quot;</span>
                <span class="s2">&quot;to sample multiple observations, use `.sample_batched()`. &quot;</span>
                <span class="s2">&quot;If you intend to sample i.i.d. observations, set up the &quot;</span>
                <span class="s2">&quot;posterior density estimator with an appropriate permutation &quot;</span>
                <span class="s2">&quot;invariant embedding net.&quot;</span>
            <span class="p">)</span>

        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">rejection</span><span class="o">.</span><span class="n">accept_reject_sample</span><span class="p">(</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;condition&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
            <span class="n">alternative_method</span><span class="o">=</span><span class="s2">&quot;build_posterior(..., sample_with=&#39;mcmc&#39;)&quot;</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># [0] to return only samples, not acceptance probabilities.</span>

        <span class="k">return</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Remove batch dimension.</span>

    <span class="k">def</span> <span class="nf">sample_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given a batch of observations [x_1, ..., x_B] this function samples from</span>
<span class="sd">        posteriors $p(\theta|x_1)$, ... ,$p(\theta|x_B)$, in a batched (i.e. vectorized)</span>
<span class="sd">        manner.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from the posterior</span>
<span class="sd">                given every observation.</span>
<span class="sd">            x: A batch of observations, of shape `(batch_dim, event_shape_x)`.</span>
<span class="sd">                `batch_dim` corresponds to the number of observations to be drawn.</span>
<span class="sd">            max_sampling_batch_size: Maximum batch size for rejection sampling.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from the posteriors of shape (*sample_shape, B, *input_shape)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">condition_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="n">condition_shape</span><span class="p">)</span>

        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">rejection</span><span class="o">.</span><span class="n">accept_reject_sample</span><span class="p">(</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;condition&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
            <span class="n">alternative_method</span><span class="o">=</span><span class="s2">&quot;build_posterior(..., sample_with=&#39;mcmc&#39;)&quot;</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">samples</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">leakage_correction_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of the posterior $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">                Renormalization of the posterior is useful when some</span>
<span class="sd">                probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">                The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">                need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">                `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">                -∞ outside of the prior support regardless of this setting.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>
<span class="sd">            leakage_correction_params: A `dict` of keyword arguments to override the</span>
<span class="sd">                default values of `leakage_correction()`. Possible options are:</span>
<span class="sd">                `num_rejection_samples`, `force_update`, `show_progress_bars`, and</span>
<span class="sd">                `rejection_sampling_batch_size`.</span>
<span class="sd">                These parameters only have an effect if `norm_posterior=True`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `(len(θ),)`-shaped log posterior probability $\log p(\theta|x)$ for θ in the</span>
<span class="sd">            support of the prior, -∞ (corresponding to 0 probability) outside.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="n">theta_density_estimator</span> <span class="o">=</span> <span class="n">reshape_to_sample_batch_event</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">leading_is_sample</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">x_density_estimator</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">x_density_estimator</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;.log_prob() supports only `batchsize == 1`. If you intend &quot;</span>
                <span class="s2">&quot;to evaluate given multiple observations, use `.log_prob_batched()`. &quot;</span>
                <span class="s2">&quot;If you intend to evaluate given i.i.d. observations, set up the &quot;</span>
                <span class="s2">&quot;posterior density estimator with an appropriate permutation &quot;</span>
                <span class="s2">&quot;invariant embedding net.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="c1"># Evaluate on device, move back to cpu for comparison with prior.</span>
            <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
                <span class="n">theta_density_estimator</span><span class="p">,</span> <span class="n">condition</span><span class="o">=</span><span class="n">x_density_estimator</span>
            <span class="p">)</span>
            <span class="c1"># `log_prob` supports only a single observation (i.e. `batchsize==1`).</span>
            <span class="c1"># We now remove this additional dimension.</span>
            <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="n">unnorm_log_prob</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Force probability to be zero outside prior support.</span>
            <span class="n">in_prior_support</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

            <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">in_prior_support</span><span class="p">,</span>
                <span class="n">unnorm_log_prob</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">leakage_correction_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">leakage_correction_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
            <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">leakage_correction_params</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">norm_posterior</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>

    <span class="k">def</span> <span class="nf">log_prob_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">leakage_correction_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Given a batch of observations [x_1, ..., x_B] and a batch of parameters \</span>
<span class="sd">            [$\theta_1$,..., $\theta_B$] this function evalautes the log-probabilities \</span>
<span class="sd">            of the posteriors $p(\theta_1|x_1)$, ..., $p(\theta_B|x_B)$ in a batched \</span>
<span class="sd">            (i.e. vectorized) manner.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters $\theta$ of shape \</span>
<span class="sd">                `(*sample_shape, batch_dim, *theta_shape)`.</span>
<span class="sd">            x: Batch of observations $x$ of shape \</span>
<span class="sd">                `(batch_dim, *condition_shape)`.</span>
<span class="sd">            norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">                Renormalization of the posterior is useful when some</span>
<span class="sd">                probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">                The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">                need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">                `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">                -∞ outside of the prior support regardless of this setting.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>
<span class="sd">            leakage_correction_params: A `dict` of keyword arguments to override the</span>
<span class="sd">                default values of `leakage_correction()`. Possible options are:</span>
<span class="sd">                `num_rejection_samples`, `force_update`, `show_progress_bars`, and</span>
<span class="sd">                `rejection_sampling_batch_size`.</span>
<span class="sd">                These parameters only have an effect if `norm_posterior=True`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `(len(θ), B)`-shaped log posterior probability $\\log p(\theta|x)$\\ for θ \</span>
<span class="sd">            in the support of the prior, -∞ (corresponding to 0 probability) outside.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="n">event_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">input_shape</span>
        <span class="n">theta_density_estimator</span> <span class="o">=</span> <span class="n">reshape_to_sample_batch_event</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">event_shape</span><span class="p">,</span> <span class="n">leading_is_sample</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">x_density_estimator</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="c1"># Evaluate on device, move back to cpu for comparison with prior.</span>
            <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
                <span class="n">theta_density_estimator</span><span class="p">,</span> <span class="n">condition</span><span class="o">=</span><span class="n">x_density_estimator</span>
            <span class="p">)</span>

            <span class="c1"># Force probability to be zero outside prior support.</span>
            <span class="n">in_prior_support</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

            <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">in_prior_support</span><span class="p">,</span>
                <span class="n">unnorm_log_prob</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">leakage_correction_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">leakage_correction_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
            <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">leakage_correction_params</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">norm_posterior</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">leakage_correction</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">num_rejection_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">rejection_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return leakage correction factor for a leaky posterior density estimate.</span>

<span class="sd">        The factor is estimated from the acceptance probability during rejection</span>
<span class="sd">        sampling from the posterior.</span>

<span class="sd">        This is to avoid re-estimating the acceptance probability from scratch</span>
<span class="sd">        whenever `log_prob` is called and `norm_posterior=True`. Here, it</span>
<span class="sd">        is estimated only once for `self.default_x` and saved for later. We</span>
<span class="sd">        re-evaluate only whenever a new `x` is passed.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            num_rejection_samples: Number of samples used to estimate correction factor.</span>
<span class="sd">            show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">            rejection_sampling_batch_size: Batch size for rejection sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Saved or newly-estimated correction factor (as a scalar `Tensor`).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
            <span class="c1"># [1:] to remove batch-dimension for `reshape_to_batch_event`.</span>
            <span class="k">return</span> <span class="n">rejection</span><span class="o">.</span><span class="n">accept_reject_sample</span><span class="p">(</span>
                <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
                <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_rejection_samples</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="n">sample_for_correction_factor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">rejection_sampling_batch_size</span><span class="p">,</span>
                <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span>
                    <span class="s2">&quot;condition&quot;</span><span class="p">:</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
                        <span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
                    <span class="p">)</span>
                <span class="p">},</span>
            <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
        <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
            <span class="k">return</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;posterior&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from the</span>
<span class="sd">                posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">posterior_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="torch.distributions.Distribution">Distribution</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior distribution with <code>.log_prob()</code> and <code>.sample()</code>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>posterior_estimator</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.neural_nets.estimators.base.ConditionalDensityEstimator" href="../models/#sbi.neural_nets.estimators.ConditionalDensityEstimator">ConditionalDensityEstimator</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The trained neural posterior.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_sampling_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Batchsize of samples being drawn from
the proposal at every iteration.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x_shape</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Size">Size</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated, should not be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>enable_transform</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to transform parameters to unconstrained space
during MAP optimization. When False, an identity transform will be
returned for <code>theta_transform</code>.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">posterior_estimator</span><span class="p">:</span> <span class="n">ConditionalDensityEstimator</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">        posterior_estimator: The trained neural posterior.</span>
<span class="sd">        max_sampling_batch_size: Batchsize of samples being drawn from</span>
<span class="sd">            the proposal at every iteration.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Deprecated, should not be passed.</span>
<span class="sd">        enable_transform: Whether to transform parameters to unconstrained space</span>
<span class="sd">            during MAP optimization. When False, an identity transform will be</span>
<span class="sd">            returned for `theta_transform`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Because `DirectPosterior` does not take the `potential_fn` as input, it</span>
    <span class="c1"># builds it itself. The `potential_fn` and `theta_transform` are used only for</span>
    <span class="c1"># obtaining the MAP.</span>
    <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
    <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">posterior_estimator_based_potential</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">posterior_estimator</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;It samples the posterior network and rejects samples that</span>
<span class="s2">        lie outside of the prior bounds.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction" class="doc doc-heading">
            <code class=" language-python"><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_rejection_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rejection_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.leakage_correction" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return leakage correction factor for a leaky posterior density estimate.</p>
<p>The factor is estimated from the acceptance probability during rejection
sampling from the posterior.</p>
<p>This is to avoid re-estimating the acceptance probability from scratch
whenever <code>log_prob</code> is called and <code>norm_posterior=True</code>. Here, it
is estimated only once for <code>self.default_x</code> and saved for later. We
re-evaluate only whenever a new <code>x</code> is passed.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>num_rejection_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of samples used to estimate correction factor.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progress bar during sampling.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rejection_sampling_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Batch size for rejection sampling.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Saved or newly-estimated correction factor (as a scalar <code>Tensor</code>).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">leakage_correction</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_rejection_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">rejection_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return leakage correction factor for a leaky posterior density estimate.</span>

<span class="sd">    The factor is estimated from the acceptance probability during rejection</span>
<span class="sd">    sampling from the posterior.</span>

<span class="sd">    This is to avoid re-estimating the acceptance probability from scratch</span>
<span class="sd">    whenever `log_prob` is called and `norm_posterior=True`. Here, it</span>
<span class="sd">    is estimated only once for `self.default_x` and saved for later. We</span>
<span class="sd">    re-evaluate only whenever a new `x` is passed.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        num_rejection_samples: Number of samples used to estimate correction factor.</span>
<span class="sd">        show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">        rejection_sampling_batch_size: Batch size for rejection sampling.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Saved or newly-estimated correction factor (as a scalar `Tensor`).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># [1:] to remove batch-dimension for `reshape_to_batch_event`.</span>
        <span class="k">return</span> <span class="n">rejection</span><span class="o">.</span><span class="n">accept_reject_sample</span><span class="p">(</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
            <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_rejection_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">sample_for_correction_factor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">rejection_sampling_batch_size</span><span class="p">,</span>
            <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;condition&quot;</span><span class="p">:</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
                    <span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
                <span class="p">)</span>
            <span class="p">},</span>
        <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
    <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
        <span class="k">return</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span> <span class="o">=</span> <span class="n">acceptance_at</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leakage_density_correction_factor</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob" class="doc doc-heading">
            <code class=" language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">leakage_correction_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the log-probability of the posterior <span class="arithmatex">\(p(\theta|x)\)</span>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>theta</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters <span class="arithmatex">\(\theta\)</span>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>norm_posterior</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to enforce a normalized posterior density.
Renormalization of the posterior is useful when some
probability falls out or leaks out of the prescribed prior support.
The normalizing factor is calculated via rejection sampling, so if you
need speedier but unnormalized log posterior estimates set here
<code>norm_posterior=False</code>. The returned log posterior is set to
-∞ outside of the prior support regardless of this setting.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>track_gradients</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>leakage_correction_params</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[dict]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A <code>dict</code> of keyword arguments to override the
default values of <code>leakage_correction()</code>. Possible options are:
<code>num_rejection_samples</code>, <code>force_update</code>, <code>show_progress_bars</code>, and
<code>rejection_sampling_batch_size</code>.
These parameters only have an effect if <code>norm_posterior=True</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>(len(θ),)</code>-shaped log posterior probability <span class="arithmatex">\(\log p(\theta|x)\)</span> for θ in the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>support of the prior, -∞ (corresponding to 0 probability) outside.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">leakage_correction_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of the posterior $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">            Renormalization of the posterior is useful when some</span>
<span class="sd">            probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">            The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">            need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">            `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">            -∞ outside of the prior support regardless of this setting.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">        leakage_correction_params: A `dict` of keyword arguments to override the</span>
<span class="sd">            default values of `leakage_correction()`. Possible options are:</span>
<span class="sd">            `num_rejection_samples`, `force_update`, `show_progress_bars`, and</span>
<span class="sd">            `rejection_sampling_batch_size`.</span>
<span class="sd">            These parameters only have an effect if `norm_posterior=True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(θ),)`-shaped log posterior probability $\log p(\theta|x)$ for θ in the</span>
<span class="sd">        support of the prior, -∞ (corresponding to 0 probability) outside.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="n">theta_density_estimator</span> <span class="o">=</span> <span class="n">reshape_to_sample_batch_event</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">leading_is_sample</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">x_density_estimator</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">x_density_estimator</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;.log_prob() supports only `batchsize == 1`. If you intend &quot;</span>
            <span class="s2">&quot;to evaluate given multiple observations, use `.log_prob_batched()`. &quot;</span>
            <span class="s2">&quot;If you intend to evaluate given i.i.d. observations, set up the &quot;</span>
            <span class="s2">&quot;posterior density estimator with an appropriate permutation &quot;</span>
            <span class="s2">&quot;invariant embedding net.&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="c1"># Evaluate on device, move back to cpu for comparison with prior.</span>
        <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="n">theta_density_estimator</span><span class="p">,</span> <span class="n">condition</span><span class="o">=</span><span class="n">x_density_estimator</span>
        <span class="p">)</span>
        <span class="c1"># `log_prob` supports only a single observation (i.e. `batchsize==1`).</span>
        <span class="c1"># We now remove this additional dimension.</span>
        <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="n">unnorm_log_prob</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Force probability to be zero outside prior support.</span>
        <span class="n">in_prior_support</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

        <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">in_prior_support</span><span class="p">,</span>
            <span class="n">unnorm_log_prob</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">leakage_correction_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">leakage_correction_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
        <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">leakage_correction_params</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">norm_posterior</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob_batched" class="doc doc-heading">
            <code class=" language-python"><span class="n">log_prob_batched</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">norm_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">leakage_correction_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.log_prob_batched" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Given a batch of observations [x_1, &hellip;, x_B] and a batch of parameters             [$  heta_1$,&hellip;, $  heta_B$] this function evalautes the log-probabilities             of the posteriors <span class="arithmatex">\(p(        heta_1|x_1)\)</span>, &hellip;, <span class="arithmatex">\(p(  heta_B|x_B)\)</span> in a batched             (i.e. vectorized) manner.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>theta</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Batch of parameters $        heta$ of shape                 <code>(*sample_shape, batch_dim, *theta_shape)</code>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Batch of observations <span class="arithmatex">\(x\)</span> of shape                 <code>(batch_dim, *condition_shape)</code>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>norm_posterior</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to enforce a normalized posterior density.
Renormalization of the posterior is useful when some
probability falls out or leaks out of the prescribed prior support.
The normalizing factor is calculated via rejection sampling, so if you
need speedier but unnormalized log posterior estimates set here
<code>norm_posterior=False</code>. The returned log posterior is set to
-∞ outside of the prior support regardless of this setting.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>track_gradients</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>leakage_correction_params</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[dict]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A <code>dict</code> of keyword arguments to override the
default values of <code>leakage_correction()</code>. Possible options are:
<code>num_rejection_samples</code>, <code>force_update</code>, <code>show_progress_bars</code>, and
<code>rejection_sampling_batch_size</code>.
These parameters only have an effect if <code>norm_posterior=True</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>(len(θ), B)</code>-shaped log posterior probability <span class="arithmatex">\(\log p(     heta|x)\)</span> for θ             in the support of the prior, -∞ (corresponding to 0 probability) outside.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob_batched</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">norm_posterior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">leakage_correction_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given a batch of observations [x_1, ..., x_B] and a batch of parameters \</span>
<span class="sd">        [$\theta_1$,..., $\theta_B$] this function evalautes the log-probabilities \</span>
<span class="sd">        of the posteriors $p(\theta_1|x_1)$, ..., $p(\theta_B|x_B)$ in a batched \</span>
<span class="sd">        (i.e. vectorized) manner.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Batch of parameters $\theta$ of shape \</span>
<span class="sd">            `(*sample_shape, batch_dim, *theta_shape)`.</span>
<span class="sd">        x: Batch of observations $x$ of shape \</span>
<span class="sd">            `(batch_dim, *condition_shape)`.</span>
<span class="sd">        norm_posterior: Whether to enforce a normalized posterior density.</span>
<span class="sd">            Renormalization of the posterior is useful when some</span>
<span class="sd">            probability falls out or leaks out of the prescribed prior support.</span>
<span class="sd">            The normalizing factor is calculated via rejection sampling, so if you</span>
<span class="sd">            need speedier but unnormalized log posterior estimates set here</span>
<span class="sd">            `norm_posterior=False`. The returned log posterior is set to</span>
<span class="sd">            -∞ outside of the prior support regardless of this setting.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">        leakage_correction_params: A `dict` of keyword arguments to override the</span>
<span class="sd">            default values of `leakage_correction()`. Possible options are:</span>
<span class="sd">            `num_rejection_samples`, `force_update`, `show_progress_bars`, and</span>
<span class="sd">            `rejection_sampling_batch_size`.</span>
<span class="sd">            These parameters only have an effect if `norm_posterior=True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(θ), B)`-shaped log posterior probability $\\log p(\theta|x)$\\ for θ \</span>
<span class="sd">        in the support of the prior, -∞ (corresponding to 0 probability) outside.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="n">event_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">input_shape</span>
    <span class="n">theta_density_estimator</span> <span class="o">=</span> <span class="n">reshape_to_sample_batch_event</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">event_shape</span><span class="p">,</span> <span class="n">leading_is_sample</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">x_density_estimator</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="c1"># Evaluate on device, move back to cpu for comparison with prior.</span>
        <span class="n">unnorm_log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="n">theta_density_estimator</span><span class="p">,</span> <span class="n">condition</span><span class="o">=</span><span class="n">x_density_estimator</span>
        <span class="p">)</span>

        <span class="c1"># Force probability to be zero outside prior support.</span>
        <span class="n">in_prior_support</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

        <span class="n">masked_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">in_prior_support</span><span class="p">,</span>
            <span class="n">unnorm_log_prob</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">leakage_correction_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">leakage_correction_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
        <span class="n">log_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leakage_correction</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">leakage_correction_params</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">norm_posterior</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">masked_log_prob</span> <span class="o">-</span> <span class="n">log_factor</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.map" class="doc doc-heading">
            <code class=" language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.map" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iter</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of optimization steps that the algorithm takes
to find the MAP.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate of the optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.01</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_method</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p>
              </div>
            </td>
            <td>
                  <code>&#39;posterior&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_init_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_to_optimize</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p>
              </div>
            </td>
            <td>
                  <code>100</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_best_every</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during sampling from the
posterior.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_update</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>log_prob_kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The MAP estimate.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;posterior&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from the</span>
<span class="sd">            posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.sample" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return samples from posterior distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>()</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_with</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show sampling progress monitor.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;.sample() supports only `batchsize == 1`. If you intend &quot;</span>
            <span class="s2">&quot;to sample multiple observations, use `.sample_batched()`. &quot;</span>
            <span class="s2">&quot;If you intend to sample i.i.d. observations, set up the &quot;</span>
            <span class="s2">&quot;posterior density estimator with an appropriate permutation &quot;</span>
            <span class="s2">&quot;invariant embedding net.&quot;</span>
        <span class="p">)</span>

    <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
        <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="n">rejection</span><span class="o">.</span><span class="n">accept_reject_sample</span><span class="p">(</span>
        <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
        <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
        <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;condition&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
        <span class="n">alternative_method</span><span class="o">=</span><span class="s2">&quot;build_posterior(..., sample_with=&#39;mcmc&#39;)&quot;</span><span class="p">,</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># [0] to return only samples, not acceptance probabilities.</span>

    <span class="k">return</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Remove batch dimension.</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.direct_posterior.DirectPosterior.sample_batched" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample_batched</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.direct_posterior.DirectPosterior.sample_batched" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Given a batch of observations [x_1, &hellip;, x_B] this function samples from
posteriors <span class="arithmatex">\(p(\theta|x_1)\)</span>, &hellip; ,<span class="arithmatex">\(p(\theta|x_B)\)</span>, in a batched (i.e. vectorized)
manner.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Desired shape of samples that are drawn from the posterior
given every observation.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A batch of observations, of shape <code>(batch_dim, event_shape_x)</code>.
<code>batch_dim</code> corresponds to the number of observations to be drawn.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_sampling_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum batch size for rejection sampling.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show sampling progress monitor.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Samples from the posteriors of shape (*sample_shape, B, *input_shape)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/direct_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample_batched</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given a batch of observations [x_1, ..., x_B] this function samples from</span>
<span class="sd">    posteriors $p(\theta|x_1)$, ... ,$p(\theta|x_B)$, in a batched (i.e. vectorized)</span>
<span class="sd">    manner.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from the posterior</span>
<span class="sd">            given every observation.</span>
<span class="sd">        x: A batch of observations, of shape `(batch_dim, event_shape_x)`.</span>
<span class="sd">            `batch_dim` corresponds to the number of observations to be drawn.</span>
<span class="sd">        max_sampling_batch_size: Maximum batch size for rejection sampling.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from the posteriors of shape (*sample_shape, B, *input_shape)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">condition_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">condition_shape</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="n">condition_shape</span><span class="p">)</span>

    <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
        <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
    <span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="n">rejection</span><span class="o">.</span><span class="n">accept_reject_sample</span><span class="p">(</span>
        <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="p">,</span>
        <span class="n">accept_reject_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
        <span class="n">proposal_sampling_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;condition&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
        <span class="n">alternative_method</span><span class="o">=</span><span class="s2">&quot;build_posterior(..., sample_with=&#39;mcmc&#39;)&quot;</span><span class="p">,</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior" class="doc doc-heading">
            <code>ImportanceSamplingPosterior</code>


<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>


        <p>Provides importance sampling to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>ImportanceSamplingPosterior</code> allows to estimate the posterior log-probability by
estimating the normlalization constant with importance sampling. It also allows to
perform importance sampling (with <code>.sample()</code>) and to draw approximate samples with
sampling-importance-resampling (SIR) (with <code>.sir_sample()</code>)</p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/posteriors/importance_posterior.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ImportanceSamplingPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides importance sampling to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `ImportanceSamplingPosterior` allows to estimate the posterior log-probability by</span>
<span class="sd">    estimating the normlalization constant with importance sampling. It also allows to</span>
<span class="sd">    perform importance sampling (with `.sample()`) and to draw approximate samples with</span>
<span class="sd">    sampling-importance-resampling (SIR) (with `.sir_sample()`)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BasePotential</span><span class="p">],</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sir&quot;</span><span class="p">,</span>
        <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples. Must be a</span>
<span class="sd">                `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.</span>
<span class="sd">            proposal: The proposal distribution.</span>
<span class="sd">            theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">                during but only when calling `.map()`.</span>
<span class="sd">            method: Either of [`sir`|`importance`]. This sets the behavior of the</span>
<span class="sd">                `.sample()` method. With `sir`, approximate posterior samples are</span>
<span class="sd">                generated with sampling importance resampling (SIR). With</span>
<span class="sd">                `importance`, the `.sample()` method returns a tuple of samples and</span>
<span class="sd">                corresponding importance weights.</span>
<span class="sd">            oversampling_factor: Number of proposed samples from which only one is</span>
<span class="sd">                selected based on its importance weight.</span>
<span class="sd">            max_sampling_batch_size: The batch size of samples being drawn from the</span>
<span class="sd">                proposal at every iteration.</span>
<span class="sd">            device: Device on which to sample, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If</span>
<span class="sd">                None, `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Deprecated, should not be passed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oversampling_factor</span> <span class="o">=</span> <span class="n">oversampling_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides sampling-importance resampling (SIR) to .sample() from the &quot;</span>
            <span class="s2">&quot;posterior and can evaluate the _unnormalized_ posterior density with &quot;</span>
            <span class="s2">&quot;.log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">normalization_constant_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">        The normalization constant is estimated with importance sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>
<span class="sd">            normalization_constant_params: Parameters passed on to</span>
<span class="sd">                `estimate_normalization_constant()`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="n">potential_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
                <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">normalization_constant_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">normalization_constant_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
            <span class="n">normalization_constant</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimate_normalization_constant</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">normalization_constant_params</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="p">(</span><span class="n">potential_values</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">normalization_constant</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
            <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">estimate_normalization_constant</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span> <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the normalization constant via importance sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Number of importance samples used for the estimate.</span>
<span class="sd">            force_update: Whether to re-calculate the normlization constant when x is</span>
<span class="sd">                unchanged and have a cached value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
        <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_importance_weights</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_importance_weights</span><span class="p">))</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return samples from the approximate posterior distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Shape of samples that are drawn from posterior.</span>
<span class="sd">            x: Observed data.</span>
<span class="sd">            method: Either of [`sir`|`importance`]. This sets the behavior of the</span>
<span class="sd">                `.sample()` method. With `sir`, approximate posterior samples are</span>
<span class="sd">                generated with sampling importance resampling (SIR). With</span>
<span class="sd">                `importance`, the `.sample()` method returns a tuple of samples and</span>
<span class="sd">                corresponding importance weights.</span>
<span class="sd">            oversampling_factor: Number of proposed samples from which only one is</span>
<span class="sd">                selected based on its importance weight.</span>
<span class="sd">            max_sampling_batch_size: The batch size of samples being drawn from the</span>
<span class="sd">                proposal at every iteration.</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>

        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sir_sample</span><span class="p">(</span>
                <span class="n">sample_shape</span><span class="p">,</span>
                <span class="n">oversampling_factor</span><span class="o">=</span><span class="n">oversampling_factor</span><span class="p">,</span>
                <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;importance&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_importance_sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NameError</span>

    <span class="k">def</span> <span class="nf">sample_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Batched sampling is not implemented for ImportanceSamplingPosterior. </span><span class="se">\</span>
<span class="s2">            Alternatively you can use `sample` in a loop </span><span class="se">\</span>
<span class="s2">            [posterior.sample(theta, x_o) for x_o in x].&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_importance_sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns samples from the proposal and log of their importance weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples and logarithm of corresponding importance weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">samples</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span><span class="p">,</span> <span class="n">log_importance_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sir_sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns approximate samples from posterior $p(\theta|x)$ via SIR.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            x: Observed data.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            oversampling_factor: Number of proposed samples form which only one is</span>
<span class="sd">                selected based on its importance weight.</span>
<span class="sd">            max_sampling_batch_size: The batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration. Used only in `sir_sample()`.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">oversampling_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oversampling_factor</span>
            <span class="k">if</span> <span class="n">oversampling_factor</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">oversampling_factor</span>
        <span class="p">)</span>
        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>

        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">sampling_importance_resampling</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">num_candidate_samples</span><span class="o">=</span><span class="n">oversampling_factor</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from the</span>
<span class="sd">                posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;sir&#39;</span><span class="p">,</span> <span class="n">oversampling_factor</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>potential_fn</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, <span title="sbi.inference.potentials.base_potential.BasePotential">BasePotential</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The potential function from which to draw samples. Must be a
<code>BasePotential</code> or a <code>Callable</code> which takes <code>theta</code> and <code>x_o</code> as inputs.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proposal</code>
            </td>
            <td>
                  <code><span title="typing.Any">Any</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The proposal distribution.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>theta_transform</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchTransform">TorchTransform</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Transformation that is applied to parameters. Is not used
during but only when calling <code>.map()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>method</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Either of [<code>sir</code>|<code>importance</code>]. This sets the behavior of the
<code>.sample()</code> method. With <code>sir</code>, approximate posterior samples are
generated with sampling importance resampling (SIR). With
<code>importance</code>, the <code>.sample()</code> method returns a tuple of samples and
corresponding importance weights.</p>
              </div>
            </td>
            <td>
                  <code>&#39;sir&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>oversampling_factor</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of proposed samples from which only one is
selected based on its importance weight.</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_sampling_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The batch size of samples being drawn from the
proposal at every iteration.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Device on which to sample, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If
None, <code>potential_fn.device</code> is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x_shape</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Size">Size</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated, should not be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/importance_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BasePotential</span><span class="p">],</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sir&quot;</span><span class="p">,</span>
    <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples. Must be a</span>
<span class="sd">            `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.</span>
<span class="sd">        proposal: The proposal distribution.</span>
<span class="sd">        theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">            during but only when calling `.map()`.</span>
<span class="sd">        method: Either of [`sir`|`importance`]. This sets the behavior of the</span>
<span class="sd">            `.sample()` method. With `sir`, approximate posterior samples are</span>
<span class="sd">            generated with sampling importance resampling (SIR). With</span>
<span class="sd">            `importance`, the `.sample()` method returns a tuple of samples and</span>
<span class="sd">            corresponding importance weights.</span>
<span class="sd">        oversampling_factor: Number of proposed samples from which only one is</span>
<span class="sd">            selected based on its importance weight.</span>
<span class="sd">        max_sampling_batch_size: The batch size of samples being drawn from the</span>
<span class="sd">            proposal at every iteration.</span>
<span class="sd">        device: Device on which to sample, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If</span>
<span class="sd">            None, `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Deprecated, should not be passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">oversampling_factor</span> <span class="o">=</span> <span class="n">oversampling_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides sampling-importance resampling (SIR) to .sample() from the &quot;</span>
        <span class="s2">&quot;posterior and can evaluate the _unnormalized_ posterior density with &quot;</span>
        <span class="s2">&quot;.log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.estimate_normalization_constant" class="doc doc-heading">
            <code class=" language-python"><span class="n">estimate_normalization_constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.estimate_normalization_constant" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the normalization constant via importance sampling.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>num_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of importance samples used for the estimate.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_update</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to re-calculate the normlization constant when x is
unchanged and have a cached value.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/importance_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">estimate_normalization_constant</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span> <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the normalization constant via importance sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_samples: Number of importance samples used for the estimate.</span>
<span class="sd">        force_update: Whether to re-calculate the normlization constant when x is</span>
<span class="sd">            unchanged and have a cached value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check if the provided x matches the default x (short-circuit on identity).</span>
    <span class="n">is_new_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">not_saved_at_default_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_new_x</span><span class="p">:</span>  <span class="c1"># Calculate at x; don&#39;t save.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_importance_weights</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">not_saved_at_default_x</span> <span class="ow">or</span> <span class="n">force_update</span><span class="p">:</span>  <span class="c1"># Calculate at default_x; save.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">log_importance_weights</span> <span class="o">=</span> <span class="n">importance_sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_importance_weights</span><span class="p">))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_constant</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.log_prob" class="doc doc-heading">
            <code class=" language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">normalization_constant_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the log-probability of theta under the posterior.</p>
<p>The normalization constant is estimated with importance sampling.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>theta</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters <span class="arithmatex">\(\theta\)</span>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>track_gradients</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>normalization_constant_params</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[dict]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters passed on to
<code>estimate_normalization_constant()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>len($\theta$)</code>-shaped log-probability.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/importance_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">normalization_constant_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">    The normalization constant is estimated with importance sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">        normalization_constant_params: Parameters passed on to</span>
<span class="sd">            `estimate_normalization_constant()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="n">potential_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">normalization_constant_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">normalization_constant_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  <span class="c1"># use defaults</span>
        <span class="n">normalization_constant</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimate_normalization_constant</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">normalization_constant_params</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">potential_values</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">normalization_constant</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.map" class="doc doc-heading">
            <code class=" language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.map" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iter</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of optimization steps that the algorithm takes
to find the MAP.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate of the optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.01</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_method</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p>
              </div>
            </td>
            <td>
                  <code>&#39;proposal&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_init_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_to_optimize</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p>
              </div>
            </td>
            <td>
                  <code>100</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_best_every</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during sampling from the
posterior.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_update</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>log_prob_kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The MAP estimate.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/importance_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from the</span>
<span class="sd">            posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.sample" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">oversampling_factor</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.importance_posterior.ImportanceSamplingPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return samples from the approximate posterior distribution.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Shape of samples that are drawn from posterior.</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>()</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Observed data.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>method</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Either of [<code>sir</code>|<code>importance</code>]. This sets the behavior of the
<code>.sample()</code> method. With <code>sir</code>, approximate posterior samples are
generated with sampling importance resampling (SIR). With
<code>importance</code>, the <code>.sample()</code> method returns a tuple of samples and
corresponding importance weights.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>oversampling_factor</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of proposed samples from which only one is
selected based on its importance weight.</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_sampling_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The batch size of samples being drawn from the
proposal at every iteration.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during sampling.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/importance_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">oversampling_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return samples from the approximate posterior distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Shape of samples that are drawn from posterior.</span>
<span class="sd">        x: Observed data.</span>
<span class="sd">        method: Either of [`sir`|`importance`]. This sets the behavior of the</span>
<span class="sd">            `.sample()` method. With `sir`, approximate posterior samples are</span>
<span class="sd">            generated with sampling importance resampling (SIR). With</span>
<span class="sd">            `importance`, the `.sample()` method returns a tuple of samples and</span>
<span class="sd">            corresponding importance weights.</span>
<span class="sd">        oversampling_factor: Number of proposed samples from which only one is</span>
<span class="sd">            selected based on its importance weight.</span>
<span class="sd">        max_sampling_batch_size: The batch size of samples being drawn from the</span>
<span class="sd">            proposal at every iteration.</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sir_sample</span><span class="p">(</span>
            <span class="n">sample_shape</span><span class="p">,</span>
            <span class="n">oversampling_factor</span><span class="o">=</span><span class="n">oversampling_factor</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;importance&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_importance_sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NameError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="doc doc-heading">
            <code>MCMCPosterior</code>


<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>


        <p>Provides MCMC to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>MCMCPosterior</code> allows to sample from the posterior with MCMC.</p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  39</span>
<span class="normal">  40</span>
<span class="normal">  41</span>
<span class="normal">  42</span>
<span class="normal">  43</span>
<span class="normal">  44</span>
<span class="normal">  45</span>
<span class="normal">  46</span>
<span class="normal">  47</span>
<span class="normal">  48</span>
<span class="normal">  49</span>
<span class="normal">  50</span>
<span class="normal">  51</span>
<span class="normal">  52</span>
<span class="normal">  53</span>
<span class="normal">  54</span>
<span class="normal">  55</span>
<span class="normal">  56</span>
<span class="normal">  57</span>
<span class="normal">  58</span>
<span class="normal">  59</span>
<span class="normal">  60</span>
<span class="normal">  61</span>
<span class="normal">  62</span>
<span class="normal">  63</span>
<span class="normal">  64</span>
<span class="normal">  65</span>
<span class="normal">  66</span>
<span class="normal">  67</span>
<span class="normal">  68</span>
<span class="normal">  69</span>
<span class="normal">  70</span>
<span class="normal">  71</span>
<span class="normal">  72</span>
<span class="normal">  73</span>
<span class="normal">  74</span>
<span class="normal">  75</span>
<span class="normal">  76</span>
<span class="normal">  77</span>
<span class="normal">  78</span>
<span class="normal">  79</span>
<span class="normal">  80</span>
<span class="normal">  81</span>
<span class="normal">  82</span>
<span class="normal">  83</span>
<span class="normal">  84</span>
<span class="normal">  85</span>
<span class="normal">  86</span>
<span class="normal">  87</span>
<span class="normal">  88</span>
<span class="normal">  89</span>
<span class="normal">  90</span>
<span class="normal">  91</span>
<span class="normal">  92</span>
<span class="normal">  93</span>
<span class="normal">  94</span>
<span class="normal">  95</span>
<span class="normal">  96</span>
<span class="normal">  97</span>
<span class="normal">  98</span>
<span class="normal">  99</span>
<span class="normal"> 100</span>
<span class="normal"> 101</span>
<span class="normal"> 102</span>
<span class="normal"> 103</span>
<span class="normal"> 104</span>
<span class="normal"> 105</span>
<span class="normal"> 106</span>
<span class="normal"> 107</span>
<span class="normal"> 108</span>
<span class="normal"> 109</span>
<span class="normal"> 110</span>
<span class="normal"> 111</span>
<span class="normal"> 112</span>
<span class="normal"> 113</span>
<span class="normal"> 114</span>
<span class="normal"> 115</span>
<span class="normal"> 116</span>
<span class="normal"> 117</span>
<span class="normal"> 118</span>
<span class="normal"> 119</span>
<span class="normal"> 120</span>
<span class="normal"> 121</span>
<span class="normal"> 122</span>
<span class="normal"> 123</span>
<span class="normal"> 124</span>
<span class="normal"> 125</span>
<span class="normal"> 126</span>
<span class="normal"> 127</span>
<span class="normal"> 128</span>
<span class="normal"> 129</span>
<span class="normal"> 130</span>
<span class="normal"> 131</span>
<span class="normal"> 132</span>
<span class="normal"> 133</span>
<span class="normal"> 134</span>
<span class="normal"> 135</span>
<span class="normal"> 136</span>
<span class="normal"> 137</span>
<span class="normal"> 138</span>
<span class="normal"> 139</span>
<span class="normal"> 140</span>
<span class="normal"> 141</span>
<span class="normal"> 142</span>
<span class="normal"> 143</span>
<span class="normal"> 144</span>
<span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MCMCPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides MCMC to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `MCMCPosterior` allows to sample from the posterior with MCMC.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BasePotential</span><span class="p">],</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;resample&quot;</span><span class="p">,</span>
        <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">mp_context</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;spawn&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples. Must be a</span>
<span class="sd">                `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.</span>
<span class="sd">            proposal: Proposal distribution that is used to initialize the MCMC chain.</span>
<span class="sd">            theta_transform: Transformation that will be applied during sampling.</span>
<span class="sd">                Allows to perform MCMC in unconstrained space.</span>
<span class="sd">            method: Method used for MCMC sampling, one of `slice_np`,</span>
<span class="sd">                `slice_np_vectorized`, `hmc_pyro`, `nuts_pyro`, `slice_pymc`,</span>
<span class="sd">                `hmc_pymc`, `nuts_pymc`. `slice_np` is a custom</span>
<span class="sd">                numpy implementation of slice sampling. `slice_np_vectorized` is</span>
<span class="sd">                identical to `slice_np`, but if `num_chains&gt;1`, the chains are</span>
<span class="sd">                vectorized for `slice_np_vectorized` whereas they are run sequentially</span>
<span class="sd">                for `slice_np`. The samplers ending on `_pyro` are using Pyro, and</span>
<span class="sd">                likewise the samplers ending on `_pymc` are using PyMC.</span>
<span class="sd">            thin: The thinning factor for the chain, default 1 (no thinning).</span>
<span class="sd">            warmup_steps: The initial number of samples to discard.</span>
<span class="sd">            num_chains: The number of chains. Should generally be at most</span>
<span class="sd">                `num_workers - 1`.</span>
<span class="sd">            init_strategy: The initialisation strategy for chains; `proposal` will draw</span>
<span class="sd">                init locations from `proposal`, whereas `sir` will use Sequential-</span>
<span class="sd">                Importance-Resampling (SIR). SIR initially samples</span>
<span class="sd">                `init_strategy_num_candidates` from the `proposal`, evaluates all of</span>
<span class="sd">                them under the `potential_fn` and `proposal`, and then resamples the</span>
<span class="sd">                initial locations with weights proportional to `exp(potential_fn -</span>
<span class="sd">                proposal.log_prob`. `resample` is the same as `sir` but</span>
<span class="sd">                uses `exp(potential_fn)` as weights.</span>
<span class="sd">            init_strategy_parameters: Dictionary of keyword arguments passed to the</span>
<span class="sd">                init strategy, e.g., for `init_strategy=sir` this could be</span>
<span class="sd">                `num_candidate_samples`, i.e., the number of candidates to find init</span>
<span class="sd">                locations (internal default is `1000`), or `device`.</span>
<span class="sd">            init_strategy_num_candidates: Number of candidates to find init</span>
<span class="sd">                 locations in `init_strategy=sir` (deprecated, use</span>
<span class="sd">                 init_strategy_parameters instead).</span>
<span class="sd">            num_workers: number of cpu cores used to parallelize mcmc</span>
<span class="sd">            mp_context: Multiprocessing start method, either `&quot;fork&quot;` or `&quot;spawn&quot;`</span>
<span class="sd">                (default), used by Pyro and PyMC samplers. `&quot;fork&quot;` can be significantly</span>
<span class="sd">                faster than `&quot;spawn&quot;` but is only supported on POSIX-based systems</span>
<span class="sd">                (e.g. Linux and macOS, not Windows).</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Deprecated, should not be passed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice&quot;</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The Pyro-based slice sampler is deprecated, and the method `slice` &quot;</span>
                <span class="s2">&quot;has been changed to `slice_np`, i.e., the custom &quot;</span>
                <span class="s2">&quot;numpy-based slice sampler.&quot;</span><span class="p">,</span>
                <span class="ne">DeprecationWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span>

        <span class="n">thin</span> <span class="o">=</span> <span class="n">_process_thin_default</span><span class="p">(</span><span class="n">thin</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="n">thin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="o">=</span> <span class="n">num_chains</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="o">=</span> <span class="n">init_strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="n">init_strategy_parameters</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mp_context</span> <span class="o">=</span> <span class="n">mp_context</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Hardcode parameter name to reduce clutter kwargs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_name</span> <span class="o">=</span> <span class="s2">&quot;theta&quot;</span>

        <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Passing `init_strategy_num_candidates` is deprecated as of sbi &quot;</span>
                <span class="s2">&quot;v0.19.0. Instead, use e.g., `init_strategy_parameters &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;=</span><span class="si">{</span><span class="s1">&#39;num_candidate_samples&#39;</span><span class="si">:</span><span class="s2"> 1000</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span><span class="p">[</span><span class="s2">&quot;num_candidate_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">init_strategy_num_candidates</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides MCMC to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns MCMC method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span>

    <span class="nd">@mcmc_method</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;See `set_mcmc_method`.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_mcmc_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">posterior_sampler</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns sampler created by `sample`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span>

    <span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: Method to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `NeuralPosterior` for chainable calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`.log_prob()` is deprecated for methods that can only evaluate the &quot;</span>
            <span class="s2">&quot;log-probability up to a normalizing constant. Use `.potential()` instead.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mp_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$ with MCMC.</span>

<span class="sd">        Check the `__init__()` method for a description of all arguments as well as</span>
<span class="sd">        their default values.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            mcmc_parameters: Dictionary that is passed only to support the API of</span>
<span class="sd">                `sbi` v0.17.2 or older.</span>
<span class="sd">            mcmc_method: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. Please use `method` instead.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">thin</span>
        <span class="n">warmup_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="k">if</span> <span class="n">warmup_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">warmup_steps</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
        <span class="n">init_strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="k">if</span> <span class="n">init_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">init_strategy</span>
        <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="k">if</span> <span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_workers</span>
        <span class="n">mp_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mp_context</span> <span class="k">if</span> <span class="n">mp_context</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mp_context</span>
        <span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span>
            <span class="k">if</span> <span class="n">init_strategy_parameters</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">init_strategy_parameters</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Passing `init_strategy_num_candidates` is deprecated as of sbi &quot;</span>
                <span class="s2">&quot;v0.19.0. Instead, use e.g., &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`init_strategy_parameters=</span><span class="si">{</span><span class="s1">&#39;num_candidate_samples&#39;</span><span class="si">:</span><span class="s2"> 1000</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span><span class="p">[</span><span class="s2">&quot;num_candidate_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">init_strategy_num_candidates</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">mcmc_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
                <span class="s2">&quot;is deprecated and will be removed in a future release. Use `method` &quot;</span>
                <span class="s2">&quot;instead of `mcmc_method`.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">mcmc_method</span>
        <span class="k">if</span> <span class="n">mcmc_parameters</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
                <span class="s2">&quot;is deprecated and will be removed in a future release. Instead, pass &quot;</span>
                <span class="s2">&quot;the variable to `.sample()` directly, e.g. &quot;</span>
                <span class="s2">&quot;`posterior.sample((1,), num_chains=5)`.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># The following lines are only for backwards compatibility with sbi v0.17.2 or</span>
        <span class="c1"># older.</span>
        <span class="n">m_p</span> <span class="o">=</span> <span class="n">mcmc_parameters</span> <span class="ow">or</span> <span class="p">{}</span>  <span class="c1"># define to shorten the variable name</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="s2">&quot;mcmc_method&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">thin</span><span class="p">,</span> <span class="s2">&quot;thin&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="s2">&quot;warmup_steps&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="s2">&quot;num_chains&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="n">init_strategy</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">init_strategy</span><span class="p">,</span> <span class="s2">&quot;init_strategy&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="n">initial_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_params</span><span class="p">(</span>
            <span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">num_chains</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">num_workers</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="o">**</span><span class="n">init_strategy_parameters</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="n">track_gradients</span> <span class="o">=</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc_pyro&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pyro&quot;</span><span class="p">,</span> <span class="s2">&quot;hmc_pymc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pymc&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;slice_np&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_np_mcmc</span><span class="p">(</span>
                    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                    <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                    <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                    <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">vectorized</span><span class="o">=</span><span class="p">(</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">),</span>
                    <span class="n">interchangeable_chains</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                    <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc_pyro&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pyro&quot;</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pyro_mcmc</span><span class="p">(</span>
                    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                    <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                    <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                    <span class="n">mcmc_method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                    <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                    <span class="n">mp_context</span><span class="o">=</span><span class="n">mp_context</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc_pymc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pymc&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_pymc&quot;</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pymc_mcmc</span><span class="p">(</span>
                    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                    <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                    <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                    <span class="n">mcmc_method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                    <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                    <span class="n">mp_context</span><span class="o">=</span><span class="n">mp_context</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The sampling method </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2"> is not implemented!&quot;</span><span class="p">)</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span>
        <span class="c1"># NOTE: Currently MCMCPosteriors will require a single dimension for the</span>
        <span class="c1"># parameter dimension. With recent ConditionalDensity(Ratio) estimators, we</span>
        <span class="c1"># can have multiple dimensions for the parameter dimension.</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># type: ignore</span>

        <span class="k">return</span> <span class="n">samples</span>

    <span class="k">def</span> <span class="nf">sample_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mp_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given a batch of observations [x_1, ..., x_B] this function samples from</span>
<span class="sd">        posteriors $p(\theta|x_1)$, ... ,$p(\theta|x_B)$, in a batched (i.e. vectorized)</span>
<span class="sd">        manner.</span>

<span class="sd">        Check the `__init__()` method for a description of all arguments as well as</span>
<span class="sd">        their default values.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from the posterior</span>
<span class="sd">                given every observation.</span>
<span class="sd">            x: A batch of observations, of shape `(batch_dim, event_shape_x)`.</span>
<span class="sd">                `batch_dim` corresponds to the number of observations to be</span>
<span class="sd">                drawn.</span>
<span class="sd">            method: Method used for MCMC sampling, e.g., &quot;slice_np_vectorized&quot;.</span>
<span class="sd">            thin: The thinning factor for the chain, default 1 (no thinning).</span>
<span class="sd">            warmup_steps: The initial number of samples to discard.</span>
<span class="sd">            num_chains: The number of chains used for each `x` passed in the batch.</span>
<span class="sd">            init_strategy: The initialisation strategy for chains.</span>
<span class="sd">            init_strategy_parameters: Dictionary of keyword arguments passed to</span>
<span class="sd">                the init strategy.</span>
<span class="sd">            num_workers: number of cpu cores used to parallelize initial</span>
<span class="sd">                parameter generation and mcmc sampling.</span>
<span class="sd">            mp_context: Multiprocessing start method, either `&quot;fork&quot;` or `&quot;spawn&quot;`</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from the posteriors of shape (*sample_shape, B, *input_shape)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">thin</span>
        <span class="n">warmup_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="k">if</span> <span class="n">warmup_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">warmup_steps</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
        <span class="n">init_strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="k">if</span> <span class="n">init_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">init_strategy</span>
        <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="k">if</span> <span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_workers</span>
        <span class="n">mp_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mp_context</span> <span class="k">if</span> <span class="n">mp_context</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mp_context</span>
        <span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span>
            <span class="k">if</span> <span class="n">init_strategy_parameters</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">init_strategy_parameters</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span>
        <span class="p">),</span> <span class="s2">&quot;Batched sampling only supported for vectorized samplers!&quot;</span>

        <span class="c1"># warn if num_chains is larger than num requested samples</span>
        <span class="k">if</span> <span class="n">num_chains</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The passed number of MCMC chains is larger than the number of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;requested samples: </span><span class="si">{</span><span class="n">num_chains</span><span class="si">}</span><span class="s2"> &gt; </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">,&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; resetting it to </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">num_chains</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="c1"># custom shape handling to make sure to match the batch size of x and theta</span>
        <span class="c1"># without unnecessary combinations.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

        <span class="c1"># For batched sampling, we want `num_chains` for each observation in the batch.</span>
        <span class="c1"># Here we repeat the observations ABC -&gt; AAABBBCCC, so that the chains are</span>
        <span class="c1"># in the order of the observations.</span>
        <span class="n">x_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">x_is_iid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># For each observation in the batch, we have num_chains independent chains.</span>
        <span class="n">num_chains_extended</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_chains</span>
        <span class="k">if</span> <span class="n">num_chains_extended</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Note that for batched sampling, we use num_chains many chains for each&quot;</span>
                <span class="s2">&quot; x in the batch. With the given settings, this results in a large &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;number large number of chains (</span><span class="si">{</span><span class="n">num_chains_extended</span><span class="si">}</span><span class="s2">), which can be &quot;</span>
                <span class="s2">&quot;slow and memory-intensive for vectorized MCMC. Consider reducing the &quot;</span>
                <span class="s2">&quot;number of chains.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">init_strategy_parameters</span><span class="p">[</span><span class="s2">&quot;num_return_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_chains_extended</span>
        <span class="n">initial_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_params_batched</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">num_chains</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">num_workers</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="o">**</span><span class="n">init_strategy_parameters</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># We need num_samples from each posterior in the batch</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">batch_size</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_np_mcmc</span><span class="p">(</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">vectorized</span><span class="o">=</span><span class="p">(</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">),</span>
                <span class="n">interchangeable_chains</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># (num_chains_extended, samples_per_chain, *input_shape)</span>
        <span class="n">samples_per_chain</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="n">dim_theta</span> <span class="o">=</span> <span class="n">samples_per_chain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># We need to collect samples for each x from the respective chains.</span>
        <span class="c1"># However, using samples.reshape(*sample_shape, batch_size, dim_theta)</span>
        <span class="c1"># does not combine the samples in the right order, since this mixes</span>
        <span class="c1"># samples that belong to different `x`. The following permute is a</span>
        <span class="c1"># workaround to reshape the samples in the right order.</span>
        <span class="n">samples_per_x</span> <span class="o">=</span> <span class="n">samples_per_chain</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="c1"># We are flattening the sample shape here using -1 because we might have</span>
            <span class="c1"># generated more samples than requested (more chains, or multiple of</span>
            <span class="c1"># chains not matching sample_shape)</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">dim_theta</span><span class="p">,</span>
        <span class="p">))</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Shape is now (-1, batch_size, dim_theta)</span>
        <span class="c1"># We can now select the number of requested samples</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples_per_x</span><span class="p">[:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()]</span>
        <span class="c1"># and reshape into (*sample_shape, batch_size, dim_theta)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim_theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">samples</span>

    <span class="k">def</span> <span class="nf">_build_mcmc_init_fn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">Transform</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return function that, when called, creates an initial parameter set for MCMC.</span>

<span class="sd">        Args:</span>
<span class="sd">            proposal: Proposal distribution.</span>
<span class="sd">            potential_fn: Potential function that the candidate samples are weighted</span>
<span class="sd">                with.</span>
<span class="sd">            init_strategy: Specifies the initialization method. Either of</span>
<span class="sd">                [`proposal`|`sir`|`resample`|`latest_sample`].</span>
<span class="sd">            kwargs: Passed on to init function. This way, init specific keywords can</span>
<span class="sd">                be set through `mcmc_parameters`. Unused arguments will be absorbed by</span>
<span class="sd">                the intitialization method.</span>

<span class="sd">        Returns: Initialization function.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;proposal&quot;</span> <span class="ow">or</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;prior&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;prior&quot;</span><span class="p">:</span>
                <span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;You set `init_strategy=prior`. As of sbi v0.18.0, this is &quot;</span>
                    <span class="s2">&quot;deprecated and it will be removed in a future release. Use &quot;</span>
                    <span class="s2">&quot;`init_strategy=proposal` instead.&quot;</span><span class="p">,</span>
                    <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">proposal_init</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;As of sbi v0.19.0, the behavior of the SIR initialization for MCMC &quot;</span>
                <span class="s2">&quot;has changed. If you wish to restore the behavior of sbi v0.18.0, set &quot;</span>
                <span class="s2">&quot;`init_strategy=&#39;resample&#39;.`&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">sir_init</span><span class="p">(</span>
                <span class="n">proposal</span><span class="p">,</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;resample&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">resample_given_potential_fn</span><span class="p">(</span>
                <span class="n">proposal</span><span class="p">,</span> <span class="n">potential_fn</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;latest_sample&quot;</span><span class="p">:</span>
            <span class="n">latest_sample</span> <span class="o">=</span> <span class="n">IterateParameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_init_params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">latest_sample</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_get_initial_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return initial parameters for MCMC obtained with given init strategy.</span>

<span class="sd">        Parallelizes across CPU cores only for resample and SIR.</span>

<span class="sd">        Args:</span>
<span class="sd">            init_strategy: Specifies the initialization method. Either of</span>
<span class="sd">                [`proposal`|`sir`|`resample`|`latest_sample`].</span>
<span class="sd">            num_chains: number of MCMC chains, generates initial params for each</span>
<span class="sd">            num_workers: number of CPU cores for parallization</span>
<span class="sd">            show_progress_bars: whether to show progress bars for SIR init</span>
<span class="sd">            kwargs: Passed on to `_build_mcmc_init_fn`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: initial parameters, one for each chain</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Build init function</span>
        <span class="n">init_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_mcmc_init_fn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">init_strategy</span><span class="o">=</span><span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Parallelize inits for resampling only.</span>
        <span class="k">if</span> <span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;resample&quot;</span> <span class="ow">or</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span><span class="p">):</span>

            <span class="k">def</span> <span class="nf">seeded_init_fn</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">init_fn</span><span class="p">()</span>

            <span class="n">seeds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,))</span>

            <span class="c1"># Generate initial params parallelized over num_workers.</span>
            <span class="n">initial_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
                <span class="n">tqdm</span><span class="p">(</span>
                    <span class="n">Parallel</span><span class="p">(</span><span class="n">return_as</span><span class="o">=</span><span class="s2">&quot;generator&quot;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)(</span>
                        <span class="n">delayed</span><span class="p">(</span><span class="n">seeded_init_fn</span><span class="p">)(</span><span class="n">seed</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span>
                    <span class="p">),</span>
                    <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">seeds</span><span class="p">),</span>
                    <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Generating </span><span class="si">{</span><span class="n">num_chains</span><span class="si">}</span><span class="s2"> MCMC inits with</span>
<span class="s2">                            </span><span class="si">{</span><span class="n">num_workers</span><span class="si">}</span><span class="s2"> workers.&quot;&quot;&quot;</span><span class="p">,</span>
                    <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">initial_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">initial_params</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">initial_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">init_fn</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chains</span><span class="p">)]</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">initial_params</span>

    <span class="k">def</span> <span class="nf">_get_initial_params_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_chains_per_x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return initial parameters for MCMC for a batch of `x`, obtained with given</span>
<span class="sd">           init strategy.</span>

<span class="sd">        Parallelizes across CPU cores only for resample and SIR.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Batch of observations to create different initial parameters for.</span>
<span class="sd">            init_strategy: Specifies the initialization method. Either of</span>
<span class="sd">                [`proposal`|`sir`|`resample`|`latest_sample`].</span>
<span class="sd">            num_chains_per_x: number of MCMC chains for each x, generates initial params</span>
<span class="sd">                for each x</span>
<span class="sd">            num_workers: number of CPU cores for parallization</span>
<span class="sd">            show_progress_bars: whether to show progress bars for SIR init</span>
<span class="sd">            kwargs: Passed on to `_build_mcmc_init_fn`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: initial parameters, one for each chain</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">potential_</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">)</span>
        <span class="n">initial_params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">init_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_mcmc_init_fn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">init_strategy</span><span class="o">=</span><span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
            <span class="c1"># Build init function</span>
            <span class="n">potential_</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>

            <span class="c1"># Parallelize inits for resampling or sir.</span>
            <span class="k">if</span> <span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;resample&quot;</span> <span class="ow">or</span> <span class="n">init_strategy</span> <span class="o">==</span> <span class="s2">&quot;sir&quot;</span>
            <span class="p">):</span>

                <span class="k">def</span> <span class="nf">seeded_init_fn</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">init_fn</span><span class="p">()</span>

                <span class="n">seeds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_chains_per_x</span><span class="p">,))</span>

                <span class="c1"># Generate initial params parallelized over num_workers.</span>
                <span class="n">initial_params</span> <span class="o">=</span> <span class="n">initial_params</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span>
                    <span class="n">tqdm</span><span class="p">(</span>
                        <span class="n">Parallel</span><span class="p">(</span><span class="n">return_as</span><span class="o">=</span><span class="s2">&quot;generator&quot;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)(</span>
                            <span class="n">delayed</span><span class="p">(</span><span class="n">seeded_init_fn</span><span class="p">)(</span><span class="n">seed</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seeds</span>
                        <span class="p">),</span>
                        <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">seeds</span><span class="p">),</span>
                        <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Generating </span><span class="si">{</span><span class="n">num_chains_per_x</span><span class="si">}</span><span class="s2"> MCMC inits with</span>
<span class="s2">                                </span><span class="si">{</span><span class="n">num_workers</span><span class="si">}</span><span class="s2"> workers.&quot;&quot;&quot;</span><span class="p">,</span>
                        <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bars</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">initial_params</span> <span class="o">=</span> <span class="n">initial_params</span> <span class="o">+</span> <span class="p">[</span>
                    <span class="n">init_fn</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chains_per_x</span><span class="p">)</span>
                <span class="p">]</span>  <span class="c1"># type: ignore</span>

        <span class="n">initial_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">initial_params</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">initial_params</span>

    <span class="k">def</span> <span class="nf">_slice_np_mcmc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">potential_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">initial_params</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">vectorized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">interchangeable_chains</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">init_width</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom implementation of slice sampling using Numpy.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Desired number of samples.</span>
<span class="sd">            potential_function: A callable **class**.</span>
<span class="sd">            initial_params: Initial parameters for MCMC chain.</span>
<span class="sd">            thin: Thinning (subsampling) factor, default 1 (no thinning).</span>
<span class="sd">            warmup_steps: Initial number of samples to discard.</span>
<span class="sd">            vectorized: Whether to use a vectorized implementation of the</span>
<span class="sd">                `SliceSampler`.</span>
<span class="sd">            interchangeable_chains: Whether chains are interchangeable, i.e., whether</span>
<span class="sd">                we can mix samples between chains.</span>
<span class="sd">            num_workers: Number of CPU cores to use.</span>
<span class="sd">            init_width: Inital width of brackets.</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling;</span>
<span class="sd">                can only be turned off for vectorized sampler.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor of shape (num_samples, shape_of_single_theta).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_chains</span><span class="p">,</span> <span class="n">dim_samples</span> <span class="o">=</span> <span class="n">initial_params</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">vectorized</span><span class="p">:</span>
            <span class="n">SliceSamplerMultiChain</span> <span class="o">=</span> <span class="n">SliceSamplerSerial</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">SliceSamplerMultiChain</span> <span class="o">=</span> <span class="n">SliceSamplerVectorized</span>

        <span class="k">def</span> <span class="nf">multi_obs_potential</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="c1"># Params are of shape (num_chains * num_obs, event).</span>
            <span class="n">all_potentials</span> <span class="o">=</span> <span class="n">potential_function</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>  <span class="c1"># Shape: (num_chains, num_obs)</span>
            <span class="k">return</span> <span class="n">all_potentials</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="n">posterior_sampler</span> <span class="o">=</span> <span class="n">SliceSamplerMultiChain</span><span class="p">(</span>
            <span class="n">init_params</span><span class="o">=</span><span class="n">tensor2numpy</span><span class="p">(</span><span class="n">initial_params</span><span class="p">),</span>
            <span class="n">log_prob_fn</span><span class="o">=</span><span class="n">multi_obs_potential</span><span class="p">,</span>
            <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
            <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">init_width</span><span class="o">=</span><span class="n">init_width</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">warmup_</span> <span class="o">=</span> <span class="n">warmup_steps</span> <span class="o">*</span> <span class="n">thin</span>
        <span class="n">num_samples_</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">((</span><span class="n">num_samples</span> <span class="o">*</span> <span class="n">thin</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_chains</span><span class="p">)</span>
        <span class="c1"># Run mcmc including warmup</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">posterior_sampler</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">warmup_</span> <span class="o">+</span> <span class="n">num_samples_</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[:,</span> <span class="n">warmup_steps</span><span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># discard warmup steps</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>  <span class="c1"># chains x samples x dim</span>

        <span class="c1"># Save posterior sampler.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="n">posterior_sampler</span>

        <span class="c1"># Save sample as potential next init (if init_strategy == &#39;latest_sample&#39;).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_init_params</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="n">dim_samples</span><span class="p">)</span>

        <span class="c1"># Update: If chains are interchangeable, return concatenated samples. Otherwise</span>
        <span class="c1"># return samples per chain.</span>
        <span class="k">if</span> <span class="n">interchangeable_chains</span><span class="p">:</span>
            <span class="c1"># Collect samples from all chains.</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim_samples</span><span class="p">)[:</span><span class="n">num_samples</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_pyro_mcmc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">potential_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">initial_params</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nuts_pyro&quot;</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">mp_context</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;spawn&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples obtained using Pyro&#39;s HMC or NUTS sampler.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Desired number of samples.</span>
<span class="sd">            potential_function: A callable **class**. A class, but not a function,</span>
<span class="sd">                is picklable for Pyro MCMC to use it across chains in parallel,</span>
<span class="sd">                even when the potential function requires evaluating a neural network.</span>
<span class="sd">            initial_params: Initial parameters for MCMC chain.</span>
<span class="sd">            mcmc_method: Pyro MCMC method to use, either `&quot;hmc_pyro&quot;` or</span>
<span class="sd">                `&quot;nuts_pyro&quot;` (default).</span>
<span class="sd">            thin: Thinning (subsampling) factor, default 1 (no thinning).</span>
<span class="sd">            warmup_steps: Initial number of samples to discard.</span>
<span class="sd">            num_chains: Whether to sample in parallel. If None, use all but one CPU.</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor of shape (num_samples, shape_of_single_theta).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="n">_process_thin_default</span><span class="p">(</span><span class="n">thin</span><span class="p">)</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
        <span class="n">kernels</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">hmc_pyro</span><span class="o">=</span><span class="n">HMC</span><span class="p">,</span> <span class="n">nuts_pyro</span><span class="o">=</span><span class="n">NUTS</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">kernels</span><span class="p">[</span><span class="n">mcmc_method</span><span class="p">](</span><span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_function</span><span class="p">),</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">ceil</span><span class="p">((</span><span class="n">thin</span> <span class="o">*</span> <span class="n">num_samples</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_chains</span><span class="p">),</span>
            <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
            <span class="n">initial_params</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">param_name</span><span class="p">:</span> <span class="n">initial_params</span><span class="p">},</span>
            <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
            <span class="n">mp_context</span><span class="o">=</span><span class="n">mp_context</span><span class="p">,</span>
            <span class="n">disable_progbar</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">transforms</span><span class="o">=</span><span class="p">{},</span>
        <span class="p">)</span>
        <span class="n">sampler</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">initial_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># .shape[1] = dim of theta</span>
        <span class="p">)</span>

        <span class="c1"># Save posterior sampler.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="n">sampler</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[::</span><span class="n">thin</span><span class="p">][:</span><span class="n">num_samples</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_pymc_mcmc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">potential_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">initial_params</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nuts_pymc&quot;</span><span class="p">,</span>
        <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">mp_context</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;spawn&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples obtained using PyMC&#39;s HMC, NUTS or slice samplers.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Desired number of samples.</span>
<span class="sd">            potential_function: A callable **class**. A class, but not a function,</span>
<span class="sd">                is picklable for PyMC MCMC to use it across chains in parallel,</span>
<span class="sd">                even when the potential function requires evaluating a neural network.</span>
<span class="sd">            initial_params: Initial parameters for MCMC chain.</span>
<span class="sd">            mcmc_method: mcmc_method: Pyro MCMC method to use, either `&quot;hmc_pymc&quot;` or</span>
<span class="sd">                `&quot;slice_pymc&quot;`, or `&quot;nuts_pymc&quot;` (default).</span>
<span class="sd">            thin: Thinning (subsampling) factor, default 1 (no thinning).</span>
<span class="sd">            warmup_steps: Initial number of samples to discard.</span>
<span class="sd">            num_chains: Whether to sample in parallel. If None, use all but one CPU.</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor of shape (num_samples, shape_of_single_theta).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">thin</span> <span class="o">=</span> <span class="n">_process_thin_default</span><span class="p">(</span><span class="n">thin</span><span class="p">)</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">slice_pymc</span><span class="o">=</span><span class="s2">&quot;slice&quot;</span><span class="p">,</span> <span class="n">hmc_pymc</span><span class="o">=</span><span class="s2">&quot;hmc&quot;</span><span class="p">,</span> <span class="n">nuts_pymc</span><span class="o">=</span><span class="s2">&quot;nuts&quot;</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">PyMCSampler</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_function</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="n">steps</span><span class="p">[</span><span class="n">mcmc_method</span><span class="p">],</span>
            <span class="n">initvals</span><span class="o">=</span><span class="n">tensor2numpy</span><span class="p">(</span><span class="n">initial_params</span><span class="p">),</span>
            <span class="n">draws</span><span class="o">=</span><span class="n">ceil</span><span class="p">((</span><span class="n">thin</span> <span class="o">*</span> <span class="n">num_samples</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_chains</span><span class="p">),</span>
            <span class="n">tune</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
            <span class="n">chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
            <span class="n">mp_ctx</span><span class="o">=</span><span class="n">mp_context</span><span class="p">,</span>
            <span class="n">progressbar</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">param_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_name</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">initial_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Save posterior sampler.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="n">sampler</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[::</span><span class="n">thin</span><span class="p">][:</span><span class="n">num_samples</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">samples</span>

    <span class="k">def</span> <span class="nf">_prepare_potential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Combines potential and transform and takes care of gradients and pyro.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: Which MCMC method to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A potential function that is ready to be used in MCMC.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc_pyro&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pyro&quot;</span><span class="p">):</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc_pymc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pymc&quot;</span><span class="p">):</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;slice_np&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_pymc&quot;</span><span class="p">):</span>
            <span class="n">track_gradients</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">pyro</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;hmc&quot;</span> <span class="ow">in</span> <span class="n">method</span> <span class="ow">or</span> <span class="s2">&quot;nuts&quot;</span> <span class="ow">in</span> <span class="n">method</span><span class="p">:</span>
                <span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;The kwargs &#39;hmc&#39; and &#39;nuts&#39; are deprecated. Use &#39;hmc_pyro&#39;, &quot;</span>
                    <span class="s2">&quot;&#39;nuts_pyro&#39;, &#39;hmc_pymc&#39;, or &#39;nuts_pymc&#39; instead.&quot;</span><span class="p">,</span>
                    <span class="ne">DeprecationWarning</span><span class="p">,</span>
                    <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MCMC method </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2"> is not implemented.&quot;</span><span class="p">)</span>

        <span class="n">prepared_potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">transformed_potential</span><span class="p">,</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">pyro</span><span class="p">:</span>
            <span class="n">prepared_potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">pyro_potential_wrapper</span><span class="p">,</span> <span class="n">potential</span><span class="o">=</span><span class="n">prepared_potential</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">prepared_potential</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_arviz_inference_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InferenceData</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns arviz InferenceData object constructed most recent samples.</span>

<span class="sd">        Note: the InferenceData is constructed using the posterior samples generated in</span>
<span class="sd">        most recent call to `.sample(...)`.</span>

<span class="sd">        For Pyro and PyMC samplers, InferenceData will contain diagnostics, but for</span>
<span class="sd">        sbi slice samplers, only the samples are added.</span>

<span class="sd">        Returns:</span>
<span class="sd">            inference_data: Arviz InferenceData object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;No samples have been generated, call .sample() first.&quot;&quot;&quot;</span>

        <span class="n">sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">MCMC</span><span class="p">,</span> <span class="n">SliceSamplerSerial</span><span class="p">,</span> <span class="n">SliceSamplerVectorized</span><span class="p">,</span> <span class="n">PyMCSampler</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span>

        <span class="c1"># If Pyro sampler and samples not transformed, use arviz&#39; from_pyro.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="p">(</span><span class="n">HMC</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">IndependentTransform</span>
        <span class="p">):</span>
            <span class="n">inference_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pyro</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>
        <span class="c1"># If PyMC sampler and samples not transformed, get cached InferenceData.</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">PyMCSampler</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">IndependentTransform</span>
        <span class="p">):</span>
            <span class="n">inference_data</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_inference_data</span><span class="p">()</span>

        <span class="c1"># otherwise get samples from sampler and transform to original space.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_samples</span><span class="p">(</span><span class="n">group_by_chain</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Pyro samplers returns dicts, get values.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                <span class="c1"># popitem gets last items, [1] get the values as tensor.</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">transformed_samples</span><span class="o">.</span><span class="n">popitem</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># Our slice samplers return numpy arrays.</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">):</span>
                <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
                <span class="p">)</span>
            <span class="c1"># For MultipleIndependent priors transforms first dim must be batch dim.</span>
            <span class="c1"># thus, reshape back and forth to have batch dim in front.</span>
            <span class="n">samples_shape</span> <span class="o">=</span> <span class="n">transformed_samples</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">transformed_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">samples_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="o">*</span><span class="n">samples_shape</span>
            <span class="p">)</span>

            <span class="n">inference_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">convert_to_inference_data</span><span class="p">({</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">samples</span>
            <span class="p">})</span>

        <span class="k">return</span> <span class="n">inference_data</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get state of MCMCPosterior.</span>

<span class="sd">        Removes the posterior sampler from the state, as it may not be picklable.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict: State of MCMCPosterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_posterior_sampler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">state</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.mcmc_method" class="doc doc-heading">
            <code class=" language-python"><span class="n">mcmc_method</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.mcmc_method" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns MCMC method.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.posterior_sampler" class="doc doc-heading">
            <code class=" language-python"><span class="n">posterior_sampler</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.posterior_sampler" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns sampler created by <code>sample</code>.</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__getstate__" class="doc doc-heading">
            <code class=" language-python"><span class="n">__getstate__</span><span class="p">()</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__getstate__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Get state of MCMCPosterior.</p>
<p>Removes the posterior sampler from the state, as it may not be picklable.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>Dict</code></td>            <td>
                  <code><span title="typing.Dict">Dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>State of MCMCPosterior.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get state of MCMCPosterior.</span>

<span class="sd">    Removes the posterior sampler from the state, as it may not be picklable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict: State of MCMCPosterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_posterior_sampler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;slice_np_vectorized&#39;</span><span class="p">,</span> <span class="n">thin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">init_strategy</span><span class="o">=</span><span class="s1">&#39;resample&#39;</span><span class="p">,</span> <span class="n">init_strategy_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy_num_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mp_context</span><span class="o">=</span><span class="s1">&#39;spawn&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>potential_fn</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, <span title="sbi.inference.potentials.base_potential.BasePotential">BasePotential</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The potential function from which to draw samples. Must be a
<code>BasePotential</code> or a <code>Callable</code> which takes <code>theta</code> and <code>x_o</code> as inputs.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proposal</code>
            </td>
            <td>
                  <code><span title="typing.Any">Any</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Proposal distribution that is used to initialize the MCMC chain.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>theta_transform</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchTransform">TorchTransform</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Transformation that will be applied during sampling.
Allows to perform MCMC in unconstrained space.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>method</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method used for MCMC sampling, one of <code>slice_np</code>,
<code>slice_np_vectorized</code>, <code>hmc_pyro</code>, <code>nuts_pyro</code>, <code>slice_pymc</code>,
<code>hmc_pymc</code>, <code>nuts_pymc</code>. <code>slice_np</code> is a custom
numpy implementation of slice sampling. <code>slice_np_vectorized</code> is
identical to <code>slice_np</code>, but if <code>num_chains&gt;1</code>, the chains are
vectorized for <code>slice_np_vectorized</code> whereas they are run sequentially
for <code>slice_np</code>. The samplers ending on <code>_pyro</code> are using Pyro, and
likewise the samplers ending on <code>_pymc</code> are using PyMC.</p>
              </div>
            </td>
            <td>
                  <code>&#39;slice_np_vectorized&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>thin</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The thinning factor for the chain, default 1 (no thinning).</p>
              </div>
            </td>
            <td>
                  <code>-1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>warmup_steps</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The initial number of samples to discard.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_chains</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of chains. Should generally be at most
<code>num_workers - 1</code>.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_strategy</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The initialisation strategy for chains; <code>proposal</code> will draw
init locations from <code>proposal</code>, whereas <code>sir</code> will use Sequential-
Importance-Resampling (SIR). SIR initially samples
<code>init_strategy_num_candidates</code> from the <code>proposal</code>, evaluates all of
them under the <code>potential_fn</code> and <code>proposal</code>, and then resamples the
initial locations with weights proportional to <code>exp(potential_fn -
proposal.log_prob</code>. <code>resample</code> is the same as <code>sir</code> but
uses <code>exp(potential_fn)</code> as weights.</p>
              </div>
            </td>
            <td>
                  <code>&#39;resample&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_strategy_parameters</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dictionary of keyword arguments passed to the
init strategy, e.g., for <code>init_strategy=sir</code> this could be
<code>num_candidate_samples</code>, i.e., the number of candidates to find init
locations (internal default is <code>1000</code>), or <code>device</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_strategy_num_candidates</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of candidates to find init
 locations in <code>init_strategy=sir</code> (deprecated, use
 init_strategy_parameters instead).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_workers</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of cpu cores used to parallelize mcmc</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mp_context</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiprocessing start method, either <code>"fork"</code> or <code>"spawn"</code>
(default), used by Pyro and PyMC samplers. <code>"fork"</code> can be significantly
faster than <code>"spawn"</code> but is only supported on POSIX-based systems
(e.g. Linux and macOS, not Windows).</p>
              </div>
            </td>
            <td>
                  <code>&#39;spawn&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x_shape</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Size">Size</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated, should not be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BasePotential</span><span class="p">],</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">,</span>
    <span class="n">thin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_chains</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">init_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;resample&quot;</span><span class="p">,</span>
    <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">mp_context</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;spawn&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples. Must be a</span>
<span class="sd">            `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.</span>
<span class="sd">        proposal: Proposal distribution that is used to initialize the MCMC chain.</span>
<span class="sd">        theta_transform: Transformation that will be applied during sampling.</span>
<span class="sd">            Allows to perform MCMC in unconstrained space.</span>
<span class="sd">        method: Method used for MCMC sampling, one of `slice_np`,</span>
<span class="sd">            `slice_np_vectorized`, `hmc_pyro`, `nuts_pyro`, `slice_pymc`,</span>
<span class="sd">            `hmc_pymc`, `nuts_pymc`. `slice_np` is a custom</span>
<span class="sd">            numpy implementation of slice sampling. `slice_np_vectorized` is</span>
<span class="sd">            identical to `slice_np`, but if `num_chains&gt;1`, the chains are</span>
<span class="sd">            vectorized for `slice_np_vectorized` whereas they are run sequentially</span>
<span class="sd">            for `slice_np`. The samplers ending on `_pyro` are using Pyro, and</span>
<span class="sd">            likewise the samplers ending on `_pymc` are using PyMC.</span>
<span class="sd">        thin: The thinning factor for the chain, default 1 (no thinning).</span>
<span class="sd">        warmup_steps: The initial number of samples to discard.</span>
<span class="sd">        num_chains: The number of chains. Should generally be at most</span>
<span class="sd">            `num_workers - 1`.</span>
<span class="sd">        init_strategy: The initialisation strategy for chains; `proposal` will draw</span>
<span class="sd">            init locations from `proposal`, whereas `sir` will use Sequential-</span>
<span class="sd">            Importance-Resampling (SIR). SIR initially samples</span>
<span class="sd">            `init_strategy_num_candidates` from the `proposal`, evaluates all of</span>
<span class="sd">            them under the `potential_fn` and `proposal`, and then resamples the</span>
<span class="sd">            initial locations with weights proportional to `exp(potential_fn -</span>
<span class="sd">            proposal.log_prob`. `resample` is the same as `sir` but</span>
<span class="sd">            uses `exp(potential_fn)` as weights.</span>
<span class="sd">        init_strategy_parameters: Dictionary of keyword arguments passed to the</span>
<span class="sd">            init strategy, e.g., for `init_strategy=sir` this could be</span>
<span class="sd">            `num_candidate_samples`, i.e., the number of candidates to find init</span>
<span class="sd">            locations (internal default is `1000`), or `device`.</span>
<span class="sd">        init_strategy_num_candidates: Number of candidates to find init</span>
<span class="sd">             locations in `init_strategy=sir` (deprecated, use</span>
<span class="sd">             init_strategy_parameters instead).</span>
<span class="sd">        num_workers: number of cpu cores used to parallelize mcmc</span>
<span class="sd">        mp_context: Multiprocessing start method, either `&quot;fork&quot;` or `&quot;spawn&quot;`</span>
<span class="sd">            (default), used by Pyro and PyMC samplers. `&quot;fork&quot;` can be significantly</span>
<span class="sd">            faster than `&quot;spawn&quot;` but is only supported on POSIX-based systems</span>
<span class="sd">            (e.g. Linux and macOS, not Windows).</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Deprecated, should not be passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice&quot;</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The Pyro-based slice sampler is deprecated, and the method `slice` &quot;</span>
            <span class="s2">&quot;has been changed to `slice_np`, i.e., the custom &quot;</span>
            <span class="s2">&quot;numpy-based slice sampler.&quot;</span><span class="p">,</span>
            <span class="ne">DeprecationWarning</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">method</span> <span class="o">=</span> <span class="s2">&quot;slice_np&quot;</span>

    <span class="n">thin</span> <span class="o">=</span> <span class="n">_process_thin_default</span><span class="p">(</span><span class="n">thin</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="o">=</span> <span class="n">thin</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="o">=</span> <span class="n">num_chains</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="o">=</span> <span class="n">init_strategy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="n">init_strategy_parameters</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mp_context</span> <span class="o">=</span> <span class="n">mp_context</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Hardcode parameter name to reduce clutter kwargs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">param_name</span> <span class="o">=</span> <span class="s2">&quot;theta&quot;</span>

    <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Passing `init_strategy_num_candidates` is deprecated as of sbi &quot;</span>
            <span class="s2">&quot;v0.19.0. Instead, use e.g., `init_strategy_parameters &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;=</span><span class="si">{</span><span class="s1">&#39;num_candidate_samples&#39;</span><span class="si">:</span><span class="s2"> 1000</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span><span class="p">[</span><span class="s2">&quot;num_candidate_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">init_strategy_num_candidates</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides MCMC to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.get_arviz_inference_data" class="doc doc-heading">
            <code class=" language-python"><span class="n">get_arviz_inference_data</span><span class="p">()</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.get_arviz_inference_data" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns arviz InferenceData object constructed most recent samples.</p>
<p>Note: the InferenceData is constructed using the posterior samples generated in
most recent call to <code>.sample(...)</code>.</p>
<p>For Pyro and PyMC samplers, InferenceData will contain diagnostics, but for
sbi slice samplers, only the samples are added.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>inference_data</code></td>            <td>
                  <code><span title="arviz.data.InferenceData">InferenceData</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Arviz InferenceData object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_arviz_inference_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InferenceData</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns arviz InferenceData object constructed most recent samples.</span>

<span class="sd">    Note: the InferenceData is constructed using the posterior samples generated in</span>
<span class="sd">    most recent call to `.sample(...)`.</span>

<span class="sd">    For Pyro and PyMC samplers, InferenceData will contain diagnostics, but for</span>
<span class="sd">    sbi slice samplers, only the samples are added.</span>

<span class="sd">    Returns:</span>
<span class="sd">        inference_data: Arviz InferenceData object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">&quot;&quot;&quot;No samples have been generated, call .sample() first.&quot;&quot;&quot;</span>

    <span class="n">sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">MCMC</span><span class="p">,</span> <span class="n">SliceSamplerSerial</span><span class="p">,</span> <span class="n">SliceSamplerVectorized</span><span class="p">,</span> <span class="n">PyMCSampler</span>
    <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_sampler</span>

    <span class="c1"># If Pyro sampler and samples not transformed, use arviz&#39; from_pyro.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="p">(</span><span class="n">HMC</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">IndependentTransform</span>
    <span class="p">):</span>
        <span class="n">inference_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pyro</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>
    <span class="c1"># If PyMC sampler and samples not transformed, get cached InferenceData.</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">PyMCSampler</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="p">,</span> <span class="n">torch_tf</span><span class="o">.</span><span class="n">IndependentTransform</span>
    <span class="p">):</span>
        <span class="n">inference_data</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_inference_data</span><span class="p">()</span>

    <span class="c1"># otherwise get samples from sampler and transform to original space.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_samples</span><span class="p">(</span><span class="n">group_by_chain</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Pyro samplers returns dicts, get values.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="c1"># popitem gets last items, [1] get the values as tensor.</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">transformed_samples</span><span class="o">.</span><span class="n">popitem</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Our slice samplers return numpy arrays.</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">)</span>
        <span class="c1"># For MultipleIndependent priors transforms first dim must be batch dim.</span>
        <span class="c1"># thus, reshape back and forth to have batch dim in front.</span>
        <span class="n">samples_shape</span> <span class="o">=</span> <span class="n">transformed_samples</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="n">transformed_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">samples_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="o">*</span><span class="n">samples_shape</span>
        <span class="p">)</span>

        <span class="n">inference_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">convert_to_inference_data</span><span class="p">({</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">samples</span>
        <span class="p">})</span>

    <span class="k">return</span> <span class="n">inference_data</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.log_prob" class="doc doc-heading">
            <code class=" language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the log-probability of theta under the posterior.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>theta</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters <span class="arithmatex">\(\theta\)</span>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>track_gradients</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>len($\theta$)</code>-shaped log-probability.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;`.log_prob()` is deprecated for methods that can only evaluate the &quot;</span>
        <span class="s2">&quot;log-probability up to a normalizing constant. Use `.potential()` instead.&quot;</span><span class="p">,</span>
        <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.map" class="doc doc-heading">
            <code class=" language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.map" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iter</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of optimization steps that the algorithm takes
to find the MAP.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate of the optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.01</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_method</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p>
              </div>
            </td>
            <td>
                  <code>&#39;proposal&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_init_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_to_optimize</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p>
              </div>
            </td>
            <td>
                  <code>100</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_best_every</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during sampling from
the posterior.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_update</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>log_prob_kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The MAP estimate.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span>
<span class="normal">978</span>
<span class="normal">979</span>
<span class="normal">980</span>
<span class="normal">981</span>
<span class="normal">982</span>
<span class="normal">983</span>
<span class="normal">984</span>
<span class="normal">985</span>
<span class="normal">986</span>
<span class="normal">987</span>
<span class="normal">988</span>
<span class="normal">989</span>
<span class="normal">990</span>
<span class="normal">991</span>
<span class="normal">992</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy_num_candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mcmc_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mp_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return samples from posterior distribution <span class="arithmatex">\(p(\theta|x)\)</span> with MCMC.</p>
<p>Check the <code>__init__()</code> method for a description of all arguments as well as
their default values.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>()</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mcmc_parameters</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dictionary that is passed only to support the API of
<code>sbi</code> v0.17.2 or older.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mcmc_method</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. Please use <code>method</code> instead.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_with</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show sampling progress monitor.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Samples from posterior.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">thin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy_num_candidates</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mcmc_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mp_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$ with MCMC.</span>

<span class="sd">    Check the `__init__()` method for a description of all arguments as well as</span>
<span class="sd">    their default values.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        mcmc_parameters: Dictionary that is passed only to support the API of</span>
<span class="sd">            `sbi` v0.17.2 or older.</span>
<span class="sd">        mcmc_method: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. Please use `method` instead.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># Replace arguments that were not passed with their default.</span>
    <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>
    <span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">thin</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="k">if</span> <span class="n">warmup_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">warmup_steps</span>
    <span class="n">num_chains</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
    <span class="n">init_strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="k">if</span> <span class="n">init_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">init_strategy</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="k">if</span> <span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_workers</span>
    <span class="n">mp_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mp_context</span> <span class="k">if</span> <span class="n">mp_context</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mp_context</span>
    <span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span>
        <span class="k">if</span> <span class="n">init_strategy_parameters</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">init_strategy_parameters</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">init_strategy_num_candidates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Passing `init_strategy_num_candidates` is deprecated as of sbi &quot;</span>
            <span class="s2">&quot;v0.19.0. Instead, use e.g., &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`init_strategy_parameters=</span><span class="si">{</span><span class="s1">&#39;num_candidate_samples&#39;</span><span class="si">:</span><span class="s2"> 1000</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span><span class="p">[</span><span class="s2">&quot;num_candidate_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">init_strategy_num_candidates</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">mcmc_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
            <span class="s2">&quot;is deprecated and will be removed in a future release. Use `method` &quot;</span>
            <span class="s2">&quot;instead of `mcmc_method`.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">mcmc_method</span>
    <span class="k">if</span> <span class="n">mcmc_parameters</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this &quot;</span>
            <span class="s2">&quot;is deprecated and will be removed in a future release. Instead, pass &quot;</span>
            <span class="s2">&quot;the variable to `.sample()` directly, e.g. &quot;</span>
            <span class="s2">&quot;`posterior.sample((1,), num_chains=5)`.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="c1"># The following lines are only for backwards compatibility with sbi v0.17.2 or</span>
    <span class="c1"># older.</span>
    <span class="n">m_p</span> <span class="o">=</span> <span class="n">mcmc_parameters</span> <span class="ow">or</span> <span class="p">{}</span>  <span class="c1"># define to shorten the variable name</span>
    <span class="n">method</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="s2">&quot;mcmc_method&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">thin</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">thin</span><span class="p">,</span> <span class="s2">&quot;thin&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="s2">&quot;warmup_steps&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">num_chains</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="s2">&quot;num_chains&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="n">init_strategy</span> <span class="o">=</span> <span class="n">_maybe_use_dict_entry</span><span class="p">(</span><span class="n">init_strategy</span><span class="p">,</span> <span class="s2">&quot;init_strategy&quot;</span><span class="p">,</span> <span class="n">m_p</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="n">initial_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_params</span><span class="p">(</span>
        <span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">num_chains</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">num_workers</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="o">**</span><span class="n">init_strategy_parameters</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="n">track_gradients</span> <span class="o">=</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc_pyro&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pyro&quot;</span><span class="p">,</span> <span class="s2">&quot;hmc_pymc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pymc&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;slice_np&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_np_mcmc</span><span class="p">(</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">vectorized</span><span class="o">=</span><span class="p">(</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">),</span>
                <span class="n">interchangeable_chains</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc_pyro&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pyro&quot;</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pyro_mcmc</span><span class="p">(</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                <span class="n">mcmc_method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="n">mp_context</span><span class="o">=</span><span class="n">mp_context</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;hmc_pymc&quot;</span><span class="p">,</span> <span class="s2">&quot;nuts_pymc&quot;</span><span class="p">,</span> <span class="s2">&quot;slice_pymc&quot;</span><span class="p">):</span>
            <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pymc_mcmc</span><span class="p">(</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
                <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
                <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
                <span class="n">mcmc_method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">num_chains</span><span class="o">=</span><span class="n">num_chains</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="n">mp_context</span><span class="o">=</span><span class="n">mp_context</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The sampling method </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2"> is not implemented!&quot;</span><span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span>
    <span class="c1"># NOTE: Currently MCMCPosteriors will require a single dimension for the</span>
    <span class="c1"># parameter dimension. With recent ConditionalDensity(Ratio) estimators, we</span>
    <span class="c1"># can have multiple dimensions for the parameter dimension.</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># type: ignore</span>

    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample_batched" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample_batched</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_strategy_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mp_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.sample_batched" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Given a batch of observations [x_1, &hellip;, x_B] this function samples from
posteriors <span class="arithmatex">\(p(\theta|x_1)\)</span>, &hellip; ,<span class="arithmatex">\(p(\theta|x_B)\)</span>, in a batched (i.e. vectorized)
manner.</p>
<p>Check the <code>__init__()</code> method for a description of all arguments as well as
their default values.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Desired shape of samples that are drawn from the posterior
given every observation.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A batch of observations, of shape <code>(batch_dim, event_shape_x)</code>.
<code>batch_dim</code> corresponds to the number of observations to be
drawn.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>method</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method used for MCMC sampling, e.g., &ldquo;slice_np_vectorized&rdquo;.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>thin</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The thinning factor for the chain, default 1 (no thinning).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>warmup_steps</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The initial number of samples to discard.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_chains</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of chains used for each <code>x</code> passed in the batch.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_strategy</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The initialisation strategy for chains.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_strategy_parameters</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dictionary of keyword arguments passed to
the init strategy.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_workers</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of cpu cores used to parallelize initial
parameter generation and mcmc sampling.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mp_context</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiprocessing start method, either <code>"fork"</code> or <code>"spawn"</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show sampling progress monitor.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Samples from the posteriors of shape (*sample_shape, B, *input_shape)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample_batched</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">thin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_chains</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_strategy_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mp_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given a batch of observations [x_1, ..., x_B] this function samples from</span>
<span class="sd">    posteriors $p(\theta|x_1)$, ... ,$p(\theta|x_B)$, in a batched (i.e. vectorized)</span>
<span class="sd">    manner.</span>

<span class="sd">    Check the `__init__()` method for a description of all arguments as well as</span>
<span class="sd">    their default values.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from the posterior</span>
<span class="sd">            given every observation.</span>
<span class="sd">        x: A batch of observations, of shape `(batch_dim, event_shape_x)`.</span>
<span class="sd">            `batch_dim` corresponds to the number of observations to be</span>
<span class="sd">            drawn.</span>
<span class="sd">        method: Method used for MCMC sampling, e.g., &quot;slice_np_vectorized&quot;.</span>
<span class="sd">        thin: The thinning factor for the chain, default 1 (no thinning).</span>
<span class="sd">        warmup_steps: The initial number of samples to discard.</span>
<span class="sd">        num_chains: The number of chains used for each `x` passed in the batch.</span>
<span class="sd">        init_strategy: The initialisation strategy for chains.</span>
<span class="sd">        init_strategy_parameters: Dictionary of keyword arguments passed to</span>
<span class="sd">            the init strategy.</span>
<span class="sd">        num_workers: number of cpu cores used to parallelize initial</span>
<span class="sd">            parameter generation and mcmc sampling.</span>
<span class="sd">        mp_context: Multiprocessing start method, either `&quot;fork&quot;` or `&quot;spawn&quot;`</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from the posteriors of shape (*sample_shape, B, *input_shape)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Replace arguments that were not passed with their default.</span>
    <span class="n">method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">method</span>
    <span class="n">thin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">thin</span> <span class="k">if</span> <span class="n">thin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">thin</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="k">if</span> <span class="n">warmup_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">warmup_steps</span>
    <span class="n">num_chains</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chains</span> <span class="k">if</span> <span class="n">num_chains</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_chains</span>
    <span class="n">init_strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy</span> <span class="k">if</span> <span class="n">init_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">init_strategy</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="k">if</span> <span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_workers</span>
    <span class="n">mp_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mp_context</span> <span class="k">if</span> <span class="n">mp_context</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mp_context</span>
    <span class="n">init_strategy_parameters</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_strategy_parameters</span>
        <span class="k">if</span> <span class="n">init_strategy_parameters</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">init_strategy_parameters</span>
    <span class="p">)</span>

    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span>
    <span class="p">),</span> <span class="s2">&quot;Batched sampling only supported for vectorized samplers!&quot;</span>

    <span class="c1"># warn if num_chains is larger than num requested samples</span>
    <span class="k">if</span> <span class="n">num_chains</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The passed number of MCMC chains is larger than the number of &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;requested samples: </span><span class="si">{</span><span class="n">num_chains</span><span class="si">}</span><span class="s2"> &gt; </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; resetting it to </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">num_chains</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="c1"># custom shape handling to make sure to match the batch size of x and theta</span>
    <span class="c1"># without unnecessary combinations.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

    <span class="c1"># For batched sampling, we want `num_chains` for each observation in the batch.</span>
    <span class="c1"># Here we repeat the observations ABC -&gt; AAABBBCCC, so that the chains are</span>
    <span class="c1"># in the order of the observations.</span>
    <span class="n">x_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_chains</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">x_is_iid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_potential</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="c1"># For each observation in the batch, we have num_chains independent chains.</span>
    <span class="n">num_chains_extended</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_chains</span>
    <span class="k">if</span> <span class="n">num_chains_extended</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Note that for batched sampling, we use num_chains many chains for each&quot;</span>
            <span class="s2">&quot; x in the batch. With the given settings, this results in a large &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;number large number of chains (</span><span class="si">{</span><span class="n">num_chains_extended</span><span class="si">}</span><span class="s2">), which can be &quot;</span>
            <span class="s2">&quot;slow and memory-intensive for vectorized MCMC. Consider reducing the &quot;</span>
            <span class="s2">&quot;number of chains.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">init_strategy_parameters</span><span class="p">[</span><span class="s2">&quot;num_return_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_chains_extended</span>
    <span class="n">initial_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_params_batched</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">init_strategy</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">num_chains</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">num_workers</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="o">**</span><span class="n">init_strategy_parameters</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># We need num_samples from each posterior in the batch</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">batch_size</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">transformed_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_slice_np_mcmc</span><span class="p">(</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">potential_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_</span><span class="p">,</span>
            <span class="n">initial_params</span><span class="o">=</span><span class="n">initial_params</span><span class="p">,</span>
            <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">vectorized</span><span class="o">=</span><span class="p">(</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;slice_np_vectorized&quot;</span><span class="p">),</span>
            <span class="n">interchangeable_chains</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># (num_chains_extended, samples_per_chain, *input_shape)</span>
    <span class="n">samples_per_chain</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">transformed_samples</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
    <span class="n">dim_theta</span> <span class="o">=</span> <span class="n">samples_per_chain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># We need to collect samples for each x from the respective chains.</span>
    <span class="c1"># However, using samples.reshape(*sample_shape, batch_size, dim_theta)</span>
    <span class="c1"># does not combine the samples in the right order, since this mixes</span>
    <span class="c1"># samples that belong to different `x`. The following permute is a</span>
    <span class="c1"># workaround to reshape the samples in the right order.</span>
    <span class="n">samples_per_x</span> <span class="o">=</span> <span class="n">samples_per_chain</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="c1"># We are flattening the sample shape here using -1 because we might have</span>
        <span class="c1"># generated more samples than requested (more chains, or multiple of</span>
        <span class="c1"># chains not matching sample_shape)</span>
        <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">dim_theta</span><span class="p">,</span>
    <span class="p">))</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Shape is now (-1, batch_size, dim_theta)</span>
    <span class="c1"># We can now select the number of requested samples</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">samples_per_x</span><span class="p">[:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()]</span>
    <span class="c1"># and reshape into (*sample_shape, batch_size, dim_theta)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim_theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_mcmc_method" class="doc doc-heading">
            <code class=" language-python"><span class="n">set_mcmc_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.mcmc_posterior.MCMCPosterior.set_mcmc_method" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Sets sampling method to for MCMC and returns <code>NeuralPosterior</code>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>method</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>NeuralPosterior</code> for chainable calls.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/mcmc_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_mcmc_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NeuralPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets sampling method to for MCMC and returns `NeuralPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: Method to use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `NeuralPosterior` for chainable calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mcmc_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="doc doc-heading">
            <code>RejectionPosterior</code>


<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>


        <p>Provides rejection sampling to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>RejectionPosterior</code> allows to sample from the posterior with rejection sampling.</p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">RejectionPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides rejection sampling to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `RejectionPosterior` allows to sample from the posterior with rejection sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BasePotential</span><span class="p">],</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples. Must be a</span>
<span class="sd">                `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.</span>
<span class="sd">            proposal: The proposal distribution.</span>
<span class="sd">            theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">                during but only when calling `.map()`.</span>
<span class="sd">            max_sampling_batch_size: The batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration.</span>
<span class="sd">            num_samples_to_find_max: The number of samples that are used to find the</span>
<span class="sd">                maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">            num_iter_to_find_max: The number of gradient ascent iterations to find the</span>
<span class="sd">                maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">            m: Multiplier to the `potential_fn / proposal` ratio.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            x_shape: Deprecated, should not be passed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="n">num_samples_to_find_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="n">num_iter_to_find_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides rejection sampling to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`.log_prob()` is deprecated for methods that can only evaluate the &quot;</span>
            <span class="s2">&quot;log-probability up to a normalizing constant. Use `.potential()` instead.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior $p(\theta|x)$ via rejection sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">                sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">                samples and then reshape into the desired shape.</span>
<span class="sd">            sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">                `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">            show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Replace arguments that were not passed with their default.</span>
        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>
        <span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span>
            <span class="k">if</span> <span class="n">num_samples_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">num_samples_to_find_max</span>
        <span class="p">)</span>
        <span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span>
            <span class="k">if</span> <span class="n">num_iter_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">num_iter_to_find_max</span>
        <span class="p">)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">m</span>

        <span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rejection_sample</span><span class="p">(</span>
            <span class="n">potential</span><span class="p">,</span>
            <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">warn_acceptance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="n">num_samples_to_find_max</span><span class="p">,</span>
            <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="n">num_iter_to_find_max</span><span class="p">,</span>
            <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">sample_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Batched sampling is not implemented for RejectionPosterior. </span><span class="se">\</span>
<span class="s2">            Alternatively you can use `sample` in a loop </span><span class="se">\</span>
<span class="s2">            [posterior.sample(theta, x_o) for x_o in x].&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>potential_fn</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, <span title="sbi.inference.potentials.base_potential.BasePotential">BasePotential</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The potential function from which to draw samples. Must be a
<code>BasePotential</code> or a <code>Callable</code> which takes <code>theta</code> and <code>x_o</code> as inputs.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proposal</code>
            </td>
            <td>
                  <code><span title="typing.Any">Any</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The proposal distribution.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>theta_transform</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchTransform">TorchTransform</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Transformation that is applied to parameters. Is not used
during but only when calling <code>.map()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_sampling_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The batchsize of samples being drawn from
the proposal at every iteration.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_samples_to_find_max</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of samples that are used to find the
maximum of the <code>potential_fn / proposal</code> ratio.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iter_to_find_max</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of gradient ascent iterations to find the
maximum of the <code>potential_fn / proposal</code> ratio.</p>
              </div>
            </td>
            <td>
                  <code>100</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>m</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiplier to the <code>potential_fn / proposal</code> ratio.</p>
              </div>
            </td>
            <td>
                  <code>1.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x_shape</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Size">Size</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated, should not be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BasePotential</span><span class="p">],</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">m</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples. Must be a</span>
<span class="sd">            `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.</span>
<span class="sd">        proposal: The proposal distribution.</span>
<span class="sd">        theta_transform: Transformation that is applied to parameters. Is not used</span>
<span class="sd">            during but only when calling `.map()`.</span>
<span class="sd">        max_sampling_batch_size: The batchsize of samples being drawn from</span>
<span class="sd">            the proposal at every iteration.</span>
<span class="sd">        num_samples_to_find_max: The number of samples that are used to find the</span>
<span class="sd">            maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">        num_iter_to_find_max: The number of gradient ascent iterations to find the</span>
<span class="sd">            maximum of the `potential_fn / proposal` ratio.</span>
<span class="sd">        m: Multiplier to the `potential_fn / proposal` ratio.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        x_shape: Deprecated, should not be passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="n">proposal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="n">num_samples_to_find_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="n">num_iter_to_find_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides rejection sampling to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _unnormalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.log_prob" class="doc doc-heading">
            <code class=" language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the log-probability of theta under the posterior.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>theta</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters <span class="arithmatex">\(\theta\)</span>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>track_gradients</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>len($\theta$)</code>-shaped log-probability.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;`.log_prob()` is deprecated for methods that can only evaluate the &quot;</span>
        <span class="s2">&quot;log-probability up to a normalizing constant. Use `.potential()` instead.&quot;</span><span class="p">,</span>
        <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The log-probability is unnormalized!&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.map" class="doc doc-heading">
            <code class=" language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.map" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iter</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of optimization steps that the algorithm takes
to find the MAP.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate of the optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.01</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_method</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p>
              </div>
            </td>
            <td>
                  <code>&#39;proposal&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_init_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_to_optimize</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p>
              </div>
            </td>
            <td>
                  <code>100</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_best_every</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during sampling from
the posterior.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_update</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>log_prob_kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The MAP estimate.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.rejection_posterior.RejectionPosterior.sample" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.rejection_posterior.RejectionPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return samples from posterior <span class="arithmatex">\(p(\theta|x)\)</span> via rejection sampling.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Desired shape of samples that are drawn from posterior. If
sample_shape is multidimensional we simply draw <code>sample_shape.numel()</code>
samples and then reshape into the desired shape.</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>()</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_with</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This argument only exists to keep backward-compatibility with
<code>sbi</code> v0.17.2 or older. If it is set, we instantly raise an error.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show sampling progress monitor.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Samples from posterior.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/rejection_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_samples_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter_to_find_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">m</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior $p(\theta|x)$ via rejection sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Desired shape of samples that are drawn from posterior. If</span>
<span class="sd">            sample_shape is multidimensional we simply draw `sample_shape.numel()`</span>
<span class="sd">            samples and then reshape into the desired shape.</span>
<span class="sd">        sample_with: This argument only exists to keep backward-compatibility with</span>
<span class="sd">            `sbi` v0.17.2 or older. If it is set, we instantly raise an error.</span>
<span class="sd">        show_progress_bars: Whether to show sampling progress monitor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">potential</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>
    <span class="c1"># Replace arguments that were not passed with their default.</span>
    <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
        <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
    <span class="p">)</span>
    <span class="n">num_samples_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples_to_find_max</span>
        <span class="k">if</span> <span class="n">num_samples_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">num_samples_to_find_max</span>
    <span class="p">)</span>
    <span class="n">num_iter_to_find_max</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_iter_to_find_max</span>
        <span class="k">if</span> <span class="n">num_iter_to_find_max</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">num_iter_to_find_max</span>
    <span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">m</span>

    <span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rejection_sample</span><span class="p">(</span>
        <span class="n">potential</span><span class="p">,</span>
        <span class="n">proposal</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proposal</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">warn_acceptance</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
        <span class="n">num_samples_to_find_max</span><span class="o">=</span><span class="n">num_samples_to_find_max</span><span class="p">,</span>
        <span class="n">num_iter_to_find_max</span><span class="o">=</span><span class="n">num_iter_to_find_max</span><span class="p">,</span>
        <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.posteriors.score_posterior.ScorePosterior" class="doc doc-heading">
            <code>ScorePosterior</code>


<a href="#sbi.inference.posteriors.score_posterior.ScorePosterior" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>


        <p>Posterior <span class="arithmatex">\(p(\theta|x_o)\)</span> with <code>log_prob()</code> and <code>sample()</code> methods. It samples
from the diffusion model given the score_estimator and rejects samples that lie
outside of the prior bounds.</p>
<p>The posterior is defined by a score estimator and a prior. The score estimator
provides the gradient of the log-posterior with respect to the parameters. The prior
is used to reject samples that lie outside of the prior bounds.</p>
<p>Sampling is done by running a diffusion process with a predictor and optionally a
corrector.</p>
<p>Log probabilities are obtained by calling the potential function, which in turn uses
zuko probabilistic ODEs to compute the log-probability.</p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/posteriors/score_posterior.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ScorePosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Posterior $p(\theta|x_o)$ with `log_prob()` and `sample()` methods. It samples</span>
<span class="sd">    from the diffusion model given the score_estimator and rejects samples that lie</span>
<span class="sd">    outside of the prior bounds.</span>

<span class="sd">    The posterior is defined by a score estimator and a prior. The score estimator</span>
<span class="sd">    provides the gradient of the log-posterior with respect to the parameters. The prior</span>
<span class="sd">    is used to reject samples that lie outside of the prior bounds.</span>

<span class="sd">    Sampling is done by running a diffusion process with a predictor and optionally a</span>
<span class="sd">    corrector.</span>

<span class="sd">    Log probabilities are obtained by calling the potential function, which in turn uses</span>
<span class="sd">    zuko probabilistic ODEs to compute the log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">score_estimator</span><span class="p">:</span> <span class="n">ConditionalScoreEstimator</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sde&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">            score_estimator: The trained neural score estimator.</span>
<span class="sd">            max_sampling_batch_size: Batchsize of samples being drawn from</span>
<span class="sd">                the proposal at every iteration.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">                `potential_fn.device` is used.</span>
<span class="sd">            enable_transform: Whether to transform parameters to unconstrained space</span>
<span class="sd">                during MAP optimization. When False, an identity transform will be</span>
<span class="sd">                returned for `theta_transform`. True is not supported yet.</span>
<span class="sd">            sample_with: Whether to sample from the posterior using the ODE-based</span>
<span class="sd">                sampler or the SDE-based sampler.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
        <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">score_estimator_based_potential</span><span class="p">(</span>
            <span class="n">score_estimator</span><span class="p">,</span>
            <span class="n">prior</span><span class="p">,</span>
            <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Set the potential function type.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">:</span> <span class="n">PosteriorScoreBasedPotential</span> <span class="o">=</span> <span class="n">potential_fn</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span> <span class="o">=</span> <span class="n">score_estimator</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span> <span class="o">=</span> <span class="n">sample_with</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;ode&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sde&quot;</span><span class="p">,</span>
        <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;sample_with must be &#39;ode&#39; or &#39;sde&#39;, but is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;It samples from the diffusion model given the </span><span class="se">\</span>
<span class="s2">            score_estimator.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">predictor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Predictor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;euler_maruyama&quot;</span><span class="p">,</span>
        <span class="n">corrector</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Corrector</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">predictor_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">corrector_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
        <span class="n">ts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Shape of the samples to be drawn.</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.sample()`.</span>
<span class="sd">            predictor: The predictor for the diffusion-based sampler. Can be a string or</span>
<span class="sd">                a custom predictor following the API in `sbi.samplers.score.predictors`.</span>
<span class="sd">                Currently, only `euler_maruyama` is implemented.</span>
<span class="sd">            corrector: The corrector for the diffusion-based sampler. Either of</span>
<span class="sd">                [None].</span>
<span class="sd">            predictor_params: Additional parameters passed to predictor.</span>
<span class="sd">            corrector_params: Additional parameters passed to corrector.</span>
<span class="sd">            steps: Number of steps to take for the Euler-Maruyama method.</span>
<span class="sd">            ts: Time points at which to evaluate the diffusion process. If None, a</span>
<span class="sd">                linear grid between t_max and t_min is used.</span>
<span class="sd">            max_sampling_batch_size: Maximum batch size for sampling.</span>
<span class="sd">            sample_with: Deprecated - use `.build_posterior(sample_with=...)` prior to</span>
<span class="sd">                `.sample()`.</span>
<span class="sd">            show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
            <span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">condition_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;ode&quot;</span><span class="p">:</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_via_zuko</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;sde&quot;</span><span class="p">:</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_via_diffusion</span><span class="p">(</span>
                <span class="n">sample_shape</span><span class="o">=</span><span class="n">sample_shape</span><span class="p">,</span>
                <span class="n">predictor</span><span class="o">=</span><span class="n">predictor</span><span class="p">,</span>
                <span class="n">corrector</span><span class="o">=</span><span class="n">corrector</span><span class="p">,</span>
                <span class="n">predictor_params</span><span class="o">=</span><span class="n">predictor_params</span><span class="p">,</span>
                <span class="n">corrector_params</span><span class="o">=</span><span class="n">corrector_params</span><span class="p">,</span>
                <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span>
                <span class="n">ts</span><span class="o">=</span><span class="n">ts</span><span class="p">,</span>
                <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
                <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span>

    <span class="k">def</span> <span class="nf">_sample_via_diffusion</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">predictor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Predictor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;euler_maruyama&quot;</span><span class="p">,</span>
        <span class="n">corrector</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Corrector</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">predictor_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">corrector_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
        <span class="n">ts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Shape of the samples to be drawn.</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.sample()`.</span>
<span class="sd">            predictor: The predictor for the diffusion-based sampler. Can be a string or</span>
<span class="sd">                a custom predictor following the API in `sbi.samplers.score.predictors`.</span>
<span class="sd">                Currently, only `euler_maruyama` is implemented.</span>
<span class="sd">            corrector: The corrector for the diffusion-based sampler. Either of</span>
<span class="sd">                [None].</span>
<span class="sd">            steps: Number of steps to take for the Euler-Maruyama method.</span>
<span class="sd">            ts: Time points at which to evaluate the diffusion process. If None, a</span>
<span class="sd">                linear grid between t_max and t_min is used.</span>
<span class="sd">            max_sampling_batch_size: Maximum batch size for sampling.</span>
<span class="sd">            sample_with: Deprecated - use `.build_posterior(sample_with=...)` prior to</span>
<span class="sd">                `.sample()`.</span>
<span class="sd">            show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span>
            <span class="k">if</span> <span class="n">max_sampling_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">max_sampling_batch_size</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">ts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">t_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">t_max</span>
            <span class="n">t_min</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">t_min</span>
            <span class="n">ts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">t_max</span><span class="p">,</span> <span class="n">t_min</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>

        <span class="n">diffuser</span> <span class="o">=</span> <span class="n">Diffuser</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="n">predictor</span><span class="o">=</span><span class="n">predictor</span><span class="p">,</span>
            <span class="n">corrector</span><span class="o">=</span><span class="n">corrector</span><span class="p">,</span>
            <span class="n">predictor_params</span><span class="o">=</span><span class="n">predictor_params</span><span class="p">,</span>
            <span class="n">corrector_params</span><span class="o">=</span><span class="n">corrector_params</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_sampling_batch_size</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">num_iter</span> <span class="o">=</span> <span class="n">num_samples</span> <span class="o">//</span> <span class="n">max_sampling_batch_size</span>
        <span class="n">num_iter</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">num_iter</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="p">(</span><span class="n">num_samples</span> <span class="o">%</span> <span class="n">max_sampling_batch_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">num_iter</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">diffuser</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                    <span class="n">num_samples</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
                    <span class="n">ts</span><span class="o">=</span><span class="n">ts</span><span class="p">,</span>
                    <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[:</span><span class="n">num_samples</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sample_shape</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample_via_zuko</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution with probability flow ODE.</span>

<span class="sd">        This build the probability flow ODE and then samples from the corresponding</span>
<span class="sd">        flow. This is implemented via the zuko library.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Condition.</span>
<span class="sd">            sample_shape: The shape of the samples to be returned.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="n">flow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">get_continuous_normalizing_flow</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,)))</span>

        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sample_shape</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">exact</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of the posterior $p(\theta|x)$.</span>

<span class="sd">        This requires building and evaluating the probability flow ODE.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters $\theta$.</span>
<span class="sd">            x: Observed data $x_o$. If None, the default $x_o$ is used.</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">                consumption.</span>
<span class="sd">            atol: Absolute tolerance for the ODE solver.</span>
<span class="sd">            rtol: Relative tolerance for the ODE solver.</span>
<span class="sd">            exact: Whether to use the exact Jacobian of the transformation or an</span>
<span class="sd">                stochastic approximation, which is faster but less accurate.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `(len(θ),)`-shaped log posterior probability $\log p(\theta|x)$ for θ in the</span>
<span class="sd">            support of the prior, -∞ (corresponding to 0 probability) outside.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span><span class="p">,</span>
            <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span>
            <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span>
            <span class="n">exact</span><span class="o">=</span><span class="n">exact</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Batched sampling is not implemented for ScorePosterior.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;posterior&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;MAP estimation is currently not working accurately for ScorePosterior.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.score_posterior.ScorePosterior.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">score_estimator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">enable_transform</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="s1">&#39;sde&#39;</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="torch.distributions.Distribution">Distribution</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior distribution with <code>.log_prob()</code> and <code>.sample()</code>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>score_estimator</code>
            </td>
            <td>
                  <code><span title="sbi.neural_nets.estimators.score_estimator.ConditionalScoreEstimator">ConditionalScoreEstimator</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The trained neural score estimator.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_sampling_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Batchsize of samples being drawn from
the proposal at every iteration.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:0&rdquo;. If None,
<code>potential_fn.device</code> is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>enable_transform</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to transform parameters to unconstrained space
during MAP optimization. When False, an identity transform will be
returned for <code>theta_transform</code>. True is not supported yet.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_with</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to sample from the posterior using the ODE-based
sampler or the SDE-based sampler.</p>
              </div>
            </td>
            <td>
                  <code>&#39;sde&#39;</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/score_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">score_estimator</span><span class="p">:</span> <span class="n">ConditionalScoreEstimator</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">enable_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sde&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        prior: Prior distribution with `.log_prob()` and `.sample()`.</span>
<span class="sd">        score_estimator: The trained neural score estimator.</span>
<span class="sd">        max_sampling_batch_size: Batchsize of samples being drawn from</span>
<span class="sd">            the proposal at every iteration.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:0&quot;. If None,</span>
<span class="sd">            `potential_fn.device` is used.</span>
<span class="sd">        enable_transform: Whether to transform parameters to unconstrained space</span>
<span class="sd">            during MAP optimization. When False, an identity transform will be</span>
<span class="sd">            returned for `theta_transform`. True is not supported yet.</span>
<span class="sd">        sample_with: Whether to sample from the posterior using the ODE-based</span>
<span class="sd">            sampler or the SDE-based sampler.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
    <span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span> <span class="o">=</span> <span class="n">score_estimator_based_potential</span><span class="p">(</span>
        <span class="n">score_estimator</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">x_o</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_transform</span><span class="o">=</span><span class="n">enable_transform</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">potential_fn</span><span class="o">=</span><span class="n">potential_fn</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="o">=</span><span class="n">theta_transform</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Set the potential function type.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">:</span> <span class="n">PosteriorScoreBasedPotential</span> <span class="o">=</span> <span class="n">potential_fn</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span> <span class="o">=</span> <span class="n">score_estimator</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span> <span class="o">=</span> <span class="n">sample_with</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s2">&quot;ode&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sde&quot;</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;sample_with must be &#39;ode&#39; or &#39;sde&#39;, but is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">.&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_sampling_batch_size</span> <span class="o">=</span> <span class="n">max_sampling_batch_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;It samples from the diffusion model given the </span><span class="se">\</span>
<span class="s2">        score_estimator.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.score_posterior.ScorePosterior.log_prob" class="doc doc-heading">
            <code class=" language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">exact</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the log-probability of the posterior <span class="arithmatex">\(p(\theta|x)\)</span>.</p>
<p>This requires building and evaluating the probability flow ODE.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>theta</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters <span class="arithmatex">\(\theta\)</span>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Observed data <span class="arithmatex">\(x_o\)</span>. If None, the default <span class="arithmatex">\(x_o\)</span> is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>track_gradients</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis, but increases memory
consumption.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>atol</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Absolute tolerance for the ODE solver.</p>
              </div>
            </td>
            <td>
                  <code>1e-05</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rtol</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Relative tolerance for the ODE solver.</p>
              </div>
            </td>
            <td>
                  <code>1e-06</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>exact</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to use the exact Jacobian of the transformation or an
stochastic approximation, which is faster but less accurate.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>(len(θ),)</code>-shaped log posterior probability <span class="arithmatex">\(\log p(\theta|x)\)</span> for θ in the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>support of the prior, -∞ (corresponding to 0 probability) outside.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/score_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">exact</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of the posterior $p(\theta|x)$.</span>

<span class="sd">    This requires building and evaluating the probability flow ODE.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters $\theta$.</span>
<span class="sd">        x: Observed data $x_o$. If None, the default $x_o$ is used.</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis, but increases memory</span>
<span class="sd">            consumption.</span>
<span class="sd">        atol: Absolute tolerance for the ODE solver.</span>
<span class="sd">        rtol: Relative tolerance for the ODE solver.</span>
<span class="sd">        exact: Whether to use the exact Jacobian of the transformation or an</span>
<span class="sd">            stochastic approximation, which is faster but less accurate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `(len(θ),)`-shaped log posterior probability $\log p(\theta|x)$ for θ in the</span>
<span class="sd">        support of the prior, -∞ (corresponding to 0 probability) outside.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
        <span class="n">track_gradients</span><span class="o">=</span><span class="n">track_gradients</span><span class="p">,</span>
        <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span>
        <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span>
        <span class="n">exact</span><span class="o">=</span><span class="n">exact</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.score_posterior.ScorePosterior.map" class="doc doc-heading">
            <code class=" language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.map" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iter</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of optimization steps that the algorithm takes
to find the MAP.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_to_optimize</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate of the optimizer.</p>
              </div>
            </td>
            <td>
                  <code>1e-05</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_method</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p>
              </div>
            </td>
            <td>
                  <code>&#39;posterior&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_init_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_best_every</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during sampling from
the posterior.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_update</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The MAP estimate.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/score_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;posterior&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;MAP estimation is currently not working accurately for ScorePosterior.&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.score_posterior.ScorePosterior.sample" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">predictor</span><span class="o">=</span><span class="s1">&#39;euler_maruyama&#39;</span><span class="p">,</span> <span class="n">corrector</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">predictor_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">corrector_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">ts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.sample" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return samples from posterior distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Shape of the samples to be drawn.</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>()</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.sample()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>predictor</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="sbi.samplers.score.Predictor">Predictor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The predictor for the diffusion-based sampler. Can be a string or
a custom predictor following the API in <code>sbi.samplers.score.predictors</code>.
Currently, only <code>euler_maruyama</code> is implemented.</p>
              </div>
            </td>
            <td>
                  <code>&#39;euler_maruyama&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>corrector</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[str, <span title="sbi.samplers.score.Corrector">Corrector</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The corrector for the diffusion-based sampler. Either of
[None].</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>predictor_params</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional parameters passed to predictor.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>corrector_params</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional parameters passed to corrector.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>steps</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of steps to take for the Euler-Maruyama method.</p>
              </div>
            </td>
            <td>
                  <code>500</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ts</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Time points at which to evaluate the diffusion process. If None, a
linear grid between t_max and t_min is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_sampling_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum batch size for sampling.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_with</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated - use <code>.build_posterior(sample_with=...)</code> prior to
<code>.sample()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progress bar during sampling.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/score_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">predictor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Predictor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;euler_maruyama&quot;</span><span class="p">,</span>
    <span class="n">corrector</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Corrector</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">predictor_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">corrector_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">ts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Shape of the samples to be drawn.</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.sample()`.</span>
<span class="sd">        predictor: The predictor for the diffusion-based sampler. Can be a string or</span>
<span class="sd">            a custom predictor following the API in `sbi.samplers.score.predictors`.</span>
<span class="sd">            Currently, only `euler_maruyama` is implemented.</span>
<span class="sd">        corrector: The corrector for the diffusion-based sampler. Either of</span>
<span class="sd">            [None].</span>
<span class="sd">        predictor_params: Additional parameters passed to predictor.</span>
<span class="sd">        corrector_params: Additional parameters passed to corrector.</span>
<span class="sd">        steps: Number of steps to take for the Euler-Maruyama method.</span>
<span class="sd">        ts: Time points at which to evaluate the diffusion process. If None, a</span>
<span class="sd">            linear grid between t_max and t_min is used.</span>
<span class="sd">        max_sampling_batch_size: Maximum batch size for sampling.</span>
<span class="sd">        sample_with: Deprecated - use `.build_posterior(sample_with=...)` prior to</span>
<span class="sd">            `.sample()`.</span>
<span class="sd">        show_progress_bars: Whether to show a progress bar during sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">sample_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You set `sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">`. As of sbi v0.18.0, setting &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`sample_with` is no longer supported. You have to rerun &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`.build_posterior(sample_with=</span><span class="si">{</span><span class="n">sample_with</span><span class="si">}</span><span class="s2">).`&quot;</span>
        <span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">condition_shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">set_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;ode&quot;</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_via_zuko</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_with</span> <span class="o">==</span> <span class="s2">&quot;sde&quot;</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_via_diffusion</span><span class="p">(</span>
            <span class="n">sample_shape</span><span class="o">=</span><span class="n">sample_shape</span><span class="p">,</span>
            <span class="n">predictor</span><span class="o">=</span><span class="n">predictor</span><span class="p">,</span>
            <span class="n">corrector</span><span class="o">=</span><span class="n">corrector</span><span class="p">,</span>
            <span class="n">predictor_params</span><span class="o">=</span><span class="n">predictor_params</span><span class="p">,</span>
            <span class="n">corrector_params</span><span class="o">=</span><span class="n">corrector_params</span><span class="p">,</span>
            <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span>
            <span class="n">ts</span><span class="o">=</span><span class="n">ts</span><span class="p">,</span>
            <span class="n">max_sampling_batch_size</span><span class="o">=</span><span class="n">max_sampling_batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.score_posterior.ScorePosterior.sample_via_zuko" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample_via_zuko</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">())</span></code>

<a href="#sbi.inference.posteriors.score_posterior.ScorePosterior.sample_via_zuko" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return samples from posterior distribution with probability flow ODE.</p>
<p>This build the probability flow ODE and then samples from the corresponding
flow. This is implemented via the zuko library.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Condition.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The shape of the samples to be returned.</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>()</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Samples.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/score_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample_via_zuko</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return samples from posterior distribution with probability flow ODE.</span>

<span class="sd">    This build the probability flow ODE and then samples from the corresponding</span>
<span class="sd">    flow. This is implemented via the zuko library.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Condition.</span>
<span class="sd">        sample_shape: The shape of the samples to be returned.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

    <span class="n">flow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">get_continuous_normalizing_flow</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">flow</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,)))</span>

    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sample_shape</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.posteriors.vi_posterior.VIPosterior" class="doc doc-heading">
            <code>VIPosterior</code>


<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.posteriors.base_posterior.NeuralPosterior">NeuralPosterior</span></code></p>


        <p>Provides VI (Variational Inference) to sample from the posterior.<br/><br/>
SNLE or SNRE train neural networks to approximate the likelihood(-ratios).
<code>VIPosterior</code> allows to learn a tractable variational posterior <span class="arithmatex">\(q(\theta)\)</span> which
approximates the true posterior <span class="arithmatex">\(p(\theta|x_o)\)</span>. After this second training stage,
we can produce approximate posterior samples, by just sampling from q with no
additional cost. For additional information see [1] and [2].<br/><br/>
References:<br/>
[1] Variational methods for simulation-based inference, Manuel Glöckler, Michael
Deistler, Jakob Macke, 2022, <a href="https://openreview.net/forum?id=kZ0UYdhqkNY">https://openreview.net/forum?id=kZ0UYdhqkNY</a><br/>
[2] Sequential Neural Posterior and Likelihood Approximation, Samuel Wiqvist, Jes
Frellsen, Umberto Picchini, 2021, <a href="https://arxiv.org/abs/2102.06522">https://arxiv.org/abs/2102.06522</a></p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">VIPosterior</span><span class="p">(</span><span class="n">NeuralPosterior</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Provides VI (Variational Inference) to sample from the posterior.&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    SNLE or SNRE train neural networks to approximate the likelihood(-ratios).</span>
<span class="sd">    `VIPosterior` allows to learn a tractable variational posterior $q(\theta)$ which</span>
<span class="sd">    approximates the true posterior $p(\theta|x_o)$. After this second training stage,</span>
<span class="sd">    we can produce approximate posterior samples, by just sampling from q with no</span>
<span class="sd">    additional cost. For additional information see [1] and [2].&lt;br/&gt;&lt;br/&gt;</span>
<span class="sd">    References:&lt;br/&gt;</span>
<span class="sd">    [1] Variational methods for simulation-based inference, Manuel Glöckler, Michael</span>
<span class="sd">    Deistler, Jakob Macke, 2022, https://openreview.net/forum?id=kZ0UYdhqkNY&lt;br/&gt;</span>
<span class="sd">    [2] Sequential Neural Posterior and Likelihood Approximation, Samuel Wiqvist, Jes</span>
<span class="sd">    Frellsen, Umberto Picchini, 2021, https://arxiv.org/abs/2102.06522</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BasePotential</span><span class="p">],</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            potential_fn: The potential function from which to draw samples. Must be a</span>
<span class="sd">                `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.</span>
<span class="sd">            prior: This is the prior distribution. Note that this is only</span>
<span class="sd">                used to check/construct the variational distribution or within some</span>
<span class="sd">                quality metrics. Please make sure that this matches with the prior</span>
<span class="sd">                within the potential_fn. If `None` is given, we will try to infer it</span>
<span class="sd">                from potential_fn or q, if this fails we raise an Error.</span>
<span class="sd">            q: Variational distribution, either string, `TransformedDistribution`, or a</span>
<span class="sd">                `VIPosterior` object. This specifies a parametric class of distribution</span>
<span class="sd">                over which the best possible posterior approximation is searched. For</span>
<span class="sd">                string input, we currently support [nsf, scf, maf, mcf, gaussian,</span>
<span class="sd">                gaussian_diag]. You can also specify your own variational family by</span>
<span class="sd">                passing a pyro `TransformedDistribution`.</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns a distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms` within the</span>
<span class="sd">                `get_flow_builder` method specifying the number of transformations</span>
<span class="sd">                within a normalizing flow. If q is already a `VIPosterior`, then the</span>
<span class="sd">                arguments will be copied from it (relevant for multi-round training).</span>
<span class="sd">            theta_transform: Maps form prior support to unconstrained space. The</span>
<span class="sd">                inverse is used here to ensure that the posterior support is equal to</span>
<span class="sd">                that of the prior.</span>
<span class="sd">            vi_method: This specifies the variational methods which are used to fit q to</span>
<span class="sd">                the posterior. We currently support [rKL, fKL, IW, alpha]. Note that</span>
<span class="sd">                some of the divergences are `mode seeking` i.e. they underestimate</span>
<span class="sd">                variance and collapse on multimodal targets (`rKL`, `alpha` for alpha &gt;</span>
<span class="sd">                1) and some are `mass covering` i.e. they overestimate variance but</span>
<span class="sd">                typically cover all modes (`fKL`, `IW`, `alpha` for alpha &lt; 1).</span>
<span class="sd">            device: Training device, e.g., `cpu`, `cuda` or `cuda:0`. We will ensure</span>
<span class="sd">                that all other objects are also on this device.</span>
<span class="sd">            x_shape: Deprecated, should not be passed.</span>
<span class="sd">            parameters: List of parameters of the variational posterior. This is only</span>
<span class="sd">                required for user-defined q i.e. if q does not have a `parameters`</span>
<span class="sd">                attribute.</span>
<span class="sd">            modules: List of modules of the variational posterior. This is only</span>
<span class="sd">                required for user-defined q i.e. if q does not have a `modules`</span>
<span class="sd">                attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="c1"># Especially the prior may be on another device -&gt; move it...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="c1"># Get prior and previous builds</span>
        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">prior</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="s2">&quot;prior&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Distribution</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;We could not find a suitable prior distribution within `potential_fn` &quot;</span>
                <span class="s2">&quot;or `q` (if a VIPosterior is given). Please explicitly specify a prior.&quot;</span>
            <span class="p">)</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># In contrast to MCMC we want to project into constrained space.</span>
        <span class="k">if</span> <span class="n">theta_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span><span class="o">.</span><span class="n">inv</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span>

        <span class="c1"># This will set the variational distribution and VI method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">vi_method</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;It provides Variational inference to .sample() from the posterior and &quot;</span>
            <span class="s2">&quot;can evaluate the _normalized_ posterior density with .log_prob().&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">q</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the variational posterior.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q</span>

    <span class="nd">@q</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">q</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the variational distribution. If the distribution does not admit access</span>
<span class="sd">        through `parameters` and `modules` function, please use `set_q` if you want to</span>
<span class="sd">        explicitly specify the parameters and modules.</span>


<span class="sd">        Args:</span>
<span class="sd">            q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">                object. This specifies a parametric class of distribution over which</span>
<span class="sd">                the best possible posterior approximation is searched. For string input,</span>
<span class="sd">                we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">                course, you can also specify your own variational family by passing a</span>
<span class="sd">                `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">                Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">                parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">                using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">                is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">                (relevant for multi-round training).</span>


<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_q</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Defines the variational family.</span>

<span class="sd">        You can specify over which parameters/modules we optimize. This is required for</span>
<span class="sd">        custom distributions which e.g. do not inherit nn.Modules or has the function</span>
<span class="sd">        `parameters` or `modules` to give direct access to trainable parameters.</span>
<span class="sd">        Further, you can pass a function, which constructs a variational distribution</span>
<span class="sd">        if called.</span>

<span class="sd">        Args:</span>
<span class="sd">            q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">                object. This specifies a parametric class of distribution over which</span>
<span class="sd">                the best possible posterior approximation is searched. For string input,</span>
<span class="sd">                we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">                course, you can also specify your own variational family by passing a</span>
<span class="sd">                `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">                Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">                parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">                Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">                `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">                useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">                using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">                is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">                (relevant for multi-round training).</span>
<span class="sd">            parameters: List of parameters associated with the distribution object.</span>
<span class="sd">            modules: List of modules associated with the distribution object.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">modules</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">adapt_variational_distribution</span><span class="p">(</span>
                <span class="n">q</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
                <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="n">self_custom_q_init_cache</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">self_custom_q_init_cache</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">get_flow_builder</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span>

            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">event_shape</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_build_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_trained_on</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vi_method</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">vi_method</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_device</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_x</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_arg</span>
            <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Something went wrong when initializing the variational distribution.</span>
<span class="s2">            Please create an issue on github https://github.com/mackelab/sbi/issues&quot;&quot;&quot;</span>
        <span class="n">check_variational_distribution</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">q</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Variational inference method e.g. one of [rKL, fKL, IW, alpha].&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span>

    <span class="nd">@vi_method</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;See `set_vi_method`.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets variational inference method.</span>

<span class="sd">        Args:</span>
<span class="sd">            method: One of [rKL, fKL, IW, alpha].</span>

<span class="sd">        Returns:</span>
<span class="sd">            `VIPosterior` for chainable calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span> <span class="o">=</span> <span class="n">get_VI_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Samples from the variational posterior distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_shape: Shape of samples</span>

<span class="sd">        Returns:</span>
<span class="sd">            Samples from posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit on the specified `default_x` &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">. Please train using `posterior.train()`.&quot;</span>
            <span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">sample_batched</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">max_sampling_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Batched sampling is not implemented for VIPosterior. </span><span class="se">\</span>
<span class="s2">            Alternatively you can use `sample` in a loop </span><span class="se">\</span>
<span class="s2">            [posterior.sample(theta, x_o) for x_o in x].&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the variational posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameters</span>
<span class="sd">            track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">                This can be helpful for e.g. sensitivity analysis but increases memory</span>
<span class="sd">                consumption.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `len($\theta$)`-shaped log-probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit using observation </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.</span><span class="se">\</span>
<span class="s2">                     Please train.&quot;</span>
            <span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_particles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
        <span class="n">max_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
        <span class="n">min_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
        <span class="n">warm_up_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">reset_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">check_for_convergence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">quality_control</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This method trains the variational posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: The observation.</span>
<span class="sd">            n_particles: Number of samples to approximate expectations within the</span>
<span class="sd">                variational bounds. The larger the more accurate are gradient</span>
<span class="sd">                estimates, but the computational cost per iteration increases.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            gamma: Learning rate decay per iteration. We use an exponential decay</span>
<span class="sd">                scheduler.</span>
<span class="sd">            max_num_iters: Maximum number of iterations.</span>
<span class="sd">            min_num_iters: Minimum number of iterations.</span>
<span class="sd">            clip_value: Gradient clipping value, decreasing may help if you see invalid</span>
<span class="sd">                values.</span>
<span class="sd">            warm_up_rounds: Initialize the posterior as the prior.</span>
<span class="sd">            retrain_from_scratch: Retrain the variational distributions from scratch.</span>
<span class="sd">            reset_optimizer: Reset the divergence optimizer</span>
<span class="sd">            show_progress_bar: If any progress report should be displayed.</span>
<span class="sd">            quality_control: If False quality control is skipped.</span>
<span class="sd">            quality_control_metric: Which metric to use for evaluating the quality.</span>
<span class="sd">            kwargs: Hyperparameters check corresponding `DivergenceOptimizer` for detail</span>
<span class="sd">                eps: Determines sensitivity of convergence check.</span>
<span class="sd">                retain_graph: Boolean which decides whether to retain the computation</span>
<span class="sd">                    graph. This may be required for some `exotic` user-specified q&#39;s.</span>
<span class="sd">                optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See</span>
<span class="sd">                    `DivergenceOptimizer` for details.</span>
<span class="sd">                scheduler: A PyTorch learning rate scheduler. See</span>
<span class="sd">                    `DivergenceOptimizer` for details.</span>
<span class="sd">                alpha: Only used if vi_method=`alpha`. Determines the alpha divergence.</span>
<span class="sd">                K: Only used if vi_method=`IW`. Determines the number of importance</span>
<span class="sd">                    weighted particles.</span>
<span class="sd">                stick_the_landing: If one should use the STL estimator (only for rKL,</span>
<span class="sd">                    IW, alpha).</span>
<span class="sd">                dreg: If one should use the DREG estimator (only for rKL, IW, alpha).</span>
<span class="sd">                weight_transform: Callable applied to importance weights (only for fKL)</span>
<span class="sd">        Returns:</span>
<span class="sd">            VIPosterior: `VIPosterior` (can be used to chain calls).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update optimizer with current arguments.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="o">**</span><span class="nb">locals</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">})</span>

        <span class="c1"># Init q and the optimizer if necessary</span>
        <span class="k">if</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
                <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">reset_optimizer</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
                <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Check context</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_2d_float32_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="p">)</span>

        <span class="n">already_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

        <span class="c1"># Optimize</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">reset_loss_stats</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="n">iters</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">iters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">)</span>

        <span class="c1"># Warmup before training</span>
        <span class="k">if</span> <span class="n">reset_optimizer</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up_was_done</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">already_trained</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                    <span class="s2">&quot;Warmup phase, this may take a few seconds...&quot;</span>
                <span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up</span><span class="p">(</span><span class="n">warm_up_rounds</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iters</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">mean_loss</span><span class="p">,</span> <span class="n">std_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_loss_stats</span><span class="p">()</span>
            <span class="c1"># Update progress bar</span>
            <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">tqdm</span><span class="p">)</span>
                <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                    <span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">std_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Check for convergence</span>
            <span class="k">if</span> <span class="n">check_for_convergence</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">min_num_iters</span> <span class="ow">and</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">converged</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged with loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="c1"># Training finished:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">x</span>

        <span class="c1"># Evaluate quality</span>
        <span class="k">if</span> <span class="n">quality_control</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">quality_control_metric</span><span class="o">=</span><span class="n">quality_control_metric</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Quality control showed a low quality of the variational &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;posterior. We are automatically retraining the variational &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;posterior from scratch with a smaller learning rate. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Alternatively, if you want to skip quality control, please &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;retrain with `VIPosterior.train(..., quality_control=False)`. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The error that occured is: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
                    <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This function will evaluate the quality of the variational posterior</span>
<span class="sd">        distribution. We currently support two different metrics of type `psis`, which</span>
<span class="sd">        checks the quality based on the tails of importance weights (there should not be</span>
<span class="sd">        much with a large one), or `prop` which checks the proportionality between q</span>
<span class="sd">        and potential_fn.</span>

<span class="sd">        NOTE: In our experience `prop` is sensitive to distinguish ``good`` from ``ok``</span>
<span class="sd">        whereas `psis` is more sensitive in distinguishing `very bad` from `ok`.</span>

<span class="sd">        Args:</span>
<span class="sd">            quality_control_metric: The metric of choice, we currently support [psis,</span>
<span class="sd">                prop, prop_prior].</span>
<span class="sd">            N: Number of samples which is used to evaluate the metric.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">quality_control_fn</span><span class="p">,</span> <span class="n">quality_control_msg</span> <span class="o">=</span> <span class="n">get_quality_metric</span><span class="p">(</span>
            <span class="n">quality_control_metric</span>
        <span class="p">)</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quality_control_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)),</span> <span class="mi">3</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quality Score: </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="o">+</span> <span class="n">quality_control_msg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">        The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">        log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">        can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">        ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">        with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">        parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">        Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">        might require hand-tuning for the problem at hand.</span>

<span class="sd">        For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">        in unbounded space and transform the result back into bounded space.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">            num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">                to find the MAP.</span>
<span class="sd">            learning_rate: Learning rate of the optimizer.</span>
<span class="sd">            init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">                it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">                the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">                tensor, the tensor will be used as init locations.</span>
<span class="sd">            num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">                evaluate the log-probability of all of them.</span>
<span class="sd">            num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">                `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">                for the optimization.</span>
<span class="sd">            save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">                `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">                Computing the best log-probability creates a significant overhead</span>
<span class="sd">                (thus, the default is `10`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">                the posterior.</span>
<span class="sd">            force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">                have a cached value.</span>
<span class="sd">            log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">                {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The MAP estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
            <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
            <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
            <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
            <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This method is called when using `copy.deepcopy` on the object.</span>

<span class="sd">        It defines how the object is copied. We need to overwrite this method, since the</span>
<span class="sd">        default implementation does use __getstate__ and __setstate__ which we overwrite</span>
<span class="sd">        to enable pickling (and in particular the necessary modifications are</span>
<span class="sd">        incompatible deep copying).</span>

<span class="sd">        Args:</span>
<span class="sd">            memo (Optional[Dict], optional): Deep copy internal memo. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            VIPosterior: Deep copy of the VIPosterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">memo</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Create a new instance of the class</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
        <span class="c1"># Add to memo</span>
        <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n">result</span>
        <span class="c1"># Copy attributes</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">memo</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This method is called when pickling the object.</span>

<span class="sd">        It defines what is pickled. We need to overwrite this method, since some parts</span>
<span class="sd">        due not support pickle protocols (e.g. due to local functions, etc.).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict: All attributes of the VIPosterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__deepcopy__</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="o">.</span><span class="n">__deepcopy__</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This method is called when unpickling the object.</span>

<span class="sd">        Especially, we need to restore the removed attributes and ensure that the object</span>
<span class="sd">        e.g. remains deep copy compatible.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict: Given state dictionary, we will restore the object from it.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">state_dict</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">)</span>
        <span class="c1"># Restore removed attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">q</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.q" class="doc doc-heading">
            <code class=" language-python"><span class="n">q</span><span class="p">:</span> <span class="n">Distribution</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.q" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the variational posterior.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.vi_method" class="doc doc-heading">
            <code class=" language-python"><span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
      <small class="doc doc-label doc-label-writable"><code>writable</code></small>
  </span>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.vi_method" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Variational inference method e.g. one of [rKL, fKL, IW, alpha].</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.__deepcopy__" class="doc doc-heading">
            <code class=" language-python"><span class="n">__deepcopy__</span><span class="p">(</span><span class="n">memo</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__deepcopy__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This method is called when using <code>copy.deepcopy</code> on the object.</p>
<p>It defines how the object is copied. We need to overwrite this method, since the
default implementation does use <strong>getstate</strong> and <strong>setstate</strong> which we overwrite
to enable pickling (and in particular the necessary modifications are
incompatible deep copying).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>memo</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deep copy internal memo. Defaults to None.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>VIPosterior</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.inference.posteriors.vi_posterior.VIPosterior" href="#sbi.inference.posteriors.vi_posterior.VIPosterior">VIPosterior</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deep copy of the VIPosterior.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This method is called when using `copy.deepcopy` on the object.</span>

<span class="sd">    It defines how the object is copied. We need to overwrite this method, since the</span>
<span class="sd">    default implementation does use __getstate__ and __setstate__ which we overwrite</span>
<span class="sd">    to enable pickling (and in particular the necessary modifications are</span>
<span class="sd">    incompatible deep copying).</span>

<span class="sd">    Args:</span>
<span class="sd">        memo (Optional[Dict], optional): Deep copy internal memo. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        VIPosterior: Deep copy of the VIPosterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Create a new instance of the class</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span>
    <span class="n">result</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
    <span class="c1"># Add to memo</span>
    <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n">result</span>
    <span class="c1"># Copy attributes</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">memo</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.__getstate__" class="doc doc-heading">
            <code class=" language-python"><span class="n">__getstate__</span><span class="p">()</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__getstate__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This method is called when pickling the object.</p>
<p>It defines what is pickled. We need to overwrite this method, since some parts
due not support pickle protocols (e.g. due to local functions, etc.).</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>Dict</code></td>            <td>
                  <code><span title="typing.Dict">Dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>All attributes of the VIPosterior.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This method is called when pickling the object.</span>

<span class="sd">    It defines what is pickled. We need to overwrite this method, since some parts</span>
<span class="sd">    due not support pickle protocols (e.g. due to local functions, etc.).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict: All attributes of the VIPosterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">__deepcopy__</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="o">.</span><span class="n">__deepcopy__</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">theta_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vi_method</span><span class="o">=</span><span class="s1">&#39;rKL&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[],</span> <span class="n">modules</span><span class="o">=</span><span class="p">[])</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>potential_fn</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>, <span title="sbi.inference.potentials.base_potential.BasePotential">BasePotential</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The potential function from which to draw samples. Must be a
<code>BasePotential</code> or a <code>Callable</code> which takes <code>theta</code> and <code>x_o</code> as inputs.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchDistribution">TorchDistribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the prior distribution. Note that this is only
used to check/construct the variational distribution or within some
quality metrics. Please make sure that this matches with the prior
within the potential_fn. If <code>None</code> is given, we will try to infer it
from potential_fn or q, if this fails we raise an Error.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>q</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="sbi.sbi_types.PyroTransformedDistribution">PyroTransformedDistribution</span>, <a class="autorefs autorefs-internal" title="sbi.inference.posteriors.vi_posterior.VIPosterior" href="#sbi.inference.posteriors.vi_posterior.VIPosterior">VIPosterior</a>, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Variational distribution, either string, <code>TransformedDistribution</code>, or a
<code>VIPosterior</code> object. This specifies a parametric class of distribution
over which the best possible posterior approximation is searched. For
string input, we currently support [nsf, scf, maf, mcf, gaussian,
gaussian_diag]. You can also specify your own variational family by
passing a pyro <code>TransformedDistribution</code>.
Additionally, we allow a <code>Callable</code>, which allows you the pass a
<code>builder</code> function, which if called returns a distribution. This may be
useful for setting the hyperparameters e.g. <code>num_transfroms</code> within the
<code>get_flow_builder</code> method specifying the number of transformations
within a normalizing flow. If q is already a <code>VIPosterior</code>, then the
arguments will be copied from it (relevant for multi-round training).</p>
              </div>
            </td>
            <td>
                  <code>&#39;maf&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>theta_transform</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchTransform">TorchTransform</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maps form prior support to unconstrained space. The
inverse is used here to ensure that the posterior support is equal to
that of the prior.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vi_method</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This specifies the variational methods which are used to fit q to
the posterior. We currently support [rKL, fKL, IW, alpha]. Note that
some of the divergences are <code>mode seeking</code> i.e. they underestimate
variance and collapse on multimodal targets (<code>rKL</code>, <code>alpha</code> for alpha &gt;
1) and some are <code>mass covering</code> i.e. they overestimate variance but
typically cover all modes (<code>fKL</code>, <code>IW</code>, <code>alpha</code> for alpha &lt; 1).</p>
              </div>
            </td>
            <td>
                  <code>&#39;rKL&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., <code>cpu</code>, <code>cuda</code> or <code>cuda:0</code>. We will ensure
that all other objects are also on this device.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x_shape</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Size">Size</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated, should not be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>parameters</code>
            </td>
            <td>
                  <code><span title="typing.Iterable">Iterable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of parameters of the variational posterior. This is only
required for user-defined q i.e. if q does not have a <code>parameters</code>
attribute.</p>
              </div>
            </td>
            <td>
                  <code>[]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>modules</code>
            </td>
            <td>
                  <code><span title="typing.Iterable">Iterable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of modules of the variational posterior. This is only
required for user-defined q i.e. if q does not have a <code>modules</code>
attribute.</p>
              </div>
            </td>
            <td>
                  <code>[]</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">potential_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">BasePotential</span><span class="p">],</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">theta_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTransform</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">vi_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rKL&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">x_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        potential_fn: The potential function from which to draw samples. Must be a</span>
<span class="sd">            `BasePotential` or a `Callable` which takes `theta` and `x_o` as inputs.</span>
<span class="sd">        prior: This is the prior distribution. Note that this is only</span>
<span class="sd">            used to check/construct the variational distribution or within some</span>
<span class="sd">            quality metrics. Please make sure that this matches with the prior</span>
<span class="sd">            within the potential_fn. If `None` is given, we will try to infer it</span>
<span class="sd">            from potential_fn or q, if this fails we raise an Error.</span>
<span class="sd">        q: Variational distribution, either string, `TransformedDistribution`, or a</span>
<span class="sd">            `VIPosterior` object. This specifies a parametric class of distribution</span>
<span class="sd">            over which the best possible posterior approximation is searched. For</span>
<span class="sd">            string input, we currently support [nsf, scf, maf, mcf, gaussian,</span>
<span class="sd">            gaussian_diag]. You can also specify your own variational family by</span>
<span class="sd">            passing a pyro `TransformedDistribution`.</span>
<span class="sd">            Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">            `builder` function, which if called returns a distribution. This may be</span>
<span class="sd">            useful for setting the hyperparameters e.g. `num_transfroms` within the</span>
<span class="sd">            `get_flow_builder` method specifying the number of transformations</span>
<span class="sd">            within a normalizing flow. If q is already a `VIPosterior`, then the</span>
<span class="sd">            arguments will be copied from it (relevant for multi-round training).</span>
<span class="sd">        theta_transform: Maps form prior support to unconstrained space. The</span>
<span class="sd">            inverse is used here to ensure that the posterior support is equal to</span>
<span class="sd">            that of the prior.</span>
<span class="sd">        vi_method: This specifies the variational methods which are used to fit q to</span>
<span class="sd">            the posterior. We currently support [rKL, fKL, IW, alpha]. Note that</span>
<span class="sd">            some of the divergences are `mode seeking` i.e. they underestimate</span>
<span class="sd">            variance and collapse on multimodal targets (`rKL`, `alpha` for alpha &gt;</span>
<span class="sd">            1) and some are `mass covering` i.e. they overestimate variance but</span>
<span class="sd">            typically cover all modes (`fKL`, `IW`, `alpha` for alpha &lt; 1).</span>
<span class="sd">        device: Training device, e.g., `cpu`, `cuda` or `cuda:0`. We will ensure</span>
<span class="sd">            that all other objects are also on this device.</span>
<span class="sd">        x_shape: Deprecated, should not be passed.</span>
<span class="sd">        parameters: List of parameters of the variational posterior. This is only</span>
<span class="sd">            required for user-defined q i.e. if q does not have a `parameters`</span>
<span class="sd">            attribute.</span>
<span class="sd">        modules: List of modules of the variational posterior. This is only</span>
<span class="sd">            required for user-defined q i.e. if q does not have a `modules`</span>
<span class="sd">            attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">theta_transform</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="c1"># Especially the prior may be on another device -&gt; move it...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

    <span class="c1"># Get prior and previous builds</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span> <span class="s2">&quot;prior&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Distribution</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="o">.</span><span class="n">prior</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;We could not find a suitable prior distribution within `potential_fn` &quot;</span>
            <span class="s2">&quot;or `q` (if a VIPosterior is given). Please explicitly specify a prior.&quot;</span>
        <span class="p">)</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># In contrast to MCMC we want to project into constrained space.</span>
    <span class="k">if</span> <span class="n">theta_transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">mcmc_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span><span class="o">.</span><span class="n">inv</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span> <span class="o">=</span> <span class="n">theta_transform</span><span class="o">.</span><span class="n">inv</span>

    <span class="c1"># This will set the variational distribution and VI method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_vi_method</span><span class="p">(</span><span class="n">vi_method</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_purpose</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;It provides Variational inference to .sample() from the posterior and &quot;</span>
        <span class="s2">&quot;can evaluate the _normalized_ posterior density with .log_prob().&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.__setstate__" class="doc doc-heading">
            <code class=" language-python"><span class="n">__setstate__</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.__setstate__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This method is called when unpickling the object.</p>
<p>Especially, we need to restore the removed attributes and ensure that the object
e.g. remains deep copy compatible.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td>
                  <code><span title="typing.Dict">Dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Given state dictionary, we will restore the object from it.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This method is called when unpickling the object.</span>

<span class="sd">    Especially, we need to restore the removed attributes and ensure that the object</span>
<span class="sd">    e.g. remains deep copy compatible.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict: Given state dictionary, we will restore the object from it.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">state_dict</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">)</span>
    <span class="c1"># Restore removed attributes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_q</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">q</span>
    <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.evaluate" class="doc doc-heading">
            <code class=" language-python"><span class="n">evaluate</span><span class="p">(</span><span class="n">quality_control_metric</span><span class="o">=</span><span class="s1">&#39;psis&#39;</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">50000.0</span><span class="p">))</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.evaluate" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This function will evaluate the quality of the variational posterior
distribution. We currently support two different metrics of type <code>psis</code>, which
checks the quality based on the tails of importance weights (there should not be
much with a large one), or <code>prop</code> which checks the proportionality between q
and potential_fn.</p>
<p>NOTE: In our experience <code>prop</code> is sensitive to distinguish <code>good</code> from <code>ok</code>
whereas <code>psis</code> is more sensitive in distinguishing <code>very bad</code> from <code>ok</code>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>quality_control_metric</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The metric of choice, we currently support [psis,
prop, prop_prior].</p>
              </div>
            </td>
            <td>
                  <code>&#39;psis&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>N</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of samples which is used to evaluate the metric.</p>
              </div>
            </td>
            <td>
                  <code>int(50000.0)</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e4</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function will evaluate the quality of the variational posterior</span>
<span class="sd">    distribution. We currently support two different metrics of type `psis`, which</span>
<span class="sd">    checks the quality based on the tails of importance weights (there should not be</span>
<span class="sd">    much with a large one), or `prop` which checks the proportionality between q</span>
<span class="sd">    and potential_fn.</span>

<span class="sd">    NOTE: In our experience `prop` is sensitive to distinguish ``good`` from ``ok``</span>
<span class="sd">    whereas `psis` is more sensitive in distinguishing `very bad` from `ok`.</span>

<span class="sd">    Args:</span>
<span class="sd">        quality_control_metric: The metric of choice, we currently support [psis,</span>
<span class="sd">            prop, prop_prior].</span>
<span class="sd">        N: Number of samples which is used to evaluate the metric.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quality_control_fn</span><span class="p">,</span> <span class="n">quality_control_msg</span> <span class="o">=</span> <span class="n">get_quality_metric</span><span class="p">(</span>
        <span class="n">quality_control_metric</span>
    <span class="p">)</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quality_control_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)),</span> <span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quality Score: </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="o">+</span> <span class="n">quality_control_msg</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.log_prob" class="doc doc-heading">
            <code class=" language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">track_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.log_prob" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the log-probability of theta under the variational posterior.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>theta</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>track_gradients</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the returned tensor supports tracking gradients.
This can be helpful for e.g. sensitivity analysis but increases memory
consumption.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>len($\theta$)</code>-shaped log-probability.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">track_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the log-probability of theta under the variational posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameters</span>
<span class="sd">        track_gradients: Whether the returned tensor supports tracking gradients.</span>
<span class="sd">            This can be helpful for e.g. sensitivity analysis but increases memory</span>
<span class="sd">            consumption.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `len($\theta$)`-shaped log-probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit using observation </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.</span><span class="se">\</span>
<span class="s2">                 Please train.&quot;</span>
        <span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">track_gradients</span><span class="p">):</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">ensure_theta_batched</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.map" class="doc doc-heading">
            <code class=" language-python"><span class="nb">map</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_to_optimize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;proposal&#39;</span><span class="p">,</span> <span class="n">num_init_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">save_best_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.map" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the maximum-a-posteriori estimate (MAP).</p>
<p>The method can be interrupted (Ctrl-C) when the user sees that the
log-probability converges. The best estimate will be saved in <code>self._map</code> and
can be accessed with <code>self.map()</code>. The MAP is obtained by running gradient
ascent from a given number of starting positions (samples from the posterior
with the highest log-probability). After the optimization is done, we select the
parameter set that has the highest log-probability after the optimization.</p>
<p>Warning: The default values used by this function are not well-tested. They
might require hand-tuning for the problem at hand.</p>
<p>For developers: if the prior is a <code>BoxUniform</code>, we carry out the optimization
in unbounded space and transform the result back into bounded space.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchTensor">TorchTensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated - use <code>.set_default_x()</code> prior to <code>.map()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iter</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of optimization steps that the algorithm takes
to find the MAP.</p>
              </div>
            </td>
            <td>
                  <code>1000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate of the optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.01</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_method</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="sbi.sbi_types.TorchTensor">TorchTensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>How to select the starting parameters for the optimization. If
it is a string, it can be either [<code>posterior</code>, <code>prior</code>], which samples
the respective distribution <code>num_init_samples</code> times. If it is a
tensor, the tensor will be used as init locations.</p>
              </div>
            </td>
            <td>
                  <code>&#39;proposal&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_init_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Draw this number of samples from the posterior and
evaluate the log-probability of all of them.</p>
              </div>
            </td>
            <td>
                  <code>10000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_to_optimize</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>From the drawn <code>num_init_samples</code>, use the
<code>num_to_optimize</code> with highest log-probability as the initial points
for the optimization.</p>
              </div>
            </td>
            <td>
                  <code>100</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_best_every</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The best log-probability is computed, saved in the
<code>map</code>-attribute, and printed every <code>save_best_every</code>-th iteration.
Computing the best log-probability creates a significant overhead
(thus, the default is <code>10</code>.)</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during sampling from
the posterior.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_update</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to re-calculate the MAP when x is unchanged and
have a cached value.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>log_prob_kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Will be empty for SNLE and SNRE. Will contain
{&lsquo;norm_posterior&rsquo;: True} for SNPE.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The MAP estimate.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span>
    <span class="n">num_to_optimize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;proposal&quot;</span><span class="p">,</span>
    <span class="n">num_init_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">save_best_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_update</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum-a-posteriori estimate (MAP).</span>

<span class="sd">    The method can be interrupted (Ctrl-C) when the user sees that the</span>
<span class="sd">    log-probability converges. The best estimate will be saved in `self._map` and</span>
<span class="sd">    can be accessed with `self.map()`. The MAP is obtained by running gradient</span>
<span class="sd">    ascent from a given number of starting positions (samples from the posterior</span>
<span class="sd">    with the highest log-probability). After the optimization is done, we select the</span>
<span class="sd">    parameter set that has the highest log-probability after the optimization.</span>

<span class="sd">    Warning: The default values used by this function are not well-tested. They</span>
<span class="sd">    might require hand-tuning for the problem at hand.</span>

<span class="sd">    For developers: if the prior is a `BoxUniform`, we carry out the optimization</span>
<span class="sd">    in unbounded space and transform the result back into bounded space.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Deprecated - use `.set_default_x()` prior to `.map()`.</span>
<span class="sd">        num_iter: Number of optimization steps that the algorithm takes</span>
<span class="sd">            to find the MAP.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        init_method: How to select the starting parameters for the optimization. If</span>
<span class="sd">            it is a string, it can be either [`posterior`, `prior`], which samples</span>
<span class="sd">            the respective distribution `num_init_samples` times. If it is a</span>
<span class="sd">            tensor, the tensor will be used as init locations.</span>
<span class="sd">        num_init_samples: Draw this number of samples from the posterior and</span>
<span class="sd">            evaluate the log-probability of all of them.</span>
<span class="sd">        num_to_optimize: From the drawn `num_init_samples`, use the</span>
<span class="sd">            `num_to_optimize` with highest log-probability as the initial points</span>
<span class="sd">            for the optimization.</span>
<span class="sd">        save_best_every: The best log-probability is computed, saved in the</span>
<span class="sd">            `map`-attribute, and printed every `save_best_every`-th iteration.</span>
<span class="sd">            Computing the best log-probability creates a significant overhead</span>
<span class="sd">            (thus, the default is `10`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during sampling from</span>
<span class="sd">            the posterior.</span>
<span class="sd">        force_update: Whether to re-calculate the MAP when x is unchanged and</span>
<span class="sd">            have a cached value.</span>
<span class="sd">        log_prob_kwargs: Will be empty for SNLE and SNRE. Will contain</span>
<span class="sd">            {&#39;norm_posterior&#39;: True} for SNPE.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The MAP estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
        <span class="n">num_to_optimize</span><span class="o">=</span><span class="n">num_to_optimize</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">num_init_samples</span><span class="o">=</span><span class="n">num_init_samples</span><span class="p">,</span>
        <span class="n">save_best_every</span><span class="o">=</span><span class="n">save_best_every</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="n">force_update</span><span class="o">=</span><span class="n">force_update</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.sample" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.sample" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Samples from the variational posterior distribution.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_shape</code>
            </td>
            <td>
                  <code><span title="sbi.sbi_types.Shape">Shape</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Shape of samples</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.Size">Size</span>()</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Samples from posterior.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">Shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(),</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples from the variational posterior distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample_shape: Shape of samples</span>

<span class="sd">    Returns:</span>
<span class="sd">        Samples from posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The variational posterior was not fit on the specified `default_x` &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">. Please train using `posterior.train()`.&quot;</span>
        <span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.set_q" class="doc doc-heading">
            <code class=" language-python"><span class="n">set_q</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[],</span> <span class="n">modules</span><span class="o">=</span><span class="p">[])</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_q" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Defines the variational family.</p>
<p>You can specify over which parameters/modules we optimize. This is required for
custom distributions which e.g. do not inherit nn.Modules or has the function
<code>parameters</code> or <code>modules</code> to give direct access to trainable parameters.
Further, you can pass a function, which constructs a variational distribution
if called.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>q</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="sbi.sbi_types.PyroTransformedDistribution">PyroTransformedDistribution</span>, <a class="autorefs autorefs-internal" title="sbi.inference.posteriors.vi_posterior.VIPosterior" href="#sbi.inference.posteriors.vi_posterior.VIPosterior">VIPosterior</a>, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Variational distribution, either string, distribution, or a VIPosterior
object. This specifies a parametric class of distribution over which
the best possible posterior approximation is searched. For string input,
we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of
course, you can also specify your own variational family by passing a
<code>parameterized</code> distribution object i.e. a torch.distributions
Distribution with methods <code>parameters</code> returning an iterable of all
parameters (you can pass them within the paramters/modules attribute).
Additionally, we allow a <code>Callable</code>, which allows you the pass a
<code>builder</code> function, which if called returns an distribution. This may be
useful for setting the hyperparameters e.g. <code>num_transfroms:int</code> by
using the <code>get_flow_builder</code> method specifying the hyperparameters. If q
is already a <code>VIPosterior</code>, then the arguments will be copied from it
(relevant for multi-round training).</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>parameters</code>
            </td>
            <td>
                  <code><span title="typing.Iterable">Iterable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of parameters associated with the distribution object.</p>
              </div>
            </td>
            <td>
                  <code>[]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>modules</code>
            </td>
            <td>
                  <code><span title="typing.Iterable">Iterable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of modules associated with the distribution object.</p>
              </div>
            </td>
            <td>
                  <code>[]</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_q</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PyroTransformedDistribution</span><span class="p">,</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Defines the variational family.</span>

<span class="sd">    You can specify over which parameters/modules we optimize. This is required for</span>
<span class="sd">    custom distributions which e.g. do not inherit nn.Modules or has the function</span>
<span class="sd">    `parameters` or `modules` to give direct access to trainable parameters.</span>
<span class="sd">    Further, you can pass a function, which constructs a variational distribution</span>
<span class="sd">    if called.</span>

<span class="sd">    Args:</span>
<span class="sd">        q: Variational distribution, either string, distribution, or a VIPosterior</span>
<span class="sd">            object. This specifies a parametric class of distribution over which</span>
<span class="sd">            the best possible posterior approximation is searched. For string input,</span>
<span class="sd">            we currently support [nsf, scf, maf, mcf, gaussian, gaussian_diag]. Of</span>
<span class="sd">            course, you can also specify your own variational family by passing a</span>
<span class="sd">            `parameterized` distribution object i.e. a torch.distributions</span>
<span class="sd">            Distribution with methods `parameters` returning an iterable of all</span>
<span class="sd">            parameters (you can pass them within the paramters/modules attribute).</span>
<span class="sd">            Additionally, we allow a `Callable`, which allows you the pass a</span>
<span class="sd">            `builder` function, which if called returns an distribution. This may be</span>
<span class="sd">            useful for setting the hyperparameters e.g. `num_transfroms:int` by</span>
<span class="sd">            using the `get_flow_builder` method specifying the hyperparameters. If q</span>
<span class="sd">            is already a `VIPosterior`, then the arguments will be copied from it</span>
<span class="sd">            (relevant for multi-round training).</span>
<span class="sd">        parameters: List of parameters associated with the distribution object.</span>
<span class="sd">        modules: List of modules associated with the distribution object.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">modules</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">adapt_variational_distribution</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
            <span class="n">modules</span><span class="o">=</span><span class="n">modules</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="n">self_custom_q_init_cache</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">self_custom_q_init_cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">get_flow_builder</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">event_shape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">link_transform</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">VIPosterior</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_build_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_trained_on</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vi_method</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">vi_method</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_q_arg</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">_q_arg</span>
        <span class="n">make_object_deepcopy_compatible</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
    <span class="n">move_all_tensor_to_device</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">Distribution</span>
    <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Something went wrong when initializing the variational distribution.</span>
<span class="s2">        Please create an issue on github https://github.com/mackelab/sbi/issues&quot;&quot;&quot;</span>
    <span class="n">check_variational_distribution</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">q</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.set_vi_method" class="doc doc-heading">
            <code class=" language-python"><span class="n">set_vi_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.set_vi_method" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Sets variational inference method.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>method</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>One of [rKL, fKL, IW, alpha].</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.inference.posteriors.vi_posterior.VIPosterior" href="#sbi.inference.posteriors.vi_posterior.VIPosterior">VIPosterior</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>VIPosterior</code> for chainable calls.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_vi_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets variational inference method.</span>

<span class="sd">    Args:</span>
<span class="sd">        method: One of [rKL, fKL, IW, alpha].</span>

<span class="sd">    Returns:</span>
<span class="sd">        `VIPosterior` for chainable calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_vi_method</span> <span class="o">=</span> <span class="n">method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span> <span class="o">=</span> <span class="n">get_VI_method</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.posteriors.vi_posterior.VIPosterior.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_particles</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">max_num_iters</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">min_num_iters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">clip_value</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">warm_up_rounds</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check_for_convergence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">quality_control</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">quality_control_metric</span><span class="o">=</span><span class="s1">&#39;psis&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.inference.posteriors.vi_posterior.VIPosterior.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This method trains the variational posterior.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchTensor">TorchTensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The observation.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_particles</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of samples to approximate expectations within the
variational bounds. The larger the more accurate are gradient
estimates, but the computational cost per iteration increases.</p>
              </div>
            </td>
            <td>
                  <code>256</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate of the optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.001</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gamma</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate decay per iteration. We use an exponential decay
scheduler.</p>
              </div>
            </td>
            <td>
                  <code>0.999</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_num_iters</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of iterations.</p>
              </div>
            </td>
            <td>
                  <code>2000</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_num_iters</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum number of iterations.</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>clip_value</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Gradient clipping value, decreasing may help if you see invalid
values.</p>
              </div>
            </td>
            <td>
                  <code>10.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>warm_up_rounds</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Initialize the posterior as the prior.</p>
              </div>
            </td>
            <td>
                  <code>100</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>retrain_from_scratch</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Retrain the variational distributions from scratch.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>reset_optimizer</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Reset the divergence optimizer</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bar</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If any progress report should be displayed.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quality_control</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If False quality control is skipped.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quality_control_metric</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Which metric to use for evaluating the quality.</p>
              </div>
            </td>
            <td>
                  <code>&#39;psis&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Hyperparameters check corresponding <code>DivergenceOptimizer</code> for detail
eps: Determines sensitivity of convergence check.
retain_graph: Boolean which decides whether to retain the computation
    graph. This may be required for some <code>exotic</code> user-specified q&rsquo;s.
optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See
    <code>DivergenceOptimizer</code> for details.
scheduler: A PyTorch learning rate scheduler. See
    <code>DivergenceOptimizer</code> for details.
alpha: Only used if vi_method=<code>alpha</code>. Determines the alpha divergence.
K: Only used if vi_method=<code>IW</code>. Determines the number of importance
    weighted particles.
stick_the_landing: If one should use the STL estimator (only for rKL,
    IW, alpha).
dreg: If one should use the DREG estimator (only for rKL, IW, alpha).
weight_transform: Callable applied to importance weights (only for fKL)</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns:
    VIPosterior: <code>VIPosterior</code> (can be used to chain calls).</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/posteriors/vi_posterior.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_particles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
    <span class="n">max_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
    <span class="n">min_num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
    <span class="n">warm_up_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">reset_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_for_convergence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">quality_control</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">quality_control_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;psis&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;VIPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This method trains the variational posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The observation.</span>
<span class="sd">        n_particles: Number of samples to approximate expectations within the</span>
<span class="sd">            variational bounds. The larger the more accurate are gradient</span>
<span class="sd">            estimates, but the computational cost per iteration increases.</span>
<span class="sd">        learning_rate: Learning rate of the optimizer.</span>
<span class="sd">        gamma: Learning rate decay per iteration. We use an exponential decay</span>
<span class="sd">            scheduler.</span>
<span class="sd">        max_num_iters: Maximum number of iterations.</span>
<span class="sd">        min_num_iters: Minimum number of iterations.</span>
<span class="sd">        clip_value: Gradient clipping value, decreasing may help if you see invalid</span>
<span class="sd">            values.</span>
<span class="sd">        warm_up_rounds: Initialize the posterior as the prior.</span>
<span class="sd">        retrain_from_scratch: Retrain the variational distributions from scratch.</span>
<span class="sd">        reset_optimizer: Reset the divergence optimizer</span>
<span class="sd">        show_progress_bar: If any progress report should be displayed.</span>
<span class="sd">        quality_control: If False quality control is skipped.</span>
<span class="sd">        quality_control_metric: Which metric to use for evaluating the quality.</span>
<span class="sd">        kwargs: Hyperparameters check corresponding `DivergenceOptimizer` for detail</span>
<span class="sd">            eps: Determines sensitivity of convergence check.</span>
<span class="sd">            retain_graph: Boolean which decides whether to retain the computation</span>
<span class="sd">                graph. This may be required for some `exotic` user-specified q&#39;s.</span>
<span class="sd">            optimizer: A PyTorch Optimizer class e.g. Adam or SGD. See</span>
<span class="sd">                `DivergenceOptimizer` for details.</span>
<span class="sd">            scheduler: A PyTorch learning rate scheduler. See</span>
<span class="sd">                `DivergenceOptimizer` for details.</span>
<span class="sd">            alpha: Only used if vi_method=`alpha`. Determines the alpha divergence.</span>
<span class="sd">            K: Only used if vi_method=`IW`. Determines the number of importance</span>
<span class="sd">                weighted particles.</span>
<span class="sd">            stick_the_landing: If one should use the STL estimator (only for rKL,</span>
<span class="sd">                IW, alpha).</span>
<span class="sd">            dreg: If one should use the DREG estimator (only for rKL, IW, alpha).</span>
<span class="sd">            weight_transform: Callable applied to importance weights (only for fKL)</span>
<span class="sd">    Returns:</span>
<span class="sd">        VIPosterior: `VIPosterior` (can be used to chain calls).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Update optimizer with current arguments.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="o">**</span><span class="nb">locals</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">})</span>

    <span class="c1"># Init q and the optimizer if necessary</span>
    <span class="k">if</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_build_fn</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">reset_optimizer</span>
        <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_builder</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">potential_fn</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_value</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">n_particles</span><span class="o">=</span><span class="n">n_particles</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Check context</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_2d_float32_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x_else_default_x</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="p">)</span>

    <span class="n">already_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

    <span class="c1"># Optimize</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">reset_loss_stats</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
        <span class="n">iters</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">iters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_num_iters</span><span class="p">)</span>

    <span class="c1"># Warmup before training</span>
    <span class="k">if</span> <span class="n">reset_optimizer</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up_was_done</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">already_trained</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="s2">&quot;Warmup phase, this may take a few seconds...&quot;</span>
            <span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">warm_up</span><span class="p">(</span><span class="n">warm_up_rounds</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iters</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">mean_loss</span><span class="p">,</span> <span class="n">std_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_loss_stats</span><span class="p">()</span>
        <span class="c1"># Update progress bar</span>
        <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">tqdm</span><span class="p">)</span>
            <span class="n">iters</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">std_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Check for convergence</span>
        <span class="k">if</span> <span class="n">check_for_convergence</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">min_num_iters</span> <span class="ow">and</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">converged</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">show_progress_bar</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged with loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">break</span>
    <span class="c1"># Training finished:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trained_on</span> <span class="o">=</span> <span class="n">x</span>

    <span class="c1"># Evaluate quality</span>
    <span class="k">if</span> <span class="n">quality_control</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">quality_control_metric</span><span class="o">=</span><span class="n">quality_control_metric</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Quality control showed a low quality of the variational &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;posterior. We are automatically retraining the variational &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;posterior from scratch with a smaller learning rate. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Alternatively, if you want to skip quality control, please &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;retrain with `VIPosterior.train(..., quality_control=False)`. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The error that occured is: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
                <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">reset_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/sbi-dev/sbi" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>