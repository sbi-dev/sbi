
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://sbi-dev.github.io/sbi/dev/reference/inference/">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>Inference - sbi</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
  
  <style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.75 1.75 0 0 1 1 7.775m1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2"/></svg>');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../static/global.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#inference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="sbi" class="md-header__button md-logo" aria-label="sbi" data-md-component="logo">
      
  <img src="../../static/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            sbi
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Inference
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="http://github.com/sbi-dev/sbi" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    sbi-dev/sbi
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="sbi" class="md-nav__button md-logo" aria-label="sbi" data-md-component="logo">
      
  <img src="../../static/logo.svg" alt="logo">

    </a>
    sbi
  </label>
  
    <div class="md-nav__source">
      <a href="http://github.com/sbi-dev/sbi" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    sbi-dev/sbi
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tutorials and Examples
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Contributing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How to contribute
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code of Conduct
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../citation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Citation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../credits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Credits
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#training-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Training algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npe.npe_a.NPE_A" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NPE_A
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NPE_A">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npe.npe_a.NPE_A.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npe.npe_a.NPE_A.build_posterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;build_posterior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npe.npe_a.NPE_A.correct_for_proposal" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;correct_for_proposal
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npe.npe_a.NPE_A.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npe.npe_c.NPE_C" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NPE_C
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NPE_C">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npe.npe_c.NPE_C.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npe.npe_c.NPE_C.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.fmpe.fmpe.FMPE" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FMPE
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" FMPE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.fmpe.fmpe.FMPE.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.fmpe.fmpe.FMPE.build_posterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;build_posterior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.fmpe.fmpe.FMPE.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npse.npse.NPSE" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NPSE
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NPSE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npse.npse.NPSE.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npse.npse.NPSE.append_simulations" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;append_simulations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npse.npse.NPSE.build_posterior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;build_posterior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.npse.npse.NPSE.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nle.nle_a.NLE_A" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NLE_A
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NLE_A">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nle.nle_a.NLE_A.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_a.NRE_A" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NRE_A
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NRE_A">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_a.NRE_A.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_a.NRE_A.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_b.NRE_B" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NRE_B
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NRE_B">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_b.NRE_B.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_b.NRE_B.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_c.NRE_C" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NRE_C
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NRE_C">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_c.NRE_C.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.nre_c.NRE_C.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.bnre.BNRE" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;BNRE
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" BNRE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.bnre.BNRE.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.trainers.nre.bnre.BNRE.train" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.abc.mcabc.MCABC" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MCABC
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" MCABC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.mcabc.MCABC.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.mcabc.MCABC.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;SMCABC
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" SMCABC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.get_kernel_variance" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_kernel_variance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.get_new_kernel" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_new_kernel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.get_particle_ranges" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_particle_ranges
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;resample_if_ess_too_small
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.run_lra_update_weights" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;run_lra_update_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.run_sass_set_xo" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;run_sass_set_xo
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;sample_from_population_with_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helpers" class="md-nav__link">
    <span class="md-ellipsis">
      Helpers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.inference.trainers.base.simulate_for_sbi" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;simulate_for_sbi
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.utils.user_input_checks.process_prior" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;process_prior
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sbi.utils.user_input_checks.process_simulator" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;process_simulator
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">&para;</a></h1>
<h2 id="training-algorithms">Training algorithms<a class="headerlink" href="#training-algorithms" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.npe.npe_a.NPE_A" class="doc doc-heading">
            <code>NPE_A</code>


<a href="#sbi.inference.trainers.npe.npe_a.NPE_A" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.trainers.npe.npe_base.PosteriorEstimator">PosteriorEstimator</span></code></p>







              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/npe/npe_a.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NPE_A</span><span class="p">(</span><span class="n">PosteriorEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">,</span>
        <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;NPE-A [1].</span>

<span class="sd">        [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">            Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">            https://arxiv.org/abs/1605.06376.</span>

<span class="sd">        Like all NPE methods, this method trains a deep neural density estimator to</span>
<span class="sd">        directly approximate the posterior. Also like all other NPE methods, in the</span>
<span class="sd">        first round, this density estimator is trained with a maximum-likelihood loss.</span>

<span class="sd">        This class implements NPE-A. NPE-A trains across multiple rounds with a</span>
<span class="sd">        maximum-likelihood-loss. This will make training converge to the proposal</span>
<span class="sd">        posterior instead of the true posterior. To correct for this, SNPE-A applies a</span>
<span class="sd">        post-hoc correction after training. This correction has to be performed</span>
<span class="sd">        analytically. Thus, NPE-A is limited to Gaussian distributions for all but the</span>
<span class="sd">        last round. In the last round, NPE-A can use a Mixture of Gaussians.</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            density_estimator: If it is a string (only &quot;mdn_snpe_a&quot; is valid), use a</span>
<span class="sd">                pre-configured mixture of densities network. Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`. Note that until the last round only a</span>
<span class="sd">                single (multivariate) Gaussian component is used for training (see</span>
<span class="sd">                Algorithm 1 in [1]). In the last round, this component is replicated</span>
<span class="sd">                `num_components` times, its parameters are perturbed with a very small</span>
<span class="sd">                noise, and then the last training round is done with the expanded</span>
<span class="sd">                Gaussian mixture as estimator for the proposal posterior.</span>
<span class="sd">            num_components: Number of components of the mixture of Gaussians in the</span>
<span class="sd">                last round. This overrides the `num_components` value passed to</span>
<span class="sd">                `posterior_nn()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Catch invalid inputs.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">((</span><span class="n">density_estimator</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">callable</span><span class="p">(</span><span class="n">density_estimator</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;The `density_estimator` passed to SNPE_A needs to be a &quot;</span>
                <span class="s2">&quot;callable or the string &#39;mdn_snpe_a&#39;!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># `num_components` will be used to replicate the Gaussian in the last round.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="n">num_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
        <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
        <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
        <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span>
            <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_components&quot;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">final_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">component_perturbation</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-3</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ConditionalDensityEstimator</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the proposal posterior.</span>

<span class="sd">        [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">            Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">            https://arxiv.org/abs/1605.06376.</span>

<span class="sd">        Training is performed with maximum likelihood on samples from the latest round,</span>
<span class="sd">        which leads the algorithm to converge to the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            final_round: Whether we are in the last round of training or not. For all</span>
<span class="sd">                but the last round, Algorithm 1 from [1] is executed. In last the</span>
<span class="sd">                round, Algorithm 2 from [1] is executed once.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">                simulations `x`. See Lueckmann, Gonçalves et al., NeurIPS 2017.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">                i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">                distribution different from the prior.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round. Not supported for</span>
<span class="sd">                SNPE-A.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">            component_perturbation: The standard deviation applied to all weights and</span>
<span class="sd">                biases when, in the last round, the Mixture of Gaussians is build from</span>
<span class="sd">                a single Gaussian. This value can be problem-specific and also depends</span>
<span class="sd">                on the number of mixture components.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="n">retrain_from_scratch</span><span class="p">,</span> <span class="s2">&quot;&quot;&quot;Retraining from scratch is not supported in</span>
<span class="s2">            SNPE-A yet. The reason for this is that, if we reininitialized the density</span>
<span class="s2">            estimator, the z-scoring would change, which would break the posthoc</span>
<span class="s2">            correction. This is a pure implementation issue.&quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span>
            <span class="n">entries</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;self&quot;</span><span class="p">,</span>
                <span class="s2">&quot;__class__&quot;</span><span class="p">,</span>
                <span class="s2">&quot;final_round&quot;</span><span class="p">,</span>
                <span class="s2">&quot;component_perturbation&quot;</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># SNPE-A always discards the prior samples.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;discard_prior_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;force_first_round_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
            <span class="c1"># If there is (will be) only one round, train with Algorithm 2 from [1].</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
                <span class="p">)</span>
            <span class="c1"># Run Algorithm 2 from [1].</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span><span class="p">:</span>
                <span class="c1"># Now switch to the specified number of components. This method will</span>
                <span class="c1"># only be used if `retrain_from_scratch=True`. Otherwise,</span>
                <span class="c1"># the MDN will be built from replicating the single-component net for</span>
                <span class="c1"># `num_component` times (via `_expand_mog()`).</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
                <span class="p">)</span>

                <span class="c1"># Extend the MDN to the originally desired number of components.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_expand_mog</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">component_perturbation</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;You have already run SNPE-A with `final_round=True`. Running it&quot;</span>
                    <span class="s2">&quot;again with this setting will not allow computing the posthoc&quot;</span>
                    <span class="s2">&quot;correction applied in SNPE-A. Thus, you will get an error when &quot;</span>
                    <span class="s2">&quot;calling `.build_posterior()` after training.&quot;</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                    <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Run Algorithm 1 from [1].</span>
            <span class="c1"># Wrap the function that builds the MDN such that we can make</span>
            <span class="c1"># sure that there is only one component when running.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">correct_for_proposal</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NPE_A_MDN&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build mixture of Gaussians that approximates the posterior.</span>

<span class="sd">        Returns a `NPE_A_MDN` object, which applies the posthoc-correction required in</span>
<span class="sd">        SNPE-A.</span>

<span class="sd">        Args:</span>
<span class="sd">            density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">                If `None`, use the latest neural density estimator that was trained.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">density_estimator</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
            <span class="p">)</span>  <span class="c1"># PosteriorEstimator.train() also returns a deepcopy, mimic this here</span>
            <span class="c1"># If internal net is used device is defined.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Set proposal of the density estimator.</span>
        <span class="c1"># This also evokes the z-scoring correction if necessary.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">proposal</span><span class="p">,</span> <span class="p">(</span><span class="n">MultivariateNormal</span><span class="p">,</span> <span class="n">BoxUniform</span><span class="p">)</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Prior must be `torch.distributions.MultivariateNormal` or `sbi.utils.</span>
<span class="s2">                BoxUniform`&quot;&quot;&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">DirectPosterior</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;The proposal you passed to `append_simulations` is neither the prior</span>
<span class="s2">                nor a `DirectPosterior`. SNPE-A currently only supports these scenarios.</span>
<span class="s2">                &quot;&quot;&quot;</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Create the NPE_A_MDN</span>
        <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="n">NPE_A_MDN</span><span class="p">(</span>
            <span class="n">flow</span><span class="o">=</span><span class="n">density_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">proposal</span><span class="o">=</span><span class="n">proposal</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapped_density_estimator</span>

    <span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DirectPosterior&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">        This method first corrects the estimated density with `correct_for_proposal`</span>
<span class="sd">        and then returns a `DirectPosterior`.</span>

<span class="sd">        Args:</span>
<span class="sd">            density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">                If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">            prior: Prior distribution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">                initialization `inference = SNPE_A(prior)` or to `.build_posterior</span>
<span class="s2">                (prior=prior)`.&quot;&quot;&quot;</span>
            <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

        <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">correct_for_proposal</span><span class="p">(</span>
            <span class="n">density_estimator</span><span class="o">=</span><span class="n">density_estimator</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build_posterior</span><span class="p">(</span>
            <span class="n">density_estimator</span><span class="o">=</span><span class="n">wrapped_density_estimator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the log-probability of the proposal posterior.</span>

<span class="sd">        For SNPE-A this is the same as `self._neural_net.log_prob(theta, x)` in</span>
<span class="sd">        `_loss()` to be found in `snpe_base.py`.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters θ.</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns: Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_expand_mog</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replicate a singe Gaussian trained with Algorithm 1 before continuing</span>
<span class="sd">        with Algorithm 2. The weights and biases of the associated MDN layers</span>
<span class="sd">        are repeated `num_components` times, slightly perturbed to break the</span>
<span class="sd">        symmetry such that the gradients in the subsequent training are not</span>
<span class="sd">        all identical.</span>

<span class="sd">        Args:</span>
<span class="sd">            eps: Standard deviation for the random perturbation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">MultivariateGaussianMDN</span><span class="p">)</span>

        <span class="c1"># Increase the number of components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>

        <span class="c1"># Expand the 1-dim Gaussian.</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                <span class="n">key</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="s2">&quot;means&quot;</span><span class="p">,</span> <span class="s2">&quot;unconstrained&quot;</span><span class="p">,</span> <span class="s2">&quot;upper&quot;</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># let autograd construct a new gradient</span>
                <span class="k">elif</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># let autograd construct a new gradient</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npe.npe_a.NPE_A.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;mdn_snpe_a&#39;</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npe.npe_a.NPE_A.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>NPE-A [1].</p>
<p>[1] <em>Fast epsilon-free Inference of Simulation Models with Bayesian Conditional
    Density Estimation</em>, Papamakarios et al., NeurIPS 2016,
    <a href="https://arxiv.org/abs/1605.06376">https://arxiv.org/abs/1605.06376</a>.</p>
<p>Like all NPE methods, this method trains a deep neural density estimator to
directly approximate the posterior. Also like all other NPE methods, in the
first round, this density estimator is trained with a maximum-likelihood loss.</p>
<p>This class implements NPE-A. NPE-A trains across multiple rounds with a
maximum-likelihood-loss. This will make training converge to the proposal
posterior instead of the true posterior. To correct for this, SNPE-A applies a
post-hoc correction after training. This correction has to be performed
analytically. Thus, NPE-A is limited to Gaussian distributions for all but the
last round. In the last round, NPE-A can use a Mixture of Gaussians.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>density_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If it is a string (only &ldquo;mdn_snpe_a&rdquo; is valid), use a
pre-configured mixture of densities network. Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>. Note that until the last round only a
single (multivariate) Gaussian component is used for training (see
Algorithm 1 in [1]). In the last round, this component is replicated
<code>num_components</code> times, its parameters are perturbed with a very small
noise, and then the last training round is done with the expanded
Gaussian mixture as estimator for the proposal posterior.</p>
              </div>
            </td>
            <td>
                  <code>&#39;mdn_snpe_a&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_components</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of components of the mixture of Gaussians in the
last round. This overrides the <code>num_components</code> value passed to
<code>posterior_nn()</code>.</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
              </div>
            </td>
            <td>
                  <code>&#39;WARNING&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during training.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npe/npe_a.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">,</span>
    <span class="n">num_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;NPE-A [1].</span>

<span class="sd">    [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">        Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">        https://arxiv.org/abs/1605.06376.</span>

<span class="sd">    Like all NPE methods, this method trains a deep neural density estimator to</span>
<span class="sd">    directly approximate the posterior. Also like all other NPE methods, in the</span>
<span class="sd">    first round, this density estimator is trained with a maximum-likelihood loss.</span>

<span class="sd">    This class implements NPE-A. NPE-A trains across multiple rounds with a</span>
<span class="sd">    maximum-likelihood-loss. This will make training converge to the proposal</span>
<span class="sd">    posterior instead of the true posterior. To correct for this, SNPE-A applies a</span>
<span class="sd">    post-hoc correction after training. This correction has to be performed</span>
<span class="sd">    analytically. Thus, NPE-A is limited to Gaussian distributions for all but the</span>
<span class="sd">    last round. In the last round, NPE-A can use a Mixture of Gaussians.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        density_estimator: If it is a string (only &quot;mdn_snpe_a&quot; is valid), use a</span>
<span class="sd">            pre-configured mixture of densities network. Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`. Note that until the last round only a</span>
<span class="sd">            single (multivariate) Gaussian component is used for training (see</span>
<span class="sd">            Algorithm 1 in [1]). In the last round, this component is replicated</span>
<span class="sd">            `num_components` times, its parameters are perturbed with a very small</span>
<span class="sd">            noise, and then the last training round is done with the expanded</span>
<span class="sd">            Gaussian mixture as estimator for the proposal posterior.</span>
<span class="sd">        num_components: Number of components of the mixture of Gaussians in the</span>
<span class="sd">            last round. This overrides the `num_components` value passed to</span>
<span class="sd">            `posterior_nn()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Catch invalid inputs.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">((</span><span class="n">density_estimator</span> <span class="o">==</span> <span class="s2">&quot;mdn_snpe_a&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">callable</span><span class="p">(</span><span class="n">density_estimator</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;The `density_estimator` passed to SNPE_A needs to be a &quot;</span>
            <span class="s2">&quot;callable or the string &#39;mdn_snpe_a&#39;!&quot;</span>
        <span class="p">)</span>

    <span class="c1"># `num_components` will be used to replicate the Gaussian in the last round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span> <span class="o">=</span> <span class="n">num_components</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
    <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
    <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
    <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span>
        <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_components&quot;</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npe.npe_a.NPE_A.build_posterior" class="doc doc-heading">
            <code class=" language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npe.npe_a.NPE_A.build_posterior" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Build posterior from the neural density estimator.</p>
<p>This method first corrects the estimated density with <code>correct_for_proposal</code>
and then returns a <code>DirectPosterior</code>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>density_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchModule">TorchModule</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior distribution.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.inference.posteriors.direct_posterior.DirectPosterior" href="../posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior">DirectPosterior</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npe/npe_a.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DirectPosterior&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the neural density estimator.</span>

<span class="sd">    This method first corrects the estimated density with `correct_for_proposal`</span>
<span class="sd">    and then returns a `DirectPosterior`.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>
<span class="sd">        prior: Prior distribution.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;You did not pass a prior. You have to pass the prior either at</span>
<span class="s2">            initialization `inference = SNPE_A(prior)` or to `.build_posterior</span>
<span class="s2">            (prior=prior)`.&quot;&quot;&quot;</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

    <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">correct_for_proposal</span><span class="p">(</span>
        <span class="n">density_estimator</span><span class="o">=</span><span class="n">density_estimator</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build_posterior</span><span class="p">(</span>
        <span class="n">density_estimator</span><span class="o">=</span><span class="n">wrapped_density_estimator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npe.npe_a.NPE_A.correct_for_proposal" class="doc doc-heading">
            <code class=" language-python"><span class="n">correct_for_proposal</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npe.npe_a.NPE_A.correct_for_proposal" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Build mixture of Gaussians that approximates the posterior.</p>
<p>Returns a <code>NPE_A_MDN</code> object, which applies the posthoc-correction required in
SNPE-A.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>density_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TorchModule">TorchModule</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The density estimator that the posterior is based on.
If <code>None</code>, use the latest neural density estimator that was trained.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="sbi.inference.trainers.npe.npe_a.NPE_A_MDN">NPE_A_MDN</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npe/npe_a.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">correct_for_proposal</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NPE_A_MDN&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build mixture of Gaussians that approximates the posterior.</span>

<span class="sd">    Returns a `NPE_A_MDN` object, which applies the posthoc-correction required in</span>
<span class="sd">    SNPE-A.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: The density estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural density estimator that was trained.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">density_estimator</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="p">)</span>  <span class="c1"># PosteriorEstimator.train() also returns a deepcopy, mimic this here</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Set proposal of the density estimator.</span>
    <span class="c1"># This also evokes the z-scoring correction if necessary.</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">proposal</span><span class="p">,</span> <span class="p">(</span><span class="n">MultivariateNormal</span><span class="p">,</span> <span class="n">BoxUniform</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;Prior must be `torch.distributions.MultivariateNormal` or `sbi.utils.</span>
<span class="s2">            BoxUniform`&quot;&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">DirectPosterior</span>
        <span class="p">),</span> <span class="s2">&quot;&quot;&quot;The proposal you passed to `append_simulations` is neither the prior</span>
<span class="s2">            nor a `DirectPosterior`. SNPE-A currently only supports these scenarios.</span>
<span class="s2">            &quot;&quot;&quot;</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Create the NPE_A_MDN</span>
    <span class="n">wrapped_density_estimator</span> <span class="o">=</span> <span class="n">NPE_A_MDN</span><span class="p">(</span>
        <span class="n">flow</span><span class="o">=</span><span class="n">density_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">proposal</span><span class="o">=</span><span class="n">proposal</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">wrapped_density_estimator</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npe.npe_a.NPE_A.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">final_round</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">component_perturbation</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npe.npe_a.NPE_A.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return density estimator that approximates the proposal posterior.</p>
<p>[1] <em>Fast epsilon-free Inference of Simulation Models with Bayesian Conditional
    Density Estimation</em>, Papamakarios et al., NeurIPS 2016,
    <a href="https://arxiv.org/abs/1605.06376">https://arxiv.org/abs/1605.06376</a>.</p>
<p>Training is performed with maximum likelihood on samples from the latest round,
which leads the algorithm to converge to the proposal posterior.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>final_round</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether we are in the last round of training or not. For all
but the last round, Algorithm 1 from [1] is executed. In last the
round, Algorithm 2 from [1] is executed once.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>training_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training batch size.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate for Adam optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.0005</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>validation_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of data to use for validation.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_after_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_num_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
              </div>
            </td>
            <td>
                  <code>2 ** 31 - 1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>clip_max_norm</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
              </div>
            </td>
            <td>
                  <code>5.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>calibration_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A function to calibrate the loss with respect to the
simulations <code>x</code>. See Lueckmann, Gonçalves et al., NeurIPS 2017.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resume_training</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_first_round_loss</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, train with maximum likelihood,
i.e., potentially ignoring the correction for using a proposal
distribution different from the prior.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>retrain_from_scratch</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to retrain the conditional density
estimator for the posterior from scratch each round. Not supported for
SNPE-A.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_train_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to print the number of epochs and validation
loss and leakage after the training.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dataloader_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>component_perturbation</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The standard deviation applied to all weights and
biases when, in the last round, the Mixture of Gaussians is build from
a single Gaussian. This value can be problem-specific and also depends
on the number of mixture components.</p>
              </div>
            </td>
            <td>
                  <code>0.005</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.neural_nets.estimators.base.ConditionalDensityEstimator" href="../models/#sbi.neural_nets.estimators.ConditionalDensityEstimator">ConditionalDensityEstimator</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npe/npe_a.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">final_round</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">component_perturbation</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-3</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ConditionalDensityEstimator</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the proposal posterior.</span>

<span class="sd">    [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional</span>
<span class="sd">        Density Estimation_, Papamakarios et al., NeurIPS 2016,</span>
<span class="sd">        https://arxiv.org/abs/1605.06376.</span>

<span class="sd">    Training is performed with maximum likelihood on samples from the latest round,</span>
<span class="sd">    which leads the algorithm to converge to the proposal posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        final_round: Whether we are in the last round of training or not. For all</span>
<span class="sd">            but the last round, Algorithm 1 from [1] is executed. In last the</span>
<span class="sd">            round, Algorithm 2 from [1] is executed once.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">            simulations `x`. See Lueckmann, Gonçalves et al., NeurIPS 2017.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">            i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">            distribution different from the prior.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round. Not supported for</span>
<span class="sd">            SNPE-A.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">        component_perturbation: The standard deviation applied to all weights and</span>
<span class="sd">            biases when, in the last round, the Mixture of Gaussians is build from</span>
<span class="sd">            a single Gaussian. This value can be problem-specific and also depends</span>
<span class="sd">            on the number of mixture components.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">retrain_from_scratch</span><span class="p">,</span> <span class="s2">&quot;&quot;&quot;Retraining from scratch is not supported in</span>
<span class="s2">        SNPE-A yet. The reason for this is that, if we reininitialized the density</span>
<span class="s2">        estimator, the z-scoring would change, which would break the posthoc</span>
<span class="s2">        correction. This is a pure implementation issue.&quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span>
        <span class="n">entries</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;self&quot;</span><span class="p">,</span>
            <span class="s2">&quot;__class__&quot;</span><span class="p">,</span>
            <span class="s2">&quot;final_round&quot;</span><span class="p">,</span>
            <span class="s2">&quot;component_perturbation&quot;</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># SNPE-A always discards the prior samples.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;discard_prior_samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;force_first_round_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
        <span class="c1"># If there is (will be) only one round, train with Algorithm 2 from [1].</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
            <span class="p">)</span>
        <span class="c1"># Run Algorithm 2 from [1].</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span><span class="p">:</span>
            <span class="c1"># Now switch to the specified number of components. This method will</span>
            <span class="c1"># only be used if `retrain_from_scratch=True`. Otherwise,</span>
            <span class="c1"># the MDN will be built from replicating the single-component net for</span>
            <span class="c1"># `num_component` times (via `_expand_mog()`).</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_components</span>
            <span class="p">)</span>

            <span class="c1"># Extend the MDN to the originally desired number of components.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_expand_mog</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">component_perturbation</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You have already run SNPE-A with `final_round=True`. Running it&quot;</span>
                <span class="s2">&quot;again with this setting will not allow computing the posthoc&quot;</span>
                <span class="s2">&quot;correction applied in SNPE-A. Thus, you will get an error when &quot;</span>
                <span class="s2">&quot;calling `.build_posterior()` after training.&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Run Algorithm 1 from [1].</span>
        <span class="c1"># Wrap the function that builds the MDN such that we can make</span>
        <span class="c1"># sure that there is only one component when running.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">,</span> <span class="n">num_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">final_round</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_final_round</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.npe.npe_c.NPE_C" class="doc doc-heading">
            <code>NPE_C</code>


<a href="#sbi.inference.trainers.npe.npe_c.NPE_C" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.trainers.npe.npe_base.PosteriorEstimator">PosteriorEstimator</span></code></p>







              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/npe/npe_c.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NPE_C</span><span class="p">(</span><span class="n">PosteriorEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;NPE-C / APT [1].</span>

<span class="sd">        [1] _Automatic Posterior Transformation for Likelihood-free Inference_,</span>
<span class="sd">            Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</span>

<span class="sd">        Like all NPE methods, this method trains a deep neural density estimator to</span>
<span class="sd">        directly approximate the posterior. Also like all other NPE methods, in the</span>
<span class="sd">        first round, this density estimator is trained with a maximum-likelihood loss.</span>

<span class="sd">        For the sequential mode in which the density estimator is trained across rounds,</span>
<span class="sd">        this class implements two loss variants of NPE-C: the non-atomic and the atomic</span>
<span class="sd">        version. The atomic loss of NPE-C can be used for any density estimator,</span>
<span class="sd">        i.e. also for normalizing flows. However, it suffers from leakage issues. On</span>
<span class="sd">        the other hand, the non-atomic loss can only be used only if the proposal</span>
<span class="sd">        distribution is a mixture of Gaussians, the density estimator is a mixture of</span>
<span class="sd">        Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from</span>
<span class="sd">        leakage issues. At the beginning of each round, we print whether the non-atomic</span>
<span class="sd">        or the atomic version is used.</span>

<span class="sd">        In this codebase, we will automatically switch to the non-atomic loss if the</span>
<span class="sd">        following criteria are fulfilled:&lt;br/&gt;</span>
<span class="sd">        - proposal is a `DirectPosterior` with density_estimator `mdn`, as built</span>
<span class="sd">            with `sbi.neural_nets.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">        - the density estimator is a `mdn`, as built with</span>
<span class="sd">            `sbi.neural_nets.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">        - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or</span>
<span class="sd">            `isinstance(prior, sbi.utils.BoxUniform)`</span>

<span class="sd">        Note that custom implementations of any of these densities (or estimators) will</span>
<span class="sd">        not trigger the non-atomic loss, and the algorithm will fall back onto using</span>
<span class="sd">        the atomic loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them.</span>
<span class="sd">            density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">                provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_combined_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the distribution $p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_atoms: Number of atoms to use for classification.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">                simulations `x`. See Lueckmann, Gonçalves et al., NeurIPS 2017.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">                i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">                distribution different from the prior.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            use_combined_loss: Whether to train the neural net also on prior samples</span>
<span class="sd">                using maximum likelihood in addition to training it on all samples using</span>
<span class="sd">                atomic loss. The extra MLE loss helps prevent density leaking with</span>
<span class="sd">                bounded priors.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
        <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
        <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
        <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span> <span class="o">=</span> <span class="n">use_combined_loss</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
            <span class="nb">locals</span><span class="p">(),</span>
            <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="s2">&quot;use_combined_loss&quot;</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Set the proposal to the last proposal that was passed by the user. For</span>
            <span class="c1"># atomic SNPE, it does not matter what the proposal is. For non-atomic</span>
            <span class="c1"># SNPE, we only use the latest data that was passed, i.e. the one from the</span>
            <span class="c1"># last proposal.</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">DirectPosterior</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">check_dist_class</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">class_to_check</span><span class="o">=</span><span class="p">(</span><span class="n">Uniform</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">)</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;non-atomic&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="k">else</span> <span class="s2">&quot;atomic&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using SNPE-C with </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> loss&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
                <span class="c1"># Take care of z-scoring, pre-compute and store prior terms.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_state_for_mog_proposal</span><span class="p">()</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_state_for_mog_proposal</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set state variables that are used at each training step of non-atomic SNPE-C.</span>

<span class="sd">        Three things are computed:</span>
<span class="sd">        1) Check if z-scoring was requested. To do so, we check if the `_transform`</span>
<span class="sd">            argument of the net had been a `CompositeTransform`. See pyknos mdn.py.</span>
<span class="sd">        2) Define a (potentially standardized) prior. It&#39;s standardized if z-scoring</span>
<span class="sd">            had been requested.</span>
<span class="sd">        3) Compute (Precision * mean) for the prior. This quantity is used at every</span>
<span class="sd">            training step if the prior is Gaussian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_transform</span><span class="p">,</span> <span class="n">CompositeTransform</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_maybe_z_scored_prior</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prec_m_prod_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">precision_matrix</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_maybe_z_scored_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute and store potentially standardized prior (if z-scoring was done).</span>

<span class="sd">        The proposal posterior is:</span>
<span class="sd">        $pp(\theta|x) = 1/Z * q(\theta|x) * prop(\theta) / p(\theta)$</span>

<span class="sd">        Let&#39;s denote z-scored theta by `a`: a = (theta - mean) / std</span>
<span class="sd">        Then pp&#39;(a|x) = 1/Z_2 * q&#39;(a|x) * prop&#39;(a) / p&#39;(a)$</span>

<span class="sd">        The &#39; indicates that the evaluation occurs in standardized space. The constant</span>
<span class="sd">        scaling factor has been absorbed into Z_2.</span>
<span class="sd">        From the above equation, we see that we need to evaluate the prior **in</span>
<span class="sd">        standardized space**. We build the standardized prior in this function.</span>

<span class="sd">        The standardize transform that is applied to the samples theta does not use</span>
<span class="sd">        the exact prior mean and std (due to implementation issues). Hence, the z-scored</span>
<span class="sd">        prior will not be exactly have mean=0 and std=1.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">_transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_scale</span>
            <span class="n">shift</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">_transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_shift</span>

            <span class="c1"># Following the definintion of the linear transform in</span>
            <span class="c1"># `standardizing_transform` in `sbiutils.py`:</span>
            <span class="c1"># shift=-mean / std</span>
            <span class="c1"># scale=1 / std</span>
            <span class="c1"># Solving these equations for mean and std:</span>
            <span class="n">estim_prior_std</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">scale</span>
            <span class="n">estim_prior_mean</span> <span class="o">=</span> <span class="o">-</span><span class="n">shift</span> <span class="o">*</span> <span class="n">estim_prior_std</span>

            <span class="c1"># Compute the discrepancy of the true prior mean and std and the mean and</span>
            <span class="c1"># std that was empirically estimated from samples.</span>
            <span class="c1"># N(theta|m,s) = N((theta-m_e)/s_e|(m-m_e)/s_e, s/s_e)</span>
            <span class="c1"># Above: m,s are true prior mean and std. m_e,s_e are estimated prior mean</span>
            <span class="c1"># and std (estimated from samples and used to build standardize transform).</span>
            <span class="n">almost_zero_mean</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">mean</span> <span class="o">-</span> <span class="n">estim_prior_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">estim_prior_std</span>
            <span class="n">almost_one_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">variance</span><span class="p">)</span> <span class="o">/</span> <span class="n">estim_prior_std</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
                    <span class="n">almost_zero_mean</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">almost_one_std</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">range_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">almost_one_std</span> <span class="o">*</span> <span class="mf">3.0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="n">BoxUniform</span><span class="p">(</span>
                    <span class="n">almost_zero_mean</span> <span class="o">-</span> <span class="n">range_</span><span class="p">,</span> <span class="n">almost_zero_mean</span> <span class="o">+</span> <span class="n">range_</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">DirectPosterior</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the log-probability of the proposal posterior.</span>

<span class="sd">        If the proposal is a MoG, the density estimator is a MoG, and the prior is</span>
<span class="sd">        either Gaussian or uniform, we use non-atomic loss. Else, use atomic loss (which</span>
<span class="sd">        suffers from leakage).</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters θ.</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns: Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
                <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="s2">&quot;_distribution&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The density estimator must be a MDNtext for non-atomic loss.&quot;</span>
                <span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prob_proposal_posterior_mog</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">proposal</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="p">,</span> <span class="s2">&quot;log_prob&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The neural estimator must have a log_prob method, for</span><span class="se">\</span>
<span class="s2">                                 atomic loss. It should at best follow the </span><span class="se">\</span>
<span class="s2">                                 sbi.neural_nets &#39;DensityEstiamtor&#39; interface.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_prob_proposal_posterior_atomic</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior_atomic</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return log probability of the proposal posterior for atomic proposals.</span>

<span class="sd">        We have two main options when evaluating the proposal posterior.</span>
<span class="sd">            (1) Generate atoms from the proposal prior.</span>
<span class="sd">            (2) Generate atoms from a more targeted distribution, such as the most</span>
<span class="sd">                recent posterior.</span>
<span class="sd">        If we choose the latter, it is likely beneficial not to do this in the first</span>
<span class="sd">        round, since we would be sampling from a randomly-initialized neural density</span>
<span class="sd">        estimator.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters θ.</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            masks: Mask that is True for prior samples in the batch in order to train</span>
<span class="sd">                them with prior loss.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">num_atoms</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">clamp_and_warn</span><span class="p">(</span><span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Each set of parameter atoms is evaluated using the same x,</span>
        <span class="c1"># so we repeat rows of the data x, e.g. [1, 2] -&gt; [1, 1, 2, 2]</span>
        <span class="n">repeated_x</span> <span class="o">=</span> <span class="n">repeat_rows</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># To generate the full set of atoms for a given item in the batch,</span>
        <span class="c1"># we sample without replacement num_atoms - 1 times from the rest</span>
        <span class="c1"># of the theta in the batch.</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eye</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">choices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_atoms</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">contrasting_theta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">choices</span><span class="p">]</span>

        <span class="c1"># We can now create our sets of atoms from the contrasting parameter sets</span>
        <span class="c1"># we have generated.</span>
        <span class="n">atomic_theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">theta</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">contrasting_theta</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_atoms</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="c1"># Get (batch_size * num_atoms) log prob prior evals.</span>
        <span class="n">log_prob_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">atomic_theta</span><span class="p">)</span>
        <span class="n">log_prob_prior</span> <span class="o">=</span> <span class="n">log_prob_prior</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>
        <span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_prior</span><span class="p">,</span> <span class="s2">&quot;prior eval&quot;</span><span class="p">)</span>

        <span class="c1"># Evaluate large batch giving (batch_size * num_atoms) log prob posterior evals.</span>
        <span class="n">atomic_theta</span> <span class="o">=</span> <span class="n">reshape_to_sample_batch_event</span><span class="p">(</span>
            <span class="n">atomic_theta</span><span class="p">,</span> <span class="n">atomic_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="p">)</span>
        <span class="n">repeated_x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span>
            <span class="n">repeated_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">condition_shape</span>
        <span class="p">)</span>
        <span class="n">log_prob_posterior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">atomic_theta</span><span class="p">,</span> <span class="n">repeated_x</span><span class="p">)</span>
        <span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_posterior</span><span class="p">,</span> <span class="s2">&quot;posterior eval&quot;</span><span class="p">)</span>
        <span class="n">log_prob_posterior</span> <span class="o">=</span> <span class="n">log_prob_posterior</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># Compute unnormalized proposal posterior.</span>
        <span class="n">unnormalized_log_prob</span> <span class="o">=</span> <span class="n">log_prob_posterior</span> <span class="o">-</span> <span class="n">log_prob_prior</span>

        <span class="c1"># Normalize proposal posterior across discrete set of atoms.</span>
        <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="n">unnormalized_log_prob</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
            <span class="n">unnormalized_log_prob</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">assert_all_finite</span><span class="p">(</span><span class="n">log_prob_proposal_posterior</span><span class="p">,</span> <span class="s2">&quot;proposal posterior eval&quot;</span><span class="p">)</span>

        <span class="c1"># XXX This evaluates the posterior on _all_ prior samples</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span><span class="p">:</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">reshape_to_sample_batch_event</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_to_batch_event</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">condition_shape</span><span class="p">)</span>
            <span class="n">log_prob_posterior_non_atomic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="c1"># squeeze to remove sample dimension, which is always one during the loss</span>
            <span class="c1"># evaluation of `SNPE_C` (because we have one theta vector per x vector).</span>
            <span class="n">log_prob_posterior_non_atomic</span> <span class="o">=</span> <span class="n">log_prob_posterior_non_atomic</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">masks</span> <span class="o">*</span> <span class="n">log_prob_posterior_non_atomic</span> <span class="o">+</span> <span class="n">log_prob_proposal_posterior</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_proposal_posterior</span>

    <span class="k">def</span> <span class="nf">_log_prob_proposal_posterior_mog</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">proposal</span><span class="p">:</span> <span class="n">DirectPosterior</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return log-probability of the proposal posterior for MoG proposal.</span>

<span class="sd">        For MoG proposals and MoG density estimators, this can be done in closed form</span>
<span class="sd">        and does not require atomic loss (i.e. there will be no leakage issues).</span>

<span class="sd">        Notation:</span>

<span class="sd">        m are mean vectors.</span>
<span class="sd">        prec are precision matrices.</span>
<span class="sd">        cov are covariance matrices.</span>

<span class="sd">        _p at the end indicates that it is the proposal.</span>
<span class="sd">        _d indicates that it is the density estimator.</span>
<span class="sd">        _pp indicates the proposal posterior.</span>

<span class="sd">        All tensors will have shapes (batch_dim, num_components, ...)</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Batch of parameters θ.</span>
<span class="sd">            x: Batch of data.</span>
<span class="sd">            proposal: Proposal distribution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Log-probability of the proposal posterior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Evaluate the proposal. MDNs do not have functionality to run the embedding_net</span>
        <span class="c1"># and then get the mixture_components (**without** calling log_prob()). Hence,</span>
        <span class="c1"># we call them separately here.</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_embedding_net</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">default_x</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span>
        <span class="p">)</span>  <span class="c1"># defined to avoid ugly black formatting.</span>
        <span class="n">logits_p</span><span class="p">,</span> <span class="n">m_p</span><span class="p">,</span> <span class="n">prec_p</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_mixture_components</span><span class="p">(</span><span class="n">encoded_x</span><span class="p">)</span>
        <span class="n">norm_logits_p</span> <span class="o">=</span> <span class="n">logits_p</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits_p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Evaluate the density estimator.</span>
        <span class="n">encoded_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_embedding_net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span>  <span class="c1"># defined to avoid black formatting.</span>
        <span class="n">logits_d</span><span class="p">,</span> <span class="n">m_d</span><span class="p">,</span> <span class="n">prec_d</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_mixture_components</span><span class="p">(</span><span class="n">encoded_x</span><span class="p">)</span>
        <span class="n">norm_logits_d</span> <span class="o">=</span> <span class="n">logits_d</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># z-score theta if it z-scoring had been requested.</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_score_theta</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Compute the MoG parameters of the proposal posterior.</span>
        <span class="p">(</span>
            <span class="n">logits_pp</span><span class="p">,</span>
            <span class="n">m_pp</span><span class="p">,</span>
            <span class="n">prec_pp</span><span class="p">,</span>
            <span class="n">cov_pp</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_posterior_transformation</span><span class="p">(</span>
            <span class="n">norm_logits_p</span><span class="p">,</span> <span class="n">m_p</span><span class="p">,</span> <span class="n">prec_p</span><span class="p">,</span> <span class="n">norm_logits_d</span><span class="p">,</span> <span class="n">m_d</span><span class="p">,</span> <span class="n">prec_d</span>
        <span class="p">)</span>

        <span class="c1"># Compute the log_prob of theta under the product.</span>
        <span class="n">log_prob_proposal_posterior</span> <span class="o">=</span> <span class="n">mog_log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">logits_pp</span><span class="p">,</span> <span class="n">m_pp</span><span class="p">,</span> <span class="n">prec_pp</span><span class="p">)</span>
        <span class="n">assert_all_finite</span><span class="p">(</span>
            <span class="n">log_prob_proposal_posterior</span><span class="p">,</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;the evaluation of the MoG proposal posterior. This is likely due to a</span>
<span class="sd">            numerical instability in the training procedure. Please create an issue on</span>
<span class="sd">            Github.&quot;&quot;&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_proposal_posterior</span>

    <span class="k">def</span> <span class="nf">_automatic_posterior_transformation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">logits_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the MoG parameters of the proposal posterior.</span>

<span class="sd">        The proposal posterior is:</span>
<span class="sd">        $pp(\theta|x) = 1/Z * q(\theta|x) * prop(\theta) / p(\theta)$</span>
<span class="sd">        In words: proposal posterior = posterior estimate * proposal / prior.</span>

<span class="sd">        If the posterior estimate and the proposal are MoG and the prior is either</span>
<span class="sd">        Gaussian or uniform, we can solve this in closed-form. The is implemented in</span>
<span class="sd">        this function.</span>

<span class="sd">        This function implements Appendix A1 from Greenberg et al. 2019.</span>

<span class="sd">        We have to build L*K components. How do we do this?</span>
<span class="sd">        Example: proposal has two components, density estimator has three components.</span>
<span class="sd">        Let&#39;s call the two components of the proposal i,j and the three components</span>
<span class="sd">        of the density estimator x,y,z. We have to multiply every component of the</span>
<span class="sd">        proposal with every component of the density estimator. So, what we do is:</span>
<span class="sd">        1) for the proposal, build: i,i,i,j,j,j. Done with torch.repeat_interleave()</span>
<span class="sd">        2) for the density estimator, build: x,y,z,x,y,z. Done with torch.repeat()</span>
<span class="sd">        3) Multiply them with simple matrix operations.</span>

<span class="sd">        Args:</span>
<span class="sd">            logits_p: Component weight of each Gaussian of the proposal.</span>
<span class="sd">            means_p: Mean of each Gaussian of the proposal.</span>
<span class="sd">            precisions_p: Precision matrix of each Gaussian of the proposal.</span>
<span class="sd">            logits_d: Component weight for each Gaussian of the density estimator.</span>
<span class="sd">            means_d: Mean of each Gaussian of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrix of each Gaussian of the density estimator.</span>

<span class="sd">        Returns: (Component weight, mean, precision matrix, covariance matrix) of each</span>
<span class="sd">            Gaussian of the proposal posterior. Has L*K terms (proposal has L terms,</span>
<span class="sd">            density estimator has K terms).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_precisions_proposal_posterior</span><span class="p">(</span>
            <span class="n">precisions_p</span><span class="p">,</span> <span class="n">precisions_d</span>
        <span class="p">)</span>

        <span class="n">means_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_means_proposal_posterior</span><span class="p">(</span>
            <span class="n">covariances_pp</span><span class="p">,</span> <span class="n">means_p</span><span class="p">,</span> <span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_d</span><span class="p">,</span> <span class="n">precisions_d</span>
        <span class="p">)</span>

        <span class="n">logits_pp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logits_proposal_posterior</span><span class="p">(</span>
            <span class="n">means_pp</span><span class="p">,</span>
            <span class="n">precisions_pp</span><span class="p">,</span>
            <span class="n">covariances_pp</span><span class="p">,</span>
            <span class="n">logits_p</span><span class="p">,</span>
            <span class="n">means_p</span><span class="p">,</span>
            <span class="n">precisions_p</span><span class="p">,</span>
            <span class="n">logits_d</span><span class="p">,</span>
            <span class="n">means_d</span><span class="p">,</span>
            <span class="n">precisions_d</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">logits_pp</span><span class="p">,</span> <span class="n">means_pp</span><span class="p">,</span> <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span>

    <span class="k">def</span> <span class="nf">_precisions_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the precisions and covariances of the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: (Precisions, Covariances) of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">precisions_p_rep</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">precisions_d_rep</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">precisions_pp</span> <span class="o">=</span> <span class="n">precisions_p_rep</span> <span class="o">+</span> <span class="n">precisions_d_rep</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="n">precisions_pp</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="o">.</span><span class="n">precision_matrix</span>

        <span class="n">covariances_pp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">precisions_pp</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">precisions_pp</span><span class="p">,</span> <span class="n">covariances_pp</span>

    <span class="k">def</span> <span class="nf">_means_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">covariances_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the means of the proposal posterior.</span>

<span class="sd">        means_pp = C_ix * (P_i * m_i + P_x * m_x - P_o * m_o).</span>

<span class="sd">        Args:</span>
<span class="sd">            covariances_pp: Covariance matrices of the proposal posterior.</span>
<span class="sd">            means_p: Means of the proposal distribution.</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            means_d: Means of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: Means of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># First, compute the product P_i * m_i and P_j * m_j</span>
        <span class="n">prec_m_prod_p</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_p</span><span class="p">)</span>
        <span class="n">prec_m_prod_d</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">,</span> <span class="n">means_d</span><span class="p">)</span>

        <span class="c1"># Repeat them to allow for matrix operations: same trick as for the precisions.</span>
        <span class="n">prec_m_prod_p_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">prec_m_prod_d_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Means = C_ij * (P_i * m_i + P_x * m_x - P_o * m_o).</span>
        <span class="n">summed_cov_m_prod_rep</span> <span class="o">=</span> <span class="n">prec_m_prod_p_rep</span> <span class="o">+</span> <span class="n">prec_m_prod_d_rep</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_maybe_z_scored_prior</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">):</span>
            <span class="n">summed_cov_m_prod_rep</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prec_m_prod_prior</span>

        <span class="n">means_pp</span> <span class="o">=</span> <span class="n">batched_mixture_mv</span><span class="p">(</span><span class="n">covariances_pp</span><span class="p">,</span> <span class="n">summed_cov_m_prod_rep</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">means_pp</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_logits_proposal_posterior</span><span class="p">(</span>
        <span class="n">means_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">covariances_pp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_p</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">means_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">precisions_d</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the component weights (i.e. logits) of the proposal posterior.</span>

<span class="sd">        Args:</span>
<span class="sd">            means_pp: Means of the proposal posterior.</span>
<span class="sd">            precisions_pp: Precision matrices of the proposal posterior.</span>
<span class="sd">            covariances_pp: Covariance matrices of the proposal posterior.</span>
<span class="sd">            logits_p: Component weights (i.e. logits) of the proposal distribution.</span>
<span class="sd">            means_p: Means of the proposal distribution.</span>
<span class="sd">            precisions_p: Precision matrices of the proposal distribution.</span>
<span class="sd">            logits_d: Component weights (i.e. logits) of the density estimator.</span>
<span class="sd">            means_d: Means of the density estimator.</span>
<span class="sd">            precisions_d: Precision matrices of the density estimator.</span>

<span class="sd">        Returns: Component weights of the proposal posterior. L*K terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_comps_p</span> <span class="o">=</span> <span class="n">precisions_p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_comps_d</span> <span class="o">=</span> <span class="n">precisions_d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Compute log(alpha_i * beta_j)</span>
        <span class="n">logits_p_rep</span> <span class="o">=</span> <span class="n">logits_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">logits_d_rep</span> <span class="o">=</span> <span class="n">logits_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>
        <span class="n">logit_factors</span> <span class="o">=</span> <span class="n">logits_p_rep</span> <span class="o">+</span> <span class="n">logits_d_rep</span>

        <span class="c1"># Compute sqrt(det()/(det()*det()))</span>
        <span class="n">logdet_covariances_pp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">covariances_pp</span><span class="p">)</span>
        <span class="n">logdet_covariances_p</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">)</span>
        <span class="n">logdet_covariances_d</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">)</span>

        <span class="c1"># Repeat the proposal and density estimator terms such that there are LK terms.</span>
        <span class="c1"># Same trick as has been used above.</span>
        <span class="n">logdet_covariances_p_rep</span> <span class="o">=</span> <span class="n">logdet_covariances_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
            <span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">logdet_covariances_d_rep</span> <span class="o">=</span> <span class="n">logdet_covariances_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>

        <span class="n">log_sqrt_det_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">logdet_covariances_pp</span>
            <span class="o">-</span> <span class="p">(</span><span class="n">logdet_covariances_p_rep</span> <span class="o">+</span> <span class="n">logdet_covariances_d_rep</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Compute for proposal, density estimator, and proposal posterior:</span>
        <span class="c1"># mu_i.T * P_i * mu_i</span>
        <span class="n">exponent_p</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_p</span><span class="p">,</span> <span class="n">means_p</span><span class="p">)</span>
        <span class="n">exponent_d</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_d</span><span class="p">,</span> <span class="n">means_d</span><span class="p">)</span>
        <span class="n">exponent_pp</span> <span class="o">=</span> <span class="n">batched_mixture_vmv</span><span class="p">(</span><span class="n">precisions_pp</span><span class="p">,</span> <span class="n">means_pp</span><span class="p">)</span>

        <span class="c1"># Extend proposal and density estimator exponents to get LK terms.</span>
        <span class="n">exponent_p_rep</span> <span class="o">=</span> <span class="n">exponent_p</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_comps_d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">exponent_d_rep</span> <span class="o">=</span> <span class="n">exponent_d</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_comps_p</span><span class="p">)</span>
        <span class="n">exponent</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">exponent_p_rep</span> <span class="o">+</span> <span class="n">exponent_d_rep</span> <span class="o">-</span> <span class="n">exponent_pp</span><span class="p">)</span>

        <span class="n">logits_pp</span> <span class="o">=</span> <span class="n">logit_factors</span> <span class="o">+</span> <span class="n">log_sqrt_det_ratio</span> <span class="o">+</span> <span class="n">exponent</span>

        <span class="k">return</span> <span class="n">logits_pp</span>

    <span class="k">def</span> <span class="nf">_maybe_z_score_theta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return potentially standardized theta if z-scoring was requested.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_score_theta</span><span class="p">:</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">theta</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npe.npe_c.NPE_C.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npe.npe_c.NPE_C.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>NPE-C / APT [1].</p>
<p>[1] <em>Automatic Posterior Transformation for Likelihood-free Inference</em>,
    Greenberg et al., ICML 2019, <a href="https://arxiv.org/abs/1905.07488">https://arxiv.org/abs/1905.07488</a>.</p>
<p>Like all NPE methods, this method trains a deep neural density estimator to
directly approximate the posterior. Also like all other NPE methods, in the
first round, this density estimator is trained with a maximum-likelihood loss.</p>
<p>For the sequential mode in which the density estimator is trained across rounds,
this class implements two loss variants of NPE-C: the non-atomic and the atomic
version. The atomic loss of NPE-C can be used for any density estimator,
i.e. also for normalizing flows. However, it suffers from leakage issues. On
the other hand, the non-atomic loss can only be used only if the proposal
distribution is a mixture of Gaussians, the density estimator is a mixture of
Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from
leakage issues. At the beginning of each round, we print whether the non-atomic
or the atomic version is used.</p>
<p>In this codebase, we will automatically switch to the non-atomic loss if the
following criteria are fulfilled:<br/>
- proposal is a <code>DirectPosterior</code> with density_estimator <code>mdn</code>, as built
    with <code>sbi.neural_nets.posterior_nn()</code>.<br/>
- the density estimator is a <code>mdn</code>, as built with
    <code>sbi.neural_nets.posterior_nn()</code>.<br/>
- <code>isinstance(prior, MultivariateNormal)</code> (from <code>torch.distributions</code>) or
    <code>isinstance(prior, sbi.utils.BoxUniform)</code></p>
<p>Note that custom implementations of any of these densities (or estimators) will
not trigger the non-atomic loss, and the algorithm will fall back onto using
the atomic loss.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>density_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If it is a string, use a pre-configured network of the
provided type (one of nsf, maf, mdn, made). Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>.</p>
              </div>
            </td>
            <td>
                  <code>&#39;maf&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
              </div>
            </td>
            <td>
                  <code>&#39;WARNING&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during training.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npe/npe_c.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;NPE-C / APT [1].</span>

<span class="sd">    [1] _Automatic Posterior Transformation for Likelihood-free Inference_,</span>
<span class="sd">        Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488.</span>

<span class="sd">    Like all NPE methods, this method trains a deep neural density estimator to</span>
<span class="sd">    directly approximate the posterior. Also like all other NPE methods, in the</span>
<span class="sd">    first round, this density estimator is trained with a maximum-likelihood loss.</span>

<span class="sd">    For the sequential mode in which the density estimator is trained across rounds,</span>
<span class="sd">    this class implements two loss variants of NPE-C: the non-atomic and the atomic</span>
<span class="sd">    version. The atomic loss of NPE-C can be used for any density estimator,</span>
<span class="sd">    i.e. also for normalizing flows. However, it suffers from leakage issues. On</span>
<span class="sd">    the other hand, the non-atomic loss can only be used only if the proposal</span>
<span class="sd">    distribution is a mixture of Gaussians, the density estimator is a mixture of</span>
<span class="sd">    Gaussians, and the prior is either Gaussian or Uniform. It does not suffer from</span>
<span class="sd">    leakage issues. At the beginning of each round, we print whether the non-atomic</span>
<span class="sd">    or the atomic version is used.</span>

<span class="sd">    In this codebase, we will automatically switch to the non-atomic loss if the</span>
<span class="sd">    following criteria are fulfilled:&lt;br/&gt;</span>
<span class="sd">    - proposal is a `DirectPosterior` with density_estimator `mdn`, as built</span>
<span class="sd">        with `sbi.neural_nets.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">    - the density estimator is a `mdn`, as built with</span>
<span class="sd">        `sbi.neural_nets.posterior_nn()`.&lt;br/&gt;</span>
<span class="sd">    - `isinstance(prior, MultivariateNormal)` (from `torch.distributions`) or</span>
<span class="sd">        `isinstance(prior, sbi.utils.BoxUniform)`</span>

<span class="sd">    Note that custom implementations of any of these densities (or estimators) will</span>
<span class="sd">    not trigger the non-atomic loss, and the algorithm will fall back onto using</span>
<span class="sd">    the atomic loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them.</span>
<span class="sd">        density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">            provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npe.npe_c.NPE_C.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_first_round_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_combined_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npe.npe_c.NPE_C.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>num_atoms</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of atoms to use for classification.</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>training_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training batch size.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate for Adam optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.0005</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>validation_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of data to use for validation.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_after_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_num_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
              </div>
            </td>
            <td>
                  <code>2 ** 31 - 1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>clip_max_norm</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
              </div>
            </td>
            <td>
                  <code>5.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>calibration_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A function to calibrate the loss with respect to the
simulations <code>x</code>. See Lueckmann, Gonçalves et al., NeurIPS 2017.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resume_training</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_first_round_loss</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, train with maximum likelihood,
i.e., potentially ignoring the correction for using a proposal
distribution different from the prior.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>discard_prior_samples</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_combined_loss</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to train the neural net also on prior samples
using maximum likelihood in addition to training it on all samples using
atomic loss. The extra MLE loss helps prevent density leaking with
bounded priors.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>retrain_from_scratch</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_train_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to print the number of epochs and validation
loss and leakage after the training.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dataloader_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Density estimator that approximates the distribution <span class="arithmatex">\(p(\theta|x)\)</span>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npe/npe_c.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_combined_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return density estimator that approximates the distribution $p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_atoms: Number of atoms to use for classification.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect to the</span>
<span class="sd">            simulations `x`. See Lueckmann, Gonçalves et al., NeurIPS 2017.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">            i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">            distribution different from the prior.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        use_combined_loss: Whether to train the neural net also on prior samples</span>
<span class="sd">            using maximum likelihood in addition to training it on all samples using</span>
<span class="sd">            atomic loss. The extra MLE loss helps prevent density leaking with</span>
<span class="sd">            bounded priors.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Density estimator that approximates the distribution $p(\theta|x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># WARNING: sneaky trick ahead. We proxy the parent&#39;s `train` here,</span>
    <span class="c1"># requiring the signature to have `num_atoms`, save it for use below, and</span>
    <span class="c1"># continue. It&#39;s sneaky because we are using the object (self) as a namespace</span>
    <span class="c1"># to pass arguments between functions, and that&#39;s implicit state management.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_combined_loss</span> <span class="o">=</span> <span class="n">use_combined_loss</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span>
        <span class="nb">locals</span><span class="p">(),</span>
        <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;num_atoms&quot;</span><span class="p">,</span> <span class="s2">&quot;use_combined_loss&quot;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Set the proposal to the last proposal that was passed by the user. For</span>
        <span class="c1"># atomic SNPE, it does not matter what the proposal is. For non-atomic</span>
        <span class="c1"># SNPE, we only use the latest data that was passed, i.e. the one from the</span>
        <span class="c1"># last proposal.</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">DirectPosterior</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="o">.</span><span class="n">posterior_estimator</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">_distribution</span><span class="p">,</span> <span class="n">mdn</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">check_dist_class</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">class_to_check</span><span class="o">=</span><span class="p">(</span><span class="n">Uniform</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">)</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">algorithm</span> <span class="o">=</span> <span class="s2">&quot;non-atomic&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span> <span class="k">else</span> <span class="s2">&quot;atomic&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using SNPE-C with </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> loss&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_non_atomic_loss</span><span class="p">:</span>
            <span class="c1"># Take care of z-scoring, pre-compute and store prior terms.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_state_for_mog_proposal</span><span class="p">()</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.fmpe.fmpe.FMPE" class="doc doc-heading">
            <code>FMPE</code>


<a href="#sbi.inference.trainers.fmpe.fmpe.FMPE" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.trainers.base.NeuralInference">NeuralInference</span></code></p>


        <p>Implements the Flow Matching Posterior Estimator (FMPE) for
simulation-based inference.</p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/fmpe/fmpe.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FMPE</span><span class="p">(</span><span class="n">NeuralInference</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implements the Flow Matching Posterior Estimator (FMPE) for</span>
<span class="sd">    simulation-based inference.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">],</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mlp&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialization method for the FMPE class.</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: Prior distribution.</span>
<span class="sd">            density_estimator: Neural network architecture used to learn the vector</span>
<span class="sd">                field for flow matching. Can be a string, e.g., &#39;mlp&#39; or &#39;resnet&#39;, or a</span>
<span class="sd">                `Callable` that builds a corresponding neural network.</span>
<span class="sd">            device: Device to use for training.</span>
<span class="sd">            logging_level: Logging level.</span>
<span class="sd">            summary_writer: Summary writer for tensorboard.</span>
<span class="sd">            show_progress_bars: Whether to show progress bars.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># obtain the shape of the prior samples</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">density_estimator</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">flowmatching_nn</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">density_estimator</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">density_estimator</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">logging_level</span><span class="o">=</span><span class="n">logging_level</span><span class="p">,</span>
            <span class="n">summary_writer</span><span class="o">=</span><span class="n">summary_writer</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">append_simulations</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DirectPosterior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">data_device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuralInference</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">proposal</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="n">proposal</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
            <span class="ow">or</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">RestrictedPrior</span><span class="p">)</span> <span class="ow">and</span> <span class="n">proposal</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">current_round</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Sequential FMPE with proposal different from prior is not implemented.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">exclude_invalid_x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">exclude_invalid_x</span> <span class="o">=</span> <span class="n">current_round</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">data_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">data_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>

        <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">validate_theta_and_x</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">data_device</span><span class="o">=</span><span class="n">data_device</span><span class="p">,</span> <span class="n">training_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="p">)</span>

        <span class="n">is_valid_x</span><span class="p">,</span> <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span> <span class="o">=</span> <span class="n">handle_invalid_x</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="n">exclude_invalid_x</span>
        <span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>

        <span class="c1"># Check for problematic z-scoring</span>
        <span class="n">warn_if_zscoring_changes_data</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Check whether there are NaNs or Infs in the data and remove accordingly.</span>
        <span class="n">npe_msg_on_invalid_x</span><span class="p">(</span>
            <span class="n">num_nans</span><span class="o">=</span><span class="n">num_nans</span><span class="p">,</span>
            <span class="n">num_infs</span><span class="o">=</span><span class="n">num_infs</span><span class="p">,</span>
            <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="n">exclude_invalid_x</span><span class="p">,</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;Single-round FMPE&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_round</span><span class="p">)</span>
        <span class="n">prior_masks</span> <span class="o">=</span> <span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">current_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">),</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prior_masks</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ConditionalDensityEstimator</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Train the flow matching estimator.</span>

<span class="sd">        Args:</span>
<span class="sd">            training_batch_size: Batch size for training. Defaults to 50.</span>
<span class="sd">            learning_rate: Learning rate for training. Defaults to 5e-4.</span>
<span class="sd">            validation_fraction: Fraction of the data to use for validation.</span>
<span class="sd">            stop_after_epochs: Number of epochs to train for. Defaults to 20.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to train for.</span>
<span class="sd">            clip_max_norm: Maximum norm for gradient clipping. Defaults to 5.0.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            force_first_round_loss: Whether to allow training with</span>
<span class="sd">                simulations that have not been sampled from the prior, e.g., in a</span>
<span class="sd">                sequential inference setting. Note that can lead to biased inference</span>
<span class="sd">                results.</span>
<span class="sd">            show_train_summary: Whether to show the training summary. Defaults to False.</span>
<span class="sd">            dataloader_kwargs: Additional keyword arguments for the dataloader.</span>

<span class="sd">        Returns:</span>
<span class="sd">            DensityEstimator: Trained flow matching estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Load data from most recent round.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">force_first_round_loss</span> <span class="ow">or</span> <span class="n">resume_training</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;You have already trained this neural network. After you had trained &quot;</span>
                <span class="s2">&quot;the network, you again appended simulations with `append_simulations&quot;</span>
                <span class="s2">&quot;(theta, x)`, but you did not provide a proposal. If the new &quot;</span>
                <span class="s2">&quot;simulations are sampled from the prior, you can set &quot;</span>
                <span class="s2">&quot;`.train(..., force_first_round_loss=True`). However, if the new &quot;</span>
                <span class="s2">&quot;simulations were not sampled from the prior, you should pass the &quot;</span>
                <span class="s2">&quot;proposal, i.e. `append_simulations(theta, x, proposal)`. If &quot;</span>
                <span class="s2">&quot;your samples are not sampled from the prior and you do not pass a &quot;</span>
                <span class="s2">&quot;proposal and you set `force_first_round_loss=True`, the result of &quot;</span>
                <span class="s2">&quot;FMPE will not be the true posterior. Instead, it will be the proposal &quot;</span>
                <span class="s2">&quot;posterior, which (usually) is more narrow than the true posterior.&quot;</span>
            <span class="p">)</span>

        <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># as there is no multi-round FMPE yet</span>

        <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dataloaders</span><span class="p">(</span>
            <span class="n">start_idx</span><span class="p">,</span>
            <span class="n">training_batch_size</span><span class="p">,</span>
            <span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">resume_training</span><span class="o">=</span><span class="n">resume_training</span><span class="p">,</span>
            <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Get theta, x to initialize NN</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">(</span><span class="n">starting_round</span><span class="o">=</span><span class="n">start_idx</span><span class="p">)</span>

            <span class="c1"># Use only training data for building the neural net (z-scoring transforms)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">(</span>
                <span class="n">theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
                <span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="k">del</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span>

        <span class="c1"># Move entire net to device for training.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

        <span class="c1"># initialize optimizer and training parameters</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># NOTE: in the FMPE context we use MSE loss, not log probs.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">&lt;=</span> <span class="n">max_num_epochs</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converged</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="n">train_loss_sum</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">epoch_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="c1"># get batches on current device.</span>
                <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="p">)</span>

                <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                <span class="n">train_loss_sum</span> <span class="o">+=</span> <span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="n">train_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">clip_max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">clip_grad_norm_</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="n">clip_max_norm</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">train_loss_average</span> <span class="o">=</span> <span class="n">train_loss_sum</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss_average</span><span class="p">)</span>

            <span class="c1"># Calculate validation performance.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="n">val_loss_sum</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
                    <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                        <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="p">)</span>
                    <span class="c1"># Aggregate the validation losses.</span>
                    <span class="n">val_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">)</span>
                    <span class="n">val_loss_sum</span> <span class="o">+=</span> <span class="n">val_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># Take mean over all validation samples.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">=</span> <span class="n">val_loss_sum</span> <span class="o">/</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">val_loader</span><span class="o">.</span><span class="n">batch_size</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>
            <span class="c1"># Log validation loss for every epoch.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epoch_durations_sec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start_time</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_show_progress</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_show_progress_bars</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_report_convergence_at_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="p">)</span>

        <span class="c1"># Update summary.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epochs_trained&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;best_validation_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_best_val_loss</span><span class="p">)</span>

        <span class="c1"># Update tensorboard and summary dict.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summarize</span><span class="p">(</span><span class="n">round_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">)</span>

        <span class="c1"># Update description for progress bar.</span>
        <span class="k">if</span> <span class="n">show_train_summary</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_describe_round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">))</span>

        <span class="c1"># Avoid keeping the gradients in the resulting network, which can</span>
        <span class="c1"># cause memory leakage when benchmarking.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ConditionalDensityEstimator</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;direct&quot;</span><span class="p">,</span>
        <span class="n">direct_sampling_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DirectPosterior</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build the posterior distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            density_estimator: Density estimator for the posterior.</span>
<span class="sd">            prior: Prior distribution.</span>
<span class="sd">            sample_with: Sampling method, currently only &quot;direct&quot; is supported.</span>
<span class="sd">            direct_sampling_parameters: kwargs for DirectPosterior.</span>

<span class="sd">        Returns:</span>
<span class="sd">            DirectPosterior: Posterior distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample_with</span> <span class="o">!=</span> <span class="s2">&quot;direct&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Currently, only direct sampling is supported for FMPE.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;You did not pass a prior. You have to pass the prior either at &quot;</span>
                <span class="s2">&quot;initialization `inference = SNPE(prior)` or to &quot;</span>
                <span class="s2">&quot;`.build_posterior(prior=prior)`.&quot;</span>
            <span class="p">)</span>
            <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">posterior_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
            <span class="c1"># If internal net is used device is defined.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">density_estimator</span>
            <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">DirectPosterior</span><span class="p">(</span>
            <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">posterior_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="o">**</span><span class="n">direct_sampling_parameters</span> <span class="ow">or</span> <span class="p">{},</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.fmpe.fmpe.FMPE.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;mlp&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.fmpe.fmpe.FMPE.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Initialization method for the FMPE class.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior distribution.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>density_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Neural network architecture used to learn the vector
field for flow matching. Can be a string, e.g., &lsquo;mlp&rsquo; or &lsquo;resnet&rsquo;, or a
<code>Callable</code> that builds a corresponding neural network.</p>
              </div>
            </td>
            <td>
                  <code>&#39;mlp&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Device to use for training.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Logging level.</p>
              </div>
            </td>
            <td>
                  <code>&#39;WARNING&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.utils.tensorboard.writer.SummaryWriter">SummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Summary writer for tensorboard.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show progress bars.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/fmpe/fmpe.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">],</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mlp&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialization method for the FMPE class.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        density_estimator: Neural network architecture used to learn the vector</span>
<span class="sd">            field for flow matching. Can be a string, e.g., &#39;mlp&#39; or &#39;resnet&#39;, or a</span>
<span class="sd">            `Callable` that builds a corresponding neural network.</span>
<span class="sd">        device: Device to use for training.</span>
<span class="sd">        logging_level: Logging level.</span>
<span class="sd">        summary_writer: Summary writer for tensorboard.</span>
<span class="sd">        show_progress_bars: Whether to show progress bars.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># obtain the shape of the prior samples</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">density_estimator</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">flowmatching_nn</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">density_estimator</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">density_estimator</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="o">=</span><span class="n">logging_level</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="o">=</span><span class="n">summary_writer</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.fmpe.fmpe.FMPE.build_posterior" class="doc doc-heading">
            <code class=" language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="s1">&#39;direct&#39;</span><span class="p">,</span> <span class="n">direct_sampling_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.fmpe.fmpe.FMPE.build_posterior" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Build the posterior distribution.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>density_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="sbi.neural_nets.estimators.ConditionalDensityEstimator" href="../models/#sbi.neural_nets.estimators.ConditionalDensityEstimator">ConditionalDensityEstimator</a>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Density estimator for the posterior.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior distribution.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_with</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Sampling method, currently only &ldquo;direct&rdquo; is supported.</p>
              </div>
            </td>
            <td>
                  <code>&#39;direct&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>direct_sampling_parameters</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>kwargs for DirectPosterior.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>DirectPosterior</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.inference.posteriors.direct_posterior.DirectPosterior" href="../posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior">DirectPosterior</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Posterior distribution.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/fmpe/fmpe.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ConditionalDensityEstimator</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;direct&quot;</span><span class="p">,</span>
    <span class="n">direct_sampling_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DirectPosterior</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Build the posterior distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        density_estimator: Density estimator for the posterior.</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        sample_with: Sampling method, currently only &quot;direct&quot; is supported.</span>
<span class="sd">        direct_sampling_parameters: kwargs for DirectPosterior.</span>

<span class="sd">    Returns:</span>
<span class="sd">        DirectPosterior: Posterior distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sample_with</span> <span class="o">!=</span> <span class="s2">&quot;direct&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Currently, only direct sampling is supported for FMPE.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;You did not pass a prior. You have to pass the prior either at &quot;</span>
            <span class="s2">&quot;initialization `inference = SNPE(prior)` or to &quot;</span>
            <span class="s2">&quot;`.build_posterior(prior=prior)`.&quot;</span>
        <span class="p">)</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">density_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">posterior_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">posterior_estimator</span> <span class="o">=</span> <span class="n">density_estimator</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">density_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">DirectPosterior</span><span class="p">(</span>
        <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">posterior_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="o">**</span><span class="n">direct_sampling_parameters</span> <span class="ow">or</span> <span class="p">{},</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.fmpe.fmpe.FMPE.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">training_batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_first_round_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.fmpe.fmpe.FMPE.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Train the flow matching estimator.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>training_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Batch size for training. Defaults to 50.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate for training. Defaults to 5e-4.</p>
              </div>
            </td>
            <td>
                  <code>0.0005</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>validation_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Fraction of the data to use for validation.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_after_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of epochs to train for. Defaults to 20.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_num_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of epochs to train for.</p>
              </div>
            </td>
            <td>
                  <code>2 ** 31 - 1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>clip_max_norm</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum norm for gradient clipping. Defaults to 5.0.</p>
              </div>
            </td>
            <td>
                  <code>5.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resume_training</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_first_round_loss</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to allow training with
simulations that have not been sampled from the prior, e.g., in a
sequential inference setting. Note that can lead to biased inference
results.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_train_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show the training summary. Defaults to False.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dataloader_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[dict]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional keyword arguments for the dataloader.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>DensityEstimator</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.neural_nets.estimators.ConditionalDensityEstimator" href="../models/#sbi.neural_nets.estimators.ConditionalDensityEstimator">ConditionalDensityEstimator</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Trained flow matching estimator.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/fmpe/fmpe.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ConditionalDensityEstimator</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train the flow matching estimator.</span>

<span class="sd">    Args:</span>
<span class="sd">        training_batch_size: Batch size for training. Defaults to 50.</span>
<span class="sd">        learning_rate: Learning rate for training. Defaults to 5e-4.</span>
<span class="sd">        validation_fraction: Fraction of the data to use for validation.</span>
<span class="sd">        stop_after_epochs: Number of epochs to train for. Defaults to 20.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to train for.</span>
<span class="sd">        clip_max_norm: Maximum norm for gradient clipping. Defaults to 5.0.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        force_first_round_loss: Whether to allow training with</span>
<span class="sd">            simulations that have not been sampled from the prior, e.g., in a</span>
<span class="sd">            sequential inference setting. Note that can lead to biased inference</span>
<span class="sd">            results.</span>
<span class="sd">        show_train_summary: Whether to show the training summary. Defaults to False.</span>
<span class="sd">        dataloader_kwargs: Additional keyword arguments for the dataloader.</span>

<span class="sd">    Returns:</span>
<span class="sd">        DensityEstimator: Trained flow matching estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Load data from most recent round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">force_first_round_loss</span> <span class="ow">or</span> <span class="n">resume_training</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;You have already trained this neural network. After you had trained &quot;</span>
            <span class="s2">&quot;the network, you again appended simulations with `append_simulations&quot;</span>
            <span class="s2">&quot;(theta, x)`, but you did not provide a proposal. If the new &quot;</span>
            <span class="s2">&quot;simulations are sampled from the prior, you can set &quot;</span>
            <span class="s2">&quot;`.train(..., force_first_round_loss=True`). However, if the new &quot;</span>
            <span class="s2">&quot;simulations were not sampled from the prior, you should pass the &quot;</span>
            <span class="s2">&quot;proposal, i.e. `append_simulations(theta, x, proposal)`. If &quot;</span>
            <span class="s2">&quot;your samples are not sampled from the prior and you do not pass a &quot;</span>
            <span class="s2">&quot;proposal and you set `force_first_round_loss=True`, the result of &quot;</span>
            <span class="s2">&quot;FMPE will not be the true posterior. Instead, it will be the proposal &quot;</span>
            <span class="s2">&quot;posterior, which (usually) is more narrow than the true posterior.&quot;</span>
        <span class="p">)</span>

    <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># as there is no multi-round FMPE yet</span>

    <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dataloaders</span><span class="p">(</span>
        <span class="n">start_idx</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="o">=</span><span class="n">resume_training</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Get theta, x to initialize NN</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">(</span><span class="n">starting_round</span><span class="o">=</span><span class="n">start_idx</span><span class="p">)</span>

        <span class="c1"># Use only training data for building the neural net (z-scoring transforms)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
            <span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">del</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span>

    <span class="c1"># Move entire net to device for training.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="c1"># initialize optimizer and training parameters</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span>
            <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># NOTE: in the FMPE context we use MSE loss, not log probs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">)</span>

    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">&lt;=</span> <span class="n">max_num_epochs</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converged</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">train_loss_sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">epoch_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># get batches on current device.</span>
            <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">train_loss_sum</span> <span class="o">+=</span> <span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">train_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">clip_max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">clip_grad_norm_</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="n">clip_max_norm</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">train_loss_average</span> <span class="o">=</span> <span class="n">train_loss_sum</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss_average</span><span class="p">)</span>

        <span class="c1"># Calculate validation performance.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">val_loss_sum</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
                <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="c1"># Aggregate the validation losses.</span>
                <span class="n">val_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">)</span>
                <span class="n">val_loss_sum</span> <span class="o">+=</span> <span class="n">val_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Take mean over all validation samples.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">=</span> <span class="n">val_loss_sum</span> <span class="o">/</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">val_loader</span><span class="o">.</span><span class="n">batch_size</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>
        <span class="c1"># Log validation loss for every epoch.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epoch_durations_sec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start_time</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_show_progress</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_show_progress_bars</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_report_convergence_at_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="p">)</span>

    <span class="c1"># Update summary.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epochs_trained&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;best_validation_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_best_val_loss</span><span class="p">)</span>

    <span class="c1"># Update tensorboard and summary dict.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summarize</span><span class="p">(</span><span class="n">round_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">)</span>

    <span class="c1"># Update description for progress bar.</span>
    <span class="k">if</span> <span class="n">show_train_summary</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_describe_round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">))</span>

    <span class="c1"># Avoid keeping the gradients in the resulting network, which can</span>
    <span class="c1"># cause memory leakage when benchmarking.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.npse.npse.NPSE" class="doc doc-heading">
            <code>NPSE</code>


<a href="#sbi.inference.trainers.npse.npse.NPSE" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.NeuralInference">NeuralInference</span></code></p>







              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/npse/npse.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NPSE</span><span class="p">(</span><span class="n">NeuralInference</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">score_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mlp&quot;</span><span class="p">,</span>
        <span class="n">sde_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;ve&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Base class for Neural Posterior Score Estimation methods.</span>

<span class="sd">        Instead of performing conditonal *density* estimation, NPSE methods perform</span>
<span class="sd">        conditional *score* estimation i.e. they estimate the gradient of the log</span>
<span class="sd">        density using denoising score matching loss.</span>

<span class="sd">        NOTE: NPSE does not support multi-round inference with flexible proposals yet.</span>
<span class="sd">        You can try to run multi-round with truncated proposals, but note that this is</span>
<span class="sd">        not tested yet.</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: Prior distribution.</span>
<span class="sd">            score_estimator: Neural network architecture for the score estimator. Can be</span>
<span class="sd">                a string (e.g. &#39;mlp&#39; or &#39;ada_mlp&#39;) or a callable that returns a neural</span>
<span class="sd">                network.</span>
<span class="sd">            sde_type: Type of SDE to use. Must be one of [&#39;vp&#39;, &#39;ve&#39;, &#39;subvp&#39;].</span>
<span class="sd">            device: Device to run the training on.</span>
<span class="sd">            logging_level: Logging level for the training. Can be an integer or a</span>
<span class="sd">                string.</span>
<span class="sd">            summary_writer: Tensorboard summary writer.</span>
<span class="sd">            show_progress_bars: Whether to show progress bars during training.</span>
<span class="sd">            kwargs: Additional keyword arguments.</span>

<span class="sd">        References:</span>
<span class="sd">            - Geffner, Tomas, George Papamakarios, and Andriy Mnih. &quot;Score modeling for</span>
<span class="sd">              simulation-based inference.&quot; ICML 2023.</span>
<span class="sd">            - Sharrock, Louis, et al. &quot;Sequential neural score estimation: Likelihood-</span>
<span class="sd">              free inference with conditional score based diffusion models.&quot; ICML 2024.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">logging_level</span><span class="o">=</span><span class="n">logging_level</span><span class="p">,</span>
            <span class="n">summary_writer</span><span class="o">=</span><span class="n">summary_writer</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># As detailed in the docstring, `score_estimator` is either a string or</span>
        <span class="c1"># a callable. The function creating the neural network is attached to</span>
        <span class="c1"># `_build_neural_net`. It will be called in the first round and receive</span>
        <span class="c1"># thetas and xs as inputs, so that they can be used for shape inference and</span>
        <span class="c1"># potentially for z-scoring.</span>
        <span class="n">check_estimator_arg</span><span class="p">(</span><span class="n">score_estimator</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">score_estimator</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">posterior_score_nn</span><span class="p">(</span>
                <span class="n">sde_type</span><span class="o">=</span><span class="n">sde_type</span><span class="p">,</span> <span class="n">score_net_type</span><span class="o">=</span><span class="n">score_estimator</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">score_estimator</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">append_simulations</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DirectPosterior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">data_device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NPSE&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store parameters and simulation outputs to use them for later training.</span>

<span class="sd">        Data are stored as entries in lists for each type of variable (parameter/data).</span>

<span class="sd">        Stores $\theta$, $x$, prior_masks (indicating if simulations are coming from the</span>
<span class="sd">        prior or not) and an index indicating which round the batch of simulations came</span>
<span class="sd">        from.</span>

<span class="sd">        Args:</span>
<span class="sd">            theta: Parameter sets.</span>
<span class="sd">            x: Simulation outputs.</span>
<span class="sd">            proposal: The distribution that the parameters $\theta$ were sampled from.</span>
<span class="sd">                Pass `None` if the parameters were sampled from the prior. If not</span>
<span class="sd">                `None`, it will trigger a different loss-function.</span>
<span class="sd">            exclude_invalid_x: Whether invalid simulations are discarded during</span>
<span class="sd">                training. For single-round SNPE, it is fine to discard invalid</span>
<span class="sd">                simulations, but for multi-round SNPE (atomic), discarding invalid</span>
<span class="sd">                simulations gives systematically wrong results. If `None`, it will</span>
<span class="sd">                be `True` in the first round and `False` in later rounds.</span>
<span class="sd">            data_device: Where to store the data, default is on the same device where</span>
<span class="sd">                the training is happening. If training a large dataset on a GPU with not</span>
<span class="sd">                much VRAM can set to &#39;cpu&#39; to store data on system memory instead.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NeuralInference object (returned so that this function is chainable).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">proposal</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Multi-round NPSE is not yet implemented. Please use single-round NPSE.&quot;</span>
        <span class="n">current_round</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">exclude_invalid_x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">exclude_invalid_x</span> <span class="o">=</span> <span class="n">current_round</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">data_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">data_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>

        <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">validate_theta_and_x</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">data_device</span><span class="o">=</span><span class="n">data_device</span><span class="p">,</span> <span class="n">training_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="p">)</span>

        <span class="n">is_valid_x</span><span class="p">,</span> <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span> <span class="o">=</span> <span class="n">handle_invalid_x</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="n">exclude_invalid_x</span>
        <span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>

        <span class="c1"># Check for problematic z-scoring</span>
        <span class="n">warn_if_zscoring_changes_data</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">npe_msg_on_invalid_x</span><span class="p">(</span><span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">,</span> <span class="s2">&quot;Single-round NPE&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_round</span><span class="p">)</span>
        <span class="n">prior_masks</span> <span class="o">=</span> <span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">current_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">),</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prior_masks</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">ImproperEmpirical</span><span class="p">):</span>
            <span class="n">theta_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">ImproperEmpirical</span><span class="p">(</span>
                <span class="n">theta_prior</span><span class="p">,</span> <span class="n">ones</span><span class="p">(</span><span class="n">theta_prior</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ema_loss_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ConditionalScoreEstimator</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a score estimator that approximates the score</span>
<span class="sd">        $\nabla_\theta \log p(\theta|x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            calibration_kernel: A function to calibrate the loss with respect</span>
<span class="sd">                to the simulations `x` (optional). See Lueckmann, Gonçalves et al.,</span>
<span class="sd">                NeurIPS 2017. If `None`, no calibration is used.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">                i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">                distribution different from the prior.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Score estimator that approximates the posterior score.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load data from most recent round.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">force_first_round_loss</span> <span class="ow">or</span> <span class="n">resume_training</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;You have already trained this neural network. After you had trained &quot;</span>
                <span class="s2">&quot;the network, you again appended simulations with `append_simulations&quot;</span>
                <span class="s2">&quot;(theta, x)`, but you did not provide a proposal. If the new &quot;</span>
                <span class="s2">&quot;simulations are sampled from the prior, you can set &quot;</span>
                <span class="s2">&quot;`.train(..., force_first_round_loss=True`). However, if the new &quot;</span>
                <span class="s2">&quot;simulations were not sampled from the prior, you should pass the &quot;</span>
                <span class="s2">&quot;proposal, i.e. `append_simulations(theta, x, proposal)`. If &quot;</span>
                <span class="s2">&quot;your samples are not sampled from the prior and you do not pass a &quot;</span>
                <span class="s2">&quot;proposal and you set `force_first_round_loss=True`, the result of &quot;</span>
                <span class="s2">&quot;NPSE will not be the true posterior. Instead, it will be the proposal &quot;</span>
                <span class="s2">&quot;posterior, which (usually) is more narrow than the true posterior.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Calibration kernels proposed in Lueckmann, Gonçalves et al., 2017.</span>
        <span class="k">if</span> <span class="n">calibration_kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">default_calibration_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

            <span class="n">calibration_kernel</span> <span class="o">=</span> <span class="n">default_calibration_kernel</span>

        <span class="c1"># Starting index for the training set (1 = discard round-0 samples).</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">discard_prior_samples</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Set the proposal to the last proposal that was passed by the user. For</span>
        <span class="c1"># atomic SNPE, it does not matter what the proposal is. For non-atomic</span>
        <span class="c1"># SNPE, we only use the latest data that was passed, i.e. the one from the</span>
        <span class="c1"># last proposal.</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dataloaders</span><span class="p">(</span>
            <span class="n">start_idx</span><span class="p">,</span>
            <span class="n">training_batch_size</span><span class="p">,</span>
            <span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">resume_training</span><span class="p">,</span>
            <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># First round or if retraining from scratch:</span>
        <span class="c1"># Call the `self._build_neural_net` with the rounds&#39; thetas and xs as</span>
        <span class="c1"># arguments, which will build the neural network.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
            <span class="c1"># Get theta,x to initialize NN</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">(</span><span class="n">starting_round</span><span class="o">=</span><span class="n">start_idx</span><span class="p">)</span>
            <span class="c1"># Use only training data for building the neural net (z-scoring transforms)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">(</span>
                <span class="n">theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
                <span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="n">test_posterior_net_for_multi_d_x</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="p">,</span>
                <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
                <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="k">del</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span>

        <span class="c1"># Move entire net to device for training.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">&lt;=</span> <span class="n">max_num_epochs</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converged</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span>
        <span class="p">):</span>
            <span class="c1"># Train for a single epoch.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="n">train_loss_sum</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">epoch_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="c1"># Get batches on current device.</span>
                <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">masks_batch</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="p">)</span>

                <span class="n">train_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">(</span>
                    <span class="n">theta_batch</span><span class="p">,</span>
                    <span class="n">x_batch</span><span class="p">,</span>
                    <span class="n">masks_batch</span><span class="p">,</span>
                    <span class="n">proposal</span><span class="p">,</span>
                    <span class="n">calibration_kernel</span><span class="p">,</span>
                    <span class="n">force_first_round_loss</span><span class="o">=</span><span class="n">force_first_round_loss</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">train_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span>

                <span class="n">train_loss_sum</span> <span class="o">+=</span> <span class="n">train_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="n">train_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">clip_max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">clip_grad_norm_</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="n">clip_max_norm</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">train_loss_average</span> <span class="o">=</span> <span class="n">train_loss_sum</span> <span class="o">/</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">batch_size</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

            <span class="c1"># NOTE: Due to the inherently noisy nature we do instead log a exponential</span>
            <span class="c1"># moving average of the training loss.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss_average</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">previous_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">ema_loss_decay</span><span class="p">)</span> <span class="o">*</span> <span class="n">previous_loss</span>
                    <span class="o">+</span> <span class="n">ema_loss_decay</span> <span class="o">*</span> <span class="n">train_loss_average</span>
                <span class="p">)</span>

            <span class="c1"># Calculate validation performance.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="n">val_loss_sum</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
                    <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">masks_batch</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                        <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                        <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="p">)</span>
                    <span class="c1"># Take negative loss here to get validation log_prob.</span>
                    <span class="n">val_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">(</span>
                        <span class="n">theta_batch</span><span class="p">,</span>
                        <span class="n">x_batch</span><span class="p">,</span>
                        <span class="n">masks_batch</span><span class="p">,</span>
                        <span class="n">proposal</span><span class="p">,</span>
                        <span class="n">calibration_kernel</span><span class="p">,</span>
                        <span class="n">force_first_round_loss</span><span class="o">=</span><span class="n">force_first_round_loss</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">val_loss_sum</span> <span class="o">+=</span> <span class="n">val_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># Take mean over all validation samples.</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="n">val_loss_sum</span> <span class="o">/</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">val_loader</span><span class="o">.</span><span class="n">batch_size</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

            <span class="c1"># NOTE: Due to the inherently noisy nature we do instead log a exponential</span>
            <span class="c1"># moving average of the validation loss.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">val_loss_ema</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">previous_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">val_loss_ema</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="n">ema_loss_decay</span>
                <span class="p">)</span> <span class="o">*</span> <span class="n">previous_loss</span> <span class="o">+</span> <span class="n">ema_loss_decay</span> <span class="o">*</span> <span class="n">val_loss</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">=</span> <span class="n">val_loss_ema</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epoch_durations_sec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start_time</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_show_progress</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_show_progress_bars</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_report_convergence_at_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="p">)</span>

        <span class="c1"># Update summary.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epochs_trained&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;best_validation_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span><span class="p">)</span>

        <span class="c1"># Update tensorboard and summary dict.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summarize</span><span class="p">(</span><span class="n">round_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">)</span>

        <span class="c1"># Update description for progress bar.</span>
        <span class="k">if</span> <span class="n">show_train_summary</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_describe_round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">))</span>

        <span class="c1"># Avoid keeping the gradients in the resulting network, which can</span>
        <span class="c1"># cause memory leakage when benchmarking.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">score_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ConditionalScoreEstimator</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sde&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ScorePosterior</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the score estimator.</span>

<span class="sd">        For NPSE, the posterior distribution that is returned here implements the</span>
<span class="sd">        following functionality over the raw neural density estimator:</span>
<span class="sd">        - correct the calculation of the log probability such that it compensates for</span>
<span class="sd">            the leakage.</span>
<span class="sd">        - reject samples that lie outside of the prior bounds.</span>

<span class="sd">        Args:</span>
<span class="sd">            score_estimator: The score estimator that the posterior is based on.</span>
<span class="sd">                If `None`, use the latest neural score estimator that was trained.</span>
<span class="sd">            prior: Prior distribution.</span>
<span class="sd">            sample_with: Method to use for sampling from the posterior. Can be one of</span>
<span class="sd">                &#39;sde&#39; (default) or &#39;ode&#39;. The &#39;sde&#39; method uses the score to</span>
<span class="sd">                do a Langevin diffusion step, while the &#39;ode&#39; method uses the score to</span>
<span class="sd">                define a probabilistic ODE and solves it with a numerical ODE solver.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;You did not pass a prior. You have to pass the prior either at &quot;</span>
                <span class="s2">&quot;initialization `inference = NPSE(prior)` or to &quot;</span>
                <span class="s2">&quot;`.build_posterior(prior=prior)`.&quot;</span>
            <span class="p">)</span>
            <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">score_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">score_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
            <span class="c1"># If internal net is used device is defined.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: Add protocol for checking if the score estimator has forward and</span>
            <span class="c1"># loss methods with the correct signature.</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">posterior</span> <span class="o">=</span> <span class="n">ScorePosterior</span><span class="p">(</span>
            <span class="n">score_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">prior</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">sample_with</span><span class="o">=</span><span class="n">sample_with</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">posterior</span>
        <span class="c1"># Store models at end of each round.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_bank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss_proposal_posterior</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Multi-round NPSE is not yet implemented.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
        <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return loss from score estimator. Currently only single-round NPSE</span>
<span class="sd">         is implemented, i.e., no proposal correction is applied for later rounds.</span>

<span class="sd">        The loss is the negative log prob. Irrespective of the round or SNPE method</span>
<span class="sd">        (A, B, or C), it can be weighted with a calibration kernel.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Calibration kernel-weighted negative log prob.</span>
<span class="sd">            force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">                i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">                distribution different from the prior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">force_first_round_loss</span><span class="p">:</span>
            <span class="c1"># First round loss.</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Multi-round NPSE with arbitrary proposals is not implemented&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">calibration_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">_converged</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if training has converged.</span>

<span class="sd">        Unlike the `._converged` method in base.py, this method does not reset to the</span>
<span class="sd">        best model. We noticed that this improves performance. Deleting this method</span>
<span class="sd">        will make C2ST tests fail. This is because the loss is very stochastic, so</span>
<span class="sd">        resetting might reset to an underfitted model. Ideally, we would write a</span>
<span class="sd">        custom `._converged()` method which checks whether the loss is still going</span>
<span class="sd">        down **for all t**.</span>

<span class="sd">        Args:</span>
<span class="sd">            epoch: Current epoch.</span>
<span class="sd">            stop_after_epochs: Number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Whether training has converged.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">converged</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># No checkpointing, just check if the validation loss has improved.</span>

        <span class="c1"># (Re)-start the epoch count with the first epoch or any improvement.</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_best_val_loss</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_best_val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_epochs_since_last_improvement</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_epochs_since_last_improvement</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># If no validation improvement over many epochs, stop training.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epochs_since_last_improvement</span> <span class="o">&gt;</span> <span class="n">stop_after_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">converged</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">converged</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npse.npse.NPSE.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">score_estimator</span><span class="o">=</span><span class="s1">&#39;mlp&#39;</span><span class="p">,</span> <span class="n">sde_type</span><span class="o">=</span><span class="s1">&#39;ve&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npse.npse.NPSE.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Base class for Neural Posterior Score Estimation methods.</p>
<p>Instead of performing conditonal <em>density</em> estimation, NPSE methods perform
conditional <em>score</em> estimation i.e. they estimate the gradient of the log
density using denoising score matching loss.</p>
<p>NOTE: NPSE does not support multi-round inference with flexible proposals yet.
You can try to run multi-round with truncated proposals, but note that this is
not tested yet.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior distribution.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>score_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Neural network architecture for the score estimator. Can be
a string (e.g. &lsquo;mlp&rsquo; or &lsquo;ada_mlp&rsquo;) or a callable that returns a neural
network.</p>
              </div>
            </td>
            <td>
                  <code>&#39;mlp&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sde_type</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of SDE to use. Must be one of [&lsquo;vp&rsquo;, &lsquo;ve&rsquo;, &lsquo;subvp&rsquo;].</p>
              </div>
            </td>
            <td>
                  <code>&#39;ve&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Device to run the training on.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Logging level for the training. Can be an integer or a
string.</p>
              </div>
            </td>
            <td>
                  <code>&#39;WARNING&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.utils.tensorboard.writer.SummaryWriter">SummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Tensorboard summary writer.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show progress bars during training.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>Geffner, Tomas, George Papamakarios, and Andriy Mnih. &ldquo;Score modeling for
  simulation-based inference.&rdquo; ICML 2023.</li>
<li>Sharrock, Louis, et al. &ldquo;Sequential neural score estimation: Likelihood-
  free inference with conditional score based diffusion models.&rdquo; ICML 2024.</li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npse/npse.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">score_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mlp&quot;</span><span class="p">,</span>
    <span class="n">sde_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;ve&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Base class for Neural Posterior Score Estimation methods.</span>

<span class="sd">    Instead of performing conditonal *density* estimation, NPSE methods perform</span>
<span class="sd">    conditional *score* estimation i.e. they estimate the gradient of the log</span>
<span class="sd">    density using denoising score matching loss.</span>

<span class="sd">    NOTE: NPSE does not support multi-round inference with flexible proposals yet.</span>
<span class="sd">    You can try to run multi-round with truncated proposals, but note that this is</span>
<span class="sd">    not tested yet.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        score_estimator: Neural network architecture for the score estimator. Can be</span>
<span class="sd">            a string (e.g. &#39;mlp&#39; or &#39;ada_mlp&#39;) or a callable that returns a neural</span>
<span class="sd">            network.</span>
<span class="sd">        sde_type: Type of SDE to use. Must be one of [&#39;vp&#39;, &#39;ve&#39;, &#39;subvp&#39;].</span>
<span class="sd">        device: Device to run the training on.</span>
<span class="sd">        logging_level: Logging level for the training. Can be an integer or a</span>
<span class="sd">            string.</span>
<span class="sd">        summary_writer: Tensorboard summary writer.</span>
<span class="sd">        show_progress_bars: Whether to show progress bars during training.</span>
<span class="sd">        kwargs: Additional keyword arguments.</span>

<span class="sd">    References:</span>
<span class="sd">        - Geffner, Tomas, George Papamakarios, and Andriy Mnih. &quot;Score modeling for</span>
<span class="sd">          simulation-based inference.&quot; ICML 2023.</span>
<span class="sd">        - Sharrock, Louis, et al. &quot;Sequential neural score estimation: Likelihood-</span>
<span class="sd">          free inference with conditional score based diffusion models.&quot; ICML 2024.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="o">=</span><span class="n">logging_level</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="o">=</span><span class="n">summary_writer</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># As detailed in the docstring, `score_estimator` is either a string or</span>
    <span class="c1"># a callable. The function creating the neural network is attached to</span>
    <span class="c1"># `_build_neural_net`. It will be called in the first round and receive</span>
    <span class="c1"># thetas and xs as inputs, so that they can be used for shape inference and</span>
    <span class="c1"># potentially for z-scoring.</span>
    <span class="n">check_estimator_arg</span><span class="p">(</span><span class="n">score_estimator</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">score_estimator</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">posterior_score_nn</span><span class="p">(</span>
            <span class="n">sde_type</span><span class="o">=</span><span class="n">sde_type</span><span class="p">,</span> <span class="n">score_net_type</span><span class="o">=</span><span class="n">score_estimator</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span> <span class="o">=</span> <span class="n">score_estimator</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npse.npse.NPSE.append_simulations" class="doc doc-heading">
            <code class=" language-python"><span class="n">append_simulations</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">proposal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_device</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npse.npse.NPSE.append_simulations" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Store parameters and simulation outputs to use them for later training.</p>
<p>Data are stored as entries in lists for each type of variable (parameter/data).</p>
<p>Stores <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(x\)</span>, prior_masks (indicating if simulations are coming from the
prior or not) and an index indicating which round the batch of simulations came
from.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>theta</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameter sets.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Simulation outputs.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proposal</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="sbi.inference.posteriors.DirectPosterior" href="../posteriors/#sbi.inference.posteriors.direct_posterior.DirectPosterior">DirectPosterior</a>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The distribution that the parameters <span class="arithmatex">\(\theta\)</span> were sampled from.
Pass <code>None</code> if the parameters were sampled from the prior. If not
<code>None</code>, it will trigger a different loss-function.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>exclude_invalid_x</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether invalid simulations are discarded during
training. For single-round SNPE, it is fine to discard invalid
simulations, but for multi-round SNPE (atomic), discarding invalid
simulations gives systematically wrong results. If <code>None</code>, it will
be <code>True</code> in the first round and <code>False</code> in later rounds.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>data_device</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Where to store the data, default is on the same device where
the training is happening. If training a large dataset on a GPU with not
much VRAM can set to &lsquo;cpu&rsquo; to store data on system memory instead.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.inference.trainers.npse.npse.NPSE" href="#sbi.inference.trainers.npse.npse.NPSE">NPSE</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>NeuralInference object (returned so that this function is chainable).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npse/npse.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">append_simulations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DirectPosterior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">exclude_invalid_x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">data_device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NPSE&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Store parameters and simulation outputs to use them for later training.</span>

<span class="sd">    Data are stored as entries in lists for each type of variable (parameter/data).</span>

<span class="sd">    Stores $\theta$, $x$, prior_masks (indicating if simulations are coming from the</span>
<span class="sd">    prior or not) and an index indicating which round the batch of simulations came</span>
<span class="sd">    from.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta: Parameter sets.</span>
<span class="sd">        x: Simulation outputs.</span>
<span class="sd">        proposal: The distribution that the parameters $\theta$ were sampled from.</span>
<span class="sd">            Pass `None` if the parameters were sampled from the prior. If not</span>
<span class="sd">            `None`, it will trigger a different loss-function.</span>
<span class="sd">        exclude_invalid_x: Whether invalid simulations are discarded during</span>
<span class="sd">            training. For single-round SNPE, it is fine to discard invalid</span>
<span class="sd">            simulations, but for multi-round SNPE (atomic), discarding invalid</span>
<span class="sd">            simulations gives systematically wrong results. If `None`, it will</span>
<span class="sd">            be `True` in the first round and `False` in later rounds.</span>
<span class="sd">        data_device: Where to store the data, default is on the same device where</span>
<span class="sd">            the training is happening. If training a large dataset on a GPU with not</span>
<span class="sd">            much VRAM can set to &#39;cpu&#39; to store data on system memory instead.</span>

<span class="sd">    Returns:</span>
<span class="sd">        NeuralInference object (returned so that this function is chainable).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">proposal</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">&quot;Multi-round NPSE is not yet implemented. Please use single-round NPSE.&quot;</span>
    <span class="n">current_round</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">exclude_invalid_x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">exclude_invalid_x</span> <span class="o">=</span> <span class="n">current_round</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">data_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">data_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>

    <span class="n">theta</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">validate_theta_and_x</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">data_device</span><span class="o">=</span><span class="n">data_device</span><span class="p">,</span> <span class="n">training_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="p">)</span>

    <span class="n">is_valid_x</span><span class="p">,</span> <span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span> <span class="o">=</span> <span class="n">handle_invalid_x</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="o">=</span><span class="n">exclude_invalid_x</span>
    <span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_valid_x</span><span class="p">]</span>

    <span class="c1"># Check for problematic z-scoring</span>
    <span class="n">warn_if_zscoring_changes_data</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">npe_msg_on_invalid_x</span><span class="p">(</span><span class="n">num_nans</span><span class="p">,</span> <span class="n">num_infs</span><span class="p">,</span> <span class="n">exclude_invalid_x</span><span class="p">,</span> <span class="s2">&quot;Single-round NPE&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_round</span><span class="p">)</span>
    <span class="n">prior_masks</span> <span class="o">=</span> <span class="n">mask_sims_from_prior</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">current_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">),</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_theta_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_x_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prior_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prior_masks</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prior</span><span class="p">,</span> <span class="n">ImproperEmpirical</span><span class="p">):</span>
        <span class="n">theta_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="o">=</span> <span class="n">ImproperEmpirical</span><span class="p">(</span>
            <span class="n">theta_prior</span><span class="p">,</span> <span class="n">ones</span><span class="p">(</span><span class="n">theta_prior</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npse.npse.NPSE.build_posterior" class="doc doc-heading">
            <code class=" language-python"><span class="n">build_posterior</span><span class="p">(</span><span class="n">score_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_with</span><span class="o">=</span><span class="s1">&#39;sde&#39;</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npse.npse.NPSE.build_posterior" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Build posterior from the score estimator.</p>
<p>For NPSE, the posterior distribution that is returned here implements the
following functionality over the raw neural density estimator:
- correct the calculation of the log probability such that it compensates for
    the leakage.
- reject samples that lie outside of the prior bounds.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>score_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.neural_nets.estimators.score_estimator.ConditionalScoreEstimator">ConditionalScoreEstimator</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The score estimator that the posterior is based on.
If <code>None</code>, use the latest neural score estimator that was trained.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior distribution.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_with</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use for sampling from the posterior. Can be one of
&lsquo;sde&rsquo; (default) or &lsquo;ode&rsquo;. The &lsquo;sde&rsquo; method uses the score to
do a Langevin diffusion step, while the &lsquo;ode&rsquo; method uses the score to
define a probabilistic ODE and solves it with a numerical ODE solver.</p>
              </div>
            </td>
            <td>
                  <code>&#39;sde&#39;</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><a class="autorefs autorefs-internal" title="sbi.inference.posteriors.score_posterior.ScorePosterior" href="../posteriors/#sbi.inference.posteriors.score_posterior.ScorePosterior">ScorePosterior</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Posterior <span class="arithmatex">\(p(\theta|x)\)</span>  with <code>.sample()</code> and <code>.log_prob()</code> methods.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npse/npse.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build_posterior</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">score_estimator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ConditionalScoreEstimator</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_with</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sde&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ScorePosterior</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Build posterior from the score estimator.</span>

<span class="sd">    For NPSE, the posterior distribution that is returned here implements the</span>
<span class="sd">    following functionality over the raw neural density estimator:</span>
<span class="sd">    - correct the calculation of the log probability such that it compensates for</span>
<span class="sd">        the leakage.</span>
<span class="sd">    - reject samples that lie outside of the prior bounds.</span>

<span class="sd">    Args:</span>
<span class="sd">        score_estimator: The score estimator that the posterior is based on.</span>
<span class="sd">            If `None`, use the latest neural score estimator that was trained.</span>
<span class="sd">        prior: Prior distribution.</span>
<span class="sd">        sample_with: Method to use for sampling from the posterior. Can be one of</span>
<span class="sd">            &#39;sde&#39; (default) or &#39;ode&#39;. The &#39;sde&#39; method uses the score to</span>
<span class="sd">            do a Langevin diffusion step, while the &#39;ode&#39; method uses the score to</span>
<span class="sd">            define a probabilistic ODE and solves it with a numerical ODE solver.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Posterior $p(\theta|x)$  with `.sample()` and `.log_prob()` methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;You did not pass a prior. You have to pass the prior either at &quot;</span>
            <span class="s2">&quot;initialization `inference = NPSE(prior)` or to &quot;</span>
            <span class="s2">&quot;`.build_posterior(prior=prior)`.&quot;</span>
        <span class="p">)</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">utils</span><span class="o">.</span><span class="n">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">score_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">score_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span>
        <span class="c1"># If internal net is used device is defined.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    <span class="c1"># Otherwise, infer it from the device of the net parameters.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># TODO: Add protocol for checking if the score estimator has forward and</span>
        <span class="c1"># loss methods with the correct signature.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">score_estimator</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">posterior</span> <span class="o">=</span> <span class="n">ScorePosterior</span><span class="p">(</span>
        <span class="n">score_estimator</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">sample_with</span><span class="o">=</span><span class="n">sample_with</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span> <span class="o">=</span> <span class="n">posterior</span>
    <span class="c1"># Store models at end of each round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_bank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_posterior</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.npse.npse.NPSE.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">training_batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">calibration_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ema_loss_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_first_round_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.npse.npse.NPSE.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns a score estimator that approximates the score
<span class="arithmatex">\(\nabla_\theta \log p(\theta|x)\)</span>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>training_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training batch size.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate for Adam optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.0005</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>validation_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of data to use for validation.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_after_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_num_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
              </div>
            </td>
            <td>
                  <code>2 ** 31 - 1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>clip_max_norm</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
              </div>
            </td>
            <td>
                  <code>5.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>calibration_kernel</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A function to calibrate the loss with respect
to the simulations <code>x</code> (optional). See Lueckmann, Gonçalves et al.,
NeurIPS 2017. If <code>None</code>, no calibration is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resume_training</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_first_round_loss</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, train with maximum likelihood,
i.e., potentially ignoring the correction for using a proposal
distribution different from the prior.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>discard_prior_samples</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>retrain_from_scratch</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_train_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to print the number of epochs and validation
loss after the training.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dataloader_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[dict]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="sbi.neural_nets.estimators.score_estimator.ConditionalScoreEstimator">ConditionalScoreEstimator</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Score estimator that approximates the posterior score.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/npse/npse.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">calibration_kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ema_loss_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_first_round_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ConditionalScoreEstimator</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a score estimator that approximates the score</span>
<span class="sd">    $\nabla_\theta \log p(\theta|x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        calibration_kernel: A function to calibrate the loss with respect</span>
<span class="sd">            to the simulations `x` (optional). See Lueckmann, Gonçalves et al.,</span>
<span class="sd">            NeurIPS 2017. If `None`, no calibration is used.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        force_first_round_loss: If `True`, train with maximum likelihood,</span>
<span class="sd">            i.e., potentially ignoring the correction for using a proposal</span>
<span class="sd">            distribution different from the prior.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Score estimator that approximates the posterior score.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load data from most recent round.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_round_index</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">force_first_round_loss</span> <span class="ow">or</span> <span class="n">resume_training</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;You have already trained this neural network. After you had trained &quot;</span>
            <span class="s2">&quot;the network, you again appended simulations with `append_simulations&quot;</span>
            <span class="s2">&quot;(theta, x)`, but you did not provide a proposal. If the new &quot;</span>
            <span class="s2">&quot;simulations are sampled from the prior, you can set &quot;</span>
            <span class="s2">&quot;`.train(..., force_first_round_loss=True`). However, if the new &quot;</span>
            <span class="s2">&quot;simulations were not sampled from the prior, you should pass the &quot;</span>
            <span class="s2">&quot;proposal, i.e. `append_simulations(theta, x, proposal)`. If &quot;</span>
            <span class="s2">&quot;your samples are not sampled from the prior and you do not pass a &quot;</span>
            <span class="s2">&quot;proposal and you set `force_first_round_loss=True`, the result of &quot;</span>
            <span class="s2">&quot;NPSE will not be the true posterior. Instead, it will be the proposal &quot;</span>
            <span class="s2">&quot;posterior, which (usually) is more narrow than the true posterior.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Calibration kernels proposed in Lueckmann, Gonçalves et al., 2017.</span>
    <span class="k">if</span> <span class="n">calibration_kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">default_calibration_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

        <span class="n">calibration_kernel</span> <span class="o">=</span> <span class="n">default_calibration_kernel</span>

    <span class="c1"># Starting index for the training set (1 = discard round-0 samples).</span>
    <span class="n">start_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">discard_prior_samples</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Set the proposal to the last proposal that was passed by the user. For</span>
    <span class="c1"># atomic SNPE, it does not matter what the proposal is. For non-atomic</span>
    <span class="c1"># SNPE, we only use the latest data that was passed, i.e. the one from the</span>
    <span class="c1"># last proposal.</span>
    <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proposal_roundwise</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dataloaders</span><span class="p">(</span>
        <span class="n">start_idx</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># First round or if retraining from scratch:</span>
    <span class="c1"># Call the `self._build_neural_net` with the rounds&#39; thetas and xs as</span>
    <span class="c1"># arguments, which will build the neural network.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">retrain_from_scratch</span><span class="p">:</span>
        <span class="c1"># Get theta,x to initialize NN</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_simulations</span><span class="p">(</span><span class="n">starting_round</span><span class="o">=</span><span class="n">start_idx</span><span class="p">)</span>
        <span class="c1"># Use only training data for building the neural net (z-scoring transforms)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_neural_net</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
            <span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">test_posterior_net_for_multi_d_x</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="p">,</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
            <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">del</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span>

    <span class="c1"># Move entire net to device for training.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_training</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">)</span>

    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">&lt;=</span> <span class="n">max_num_epochs</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converged</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span>
    <span class="p">):</span>
        <span class="c1"># Train for a single epoch.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">train_loss_sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">epoch_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># Get batches on current device.</span>
            <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">masks_batch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="n">train_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">(</span>
                <span class="n">theta_batch</span><span class="p">,</span>
                <span class="n">x_batch</span><span class="p">,</span>
                <span class="n">masks_batch</span><span class="p">,</span>
                <span class="n">proposal</span><span class="p">,</span>
                <span class="n">calibration_kernel</span><span class="p">,</span>
                <span class="n">force_first_round_loss</span><span class="o">=</span><span class="n">force_first_round_loss</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">train_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span>

            <span class="n">train_loss_sum</span> <span class="o">+=</span> <span class="n">train_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">train_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">clip_max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">clip_grad_norm_</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="n">clip_max_norm</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">train_loss_average</span> <span class="o">=</span> <span class="n">train_loss_sum</span> <span class="o">/</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">batch_size</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>

        <span class="c1"># NOTE: Due to the inherently noisy nature we do instead log a exponential</span>
        <span class="c1"># moving average of the training loss.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss_average</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">previous_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;training_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">ema_loss_decay</span><span class="p">)</span> <span class="o">*</span> <span class="n">previous_loss</span>
                <span class="o">+</span> <span class="n">ema_loss_decay</span> <span class="o">*</span> <span class="n">train_loss_average</span>
            <span class="p">)</span>

        <span class="c1"># Calculate validation performance.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">val_loss_sum</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
                <span class="n">theta_batch</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">masks_batch</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                    <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="c1"># Take negative loss here to get validation log_prob.</span>
                <span class="n">val_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">(</span>
                    <span class="n">theta_batch</span><span class="p">,</span>
                    <span class="n">x_batch</span><span class="p">,</span>
                    <span class="n">masks_batch</span><span class="p">,</span>
                    <span class="n">proposal</span><span class="p">,</span>
                    <span class="n">calibration_kernel</span><span class="p">,</span>
                    <span class="n">force_first_round_loss</span><span class="o">=</span><span class="n">force_first_round_loss</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">val_loss_sum</span> <span class="o">+=</span> <span class="n">val_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Take mean over all validation samples.</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">val_loss_sum</span> <span class="o">/</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">val_loader</span><span class="o">.</span><span class="n">batch_size</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>

        <span class="c1"># NOTE: Due to the inherently noisy nature we do instead log a exponential</span>
        <span class="c1"># moving average of the validation loss.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">val_loss_ema</span> <span class="o">=</span> <span class="n">val_loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">previous_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">val_loss_ema</span> <span class="o">=</span> <span class="p">(</span>
                <span class="mi">1</span> <span class="o">-</span> <span class="n">ema_loss_decay</span>
            <span class="p">)</span> <span class="o">*</span> <span class="n">previous_loss</span> <span class="o">+</span> <span class="n">ema_loss_decay</span> <span class="o">*</span> <span class="n">val_loss</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span> <span class="o">=</span> <span class="n">val_loss_ema</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epoch_durations_sec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start_time</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_show_progress</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_show_progress_bars</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_report_convergence_at_end</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="p">)</span>

    <span class="c1"># Update summary.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;epochs_trained&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">[</span><span class="s2">&quot;best_validation_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_val_loss</span><span class="p">)</span>

    <span class="c1"># Update tensorboard and summary dict.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summarize</span><span class="p">(</span><span class="n">round_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">)</span>

    <span class="c1"># Update description for progress bar.</span>
    <span class="k">if</span> <span class="n">show_train_summary</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_describe_round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_round</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary</span><span class="p">))</span>

    <span class="c1"># Avoid keeping the gradients in the resulting network, which can</span>
    <span class="c1"># cause memory leakage when benchmarking.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_neural_net</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.nle.nle_a.NLE_A" class="doc doc-heading">
            <code>NLE_A</code>


<a href="#sbi.inference.trainers.nle.nle_a.NLE_A" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.trainers.nle.nle_base.LikelihoodEstimator">LikelihoodEstimator</span></code></p>







              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/nle/nle_a.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NLE_A</span><span class="p">(</span><span class="n">LikelihoodEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Neural Likelihood Estimation [1].</span>

<span class="sd">        [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with</span>
<span class="sd">        Autoregressive Flows_, Papamakarios et al., AISTATS 2019,</span>
<span class="sd">        https://arxiv.org/abs/1805.07226</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">                provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">                that builds a custom neural network can be provided. The function will</span>
<span class="sd">                be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">                thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">                needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">                estimator. The density estimator needs to provide the methods</span>
<span class="sd">                `.log_prob` and `.sample()`.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nle.nle_a.NLE_A.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density_estimator</span><span class="o">=</span><span class="s1">&#39;maf&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;WARNING&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nle.nle_a.NLE_A.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Neural Likelihood Estimation [1].</p>
<p>[1] Sequential Neural Likelihood: Fast Likelihood-free Inference with
Autoregressive Flows_, Papamakarios et al., AISTATS 2019,
<a href="https://arxiv.org/abs/1805.07226">https://arxiv.org/abs/1805.07226</a></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>density_estimator</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If it is a string, use a pre-configured network of the
provided type (one of nsf, maf, mdn, made). Alternatively, a function
that builds a custom neural network can be provided. The function will
be called with the first batch of simulations (theta, x), which can
thus be used for shape inference and potentially for z-scoring. It
needs to return a PyTorch <code>nn.Module</code> implementing the density
estimator. The density estimator needs to provide the methods
<code>.log_prob</code> and <code>.sample()</code>.</p>
              </div>
            </td>
            <td>
                  <code>&#39;maf&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
              </div>
            </td>
            <td>
                  <code>&#39;WARNING&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during simulation and
sampling.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nle/nle_a.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">density_estimator</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;maf&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WARNING&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Neural Likelihood Estimation [1].</span>

<span class="sd">    [1] Sequential Neural Likelihood: Fast Likelihood-free Inference with</span>
<span class="sd">    Autoregressive Flows_, Papamakarios et al., AISTATS 2019,</span>
<span class="sd">    https://arxiv.org/abs/1805.07226</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        density_estimator: If it is a string, use a pre-configured network of the</span>
<span class="sd">            provided type (one of nsf, maf, mdn, made). Alternatively, a function</span>
<span class="sd">            that builds a custom neural network can be provided. The function will</span>
<span class="sd">            be called with the first batch of simulations (theta, x), which can</span>
<span class="sd">            thus be used for shape inference and potentially for z-scoring. It</span>
<span class="sd">            needs to return a PyTorch `nn.Module` implementing the density</span>
<span class="sd">            estimator. The density estimator needs to provide the methods</span>
<span class="sd">            `.log_prob` and `.sample()`.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.nre.nre_a.NRE_A" class="doc doc-heading">
            <code>NRE_A</code>


<a href="#sbi.inference.trainers.nre.nre_a.NRE_A" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.trainers.nre.nre_base.RatioEstimator">RatioEstimator</span></code></p>







              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/nre/nre_a.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NRE_A</span><span class="p">(</span><span class="n">RatioEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;AALR[1], here known as NRE_A.</span>

<span class="sd">        [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans</span>
<span class="sd">            et al., ICML 2020, https://arxiv.org/abs/1903.04057</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">                inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">                `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">loss_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">            loss_kwargs: Additional or updated kwargs to be passed to the self._loss fn.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># AALR is defined for `num_atoms=2`.</span>
        <span class="c1"># Proxy to `super().__call__` to ensure right parameter.</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the binary cross-entropy loss for the trained classifier.</span>

<span class="sd">        The classifier takes as input a $(\theta,x)$ pair. It is trained to predict 1</span>
<span class="sd">        if the pair was sampled from the joint $p(\theta,x)$, and to predict 0 if the</span>
<span class="sd">        pair was sampled from the marginals $p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># Alternating pairs where there is one sampled from the joint and one</span>
        <span class="c1"># sampled from the marginals. The first element is sampled from the</span>
        <span class="c1"># joint p(theta, x) and is labelled 1. The second element is sampled</span>
        <span class="c1"># from the marginals p(theta)p(x) and is labelled 0. And so on.</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># two atoms</span>
        <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Binary cross entropy to learn the likelihood (AALR-specific)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nre.nre_a.NRE_A.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nre.nre_a.NRE_A.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>AALR[1], here known as NRE_A.</p>
<p>[1] <em>Likelihood-free MCMC with Amortized Approximate Likelihood Ratios</em>, Hermans
    et al., ICML 2020, <a href="https://arxiv.org/abs/1903.04057">https://arxiv.org/abs/1903.04057</a></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>classifier</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p>
              </div>
            </td>
            <td>
                  <code>&#39;resnet&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
              </div>
            </td>
            <td>
                  <code>&#39;warning&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during simulation and
sampling.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nre/nre_a.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;AALR[1], here known as NRE_A.</span>

<span class="sd">    [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans</span>
<span class="sd">        et al., ICML 2020, https://arxiv.org/abs/1903.04057</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nre.nre_a.NRE_A.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">training_batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nre.nre_a.NRE_A.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>training_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training batch size.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate for Adam optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.0005</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>validation_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of data to use for validation.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_after_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_num_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
              </div>
            </td>
            <td>
                  <code>2 ** 31 - 1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>clip_max_norm</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
              </div>
            </td>
            <td>
                  <code>5.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resume_training</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>discard_prior_samples</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>retrain_from_scratch</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_train_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to print the number of epochs and validation
loss and leakage after the training.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dataloader_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>loss_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional or updated kwargs to be passed to the self._loss fn.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nre/nre_a.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">loss_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">        loss_kwargs: Additional or updated kwargs to be passed to the self._loss fn.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># AALR is defined for `num_atoms=2`.</span>
    <span class="c1"># Proxy to `super().__call__` to ensure right parameter.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">num_atoms</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.nre.nre_b.NRE_B" class="doc doc-heading">
            <code>NRE_B</code>


<a href="#sbi.inference.trainers.nre.nre_b.NRE_B" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.trainers.nre.nre_base.RatioEstimator">RatioEstimator</span></code></p>







              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/nre/nre_b.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NRE_B</span><span class="p">(</span><span class="n">RatioEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SRE[1], here known as NRE_B.</span>

<span class="sd">        [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,</span>
<span class="sd">            ICML 2020, https://arxiv.org/pdf/2002.03712</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">                inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">                `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_atoms: Number of atoms to use for classification.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return cross-entropy (via softmax activation) loss for 1-out-of-`num_atoms`</span>
<span class="sd">        classification.</span>

<span class="sd">        The classifier takes as input `num_atoms` $(\theta,x)$ pairs. Out of these</span>
<span class="sd">        pairs, one pair was sampled from the joint $p(\theta,x)$ and all others from the</span>
<span class="sd">        marginals $p(\theta)p(x)$. The classifier is trained to predict which of the</span>
<span class="sd">        pairs was sampled from the joint $p(\theta,x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># For 1-out-of-`num_atoms` classification each datapoint consists</span>
        <span class="c1"># of `num_atoms` points, with one of them being the correct one.</span>
        <span class="c1"># We have a batch of `batch_size` such datapoints.</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>

        <span class="c1"># Index 0 is the theta-x-pair sampled from the joint p(theta,x) and hence the</span>
        <span class="c1"># &quot;correct&quot; one for the 1-out-of-N classification.</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nre.nre_b.NRE_B.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nre.nre_b.NRE_B.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>SRE[1], here known as NRE_B.</p>
<p>[1] <em>On Contrastive Learning for Likelihood-free Inference</em>, Durkan et al.,
    ICML 2020, <a href="https://arxiv.org/pdf/2002.03712">https://arxiv.org/pdf/2002.03712</a></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>classifier</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p>
              </div>
            </td>
            <td>
                  <code>&#39;resnet&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
              </div>
            </td>
            <td>
                  <code>&#39;warning&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during simulation and
sampling.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nre/nre_b.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SRE[1], here known as NRE_B.</span>

<span class="sd">    [1] _On Contrastive Learning for Likelihood-free Inference_, Durkan et al.,</span>
<span class="sd">        ICML 2020, https://arxiv.org/pdf/2002.03712</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nre.nre_b.NRE_B.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">num_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nre.nre_b.NRE_B.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>num_atoms</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of atoms to use for classification.</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>training_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training batch size.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate for Adam optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.0005</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>validation_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of data to use for validation.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_after_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_num_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
              </div>
            </td>
            <td>
                  <code>2 ** 31 - 1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>clip_max_norm</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
              </div>
            </td>
            <td>
                  <code>5.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resume_training</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>discard_prior_samples</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>retrain_from_scratch</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_train_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to print the number of epochs and validation
loss and leakage after the training.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dataloader_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nre/nre_b.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_atoms: Number of atoms to use for classification.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.nre.nre_c.NRE_C" class="doc doc-heading">
            <code>NRE_C</code>


<a href="#sbi.inference.trainers.nre.nre_c.NRE_C" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.trainers.nre.nre_base.RatioEstimator">RatioEstimator</span></code></p>







              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/nre/nre_c.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NRE_C</span><span class="p">(</span><span class="n">RatioEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;NRE-C[1] is a generalization of the non-sequential (amortized) versions of</span>
<span class="sd">        NRE_A and NRE_B. We call the algorithm NRE_C within `sbi`.</span>

<span class="sd">        NRE-C:</span>
<span class="sd">        (1) like NRE_B, features a &quot;multiclass&quot; loss function where several marginally</span>
<span class="sd">            drawn parameter-data pairs are contrasted against a jointly drawn pair.</span>
<span class="sd">        (2) like AALR/NRE_A, i.e., the non-sequential version of NRE_A, it encourages</span>
<span class="sd">            the approximate ratio $p(\theta,x)/p(\theta)p(x)$, accessed through</span>
<span class="sd">            `.potential()` within `sbi`, to be exact at optimum. This addresses the</span>
<span class="sd">            issue that NRE_B estimates this ratio only up to an arbitrary function</span>
<span class="sd">            (normalizing constant) of the data $x$.</span>

<span class="sd">        Just like for all ratio estimation algorithms, the sequential version of NRE_C</span>
<span class="sd">        will be estimated only up to a function (normalizing constant) of the data $x$</span>
<span class="sd">        in rounds after the first.</span>

<span class="sd">        [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,</span>
<span class="sd">            NeurIPS 2022, https://arxiv.org/abs/2210.06170</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">                inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">                `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_classes: Number of theta to classify against, corresponds to $K$ in</span>
<span class="sd">                _Contrastive Neural Ratio Estimation_. Minimum value is 1. Similar to</span>
<span class="sd">                `num_atoms` for SNRE_B except SNRE_C has an additional independently</span>
<span class="sd">                drawn sample. The total number of alternative parameters `NRE-C` &quot;sees&quot;</span>
<span class="sd">                is $2K-1$ or `2 * num_classes - 1` divided between two loss terms.</span>
<span class="sd">            gamma: Determines the relative weight of the sum of all $K$ dependently</span>
<span class="sd">                drawn classes against the marginally drawn one. Specifically,</span>
<span class="sd">                $p(y=k) :=p_K$, $p(y=0) := p_0$, $p_0 = 1 - K p_K$, and finally</span>
<span class="sd">                $\gamma := K p_K / p_0$.</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">                during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_atoms&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_classes&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;loss_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">)}</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return cross-entropy loss (via &#39;&#39;multi-class sigmoid&#39;&#39; activation) for</span>
<span class="sd">        1-out-of-`K + 1` classification.</span>

<span class="sd">        At optimum, this loss function returns the exact likelihood-to-evidence ratio</span>
<span class="sd">        in the first round.</span>
<span class="sd">        Details of loss computation are described in Contrastive Neural Ratio</span>
<span class="sd">        Estimation[1]. The paper does not discuss the sequential case.</span>

<span class="sd">        [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,</span>
<span class="sd">            NeurIPS 2022, https://arxiv.org/abs/2210.06170</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Reminder: K = num_classes</span>
        <span class="c1"># The algorithm is written with K, so we convert back to K format rather than</span>
        <span class="c1"># reasoning in num_atoms.</span>
        <span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_atoms</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="n">num_classes</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;num_classes = </span><span class="si">{</span><span class="n">num_classes</span><span class="si">}</span><span class="s2"> must be greater than 1.&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># We append a contrastive theta to the marginal case because we will remove</span>
        <span class="c1"># the jointly drawn</span>
        <span class="c1"># sample in the logits_marginal[:, 0] position. That makes the remaining sample</span>
        <span class="c1"># marginally drawn.</span>
        <span class="c1"># We have a batch of `batch_size` datapoints.</span>
        <span class="n">logits_marginal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">logits_joint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span>
        <span class="p">)</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="n">logits_marginal</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">logits_marginal</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># Index 0 is the theta-x-pair sampled from the joint p(theta,x) and hence</span>
        <span class="c1"># we remove the jointly drawn sample from the logits_marginal</span>
        <span class="n">logits_marginal</span> <span class="o">=</span> <span class="n">logits_marginal</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="c1"># ... and retain it in the logits_joint. Now we have two arrays with K choices.</span>

        <span class="c1"># To use logsumexp, we extend the denominator logits with loggamma</span>
        <span class="n">loggamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
        <span class="n">logK</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
        <span class="n">denominator_marginal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">loggamma</span> <span class="o">+</span> <span class="n">logits_marginal</span><span class="p">,</span> <span class="n">logK</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">denominator_joint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">loggamma</span> <span class="o">+</span> <span class="n">logits_joint</span><span class="p">,</span> <span class="n">logK</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Compute the contributions to the loss from each term in the classification.</span>
        <span class="n">log_prob_marginal</span> <span class="o">=</span> <span class="n">logK</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">denominator_marginal</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_prob_joint</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">loggamma</span> <span class="o">+</span> <span class="n">logits_joint</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">denominator_joint</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># relative weights. p_marginal := p_0, and p_joint := p_K * K from the notation.</span>
        <span class="n">p_marginal</span><span class="p">,</span> <span class="n">p_joint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_prior_probs_marginal_and_joint</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_marginal</span> <span class="o">*</span> <span class="n">log_prob_marginal</span> <span class="o">+</span> <span class="n">p_joint</span> <span class="o">*</span> <span class="n">log_prob_joint</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_prior_probs_marginal_and_joint</span><span class="p">(</span><span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return a tuple (p_marginal, p_joint) where `p_marginal := `$p_0$,</span>
<span class="sd">        `p_joint := `$p_K \cdot K$.</span>

<span class="sd">        We let the joint (dependently drawn) class to be equally likely across K</span>
<span class="sd">        options. The marginal class is therefore restricted to get the remaining</span>
<span class="sd">        probability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p_joint</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">p_marginal</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p_marginal</span><span class="p">,</span> <span class="n">p_joint</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nre.nre_c.NRE_C.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nre.nre_c.NRE_C.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>NRE-C[1] is a generalization of the non-sequential (amortized) versions of
NRE_A and NRE_B. We call the algorithm NRE_C within <code>sbi</code>.</p>
<p>NRE-C:
(1) like NRE_B, features a &ldquo;multiclass&rdquo; loss function where several marginally
    drawn parameter-data pairs are contrasted against a jointly drawn pair.
(2) like AALR/NRE_A, i.e., the non-sequential version of NRE_A, it encourages
    the approximate ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>, accessed through
    <code>.potential()</code> within <code>sbi</code>, to be exact at optimum. This addresses the
    issue that NRE_B estimates this ratio only up to an arbitrary function
    (normalizing constant) of the data <span class="arithmatex">\(x\)</span>.</p>
<p>Just like for all ratio estimation algorithms, the sequential version of NRE_C
will be estimated only up to a function (normalizing constant) of the data <span class="arithmatex">\(x\)</span>
in rounds after the first.</p>
<p>[1] <em>Contrastive Neural Ratio Estimation</em>, Benajmin Kurt Miller, et. al.,
    NeurIPS 2022, <a href="https://arxiv.org/abs/2210.06170">https://arxiv.org/abs/2210.06170</a></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>classifier</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations (theta, x), which can thus be used for shape
inference and potentially for z-scoring. It needs to return a PyTorch
<code>nn.Module</code> implementing the classifier.</p>
              </div>
            </td>
            <td>
                  <code>&#39;resnet&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
              </div>
            </td>
            <td>
                  <code>&#39;warning&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during simulation and
sampling.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nre/nre_c.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;NRE-C[1] is a generalization of the non-sequential (amortized) versions of</span>
<span class="sd">    NRE_A and NRE_B. We call the algorithm NRE_C within `sbi`.</span>

<span class="sd">    NRE-C:</span>
<span class="sd">    (1) like NRE_B, features a &quot;multiclass&quot; loss function where several marginally</span>
<span class="sd">        drawn parameter-data pairs are contrasted against a jointly drawn pair.</span>
<span class="sd">    (2) like AALR/NRE_A, i.e., the non-sequential version of NRE_A, it encourages</span>
<span class="sd">        the approximate ratio $p(\theta,x)/p(\theta)p(x)$, accessed through</span>
<span class="sd">        `.potential()` within `sbi`, to be exact at optimum. This addresses the</span>
<span class="sd">        issue that NRE_B estimates this ratio only up to an arbitrary function</span>
<span class="sd">        (normalizing constant) of the data $x$.</span>

<span class="sd">    Just like for all ratio estimation algorithms, the sequential version of NRE_C</span>
<span class="sd">    will be estimated only up to a function (normalizing constant) of the data $x$</span>
<span class="sd">    in rounds after the first.</span>

<span class="sd">    [1] _Contrastive Neural Ratio Estimation_, Benajmin Kurt Miller, et. al.,</span>
<span class="sd">        NeurIPS 2022, https://arxiv.org/abs/2210.06170</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations (theta, x), which can thus be used for shape</span>
<span class="sd">            inference and potentially for z-scoring. It needs to return a PyTorch</span>
<span class="sd">            `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nre.nre_c.NRE_C.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nre.nre_c.NRE_C.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>num_classes</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of theta to classify against, corresponds to <span class="arithmatex">\(K\)</span> in
<em>Contrastive Neural Ratio Estimation</em>. Minimum value is 1. Similar to
<code>num_atoms</code> for SNRE_B except SNRE_C has an additional independently
drawn sample. The total number of alternative parameters <code>NRE-C</code> &ldquo;sees&rdquo;
is <span class="arithmatex">\(2K-1\)</span> or <code>2 * num_classes - 1</code> divided between two loss terms.</p>
              </div>
            </td>
            <td>
                  <code>5</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gamma</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Determines the relative weight of the sum of all <span class="arithmatex">\(K\)</span> dependently
drawn classes against the marginally drawn one. Specifically,
<span class="arithmatex">\(p(y=k) :=p_K\)</span>, <span class="arithmatex">\(p(y=0) := p_0\)</span>, <span class="arithmatex">\(p_0 = 1 - K p_K\)</span>, and finally
<span class="arithmatex">\(\gamma := K p_K / p_0\)</span>.</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>training_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training batch size.</p>
              </div>
            </td>
            <td>
                  <code>200</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>learning_rate</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Learning rate for Adam optimizer.</p>
              </div>
            </td>
            <td>
                  <code>0.0005</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>validation_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of data to use for validation.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_after_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of epochs to wait for improvement on the
validation set before terminating training.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_num_epochs</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of epochs to run. If reached, we stop
training even when the validation loss is still decreasing. Otherwise,
we train until validation loss increases (see also <code>stop_after_epochs</code>).</p>
              </div>
            </td>
            <td>
                  <code>2 ** 31 - 1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>clip_max_norm</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Value at which to clip the total gradient norm in order to
prevent exploding gradients. Use None for no clipping.</p>
              </div>
            </td>
            <td>
                  <code>5.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>exclude_invalid_x</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to exclude simulation outputs <code>x=NaN</code> or <code>x=±∞</code>
during training. Expect errors, silent or explicit, when <code>False</code>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resume_training</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Can be used in case training time is limited, e.g. on a
cluster. If <code>True</code>, the split between train and validation set, the
optimizer, the number of epochs, and the best validation log-prob will
be restored from the last time <code>.train()</code> was called.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>discard_prior_samples</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to discard samples simulated in round 1, i.e.
from the prior. Training may be sped up by ignoring such less targeted
samples.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>retrain_from_scratch</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to retrain the conditional density
estimator for the posterior from scratch each round.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_train_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to print the number of epochs and validation
loss and leakage after the training.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dataloader_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional or updated kwargs to be passed to the training
and validation dataloaders (like, e.g., a collate_fn)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nre/nre_c.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_classes: Number of theta to classify against, corresponds to $K$ in</span>
<span class="sd">            _Contrastive Neural Ratio Estimation_. Minimum value is 1. Similar to</span>
<span class="sd">            `num_atoms` for SNRE_B except SNRE_C has an additional independently</span>
<span class="sd">            drawn sample. The total number of alternative parameters `NRE-C` &quot;sees&quot;</span>
<span class="sd">            is $2K-1$ or `2 * num_classes - 1` divided between two loss terms.</span>
<span class="sd">        gamma: Determines the relative weight of the sum of all $K$ dependently</span>
<span class="sd">            drawn classes against the marginally drawn one. Specifically,</span>
<span class="sd">            $p(y=k) :=p_K$, $p(y=0) := p_0$, $p_0 = 1 - K p_K$, and finally</span>
<span class="sd">            $\gamma := K p_K / p_0$.</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_atoms&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_classes&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;loss_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">)}</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.trainers.nre.bnre.BNRE" class="doc doc-heading">
            <code>BNRE</code>


<a href="#sbi.inference.trainers.nre.bnre.BNRE" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="sbi.inference.trainers.nre.nre_a.NRE_A" href="#sbi.inference.trainers.nre.nre_a.NRE_A">NRE_A</a></code></p>







              <details class="quote">
                <summary>Source code in <code>sbi/inference/trainers/nre/bnre.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BNRE</span><span class="p">(</span><span class="n">NRE_A</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Balanced neural ratio estimation (BNRE)[1].</span>

<span class="sd">        BNRE is a variation of NRE aiming to produce more conservative posterior</span>
<span class="sd">        approximations.</span>

<span class="sd">        [1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G..</span>
<span class="sd">        Towards Reliable Simulation-Based Inference with Balanced Neural Ratio</span>
<span class="sd">        Estimation.</span>
<span class="sd">        NeurIPS 2022. https://arxiv.org/abs/2208.13624</span>

<span class="sd">        Args:</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">                prior must be passed to `.build_posterior()`.</span>
<span class="sd">            classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">                a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">                linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">                neural network can be provided. The function will be called with the</span>
<span class="sd">                first batch of simulations $(\theta, x)$, which can thus be used for</span>
<span class="sd">                shape inference and potentially for z-scoring. It needs to return a</span>
<span class="sd">                PyTorch `nn.Module` implementing the classifier.</span>
<span class="sd">            device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">            logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">                INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">            summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">                file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">100.0</span><span class="p">,</span>
        <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        Args:</span>

<span class="sd">            regularization_strength: The multiplicative coefficient applied to the</span>
<span class="sd">                balancing regularizer ($\lambda$).</span>
<span class="sd">            training_batch_size: Training batch size.</span>
<span class="sd">            learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">            validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">            stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">                validation set before terminating training.</span>
<span class="sd">            max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">                training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">                we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">            clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">                prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">            exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">                during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">            resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">                cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">                optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">                be restored from the last time `.train()` was called.</span>
<span class="sd">            discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">                from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">                samples.</span>
<span class="sd">            retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">                estimator for the posterior from scratch each round.</span>
<span class="sd">            show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">                loss and leakage after the training.</span>
<span class="sd">            dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">                and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">        Returns:</span>
<span class="sd">            Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;loss_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;regularization_strength&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;regularization_strength&quot;</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the binary cross-entropy loss for the trained classifier.</span>

<span class="sd">        The classifier takes as input a $(\theta,x)$ pair. It is trained to predict 1</span>
<span class="sd">        if the pair was sampled from the joint $p(\theta,x)$, and to predict 0 if the</span>
<span class="sd">        pair was sampled from the marginals $p(\theta)p(x)$.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Batch sizes for theta and x must match.&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_classifier_logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_atoms</span><span class="p">)</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># Alternating pairs where there is one sampled from the joint and one</span>
        <span class="c1"># sampled from the marginals. The first element is sampled from the</span>
        <span class="c1"># joint p(theta, x) and is labelled 1. The second element is sampled</span>
        <span class="c1"># from the marginals p(theta)p(x) and is labelled 0. And so on.</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>  <span class="c1"># two atoms</span>
        <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Binary cross entropy to learn the likelihood (AALR-specific)</span>
        <span class="n">bce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Balancing regularizer</span>
        <span class="n">regularizer</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="o">.</span><span class="n">square</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">bce</span> <span class="o">+</span> <span class="n">regularization_strength</span> <span class="o">*</span> <span class="n">regularizer</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nre.bnre.BNRE.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="s1">&#39;resnet&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">logging_level</span><span class="o">=</span><span class="s1">&#39;warning&#39;</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nre.bnre.BNRE.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Balanced neural ratio estimation (BNRE)[1].</p>
<p>BNRE is a variation of NRE aiming to produce more conservative posterior
approximations.</p>
<p>[1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G..
Towards Reliable Simulation-Based Inference with Balanced Neural Ratio
Estimation.
NeurIPS 2022. <a href="https://arxiv.org/abs/2208.13624">https://arxiv.org/abs/2208.13624</a></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.distributions.Distribution">Distribution</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. If <code>None</code>, the
prior must be passed to <code>.build_posterior()</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>classifier</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Classifier trained to approximate likelihood ratios. If it is
a string, use a pre-configured network of the provided type (one of
linear, mlp, resnet). Alternatively, a function that builds a custom
neural network can be provided. The function will be called with the
first batch of simulations <span class="arithmatex">\((\theta, x)\)</span>, which can thus be used for
shape inference and potentially for z-scoring. It needs to return a
PyTorch <code>nn.Module</code> implementing the classifier.</p>
              </div>
            </td>
            <td>
                  <code>&#39;resnet&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training device, e.g., &ldquo;cpu&rdquo;, &ldquo;cuda&rdquo; or &ldquo;cuda:{0, 1, &hellip;}&rdquo;.</p>
              </div>
            </td>
            <td>
                  <code>&#39;cpu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logging_level</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum severity of messages to log. One of the strings
INFO, WARNING, DEBUG, ERROR and CRITICAL.</p>
              </div>
            </td>
            <td>
                  <code>&#39;warning&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>summary_writer</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="sbi.sbi_types.TensorboardSummaryWriter">TensorboardSummaryWriter</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tensorboard <code>SummaryWriter</code> to control, among others, log
file location (default is <code>&lt;current working directory&gt;/logs</code>.)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during simulation and
sampling.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nre/bnre.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Distribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;resnet&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logging_level</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;warning&quot;</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorboardSummaryWriter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Balanced neural ratio estimation (BNRE)[1].</span>

<span class="sd">    BNRE is a variation of NRE aiming to produce more conservative posterior</span>
<span class="sd">    approximations.</span>

<span class="sd">    [1] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., &amp; Louppe, G..</span>
<span class="sd">    Towards Reliable Simulation-Based Inference with Balanced Neural Ratio</span>
<span class="sd">    Estimation.</span>
<span class="sd">    NeurIPS 2022. https://arxiv.org/abs/2208.13624</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. If `None`, the</span>
<span class="sd">            prior must be passed to `.build_posterior()`.</span>
<span class="sd">        classifier: Classifier trained to approximate likelihood ratios. If it is</span>
<span class="sd">            a string, use a pre-configured network of the provided type (one of</span>
<span class="sd">            linear, mlp, resnet). Alternatively, a function that builds a custom</span>
<span class="sd">            neural network can be provided. The function will be called with the</span>
<span class="sd">            first batch of simulations $(\theta, x)$, which can thus be used for</span>
<span class="sd">            shape inference and potentially for z-scoring. It needs to return a</span>
<span class="sd">            PyTorch `nn.Module` implementing the classifier.</span>
<span class="sd">        device: Training device, e.g., &quot;cpu&quot;, &quot;cuda&quot; or &quot;cuda:{0, 1, ...}&quot;.</span>
<span class="sd">        logging_level: Minimum severity of messages to log. One of the strings</span>
<span class="sd">            INFO, WARNING, DEBUG, ERROR and CRITICAL.</span>
<span class="sd">        summary_writer: A tensorboard `SummaryWriter` to control, among others, log</span>
<span class="sd">            file location (default is `&lt;current working directory&gt;/logs`.)</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.trainers.nre.bnre.BNRE.train" class="doc doc-heading">
            <code class=" language-python"><span class="n">train</span><span class="p">(</span><span class="n">regularization_strength</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stop_after_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">clip_max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">resume_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">discard_prior_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retrain_from_scratch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_train_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.nre.bnre.BNRE.train" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.
Args:</p>
<div class="codehilite"><pre><span></span><code>regularization_strength: The multiplicative coefficient applied to the
    balancing regularizer ($\lambda$).
training_batch_size: Training batch size.
learning_rate: Learning rate for Adam optimizer.
validation_fraction: The fraction of data to use for validation.
stop_after_epochs: The number of epochs to wait for improvement on the
    validation set before terminating training.
max_num_epochs: Maximum number of epochs to run. If reached, we stop
    training even when the validation loss is still decreasing. Otherwise,
    we train until validation loss increases (see also `stop_after_epochs`).
clip_max_norm: Value at which to clip the total gradient norm in order to
    prevent exploding gradients. Use None for no clipping.
exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`
    during training. Expect errors, silent or explicit, when `False`.
resume_training: Can be used in case training time is limited, e.g. on a
    cluster. If `True`, the split between train and validation set, the
    optimizer, the number of epochs, and the best validation log-prob will
    be restored from the last time `.train()` was called.
discard_prior_samples: Whether to discard samples simulated in round 1, i.e.
    from the prior. Training may be sped up by ignoring such less targeted
    samples.
retrain_from_scratch: Whether to retrain the conditional density
    estimator for the posterior from scratch each round.
show_train_summary: Whether to print the number of epochs and validation
    loss and leakage after the training.
dataloader_kwargs: Additional or updated kwargs to be passed to the training
    and validation dataloaders (like, e.g., a collate_fn)
</code></pre></div>

<p>Returns:
    Classifier that approximates the ratio <span class="arithmatex">\(p(\theta,x)/p(\theta)p(x)\)</span>.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/trainers/nre/bnre.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">regularization_strength</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">100.0</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">validation_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">stop_after_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">resume_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">discard_prior_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">retrain_from_scratch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">show_train_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    Args:</span>

<span class="sd">        regularization_strength: The multiplicative coefficient applied to the</span>
<span class="sd">            balancing regularizer ($\lambda$).</span>
<span class="sd">        training_batch_size: Training batch size.</span>
<span class="sd">        learning_rate: Learning rate for Adam optimizer.</span>
<span class="sd">        validation_fraction: The fraction of data to use for validation.</span>
<span class="sd">        stop_after_epochs: The number of epochs to wait for improvement on the</span>
<span class="sd">            validation set before terminating training.</span>
<span class="sd">        max_num_epochs: Maximum number of epochs to run. If reached, we stop</span>
<span class="sd">            training even when the validation loss is still decreasing. Otherwise,</span>
<span class="sd">            we train until validation loss increases (see also `stop_after_epochs`).</span>
<span class="sd">        clip_max_norm: Value at which to clip the total gradient norm in order to</span>
<span class="sd">            prevent exploding gradients. Use None for no clipping.</span>
<span class="sd">        exclude_invalid_x: Whether to exclude simulation outputs `x=NaN` or `x=±∞`</span>
<span class="sd">            during training. Expect errors, silent or explicit, when `False`.</span>
<span class="sd">        resume_training: Can be used in case training time is limited, e.g. on a</span>
<span class="sd">            cluster. If `True`, the split between train and validation set, the</span>
<span class="sd">            optimizer, the number of epochs, and the best validation log-prob will</span>
<span class="sd">            be restored from the last time `.train()` was called.</span>
<span class="sd">        discard_prior_samples: Whether to discard samples simulated in round 1, i.e.</span>
<span class="sd">            from the prior. Training may be sped up by ignoring such less targeted</span>
<span class="sd">            samples.</span>
<span class="sd">        retrain_from_scratch: Whether to retrain the conditional density</span>
<span class="sd">            estimator for the posterior from scratch each round.</span>
<span class="sd">        show_train_summary: Whether to print the number of epochs and validation</span>
<span class="sd">            loss and leakage after the training.</span>
<span class="sd">        dataloader_kwargs: Additional or updated kwargs to be passed to the training</span>
<span class="sd">            and validation dataloaders (like, e.g., a collate_fn)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Classifier that approximates the ratio $p(\theta,x)/p(\theta)p(x)$.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">del_entries</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="n">entries</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">))</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;loss_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;regularization_strength&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;regularization_strength&quot;</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.abc.mcabc.MCABC" class="doc doc-heading">
            <code>MCABC</code>


<a href="#sbi.inference.abc.mcabc.MCABC" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.abc.abc_base.ABCBASE">ABCBASE</span></code></p>


        <p>Monte-Carlo Approximate Bayesian Computation (Rejection ABC).</p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MCABC</span><span class="p">(</span><span class="n">ABCBASE</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monte-Carlo Approximate Bayesian Computation (Rejection ABC).&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
        <span class="n">requires_iid_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">distance_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">distance_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</span>

<span class="sd">        [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.</span>
<span class="sd">        (1999). Population growth of human Y chromosomes: a study of Y chromosome</span>
<span class="sd">        microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</span>

<span class="sd">        Args:</span>
<span class="sd">            simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">                simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">                regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">                can be used.</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">                a custom callable function or one of `l1`, `l2`, `mse`,</span>
<span class="sd">                `mmd`, `wasserstein`.</span>
<span class="sd">            requires_iid_data: Whether to allow conditioning on iid sampled data or not.</span>
<span class="sd">                Typically, this information is inferred by the choice of the distance,</span>
<span class="sd">                but in case a custom distance is used, this information is pivotal.</span>
<span class="sd">            distance_kwargs: Configurations parameters for the distances. In particular</span>
<span class="sd">                useful for the MMD and Wasserstein distance.</span>
<span class="sd">            num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">            simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">                maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">                same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">                (simulation_batch_size, parameter_dimension).</span>
<span class="sd">            distance_batch_size: Number of simulations that the distance function</span>
<span class="sd">                evaluates against the reference observations at once. If -1, we evaluate</span>
<span class="sd">                all simulations at the same time.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
            <span class="n">requires_iid_data</span><span class="o">=</span><span class="n">requires_iid_data</span><span class="p">,</span>
            <span class="n">distance_kwargs</span><span class="o">=</span><span class="n">distance_kwargs</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
            <span class="n">distance_batch_size</span><span class="o">=</span><span class="n">distance_batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
        <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantile</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_iid_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run MCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">        Args:</span>
<span class="sd">            x_o: Observed data.</span>
<span class="sd">            num_simulations: Number of simulations to run.</span>
<span class="sd">            eps: Acceptance threshold $\epsilon$ for distance between observed and</span>
<span class="sd">                simulated data.</span>
<span class="sd">            quantile: Upper quantile of smallest distances for which the corresponding</span>
<span class="sd">                parameters are returned, e.g, q=0.01 will return the top 1%. Exactly</span>
<span class="sd">                one of quantile or `eps` have to be passed.</span>
<span class="sd">            lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">            sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">                Fearnhead &amp; Prangle 2012.</span>
<span class="sd">            sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">            sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">                sass regression, default 1 - no expansion.</span>
<span class="sd">            kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">                object from which one can sample.</span>
<span class="sd">            kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">                &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">                heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">                default &#39;cv&#39;.</span>
<span class="sd">                &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">                &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">                more details</span>
<span class="sd">            return_summary: Whether to return the distances and data corresponding to</span>
<span class="sd">                the accepted parameters.</span>
<span class="sd">            num_iid_samples: Number of simulations per parameter. Choose</span>
<span class="sd">                `num_iid_samples&gt;1`, if you have chosen a statistical distance that</span>
<span class="sd">                evaluates sets of simulations against a set of reference observations</span>
<span class="sd">                instead of a single data-point comparison.</span>

<span class="sd">        Returns:</span>
<span class="sd">            theta (if kde False): accepted parameters</span>
<span class="sd">            kde (if kde True): KDE object based on accepted parameters from which one</span>
<span class="sd">                can .sample() and .log_prob().</span>
<span class="sd">            summary (if summary True): dictionary containing the accepted paramters (if</span>
<span class="sd">                kde True), distances and simulated data x.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Exactly one of eps or quantile need to be passed.</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span>
            <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Eps or quantile must be passed, but not both.&quot;</span>
        <span class="k">if</span> <span class="n">kde_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kde_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Run SASS and change the simulator and x_o accordingly.</span>
        <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
            <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Running SASS with </span><span class="si">%s</span><span class="s2"> pilot samples.&quot;</span><span class="p">,</span> <span class="n">num_pilot_simulations</span>
            <span class="p">)</span>
            <span class="n">num_simulations</span> <span class="o">-=</span> <span class="n">num_pilot_simulations</span>

            <span class="n">pilot_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_pilot_simulations</span><span class="p">,))</span>
            <span class="n">pilot_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">pilot_theta</span><span class="p">)</span>

            <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
                <span class="n">pilot_theta</span><span class="p">,</span> <span class="n">pilot_x</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
            <span class="p">)</span>

            <span class="c1"># Add sass transform to simulator and x_o.</span>
            <span class="k">def</span> <span class="nf">simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

            <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="n">x_o</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">simulator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span>

        <span class="c1"># Simulate and calculate distances.</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>
        <span class="n">theta_repeat</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_iid_samples</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">simulator</span><span class="p">(</span><span class="n">theta_repeat</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span>
            <span class="n">num_simulations</span><span class="p">,</span>
            <span class="n">num_iid_samples</span><span class="p">,</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">))</span>  <span class="c1"># Dim(num_initial_pop, num_iid_samples, -1)</span>

        <span class="c1"># Infer x shape to test and set x_o.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="c1"># Select based on acceptance threshold epsilon.</span>
        <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">&lt;</span> <span class="n">eps</span>
            <span class="n">num_accepted</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">assert</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;No parameters accepted, eps=</span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2"> too small&quot;</span>

            <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
            <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
            <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>

        <span class="c1"># Select based on quantile on sorted distances.</span>
        <span class="k">elif</span> <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_top_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_simulations</span> <span class="o">*</span> <span class="n">quantile</span><span class="p">)</span>
            <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
            <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
            <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;One of epsilon or quantile has to be passed.&quot;</span><span class="p">)</span>

        <span class="c1"># Maybe adjust theta with LRA.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
            <span class="n">final_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">theta_accepted</span><span class="p">,</span> <span class="n">x_accepted</span><span class="p">,</span> <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">final_theta</span> <span class="o">=</span> <span class="n">theta_accepted</span>

        <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;KDE on %s samples with bandwidth option</span>
<span class="sd">                {kde_kwargs[&quot;bandwidth&quot;] if &quot;bandwidth&quot; in kde_kwargs else &quot;cv&quot;}.</span>
<span class="sd">                Beware that KDE can give unreliable results when used with too few</span>
<span class="sd">                samples and in high dimensions.&quot;&quot;&quot;</span><span class="p">,</span>
                <span class="n">final_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="p">)</span>

            <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">(</span>
                    <span class="n">kde_dist</span><span class="p">,</span>
                    <span class="nb">dict</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">final_theta</span><span class="p">,</span> <span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">kde_dist</span>
        <span class="k">elif</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_theta</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_theta</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.mcabc.MCABC.__call__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quantile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_fraction</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_iid_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.mcabc.MCABC.__call__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Run MCABC and return accepted parameters or KDE object fitted on them.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x_o</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="numpy.ndarray">ndarray</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Observed data.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_simulations</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of simulations to run.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eps</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Acceptance threshold <span class="arithmatex">\(\epsilon\)</span> for distance between observed and
simulated data.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantile</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Upper quantile of smallest distances for which the corresponding
parameters are returned, e.g, q=0.01 will return the top 1%. Exactly
one of quantile or <code>eps</code> have to be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lra</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to run linear regression adjustment as in Beaumont et al. 2002</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sass</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to determine semi-automatic summary statistics as in
Fearnhead &amp; Prangle 2012.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sass_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Fraction of simulation budget used for the initial sass run.</p>
              </div>
            </td>
            <td>
                  <code>0.25</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sass_expansion_degree</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Degree of the polynomial feature expansion for the
sass regression, default 1 - no expansion.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kde</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to run KDE on the accepted parameters to return a KDE
object from which one can sample.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kde_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>kwargs for performing KDE:
&lsquo;bandwidth=&rsquo;; either a float, or a string naming a bandwidth
heuristics, e.g., &lsquo;cv&rsquo; (cross validation), &lsquo;silvermann&rsquo; or &lsquo;scott&rsquo;,
default &lsquo;cv&rsquo;.
&lsquo;transform&rsquo;: transform applied to the parameters before doing KDE.
&lsquo;sample_weights&rsquo;: weights associated with samples. See &lsquo;get_kde&rsquo; for
more details</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to return the distances and data corresponding to
the accepted parameters.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iid_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of simulations per parameter. Choose
<code>num_iid_samples&gt;1</code>, if you have chosen a statistical distance that
evaluates sets of simulations against a set of reference observations
instead of a single data-point comparison.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>theta</code></td>            <td>
                  <code>if kde False</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>accepted parameters</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>kde</code></td>            <td>
                  <code>if kde True</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>KDE object based on accepted parameters from which one
can .sample() and .log_prob().</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>summary</code></td>            <td>
                  <code>if summary True</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary containing the accepted paramters (if
kde True), distances and simulated data x.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantile</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_iid_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run MCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_o: Observed data.</span>
<span class="sd">        num_simulations: Number of simulations to run.</span>
<span class="sd">        eps: Acceptance threshold $\epsilon$ for distance between observed and</span>
<span class="sd">            simulated data.</span>
<span class="sd">        quantile: Upper quantile of smallest distances for which the corresponding</span>
<span class="sd">            parameters are returned, e.g, q=0.01 will return the top 1%. Exactly</span>
<span class="sd">            one of quantile or `eps` have to be passed.</span>
<span class="sd">        lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">        sass: Whether to determine semi-automatic summary statistics as in</span>
<span class="sd">            Fearnhead &amp; Prangle 2012.</span>
<span class="sd">        sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">        sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">            sass regression, default 1 - no expansion.</span>
<span class="sd">        kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">            object from which one can sample.</span>
<span class="sd">        kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">            &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">            heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">            default &#39;cv&#39;.</span>
<span class="sd">            &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">            &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">            more details</span>
<span class="sd">        return_summary: Whether to return the distances and data corresponding to</span>
<span class="sd">            the accepted parameters.</span>
<span class="sd">        num_iid_samples: Number of simulations per parameter. Choose</span>
<span class="sd">            `num_iid_samples&gt;1`, if you have chosen a statistical distance that</span>
<span class="sd">            evaluates sets of simulations against a set of reference observations</span>
<span class="sd">            instead of a single data-point comparison.</span>

<span class="sd">    Returns:</span>
<span class="sd">        theta (if kde False): accepted parameters</span>
<span class="sd">        kde (if kde True): KDE object based on accepted parameters from which one</span>
<span class="sd">            can .sample() and .log_prob().</span>
<span class="sd">        summary (if summary True): dictionary containing the accepted paramters (if</span>
<span class="sd">            kde True), distances and simulated data x.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Exactly one of eps or quantile need to be passed.</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span>
        <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">&quot;Eps or quantile must be passed, but not both.&quot;</span>
    <span class="k">if</span> <span class="n">kde_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kde_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Run SASS and change the simulator and x_o accordingly.</span>
    <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
        <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Running SASS with </span><span class="si">%s</span><span class="s2"> pilot samples.&quot;</span><span class="p">,</span> <span class="n">num_pilot_simulations</span>
        <span class="p">)</span>
        <span class="n">num_simulations</span> <span class="o">-=</span> <span class="n">num_pilot_simulations</span>

        <span class="n">pilot_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_pilot_simulations</span><span class="p">,))</span>
        <span class="n">pilot_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">pilot_theta</span><span class="p">)</span>

        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
            <span class="n">pilot_theta</span><span class="p">,</span> <span class="n">pilot_x</span><span class="p">,</span> <span class="n">sass_expansion_degree</span>
        <span class="p">)</span>

        <span class="c1"># Add sass transform to simulator and x_o.</span>
        <span class="k">def</span> <span class="nf">simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

        <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="n">x_o</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">simulator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span>

    <span class="c1"># Simulate and calculate distances.</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>
    <span class="n">theta_repeat</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_iid_samples</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">simulator</span><span class="p">(</span><span class="n">theta_repeat</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span>
        <span class="n">num_simulations</span><span class="p">,</span>
        <span class="n">num_iid_samples</span><span class="p">,</span>
        <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">))</span>  <span class="c1"># Dim(num_initial_pop, num_iid_samples, -1)</span>

    <span class="c1"># Infer x shape to test and set x_o.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># Select based on acceptance threshold epsilon.</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">&lt;</span> <span class="n">eps</span>
        <span class="n">num_accepted</span> <span class="o">=</span> <span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;No parameters accepted, eps=</span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2"> too small&quot;</span>

        <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
        <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>
        <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">]</span>

    <span class="c1"># Select based on quantile on sorted distances.</span>
    <span class="k">elif</span> <span class="n">quantile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_top_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_simulations</span> <span class="o">*</span> <span class="n">quantile</span><span class="p">)</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">theta_accepted</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
        <span class="n">distances_accepted</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>
        <span class="n">x_accepted</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">][:</span><span class="n">num_top_samples</span><span class="p">]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;One of epsilon or quantile has to be passed.&quot;</span><span class="p">)</span>

    <span class="c1"># Maybe adjust theta with LRA.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
        <span class="n">final_theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">theta_accepted</span><span class="p">,</span> <span class="n">x_accepted</span><span class="p">,</span> <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">final_theta</span> <span class="o">=</span> <span class="n">theta_accepted</span>

    <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;KDE on %s samples with bandwidth option</span>
<span class="sd">            {kde_kwargs[&quot;bandwidth&quot;] if &quot;bandwidth&quot; in kde_kwargs else &quot;cv&quot;}.</span>
<span class="sd">            Beware that KDE can give unreliable results when used with too few</span>
<span class="sd">            samples and in high dimensions.&quot;&quot;&quot;</span><span class="p">,</span>
            <span class="n">final_theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_theta</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">kde_dist</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">final_theta</span><span class="p">,</span> <span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kde_dist</span>
    <span class="k">elif</span> <span class="n">return_summary</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_theta</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">distances</span><span class="o">=</span><span class="n">distances_accepted</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_accepted</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_theta</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.mcabc.MCABC.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">requires_iid_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distance_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">distance_batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.mcabc.MCABC.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</p>
<p>[1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.
(1999). Population growth of human Y chromosomes: a study of Y chromosome
microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>simulator</code>
            </td>
            <td>
                  <code><span title="typing.Callable">Callable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\mathrm{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Distance function to compare observed and simulated data. Can be
a custom callable function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>,
<code>mmd</code>, <code>wasserstein</code>.</p>
              </div>
            </td>
            <td>
                  <code>&#39;l2&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_iid_data</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[None]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to allow conditioning on iid sampled data or not.
Typically, this information is inferred by the choice of the distance,
but in case a custom distance is used, this information is pivotal.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Configurations parameters for the distances. In particular
useful for the MMD and Wasserstein distance.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_workers</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of parallel workers to use for simulations.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>simulation_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of simulations that the distance function
evaluates against the reference observations at once. If -1, we evaluate
all simulations at the same time.</p>
              </div>
            </td>
            <td>
                  <code>-1</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/mcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">,</span>
    <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
    <span class="n">requires_iid_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">distance_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">distance_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Monte-Carlo Approximate Bayesian Computation (Rejection ABC) [1].</span>

<span class="sd">    [1] Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., &amp; Feldman, M. W.</span>
<span class="sd">    (1999). Population growth of human Y chromosomes: a study of Y chromosome</span>
<span class="sd">    microsatellites. Molecular biology and evolution, 16(12), 1791-1798.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">            a custom callable function or one of `l1`, `l2`, `mse`,</span>
<span class="sd">            `mmd`, `wasserstein`.</span>
<span class="sd">        requires_iid_data: Whether to allow conditioning on iid sampled data or not.</span>
<span class="sd">            Typically, this information is inferred by the choice of the distance,</span>
<span class="sd">            but in case a custom distance is used, this information is pivotal.</span>
<span class="sd">        distance_kwargs: Configurations parameters for the distances. In particular</span>
<span class="sd">            useful for the MMD and Wasserstein distance.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        distance_batch_size: Number of simulations that the distance function</span>
<span class="sd">            evaluates against the reference observations at once. If -1, we evaluate</span>
<span class="sd">            all simulations at the same time.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">requires_iid_data</span><span class="o">=</span><span class="n">requires_iid_data</span><span class="p">,</span>
        <span class="n">distance_kwargs</span><span class="o">=</span><span class="n">distance_kwargs</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">distance_batch_size</span><span class="o">=</span><span class="n">distance_batch_size</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="sbi.inference.abc.smcabc.SMCABC" class="doc doc-heading">
            <code>SMCABC</code>


<a href="#sbi.inference.abc.smcabc.SMCABC" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sbi.inference.abc.abc_base.ABCBASE">ABCBASE</span></code></p>


        <p>Sequential Monte Carlo Approximate Bayesian Computation.</p>






              <details class="quote">
                <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
                <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SMCABC</span><span class="p">(</span><span class="n">ABCBASE</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sequential Monte Carlo Approximate Bayesian Computation.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
        <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
        <span class="n">requires_iid_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">distance_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">distance_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
        <span class="n">algorithm_variant</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Monte Carlo Approximate Bayesian Computation.</span>

<span class="sd">        We distinguish between three different SMC methods here:</span>
<span class="sd">            - A: Toni et al. 2010 (Phd Thesis)</span>
<span class="sd">            - B: Sisson et al. 2007 (with correction from 2009)</span>
<span class="sd">            - C: Beaumont et al. 2009</span>

<span class="sd">        In Toni et al. 2010 we find an overview of the differences on page 34:</span>
<span class="sd">            - B: same as A except for resampling of weights if the effective sampling</span>
<span class="sd">                size is too small.</span>
<span class="sd">            - C: same as A except for calculation of the covariance of the perturbation</span>
<span class="sd">                kernel: the kernel covariance is a scaled version of the covariance of</span>
<span class="sd">                the previous population.</span>

<span class="sd">        Args:</span>
<span class="sd">            simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">                simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">                regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">                can be used.</span>
<span class="sd">            prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">                parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">                object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">                distribution) can be used.</span>
<span class="sd">            distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">                a custom callable function or one of `l1`, `l2`, `mse`,</span>
<span class="sd">                `mmd`, `wasserstein`.</span>
<span class="sd">            requires_iid_data: Whether to allow conditioning on iid sampled data or not.</span>
<span class="sd">                Typically, this information is inferred by the choice of the distance,</span>
<span class="sd">                but in case a custom distance is used, this information is pivotal.</span>
<span class="sd">            distance_kwargs: Configurations parameters for the distances. In particular</span>
<span class="sd">                useful for the MMD and Wasserstein distance.</span>
<span class="sd">            num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">            simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">                maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">                same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">                (simulation_batch_size, parameter_dimension).</span>
<span class="sd">            distance_batch_size: Number of simulations that the distance function</span>
<span class="sd">                evaluates against the reference observations at once. If -1, we evaluate</span>
<span class="sd">                all simulations at the same time.</span>
<span class="sd">            show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">                sampling.</span>
<span class="sd">            kernel: Perturbation kernel.</span>
<span class="sd">            algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
            <span class="n">requires_iid_data</span><span class="o">=</span><span class="n">requires_iid_data</span><span class="p">,</span>
            <span class="n">distance_kwargs</span><span class="o">=</span><span class="n">distance_kwargs</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
            <span class="n">distance_batch_size</span><span class="o">=</span><span class="n">distance_batch_size</span><span class="p">,</span>
            <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">kernels</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Kernel &#39;</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported. Choose one from </span><span class="si">{</span><span class="n">kernels</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

        <span class="n">algorithm_variants</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="n">algorithm_variants</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;SMCABC variant &#39;</span><span class="si">{</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported, choose one from&quot;</span>
            <span class="s2">&quot; </span><span class="si">{algorithm_variants}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">=</span> <span class="n">algorithm_variant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance_to_x0</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Define simulator that keeps track of budget.</span>
        <span class="k">def</span> <span class="nf">simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">simulate_with_budget</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">epsilon_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">distance_based_decay</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">ess_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kde_sample_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_iid_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run SMCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">        Args:</span>
<span class="sd">            x_o: Observed data.</span>
<span class="sd">            num_particles: Number of particles in each population.</span>
<span class="sd">            num_initial_pop: Number of simulations used for initial population.</span>
<span class="sd">            num_simulations: Total number of possible simulations.</span>
<span class="sd">            epsilon_decay: Factor with which the acceptance threshold $\epsilon$ decays.</span>
<span class="sd">            distance_based_decay: Whether the $\epsilon$ decay is constant over</span>
<span class="sd">                populations or calculated from the previous populations distribution of</span>
<span class="sd">                distances.</span>
<span class="sd">            ess_min: Threshold of effective sampling size for resampling weights. Not</span>
<span class="sd">                used when None (default).</span>
<span class="sd">            kernel_variance_scale: Factor for scaling the perturbation kernel variance.</span>
<span class="sd">            use_last_pop_samples: Whether to fill up the current population with</span>
<span class="sd">                samples from the previous population when the budget is used up. If</span>
<span class="sd">                False, the current population is discarded and the previous population</span>
<span class="sd">                is returned.</span>
<span class="sd">            lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">            lra_with_weights: Whether to run lra as weighted linear regression with SMC</span>
<span class="sd">                weights</span>
<span class="sd">            sass: Whether to determine semi-automatic summary statistics (sass) as in</span>
<span class="sd">                Fearnhead &amp; Prangle 2012.</span>
<span class="sd">            sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">            sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">                sass regression, default 1 - no expansion.</span>
<span class="sd">            kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">                object from which one can sample.</span>
<span class="sd">            kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">                &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">                heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">                default &#39;cv&#39;.</span>
<span class="sd">                &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">                &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">                more details</span>
<span class="sd">            kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw</span>
<span class="sd">                particles.</span>
<span class="sd">            return_summary: Whether to return a dictionary with all accepted particles,</span>
<span class="sd">                weights, etc. at the end.</span>
<span class="sd">            num_iid_samples: Number of simulations per parameter. Choose</span>
<span class="sd">                `num_iid_samples&gt;1`, if you have chosen a statistical distance that</span>
<span class="sd">                evaluates sets of simulations against a set of reference observations</span>
<span class="sd">                instead of a single data-point comparison.</span>

<span class="sd">        Returns:</span>
<span class="sd">            theta (if kde False): accepted parameters of the last population.</span>
<span class="sd">            kde (if kde True): KDE object fitted on accepted parameters, from which one</span>
<span class="sd">                can .sample() and .log_prob().</span>
<span class="sd">            summary (if return_summary True): dictionary containing the accepted</span>
<span class="sd">                paramters (if kde True), distances and simulated data x of all</span>
<span class="sd">                populations.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">pop_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="n">num_simulations</span> <span class="o">*</span> <span class="n">num_iid_samples</span>
        <span class="k">if</span> <span class="n">kde_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kde_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">epsilon_decay</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="n">epsilon_decay</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span> <span class="ow">and</span> <span class="n">lra</span>
        <span class="p">),</span> <span class="s2">&quot;Currently there is no support to run inference &quot;</span>
        <span class="s2">&quot;on multiple observations together with lra.&quot;</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span> <span class="ow">and</span> <span class="n">sass</span>
        <span class="p">),</span> <span class="s2">&quot;Currently there is no support to run inference &quot;</span>
        <span class="s2">&quot;on multiple observations together with sass.&quot;</span>

        <span class="c1"># Pilot run for SASS.</span>
        <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
            <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Running SASS with </span><span class="si">%s</span><span class="s2"> pilot samples.&quot;</span><span class="p">,</span> <span class="n">num_pilot_simulations</span>
            <span class="p">)</span>
            <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_sass_set_xo</span><span class="p">(</span>
                <span class="n">num_particles</span><span class="p">,</span>
                <span class="n">num_pilot_simulations</span><span class="p">,</span>
                <span class="n">x_o</span><span class="p">,</span>
                <span class="n">num_iid_samples</span><span class="p">,</span>
                <span class="n">lra</span><span class="p">,</span>
                <span class="n">sass_expansion_degree</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Udpate simulator and xo</span>
            <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">sass_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">sass_simulator</span>

        <span class="c1"># run initial population</span>
        <span class="n">particles</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
            <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span><span class="p">,</span> <span class="n">num_iid_samples</span>
        <span class="p">)</span>
        <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">((</span>
            <span class="s2">&quot;population=</span><span class="si">%s</span><span class="s2">, eps=</span><span class="si">%s</span><span class="s2">, ess=</span><span class="si">%s</span><span class="s2">, num_sims=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">pop_idx</span><span class="p">,</span>
            <span class="n">epsilon</span><span class="p">,</span>
            <span class="mf">1.0</span><span class="p">,</span>
            <span class="n">num_initial_pop</span><span class="p">,</span>
        <span class="p">))</span>

        <span class="n">all_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
        <span class="n">all_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
        <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
        <span class="n">all_epsilons</span> <span class="o">=</span> <span class="p">[</span><span class="n">epsilon</span><span class="p">]</span>
        <span class="n">all_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span><span class="p">:</span>
            <span class="n">pop_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># Decay based on quantile of distances from previous pop.</span>
            <span class="k">if</span> <span class="n">distance_based_decay</span><span class="p">:</span>
                <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_epsilon</span><span class="p">(</span>
                    <span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">epsilon_decay</span>
                <span class="p">)</span>
            <span class="c1"># Constant decay.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epsilon</span> <span class="o">*=</span> <span class="n">epsilon_decay</span>

            <span class="c1"># Get kernel variance from previous pop.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_kernel_variance</span><span class="p">(</span>
                <span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span>
                <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="n">kernel_variance_scale</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_next_population</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">num_iid_samples</span><span class="o">=</span><span class="n">num_iid_samples</span><span class="p">,</span>
                <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="n">use_last_pop_samples</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Resample population if effective sampling size is too small.</span>
            <span class="k">if</span> <span class="n">ess_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_if_ess_too_small</span><span class="p">(</span>
                    <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">((</span>
                <span class="s2">&quot;population=</span><span class="si">%s</span><span class="s2"> done: eps=</span><span class="si">{epsilon:.6f}</span><span class="s2">, num_sims=</span><span class="si">%s</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">pop_idx</span><span class="p">,</span>
                <span class="n">epsilon</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="p">,</span>
            <span class="p">))</span>

            <span class="c1"># collect results</span>
            <span class="n">all_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">)</span>
            <span class="n">all_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
            <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">all_epsilons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">all_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Maybe run LRA and adjust weights.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
            <span class="n">adjusted_particles</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra_update_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">observation</span><span class="o">=</span><span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">),</span>
                <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">lra_with_weights</span><span class="o">=</span><span class="n">lra_with_weights</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">final_particles</span> <span class="o">=</span> <span class="n">adjusted_particles</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">final_particles</span> <span class="o">=</span> <span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;KDE on %s samples with bandwidth option %s. Beware that KDE can give</span>
<span class="sd">                unreliable results when used with too few samples and in high</span>
<span class="sd">                dimensions.&quot;&quot;&quot;</span><span class="p">,</span>
                <span class="n">final_particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">kde_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">,</span> <span class="s2">&quot;cv&quot;</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="c1"># Maybe get particles weights from last population for weighted KDE.</span>
            <span class="k">if</span> <span class="n">kde_sample_weights</span><span class="p">:</span>
                <span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;sample_weights&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>

            <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_particles</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">(</span>
                    <span class="n">kde_dist</span><span class="p">,</span>
                    <span class="nb">dict</span><span class="p">(</span>
                        <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                        <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                        <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                        <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                        <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">kde_dist</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">final_particles</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">(</span>
                    <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                    <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                    <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                    <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_particles</span>

    <span class="k">def</span> <span class="nf">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_iid_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return particles, epsilon and distances of initial population.&quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">num_particles</span> <span class="o">&lt;=</span> <span class="n">num_initial_pop</span>
        <span class="p">),</span> <span class="s2">&quot;number of initial round simulations must be greater than population size&quot;</span>

        <span class="k">assert</span> <span class="p">(</span><span class="n">x_o</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;Your data contain iid data-points, but the choice of &quot;</span>
            <span class="s2">&quot;your distance does not allow multiple conditioning &quot;</span>
            <span class="s2">&quot;observations.&quot;</span>
        <span class="p">)</span>

        <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_initial_pop</span><span class="p">,))</span>

        <span class="n">theta_repeat</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_iid_samples</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span><span class="p">(</span><span class="n">theta_repeat</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span>
            <span class="n">num_initial_pop</span><span class="p">,</span>
            <span class="n">num_iid_samples</span><span class="p">,</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">))</span>  <span class="c1"># Dim(num_initial_pop, num_iid_samples, -1)</span>

        <span class="c1"># Infer x shape to test and set x_o.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="o">=</span> <span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">sortidx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">particles</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">]</span>
        <span class="c1"># Take last accepted distance as epsilon.</span>
        <span class="n">initial_epsilon</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][</span><span class="n">num_particles</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">math</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">initial_epsilon</span><span class="p">):</span>
            <span class="n">initial_epsilon</span> <span class="o">=</span> <span class="mf">1e8</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span>
            <span class="n">initial_epsilon</span><span class="p">,</span>
            <span class="n">distances</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">],</span>
            <span class="n">x</span><span class="p">[</span><span class="n">sortidx</span><span class="p">][:</span><span class="n">num_particles</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sample_next_population</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">distances</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">num_iid_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return particles, weights and distances of new population.&quot;&quot;&quot;</span>

        <span class="n">new_particles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_distances</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_x</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">num_accepted_particles</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">while</span> <span class="n">num_accepted_particles</span> <span class="o">&lt;</span> <span class="n">num_particles</span><span class="p">:</span>
            <span class="c1"># Upperbound for batch size to not exceed simulation budget.</span>
            <span class="n">num_batch</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="n">num_particles</span> <span class="o">-</span> <span class="n">num_accepted_particles</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Sample from previous population and perturb.</span>
            <span class="n">particle_candidates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_and_perturb</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_batch</span>
            <span class="p">)</span>
            <span class="c1"># Simulate and select based on distance.</span>
            <span class="n">candidates_repeated</span> <span class="o">=</span> <span class="n">particle_candidates</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
                <span class="n">num_iid_samples</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">x_candidates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span><span class="p">(</span><span class="n">candidates_repeated</span><span class="p">)</span>
            <span class="n">x_candidates</span> <span class="o">=</span> <span class="n">x_candidates</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span>
                <span class="n">num_batch</span><span class="p">,</span>
                <span class="n">num_iid_samples</span><span class="p">,</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">))</span>  <span class="c1"># Dim(num_initial_pop, num_iid_samples, -1)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span><span class="p">:</span>
                <span class="n">x_candidates</span> <span class="o">=</span> <span class="n">x_candidates</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">dists</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">,</span> <span class="n">x_candidates</span><span class="p">)</span>
            <span class="n">is_accepted</span> <span class="o">=</span> <span class="n">dists</span> <span class="o">&lt;=</span> <span class="n">epsilon</span>
            <span class="n">num_accepted_batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">is_accepted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="k">if</span> <span class="n">num_accepted_batch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">new_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particle_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">new_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
                        <span class="n">particle_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">],</span>
                        <span class="n">particles</span><span class="p">,</span>
                        <span class="n">log_weights</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">new_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dists</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">new_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_candidates</span><span class="p">[</span><span class="n">is_accepted</span><span class="p">])</span>
                <span class="n">num_accepted_particles</span> <span class="o">+=</span> <span class="n">num_accepted_batch</span>

            <span class="c1"># If simulation budget was exceeded and we still need particles, take</span>
            <span class="c1"># previous population or fill up with previous population.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span>
                <span class="ow">and</span> <span class="n">num_accepted_particles</span> <span class="o">&lt;</span> <span class="n">num_particles</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">use_last_pop_samples</span><span class="p">:</span>
                    <span class="n">num_remaining</span> <span class="o">=</span> <span class="n">num_particles</span> <span class="o">-</span> <span class="n">num_accepted_particles</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
<span class="w">                        </span><span class="sd">&quot;&quot;&quot;Simulation Budget exceeded, filling up with %s</span>
<span class="sd">                        samples from last population.&quot;&quot;&quot;</span><span class="p">,</span>
                        <span class="n">num_remaining</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="c1"># Some new particles have been accepted already, therefore</span>
                    <span class="c1"># fill up the remaining once with old particles and weights.</span>
                    <span class="n">new_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">,</span> <span class="p">:])</span>
                    <span class="c1"># Recalculate weights with new particles.</span>
                    <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_particles</span><span class="p">),</span>
                            <span class="n">particles</span><span class="p">,</span>
                            <span class="n">log_weights</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                    <span class="n">new_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">])</span>
                    <span class="n">new_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">num_remaining</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;Simulation Budget exceeded, returning previous population.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">new_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
                    <span class="n">new_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
                    <span class="n">new_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
                    <span class="n">new_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

                <span class="k">break</span>

        <span class="c1"># collect lists of tensors into tensors</span>
        <span class="n">new_particles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_particles</span><span class="p">)</span>
        <span class="n">new_log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_log_weights</span><span class="p">)</span>
        <span class="n">new_distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_distances</span><span class="p">)</span>
        <span class="n">new_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>

        <span class="c1"># normalize the new weights</span>
        <span class="n">new_log_weights</span> <span class="o">-=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">new_log_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Return sorted wrt distances.</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">new_distances</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">new_particles</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_log_weights</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_distances</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
            <span class="n">new_x</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_next_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distances</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">quantile</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return epsilon for next round based on quantile of this round&#39;s distances.</span>

<span class="sd">        Note: distances are made unique to avoid repeated distances from simulations</span>
<span class="sd">        that result in the same observation.</span>

<span class="sd">        Args:</span>
<span class="sd">            distances: The distances accepted in this round.</span>
<span class="sd">            quantile: Quantile in the distance distribution to determine new epsilon.</span>

<span class="sd">        Returns:</span>
<span class="sd">            epsilon: Epsilon for the next population.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Take unique distances to skip same distances simulations (return is sorted).</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="c1"># Cumsum as cdf proxy.</span>
        <span class="n">distances_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">distances</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="c1"># Take the q quantile of distances.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">qidx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">distances_cdf</span> <span class="o">&gt;=</span> <span class="n">quantile</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">((</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;Accepted unique distances=%s don&#39;t match quantile=%s. Selecting</span>
<span class="sd">                    last distance.&quot;&quot;&quot;</span><span class="p">,</span>
                <span class="n">distances</span><span class="p">,</span>
                <span class="n">quantile</span><span class="p">,</span>
            <span class="p">))</span>
            <span class="n">qidx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># The new epsilon is given by that distance.</span>
        <span class="k">return</span> <span class="n">distances</span><span class="p">[</span><span class="n">qidx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_calculate_new_log_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">new_particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">old_particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">old_log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return new log weights following formulas in publications A,B anc C.&quot;&quot;&quot;</span>

        <span class="c1"># Prior can be batched across new particles.</span>
        <span class="n">prior_log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">new_particles</span><span class="p">)</span>

        <span class="c1"># Contstruct function to get kernel log prob for given old particle.</span>
        <span class="c1"># The kernel is centered on each old particle as in all three variants (A,B,C).</span>
        <span class="k">def</span> <span class="nf">kernel_log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_new_kernel</span><span class="p">(</span><span class="n">old_particles</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">)</span>

        <span class="c1"># We still have to loop over particles here because</span>
        <span class="c1"># the kernel log probs are already batched across old particles.</span>
        <span class="n">log_weighted_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">old_log_weights</span> <span class="o">+</span> <span class="n">kernel_log_prob</span><span class="p">(</span><span class="n">new_particle</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">new_particle</span> <span class="ow">in</span> <span class="n">new_particles</span>
            <span class="p">],</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># new weights are prior probs over weighted sum:</span>
        <span class="k">return</span> <span class="n">prior_log_probs</span> <span class="o">-</span> <span class="n">log_weighted_sum</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">sample_from_population_with_weights</span><span class="p">(</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return samples from particles sampled with weights.&quot;&quot;&quot;</span>

        <span class="c1"># define multinomial with weights as probs</span>
        <span class="n">multi</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
        <span class="c1"># sample num samples, with replacement</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">multi</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,)))</span>
        <span class="c1"># get indices of success trials</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># return those indices from trace</span>
        <span class="k">return</span> <span class="n">particles</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_sample_and_perturb</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample and perturb batch of new parameters from trace.</span>

<span class="sd">        Reject sampled and perturbed parameters outside of prior.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_accepted</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">num_accepted</span> <span class="o">&lt;</span> <span class="n">num_samples</span><span class="p">:</span>
            <span class="n">parms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span> <span class="o">-</span> <span class="n">num_accepted</span>
            <span class="p">)</span>

            <span class="c1"># Create kernel on params and perturb.</span>
            <span class="n">parms_perturbed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_new_kernel</span><span class="p">(</span><span class="n">parms</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

            <span class="n">is_within_prior</span> <span class="o">=</span> <span class="n">within_support</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">parms_perturbed</span><span class="p">)</span>
            <span class="n">num_accepted</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">is_within_prior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="k">if</span> <span class="n">num_accepted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parms_perturbed</span><span class="p">[</span><span class="n">is_within_prior</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_kernel_variance</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return kernel variance for a given population of particles and weights.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="c1"># For variant C, Beaumont et al. 2009, the kernel variance comes from the</span>
            <span class="c1"># previous population.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">==</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span>
                <span class="c1"># Calculate weighted covariance of particles.</span>
                <span class="n">population_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">aweights</span><span class="o">=</span><span class="n">weights</span><span class="p">)),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># Make sure variance is nonsingular.</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">population_cov</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                        </span><span class="sd">&quot;&quot;&quot;&quot;Singular particle covariance, using unit covariance.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                    <span class="n">population_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">population_cov</span>
            <span class="c1"># While for Toni et al. and Sisson et al. it comes from the parameter</span>
            <span class="c1"># ranges.</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">):</span>
                <span class="n">particle_ranges</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_particle_ranges</span><span class="p">(</span>
                    <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="n">samples_per_dim</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">particle_ranges</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variant, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="c1"># Variance spans the range of parameters for every dimension.</span>
            <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_particle_ranges</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="n">samples_per_dim</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return new kernel distribution for a given set of paramters.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;get kernel variance first.&quot;</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="k">return</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
                <span class="n">loc</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="n">low</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="n">high</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
            <span class="c1"># Move batch shape to event shape to get Uniform that is multivariate in</span>
            <span class="c1"># parameter dimension.</span>
            <span class="k">return</span> <span class="n">BoxUniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resample_if_ess_too_small</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">ess_min</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">pop_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return resampled particles and uniform weights if effectice sampling size is</span>
<span class="sd">        too small.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">log_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_particles</span>
        <span class="c1"># Resampling of weights for low ESS only for Sisson et al. 2007.</span>
        <span class="k">if</span> <span class="n">ess</span> <span class="o">&lt;</span> <span class="n">ess_min</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;ESS=</span><span class="si">%s</span><span class="s2"> too low, resampling pop </span><span class="si">%s</span><span class="s2">...&quot;</span><span class="p">,</span> <span class="n">ess</span><span class="p">,</span> <span class="n">pop_idx</span><span class="p">)</span>
            <span class="c1"># First resample, then set to uniform weights as in Sisson et al. 2007.</span>
            <span class="n">particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_particles</span>
            <span class="p">)</span>
            <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span>

    <span class="k">def</span> <span class="nf">run_lra_update_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">observation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return particles and weights adjusted with LRA.</span>

<span class="sd">        Runs (weighted) linear regression from xs onto particles to adjust the</span>
<span class="sd">        particles.</span>

<span class="sd">        Updates the SMC weights according to the new particles.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">adjusted_particels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">xs</span><span class="p">,</span>
            <span class="n">observation</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">log_weights</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">if</span> <span class="n">lra_with_weights</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Update SMC weights with LRA adjusted weights</span>
        <span class="n">adjusted_log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
            <span class="n">new_particles</span><span class="o">=</span><span class="n">adjusted_particels</span><span class="p">,</span>
            <span class="n">old_particles</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
            <span class="n">old_log_weights</span><span class="o">=</span><span class="n">log_weights</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">adjusted_particels</span><span class="p">,</span> <span class="n">adjusted_log_weights</span>

    <span class="k">def</span> <span class="nf">run_sass_set_xo</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_pilot_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">x_o</span><span class="p">,</span>
        <span class="n">num_iid_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return transform for semi-automatic summary statistics.</span>

<span class="sd">        Runs an single round of rejection abc with fixed budget and accepts</span>
<span class="sd">        num_particles simulations to run the regression for sass.</span>

<span class="sd">        Sets self.x_o once the x_shape can be derived from simulations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="p">(</span>
            <span class="n">pilot_particles</span><span class="p">,</span>
            <span class="n">_</span><span class="p">,</span>
            <span class="n">_</span><span class="p">,</span>
            <span class="n">pilot_xs</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
            <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">num_iid_samples</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;x_o not set yet.&quot;</span>

        <span class="c1"># Adjust with LRA.</span>
        <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
            <span class="n">pilot_particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">pilot_particles</span><span class="p">,</span> <span class="n">pilot_xs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
            <span class="n">pilot_particles</span><span class="p">,</span>
            <span class="n">pilot_xs</span><span class="p">,</span>
            <span class="n">expansion_degree</span><span class="o">=</span><span class="n">sass_expansion_degree</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sass_transform</span>

    <span class="k">def</span> <span class="nf">get_particle_ranges</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return range of particles in each parameter dimension.&quot;&quot;&quot;</span>

        <span class="c1"># get weighted samples</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span>
            <span class="n">weights</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">samples_per_dim</span> <span class="o">*</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1"># Variance spans the range of particles for every dimension.</span>
        <span class="n">particle_ranges</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
        <span class="k">assert</span> <span class="n">particle_ranges</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">particle_ranges</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.__call__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">epsilon_decay</span><span class="p">,</span> <span class="n">distance_based_decay</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ess_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kde_sample_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lra_with_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_fraction</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_iid_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.__call__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Run SMCABC and return accepted parameters or KDE object fitted on them.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x_o</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="numpy.ndarray">ndarray</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Observed data.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_particles</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of particles in each population.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_initial_pop</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of simulations used for initial population.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_simulations</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Total number of possible simulations.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>epsilon_decay</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Factor with which the acceptance threshold <span class="arithmatex">\(\epsilon\)</span> decays.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance_based_decay</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the <span class="arithmatex">\(\epsilon\)</span> decay is constant over
populations or calculated from the previous populations distribution of
distances.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ess_min</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Threshold of effective sampling size for resampling weights. Not
used when None (default).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_variance_scale</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Factor for scaling the perturbation kernel variance.</p>
              </div>
            </td>
            <td>
                  <code>1.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_last_pop_samples</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to fill up the current population with
samples from the previous population when the budget is used up. If
False, the current population is discarded and the previous population
is returned.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lra</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to run linear regression adjustment as in Beaumont et al. 2002</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lra_with_weights</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to run lra as weighted linear regression with SMC
weights</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sass</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to determine semi-automatic summary statistics (sass) as in
Fearnhead &amp; Prangle 2012.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sass_fraction</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Fraction of simulation budget used for the initial sass run.</p>
              </div>
            </td>
            <td>
                  <code>0.25</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sass_expansion_degree</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Degree of the polynomial feature expansion for the
sass regression, default 1 - no expansion.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kde</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to run KDE on the accepted parameters to return a KDE
object from which one can sample.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kde_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>kwargs for performing KDE:
&lsquo;bandwidth=&rsquo;; either a float, or a string naming a bandwidth
heuristics, e.g., &lsquo;cv&rsquo; (cross validation), &lsquo;silvermann&rsquo; or &lsquo;scott&rsquo;,
default &lsquo;cv&rsquo;.
&lsquo;transform&rsquo;: transform applied to the parameters before doing KDE.
&lsquo;sample_weights&rsquo;: weights associated with samples. See &lsquo;get_kde&rsquo; for
more details</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kde_sample_weights</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether perform weighted KDE with SMC weights or on raw
particles.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_summary</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to return a dictionary with all accepted particles,
weights, etc. at the end.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_iid_samples</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of simulations per parameter. Choose
<code>num_iid_samples&gt;1</code>, if you have chosen a statistical distance that
evaluates sets of simulations against a set of reference observations
instead of a single data-point comparison.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>theta</code></td>            <td>
                  <code>if kde False</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>accepted parameters of the last population.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>kde</code></td>            <td>
                  <code>if kde True</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>KDE object fitted on accepted parameters, from which one
can .sample() and .log_prob().</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>summary</code></td>            <td>
                  <code>if return_summary True</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary containing the accepted
paramters (if kde True), distances and simulated data x of all
populations.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">],</span>
    <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_initial_pop</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">epsilon_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">distance_based_decay</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">ess_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">use_last_pop_samples</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">return_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">kde_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kde_sample_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">num_iid_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">KDEWrapper</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">KDEWrapper</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Run SMCABC and return accepted parameters or KDE object fitted on them.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_o: Observed data.</span>
<span class="sd">        num_particles: Number of particles in each population.</span>
<span class="sd">        num_initial_pop: Number of simulations used for initial population.</span>
<span class="sd">        num_simulations: Total number of possible simulations.</span>
<span class="sd">        epsilon_decay: Factor with which the acceptance threshold $\epsilon$ decays.</span>
<span class="sd">        distance_based_decay: Whether the $\epsilon$ decay is constant over</span>
<span class="sd">            populations or calculated from the previous populations distribution of</span>
<span class="sd">            distances.</span>
<span class="sd">        ess_min: Threshold of effective sampling size for resampling weights. Not</span>
<span class="sd">            used when None (default).</span>
<span class="sd">        kernel_variance_scale: Factor for scaling the perturbation kernel variance.</span>
<span class="sd">        use_last_pop_samples: Whether to fill up the current population with</span>
<span class="sd">            samples from the previous population when the budget is used up. If</span>
<span class="sd">            False, the current population is discarded and the previous population</span>
<span class="sd">            is returned.</span>
<span class="sd">        lra: Whether to run linear regression adjustment as in Beaumont et al. 2002</span>
<span class="sd">        lra_with_weights: Whether to run lra as weighted linear regression with SMC</span>
<span class="sd">            weights</span>
<span class="sd">        sass: Whether to determine semi-automatic summary statistics (sass) as in</span>
<span class="sd">            Fearnhead &amp; Prangle 2012.</span>
<span class="sd">        sass_fraction: Fraction of simulation budget used for the initial sass run.</span>
<span class="sd">        sass_expansion_degree: Degree of the polynomial feature expansion for the</span>
<span class="sd">            sass regression, default 1 - no expansion.</span>
<span class="sd">        kde: Whether to run KDE on the accepted parameters to return a KDE</span>
<span class="sd">            object from which one can sample.</span>
<span class="sd">        kde_kwargs: kwargs for performing KDE:</span>
<span class="sd">            &#39;bandwidth=&#39;; either a float, or a string naming a bandwidth</span>
<span class="sd">            heuristics, e.g., &#39;cv&#39; (cross validation), &#39;silvermann&#39; or &#39;scott&#39;,</span>
<span class="sd">            default &#39;cv&#39;.</span>
<span class="sd">            &#39;transform&#39;: transform applied to the parameters before doing KDE.</span>
<span class="sd">            &#39;sample_weights&#39;: weights associated with samples. See &#39;get_kde&#39; for</span>
<span class="sd">            more details</span>
<span class="sd">        kde_sample_weights: Whether perform weighted KDE with SMC weights or on raw</span>
<span class="sd">            particles.</span>
<span class="sd">        return_summary: Whether to return a dictionary with all accepted particles,</span>
<span class="sd">            weights, etc. at the end.</span>
<span class="sd">        num_iid_samples: Number of simulations per parameter. Choose</span>
<span class="sd">            `num_iid_samples&gt;1`, if you have chosen a statistical distance that</span>
<span class="sd">            evaluates sets of simulations against a set of reference observations</span>
<span class="sd">            instead of a single data-point comparison.</span>

<span class="sd">    Returns:</span>
<span class="sd">        theta (if kde False): accepted parameters of the last population.</span>
<span class="sd">        kde (if kde True): KDE object fitted on accepted parameters, from which one</span>
<span class="sd">            can .sample() and .log_prob().</span>
<span class="sd">        summary (if return_summary True): dictionary containing the accepted</span>
<span class="sd">            paramters (if kde True), distances and simulated data x of all</span>
<span class="sd">            populations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">pop_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="n">num_simulations</span> <span class="o">*</span> <span class="n">num_iid_samples</span>
    <span class="k">if</span> <span class="n">kde_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kde_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">epsilon_decay</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="n">epsilon_decay</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span> <span class="ow">and</span> <span class="n">lra</span>
    <span class="p">),</span> <span class="s2">&quot;Currently there is no support to run inference &quot;</span>
    <span class="s2">&quot;on multiple observations together with lra.&quot;</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">requires_iid_data</span> <span class="ow">and</span> <span class="n">sass</span>
    <span class="p">),</span> <span class="s2">&quot;Currently there is no support to run inference &quot;</span>
    <span class="s2">&quot;on multiple observations together with sass.&quot;</span>

    <span class="c1"># Pilot run for SASS.</span>
    <span class="k">if</span> <span class="n">sass</span><span class="p">:</span>
        <span class="n">num_pilot_simulations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sass_fraction</span> <span class="o">*</span> <span class="n">num_simulations</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Running SASS with </span><span class="si">%s</span><span class="s2"> pilot samples.&quot;</span><span class="p">,</span> <span class="n">num_pilot_simulations</span>
        <span class="p">)</span>
        <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_sass_set_xo</span><span class="p">(</span>
            <span class="n">num_particles</span><span class="p">,</span>
            <span class="n">num_pilot_simulations</span><span class="p">,</span>
            <span class="n">x_o</span><span class="p">,</span>
            <span class="n">num_iid_samples</span><span class="p">,</span>
            <span class="n">lra</span><span class="p">,</span>
            <span class="n">sass_expansion_degree</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Udpate simulator and xo</span>
        <span class="n">x_o</span> <span class="o">=</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">sass_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">sass_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">sass_simulator</span>

    <span class="c1"># run initial population</span>
    <span class="n">particles</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_initial_pop</span><span class="p">,</span> <span class="n">num_iid_samples</span>
    <span class="p">)</span>
    <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">((</span>
        <span class="s2">&quot;population=</span><span class="si">%s</span><span class="s2">, eps=</span><span class="si">%s</span><span class="s2">, ess=</span><span class="si">%s</span><span class="s2">, num_sims=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">pop_idx</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">,</span>
        <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">num_initial_pop</span><span class="p">,</span>
    <span class="p">))</span>

    <span class="n">all_particles</span> <span class="o">=</span> <span class="p">[</span><span class="n">particles</span><span class="p">]</span>
    <span class="n">all_log_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_weights</span><span class="p">]</span>
    <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">distances</span><span class="p">]</span>
    <span class="n">all_epsilons</span> <span class="o">=</span> <span class="p">[</span><span class="n">epsilon</span><span class="p">]</span>
    <span class="n">all_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

    <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span><span class="p">:</span>
        <span class="n">pop_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Decay based on quantile of distances from previous pop.</span>
        <span class="k">if</span> <span class="n">distance_based_decay</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_epsilon</span><span class="p">(</span>
                <span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">epsilon_decay</span>
            <span class="p">)</span>
        <span class="c1"># Constant decay.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">*=</span> <span class="n">epsilon_decay</span>

        <span class="c1"># Get kernel variance from previous pop.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_kernel_variance</span><span class="p">(</span>
            <span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="n">kernel_variance_scale</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_next_population</span><span class="p">(</span>
            <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="n">pop_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">num_iid_samples</span><span class="o">=</span><span class="n">num_iid_samples</span><span class="p">,</span>
            <span class="n">use_last_pop_samples</span><span class="o">=</span><span class="n">use_last_pop_samples</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Resample population if effective sampling size is too small.</span>
        <span class="k">if</span> <span class="n">ess_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_if_ess_too_small</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">((</span>
            <span class="s2">&quot;population=</span><span class="si">%s</span><span class="s2"> done: eps=</span><span class="si">{epsilon:.6f}</span><span class="s2">, num_sims=</span><span class="si">%s</span><span class="s2">.&quot;</span><span class="p">,</span>
            <span class="n">pop_idx</span><span class="p">,</span>
            <span class="n">epsilon</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span><span class="p">,</span>
        <span class="p">))</span>

        <span class="c1"># collect results</span>
        <span class="n">all_particles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">particles</span><span class="p">)</span>
        <span class="n">all_log_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
        <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">all_epsilons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="n">all_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Maybe run LRA and adjust weights.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running Linear regression adjustment.&quot;</span><span class="p">)</span>
        <span class="n">adjusted_particles</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra_update_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">observation</span><span class="o">=</span><span class="n">process_x</span><span class="p">(</span><span class="n">x_o</span><span class="p">),</span>
            <span class="n">log_weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">lra_with_weights</span><span class="o">=</span><span class="n">lra_with_weights</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">final_particles</span> <span class="o">=</span> <span class="n">adjusted_particles</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">final_particles</span> <span class="o">=</span> <span class="n">all_particles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">kde</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;KDE on %s samples with bandwidth option %s. Beware that KDE can give</span>
<span class="sd">            unreliable results when used with too few samples and in high</span>
<span class="sd">            dimensions.&quot;&quot;&quot;</span><span class="p">,</span>
            <span class="n">final_particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">kde_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bandwidth&quot;</span><span class="p">,</span> <span class="s2">&quot;cv&quot;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># Maybe get particles weights from last population for weighted KDE.</span>
        <span class="k">if</span> <span class="n">kde_sample_weights</span><span class="p">:</span>
            <span class="n">kde_kwargs</span><span class="p">[</span><span class="s2">&quot;sample_weights&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_log_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>

        <span class="n">kde_dist</span> <span class="o">=</span> <span class="n">get_kde</span><span class="p">(</span><span class="n">final_particles</span><span class="p">,</span> <span class="o">**</span><span class="n">kde_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">kde_dist</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">(</span>
                    <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                    <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                    <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                    <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">kde_dist</span>

    <span class="k">if</span> <span class="n">return_summary</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">final_particles</span><span class="p">,</span>
            <span class="nb">dict</span><span class="p">(</span>
                <span class="n">particles</span><span class="o">=</span><span class="n">all_particles</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">all_log_weights</span><span class="p">,</span>
                <span class="n">epsilons</span><span class="o">=</span><span class="n">all_epsilons</span><span class="p">,</span>
                <span class="n">distances</span><span class="o">=</span><span class="n">all_distances</span><span class="p">,</span>
                <span class="n">xs</span><span class="o">=</span><span class="n">all_x</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_particles</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.__init__" class="doc doc-heading">
            <code class=" language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">requires_iid_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distance_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">distance_batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">show_progress_bars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">algorithm_variant</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Sequential Monte Carlo Approximate Bayesian Computation.</p>


<details class="we-distinguish-between-three-different-smc-methods-here" open>
  <summary>We distinguish between three different SMC methods here</summary>
  <ul>
<li>A: Toni et al. 2010 (Phd Thesis)</li>
<li>B: Sisson et al. 2007 (with correction from 2009)</li>
<li>C: Beaumont et al. 2009</li>
</ul>
</details>        <p>In Toni et al. 2010 we find an overview of the differences on page 34:
    - B: same as A except for resampling of weights if the effective sampling
        size is too small.
    - C: same as A except for calculation of the covariance of the perturbation
        kernel: the kernel covariance is a scaled version of the covariance of
        the previous population.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>simulator</code>
            </td>
            <td>
                  <code><span title="typing.Callable">Callable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\mathrm{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="torch.distributions.Distribution">Distribution</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A probability distribution that expresses prior knowledge about the
parameters, e.g. which ranges are meaningful for them. Any
object with <code>.log_prob()</code>and <code>.sample()</code> (for example, a PyTorch
distribution) can be used.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.Callable">Callable</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Distance function to compare observed and simulated data. Can be
a custom callable function or one of <code>l1</code>, <code>l2</code>, <code>mse</code>,
<code>mmd</code>, <code>wasserstein</code>.</p>
              </div>
            </td>
            <td>
                  <code>&#39;l2&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>requires_iid_data</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[None]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to allow conditioning on iid sampled data or not.
Typically, this information is inferred by the choice of the distance,
but in case a custom distance is used, this information is pivotal.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Configurations parameters for the distances. In particular
useful for the MMD and Wasserstein distance.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_workers</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of parallel workers to use for simulations.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>simulation_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of parameter sets that the simulator
maps to data x at once. If None, we simulate all parameter sets at the
same time. If &gt;= 1, the simulator has to process data of shape
(simulation_batch_size, parameter_dimension).</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of simulations that the distance function
evaluates against the reference observations at once. If -1, we evaluate
all simulations at the same time.</p>
              </div>
            </td>
            <td>
                  <code>-1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bars</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progressbar during simulation and
sampling.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Perturbation kernel.</p>
              </div>
            </td>
            <td>
                  <code>&#39;gaussian&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>algorithm_variant</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Indicating the choice of algorithm variant, A, B, or C.</p>
              </div>
            </td>
            <td>
                  <code>&#39;C&#39;</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">distance</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span>
    <span class="n">requires_iid_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">distance_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">distance_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">show_progress_bars</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="n">algorithm_variant</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sequential Monte Carlo Approximate Bayesian Computation.</span>

<span class="sd">    We distinguish between three different SMC methods here:</span>
<span class="sd">        - A: Toni et al. 2010 (Phd Thesis)</span>
<span class="sd">        - B: Sisson et al. 2007 (with correction from 2009)</span>
<span class="sd">        - C: Beaumont et al. 2009</span>

<span class="sd">    In Toni et al. 2010 we find an overview of the differences on page 34:</span>
<span class="sd">        - B: same as A except for resampling of weights if the effective sampling</span>
<span class="sd">            size is too small.</span>
<span class="sd">        - C: same as A except for calculation of the covariance of the perturbation</span>
<span class="sd">            kernel: the kernel covariance is a scaled version of the covariance of</span>
<span class="sd">            the previous population.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\mathrm{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used.</span>
<span class="sd">        prior: A probability distribution that expresses prior knowledge about the</span>
<span class="sd">            parameters, e.g. which ranges are meaningful for them. Any</span>
<span class="sd">            object with `.log_prob()`and `.sample()` (for example, a PyTorch</span>
<span class="sd">            distribution) can be used.</span>
<span class="sd">        distance: Distance function to compare observed and simulated data. Can be</span>
<span class="sd">            a custom callable function or one of `l1`, `l2`, `mse`,</span>
<span class="sd">            `mmd`, `wasserstein`.</span>
<span class="sd">        requires_iid_data: Whether to allow conditioning on iid sampled data or not.</span>
<span class="sd">            Typically, this information is inferred by the choice of the distance,</span>
<span class="sd">            but in case a custom distance is used, this information is pivotal.</span>
<span class="sd">        distance_kwargs: Configurations parameters for the distances. In particular</span>
<span class="sd">            useful for the MMD and Wasserstein distance.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets that the simulator</span>
<span class="sd">            maps to data x at once. If None, we simulate all parameter sets at the</span>
<span class="sd">            same time. If &gt;= 1, the simulator has to process data of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension).</span>
<span class="sd">        distance_batch_size: Number of simulations that the distance function</span>
<span class="sd">            evaluates against the reference observations at once. If -1, we evaluate</span>
<span class="sd">            all simulations at the same time.</span>
<span class="sd">        show_progress_bars: Whether to show a progressbar during simulation and</span>
<span class="sd">            sampling.</span>
<span class="sd">        kernel: Perturbation kernel.</span>
<span class="sd">        algorithm_variant: Indicating the choice of algorithm variant, A, B, or C.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">requires_iid_data</span><span class="o">=</span><span class="n">requires_iid_data</span><span class="p">,</span>
        <span class="n">distance_kwargs</span><span class="o">=</span><span class="n">distance_kwargs</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">simulation_batch_size</span><span class="o">=</span><span class="n">simulation_batch_size</span><span class="p">,</span>
        <span class="n">distance_batch_size</span><span class="o">=</span><span class="n">distance_batch_size</span><span class="p">,</span>
        <span class="n">show_progress_bars</span><span class="o">=</span><span class="n">show_progress_bars</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">kernels</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">kernel</span> <span class="ow">in</span> <span class="n">kernels</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Kernel &#39;</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported. Choose one from </span><span class="si">{</span><span class="n">kernels</span><span class="si">}</span><span class="s2">.&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

    <span class="n">algorithm_variants</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="n">algorithm_variants</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;SMCABC variant &#39;</span><span class="si">{</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported, choose one from&quot;</span>
        <span class="s2">&quot; </span><span class="si">{algorithm_variants}</span><span class="s2">.&quot;</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">=</span> <span class="n">algorithm_variant</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">distance_to_x0</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Define simulator that keeps track of budget.</span>
    <span class="k">def</span> <span class="nf">simulate_with_budget</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simulation_counter</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batched_simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_simulate_with_budget</span> <span class="o">=</span> <span class="n">simulate_with_budget</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.get_kernel_variance" class="doc doc-heading">
            <code class=" language-python"><span class="n">get_kernel_variance</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">kernel_variance_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.get_kernel_variance" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return kernel variance for a given population of particles and weights.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_kernel_variance</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">kernel_variance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return kernel variance for a given population of particles and weights.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
        <span class="c1"># For variant C, Beaumont et al. 2009, the kernel variance comes from the</span>
        <span class="c1"># previous population.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="o">==</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span>
            <span class="c1"># Calculate weighted covariance of particles.</span>
            <span class="n">population_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">aweights</span><span class="o">=</span><span class="n">weights</span><span class="p">)),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Make sure variance is nonsingular.</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">population_cov</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;&quot;Singular particle covariance, using unit covariance.&quot;&quot;&quot;</span>
                <span class="p">)</span>
                <span class="n">population_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">population_cov</span>
        <span class="c1"># While for Toni et al. and Sisson et al. it comes from the parameter</span>
        <span class="c1"># ranges.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">):</span>
            <span class="n">particle_ranges</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_particle_ranges</span><span class="p">(</span>
                <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="n">samples_per_dim</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">particle_ranges</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variant, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">algorithm_variant</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
        <span class="c1"># Variance spans the range of parameters for every dimension.</span>
        <span class="k">return</span> <span class="n">kernel_variance_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_particle_ranges</span><span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="n">samples_per_dim</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.get_new_kernel" class="doc doc-heading">
            <code class=" language-python"><span class="n">get_new_kernel</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.get_new_kernel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return new kernel distribution for a given set of paramters.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thetas</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Distribution</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return new kernel distribution for a given set of paramters.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;get kernel variance first.&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
        <span class="n">low</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">thetas</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_variance</span>
        <span class="c1"># Move batch shape to event shape to get Uniform that is multivariate in</span>
        <span class="c1"># parameter dimension.</span>
        <span class="k">return</span> <span class="n">BoxUniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel, &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="si">}</span><span class="s2">&#39; not supported.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.get_particle_ranges" class="doc doc-heading">
            <code class=" language-python"><span class="n">get_particle_ranges</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.get_particle_ranges" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return range of particles in each parameter dimension.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_particle_ranges</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">samples_per_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return range of particles in each parameter dimension.&quot;&quot;&quot;</span>

    <span class="c1"># get weighted samples</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
        <span class="n">particles</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">samples_per_dim</span> <span class="o">*</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="c1"># Variance spans the range of particles for every dimension.</span>
    <span class="n">particle_ranges</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="k">assert</span> <span class="n">particle_ranges</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">particle_ranges</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small" class="doc doc-heading">
            <code class=" language-python"><span class="n">resample_if_ess_too_small</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">ess_min</span><span class="p">,</span> <span class="n">pop_idx</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.resample_if_ess_too_small" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return resampled particles and uniform weights if effectice sampling size is
too small.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">resample_if_ess_too_small</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">ess_min</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">pop_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return resampled particles and uniform weights if effectice sampling size is</span>
<span class="sd">    too small.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_particles</span> <span class="o">=</span> <span class="n">particles</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">log_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_particles</span>
    <span class="c1"># Resampling of weights for low ESS only for Sisson et al. 2007.</span>
    <span class="k">if</span> <span class="n">ess</span> <span class="o">&lt;</span> <span class="n">ess_min</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;ESS=</span><span class="si">%s</span><span class="s2"> too low, resampling pop </span><span class="si">%s</span><span class="s2">...&quot;</span><span class="p">,</span> <span class="n">ess</span><span class="p">,</span> <span class="n">pop_idx</span><span class="p">)</span>
        <span class="c1"># First resample, then set to uniform weights as in Sisson et al. 2007.</span>
        <span class="n">particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_population_with_weights</span><span class="p">(</span>
            <span class="n">particles</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_particles</span>
        <span class="p">)</span>
        <span class="n">log_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_particles</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_particles</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">particles</span><span class="p">,</span> <span class="n">log_weights</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.run_lra_update_weights" class="doc doc-heading">
            <code class=" language-python"><span class="n">run_lra_update_weights</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">observation</span><span class="p">,</span> <span class="n">log_weights</span><span class="p">,</span> <span class="n">lra_with_weights</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.run_lra_update_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return particles and weights adjusted with LRA.</p>
<p>Runs (weighted) linear regression from xs onto particles to adjust the
particles.</p>
<p>Updates the SMC weights according to the new particles.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">run_lra_update_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">observation</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">log_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">lra_with_weights</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return particles and weights adjusted with LRA.</span>

<span class="sd">    Runs (weighted) linear regression from xs onto particles to adjust the</span>
<span class="sd">    particles.</span>

<span class="sd">    Updates the SMC weights according to the new particles.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">adjusted_particels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="n">xs</span><span class="p">,</span>
        <span class="n">observation</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">log_weights</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="k">if</span> <span class="n">lra_with_weights</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Update SMC weights with LRA adjusted weights</span>
    <span class="n">adjusted_log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_new_log_weights</span><span class="p">(</span>
        <span class="n">new_particles</span><span class="o">=</span><span class="n">adjusted_particels</span><span class="p">,</span>
        <span class="n">old_particles</span><span class="o">=</span><span class="n">particles</span><span class="p">,</span>
        <span class="n">old_log_weights</span><span class="o">=</span><span class="n">log_weights</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">adjusted_particels</span><span class="p">,</span> <span class="n">adjusted_log_weights</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.run_sass_set_xo" class="doc doc-heading">
            <code class=" language-python"><span class="n">run_sass_set_xo</span><span class="p">(</span><span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">x_o</span><span class="p">,</span> <span class="n">num_iid_samples</span><span class="p">,</span> <span class="n">lra</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sass_expansion_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#sbi.inference.abc.smcabc.SMCABC.run_sass_set_xo" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return transform for semi-automatic summary statistics.</p>
<p>Runs an single round of rejection abc with fixed budget and accepts
num_particles simulations to run the regression for sass.</p>
<p>Sets self.x_o once the x_shape can be derived from simulations.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">run_sass_set_xo</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_particles</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_pilot_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">x_o</span><span class="p">,</span>
    <span class="n">num_iid_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">lra</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sass_expansion_degree</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return transform for semi-automatic summary statistics.</span>

<span class="sd">    Runs an single round of rejection abc with fixed budget and accepts</span>
<span class="sd">    num_particles simulations to run the regression for sass.</span>

<span class="sd">    Sets self.x_o once the x_shape can be derived from simulations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="p">(</span>
        <span class="n">pilot_particles</span><span class="p">,</span>
        <span class="n">_</span><span class="p">,</span>
        <span class="n">_</span><span class="p">,</span>
        <span class="n">pilot_xs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_xo_and_sample_initial_population</span><span class="p">(</span>
        <span class="n">x_o</span><span class="p">,</span> <span class="n">num_particles</span><span class="p">,</span> <span class="n">num_pilot_simulations</span><span class="p">,</span> <span class="n">num_iid_samples</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;x_o not set yet.&quot;</span>

    <span class="c1"># Adjust with LRA.</span>
    <span class="k">if</span> <span class="n">lra</span><span class="p">:</span>
        <span class="n">pilot_particles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_lra</span><span class="p">(</span><span class="n">pilot_particles</span><span class="p">,</span> <span class="n">pilot_xs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_o</span><span class="p">)</span>
    <span class="n">sass_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sass_transform</span><span class="p">(</span>
        <span class="n">pilot_particles</span><span class="p">,</span>
        <span class="n">pilot_xs</span><span class="p">,</span>
        <span class="n">expansion_degree</span><span class="o">=</span><span class="n">sass_expansion_degree</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">sass_transform</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights" class="doc doc-heading">
            <code class=" language-python"><span class="n">sample_from_population_with_weights</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sbi.inference.abc.smcabc.SMCABC.sample_from_population_with_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return samples from particles sampled with weights.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/inference/abc/smcabc.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">sample_from_population_with_weights</span><span class="p">(</span>
    <span class="n">particles</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return samples from particles sampled with weights.&quot;&quot;&quot;</span>

    <span class="c1"># define multinomial with weights as probs</span>
    <span class="n">multi</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="c1"># sample num samples, with replacement</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">multi</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,)))</span>
    <span class="c1"># get indices of success trials</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># return those indices from trace</span>
    <span class="k">return</span> <span class="n">particles</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="helpers">Helpers<a class="headerlink" href="#helpers" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-function">


<h2 id="sbi.inference.trainers.base.simulate_for_sbi" class="doc doc-heading">
            <code class=" language-python"><span class="n">simulate_for_sbi</span><span class="p">(</span><span class="n">simulator</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#sbi.inference.trainers.base.simulate_for_sbi" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">

        <p>Returns (<span class="arithmatex">\(\theta, x\)</span>) pairs obtained from sampling the proposal and simulating.</p>
<p>This function performs two steps:</p>
<ul>
<li>Sample parameters <span class="arithmatex">\(\theta\)</span> from the <code>proposal</code>.</li>
<li>Simulate these parameters to obtain <span class="arithmatex">\(x\)</span>.</li>
</ul>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>simulator</code>
            </td>
            <td>
                  <code><span title="typing.Callable">Callable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A function that takes parameters <span class="arithmatex">\(\theta\)</span> and maps them to
simulations, or observations, <code>x</code>, <span class="arithmatex">\(\text{sim}(\theta)\to x\)</span>. Any
regular Python callable (i.e. function or class with <code>__call__</code> method)
can be used. Note that the simulator should be able to handle numpy
arrays for efficient parallelization. You can use
<code>process_simulator</code> to ensure this.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proposal</code>
            </td>
            <td>
                  <code><span title="typing.Any">Any</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Probability distribution that the parameters <span class="arithmatex">\(\theta\)</span> are sampled
from.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_simulations</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of simulations that are run.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_workers</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of parallel workers to use for simulations.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>simulation_batch_size</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[int, None]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of parameter sets of shape
(simulation_batch_size, parameter_dimension) that the simulator
receives per call. If None, we set
simulation_batch_size=num_simulations and simulate all parameter
sets with one call. Otherwise, we construct batches of parameter
sets and distribute them among num_workers.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>seed</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Seed for reproducibility.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>show_progress_bar</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to show a progress bar for simulating. This will not
affect whether there will be a progressbar while drawing samples from the
proposal.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns: Sampled parameters <span class="arithmatex">\(\theta\)</span> and simulation-outputs <span class="arithmatex">\(x\)</span>.</p>

            <details class="quote">
              <summary>Source code in <code>sbi/utils/simulation_utils.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">simulate_for_sbi</span><span class="p">(</span>
    <span class="n">simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">proposal</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">num_simulations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">simulation_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns ($\theta, x$) pairs obtained from sampling the proposal and simulating.</span>

<span class="sd">    This function performs two steps:</span>

<span class="sd">    - Sample parameters $\theta$ from the `proposal`.</span>
<span class="sd">    - Simulate these parameters to obtain $x$.</span>

<span class="sd">    Args:</span>
<span class="sd">        simulator: A function that takes parameters $\theta$ and maps them to</span>
<span class="sd">            simulations, or observations, `x`, $\text{sim}(\theta)\to x$. Any</span>
<span class="sd">            regular Python callable (i.e. function or class with `__call__` method)</span>
<span class="sd">            can be used. Note that the simulator should be able to handle numpy</span>
<span class="sd">            arrays for efficient parallelization. You can use</span>
<span class="sd">            `process_simulator` to ensure this.</span>
<span class="sd">        proposal: Probability distribution that the parameters $\theta$ are sampled</span>
<span class="sd">            from.</span>
<span class="sd">        num_simulations: Number of simulations that are run.</span>
<span class="sd">        num_workers: Number of parallel workers to use for simulations.</span>
<span class="sd">        simulation_batch_size: Number of parameter sets of shape</span>
<span class="sd">            (simulation_batch_size, parameter_dimension) that the simulator</span>
<span class="sd">            receives per call. If None, we set</span>
<span class="sd">            simulation_batch_size=num_simulations and simulate all parameter</span>
<span class="sd">            sets with one call. Otherwise, we construct batches of parameter</span>
<span class="sd">            sets and distribute them among num_workers.</span>
<span class="sd">        seed: Seed for reproducibility.</span>
<span class="sd">        show_progress_bar: Whether to show a progress bar for simulating. This will not</span>
<span class="sd">            affect whether there will be a progressbar while drawing samples from the</span>
<span class="sd">            proposal.</span>

<span class="sd">    Returns: Sampled parameters $\theta$ and simulation-outputs $x$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">num_simulations</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Cast theta to numpy for better joblib performance (seee #1175)</span>
        <span class="n">seed_all_backends</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">proposal</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_simulations</span><span class="p">,))</span>

        <span class="c1"># Parse the simulation_batch_size logic</span>
        <span class="k">if</span> <span class="n">simulation_batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">simulation_batch_size</span> <span class="o">=</span> <span class="n">num_simulations</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">simulation_batch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">simulation_batch_size</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_workers</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># For multiprocessing, we want to switch to numpy arrays.</span>
            <span class="c1"># The batch size will be an approximation, since np.array_split does</span>
            <span class="c1"># not take as argument the size of the batch but their total.</span>
            <span class="n">num_batches</span> <span class="o">=</span> <span class="n">num_simulations</span> <span class="o">//</span> <span class="n">simulation_batch_size</span>
            <span class="n">batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">num_batches</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">batch_seeds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">),))</span>

            <span class="c1"># define seeded simulator.</span>
            <span class="k">def</span> <span class="nf">simulator_seeded</span><span class="p">(</span><span class="n">theta</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
                <span class="n">seed_all_backends</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">simulator</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

            <span class="k">try</span><span class="p">:</span>  <span class="c1"># catch TypeError to give more informative error message</span>
                <span class="n">simulation_outputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>  <span class="c1"># pyright: ignore</span>
                    <span class="n">xx</span>
                    <span class="k">for</span> <span class="n">xx</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                        <span class="n">Parallel</span><span class="p">(</span><span class="n">return_as</span><span class="o">=</span><span class="s2">&quot;generator&quot;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)(</span>
                            <span class="n">delayed</span><span class="p">(</span><span class="n">simulator_seeded</span><span class="p">)(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batches</span><span class="p">,</span> <span class="n">batch_seeds</span><span class="p">)</span>
                        <span class="p">),</span>
                        <span class="n">total</span><span class="o">=</span><span class="n">num_simulations</span><span class="p">,</span>
                        <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bar</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">]</span>
            <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;For multiprocessing, we switch to numpy arrays. Make sure to &quot;</span>
                    <span class="s2">&quot;preprocess your simulator with `process_simulator` to handle numpy&quot;</span>
                    <span class="s2">&quot; arrays.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">err</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">simulation_outputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">simulation_batch_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">batches</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">show_progress_bar</span><span class="p">):</span>
                <span class="n">simulation_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">simulator</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>

        <span class="c1"># Correctly format the output</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">simulation_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="sbi.utils.user_input_checks.process_prior" class="doc doc-heading">
            <code class=" language-python"><span class="n">process_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">custom_prior_wrapper_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sbi.utils.user_input_checks.process_prior" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">

        <p>Return PyTorch distribution-like prior from user-provided prior.</p>
<p>NOTE: If the prior argument is a sequence of PyTorch distributions, they will be
interpreted as independent prior dimensions wrapped in a <code>MultipleIndependent</code>
pytorch Distribution. In case the elements are not PyTorch distributions, make sure
to use process_prior on each element in the list beforehand. See FAQ 7 for details.</p>
<p>NOTE: returns a tuple (processed_prior, num_params, whether_prior_returns_numpy).
The last two entries in the tuple can be passed on to <code>process_simulator</code> to prepare
the simulator as well. For example, it will take care of casting parameters to numpy
or adding a batch dimension to the simulator output, if needed.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.Sequence">Sequence</span>[<span title="torch.distributions.Distribution">Distribution</span>], <span title="torch.distributions.Distribution">Distribution</span>, <span title="scipy.stats._distn_infrastructure.rv_frozen">rv_frozen</span>, <span title="scipy.stats._multivariate.multi_rv_frozen">multi_rv_frozen</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior object with <code>.sample()</code> and <code>.log_prob()</code> as provided by the user,
or a sequence of such objects.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>custom_prior_wrapper_kwargs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>kwargs to be passed to the class that wraps a
custom prior into a pytorch Distribution, e.g., for passing bounds for a
prior with bounded support (lower_bound, upper_bound), or argument
constraints.
(arg_constraints), see pytorch.distributions.Distribution for more info.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>AttributeError</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If prior objects lacks <code>.sample()</code> or <code>.log_prob()</code>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>prior</code></td>            <td>
                  <code><span title="torch.distributions.Distribution">Distribution</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Prior that emits samples and evaluates log prob as PyTorch Tensors.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>theta_numel</code></td>            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of parameters - elements in a single sample from the prior.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>prior_returns_numpy</code></td>            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether the return type of the prior was a Numpy array.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/utils/user_input_checks.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">process_prior</span><span class="p">(</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Distribution</span><span class="p">],</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">rv_frozen</span><span class="p">,</span> <span class="n">multi_rv_frozen</span><span class="p">],</span>
    <span class="n">custom_prior_wrapper_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Distribution</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return PyTorch distribution-like prior from user-provided prior.</span>

<span class="sd">    NOTE: If the prior argument is a sequence of PyTorch distributions, they will be</span>
<span class="sd">    interpreted as independent prior dimensions wrapped in a `MultipleIndependent`</span>
<span class="sd">    pytorch Distribution. In case the elements are not PyTorch distributions, make sure</span>
<span class="sd">    to use process_prior on each element in the list beforehand. See FAQ 7 for details.</span>

<span class="sd">    NOTE: returns a tuple (processed_prior, num_params, whether_prior_returns_numpy).</span>
<span class="sd">    The last two entries in the tuple can be passed on to `process_simulator` to prepare</span>
<span class="sd">    the simulator as well. For example, it will take care of casting parameters to numpy</span>
<span class="sd">    or adding a batch dimension to the simulator output, if needed.</span>

<span class="sd">    Args:</span>
<span class="sd">        prior: Prior object with `.sample()` and `.log_prob()` as provided by the user,</span>
<span class="sd">            or a sequence of such objects.</span>
<span class="sd">        custom_prior_wrapper_kwargs: kwargs to be passed to the class that wraps a</span>
<span class="sd">            custom prior into a pytorch Distribution, e.g., for passing bounds for a</span>
<span class="sd">            prior with bounded support (lower_bound, upper_bound), or argument</span>
<span class="sd">            constraints.</span>
<span class="sd">            (arg_constraints), see pytorch.distributions.Distribution for more info.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If prior objects lacks `.sample()` or `.log_prob()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        prior: Prior that emits samples and evaluates log prob as PyTorch Tensors.</span>
<span class="sd">        theta_numel: Number of parameters - elements in a single sample from the prior.</span>
<span class="sd">        prior_returns_numpy: Whether the return type of the prior was a Numpy array.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># If prior is a sequence, assume independent components and check as PyTorch prior.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Prior was provided as a sequence of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span><span class="si">}</span><span class="s2"> priors. They will be &quot;</span>
            <span class="s2">&quot;interpreted as independent of each other and matched in order to the &quot;</span>
            <span class="s2">&quot;components of the parameter.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># process individual priors</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="p">[</span><span class="n">process_prior</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">custom_prior_wrapper_kwargs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prior</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">process_pytorch_prior</span><span class="p">(</span><span class="n">MultipleIndependent</span><span class="p">(</span><span class="n">prior</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">Distribution</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">process_pytorch_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="c1"># If prior is given as `scipy.stats` object, wrap as PyTorch.</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="p">(</span><span class="n">rv_frozen</span><span class="p">,</span> <span class="n">multi_rv_frozen</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Passing a prior as scipy.stats object is deprecated. &quot;</span>
            <span class="s2">&quot;Please pass it as a PyTorch Distribution.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Otherwise it is a custom prior - check for `.sample()` and `.log_prob()`.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">process_custom_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">custom_prior_wrapper_kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="sbi.utils.user_input_checks.process_simulator" class="doc doc-heading">
            <code class=" language-python"><span class="n">process_simulator</span><span class="p">(</span><span class="n">user_simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">is_numpy_simulator</span><span class="p">)</span></code>

<a href="#sbi.utils.user_input_checks.process_simulator" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">

        <p>Returns a simulator that meets the requirements for usage in sbi.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>user_simulator</code>
            </td>
            <td>
                  <code><span title="typing.Callable">Callable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>simulator provided by the user, possibly written in numpy.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prior</code>
            </td>
            <td>
                  <code><span title="torch.distributions.Distribution">Distribution</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>prior as pytorch distribution or processed with <code>process_prior</code>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_numpy_simulator</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>whether the simulator needs theta in numpy types, returned
from <code>process_prior</code>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>simulator</code></td>            <td>
                  <code><span title="typing.Callable">Callable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>processed simulator that returns <code>torch.Tensor</code> can handle batches
of parameters.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>sbi/utils/user_input_checks.py</code></summary>
              <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">process_simulator</span><span class="p">(</span>
    <span class="n">user_simulator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="n">is_numpy_simulator</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a simulator that meets the requirements for usage in sbi.</span>

<span class="sd">    Args:</span>
<span class="sd">        user_simulator: simulator provided by the user, possibly written in numpy.</span>
<span class="sd">        prior: prior as pytorch distribution or processed with `process_prior`.</span>
<span class="sd">        is_numpy_simulator: whether the simulator needs theta in numpy types, returned</span>
<span class="sd">            from `process_prior`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        simulator: processed simulator that returns `torch.Tensor` can handle batches</span>
<span class="sd">            of parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">user_simulator</span><span class="p">,</span> <span class="n">Callable</span><span class="p">),</span> <span class="s2">&quot;Simulator must be a function.&quot;</span>

    <span class="n">joblib_simulator</span> <span class="o">=</span> <span class="n">wrap_as_joblib_efficient_simulator</span><span class="p">(</span>
        <span class="n">user_simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">is_numpy_simulator</span>
    <span class="p">)</span>

    <span class="n">batch_simulator</span> <span class="o">=</span> <span class="n">ensure_batched_simulator</span><span class="p">(</span><span class="n">joblib_simulator</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">batch_simulator</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/sbi-dev/sbi" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>