{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ef8ff7-d8e5-43d7-9795-92e39069c9b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# More flexibility over the training loop and samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d329e-b30e-43c9-a497-d088d3ce57d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note, you can find the original version of this notebook at [tutorials/18_training_interface.ipynb](https://github.com/sbi-dev/sbi/blob/main/tutorials/18_training_interface.ipynb) in the `sbi` repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3e07e-838a-46e2-9ed6-02bfb27a4ede",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the previous tutorials, we showed how `sbi` can be used to train neural networks and sample from the posterior. If you are an `sbi` power-user, then you might want more control over individual stages of this process. For example, you might want to write a custom training loop or more flexibility over the samplers that are used. In this tutorial, we will explain how you can achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce293ed-5d23-4440-b3fc-22c66bd079a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import ones, zeros, eye, float32, as_tensor, tensor\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.analysis import pairplot\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dffec5-843a-48fc-91e8-2feb1f22a0e6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As in the previous tutorials, we first define the prior and simulator and use them to generate simulated data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19122e88-2a8d-4e09-8c4e-061a087d175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = BoxUniform(-3 * ones((2,)), 3 * ones((2,)))\n",
    "\n",
    "def simulator(theta):\n",
    "    return theta + torch.randn_like(theta) * 0.1\n",
    "\n",
    "theta = prior.sample((1000,))\n",
    "x = simulator(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877610b-c76c-4653-abba-355c387f8e06",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below, we will first describe how you can run `Neural Posterior Estimation (NPE)`. We will attach code snippets for `Neural Likelihood Estimation (NLE)` and `Neural Ratio Estimation (NRE)` at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f85c4d-fa93-44dd-85ca-24787b4cfbfc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Neural Posterior Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d207b-fd00-4c8c-ab9b-5eb9f811c21a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, we have to decide on what `DensityEstimator` to use. In this tutorial, we will use a `Neural Spline Flow` (NSF) taken from the [`nflows`](https://github.com/bayesiains/nflows) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b520f28-5f30-444a-92b7-3407f70860ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.neural_nets.net_builders import build_nsf\n",
    "\n",
    "density_estimator = build_nsf(theta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208249e-d606-4bf4-b1f5-22cc777efef1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Every `density_estimator` in `sbi` implements at least two methods: `.sample()` and `.loss()`. Their input and output shapes are:\n",
    "\n",
    "\n",
    "**`density_estimator.loss(input, condition)`:**\n",
    "```\n",
    "Args:\n",
    "    input: `(batch_dim, *event_shape_input)`\n",
    "    condition: `(batch_dim, *event_shape_condition)`\n",
    "\n",
    "Returns:\n",
    "    Loss of shape `(batch_dim,)`\n",
    "```\n",
    "\n",
    "**`density_estimator.sample(sample_shape, condition)`:**\n",
    "```\n",
    "Args:\n",
    "    sample_shape: Tuple of ints which indicates the desired number of samples.\n",
    "    condition: `(batch_dim, *event_shape_condition)`\n",
    "\n",
    "Returns:\n",
    "    Samples of shape `(sample_shape, batch_dim, *event_shape_input)`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd6333-cee6-4b28-90ce-693ff4e2931a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some `DensityEstimator`s, such as Normalizing flows, also allow to evaluate the `log probability`. In those cases, the `DensityEstimator` also has the following method:\n",
    "\n",
    "**`density_estimator.log_prob(input, condition)`:**\n",
    "```\n",
    "Args:\n",
    "    input: `(sample_dim, batch_dim, *event_shape_input)`\n",
    "    condition: `(batch_dim, *event_shape_condition)`\n",
    "\n",
    "Returns:\n",
    "    Loss of shape `(sample_dim, batch_dim,)`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a0c77-2407-4c90-a708-afd826f5d1da",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training the density estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f82c9-6ba8-4270-bda1-06c47f87f0cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now write our own custom training loop to train the above-generated `DensityEstimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27188bb7-4a3f-433f-9d68-04c18fbe6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(list(density_estimator.parameters()), lr=5e-4)\n",
    "\n",
    "for _ in range(100):\n",
    "    opt.zero_grad()\n",
    "    losses = density_estimator.loss(theta, condition=x)\n",
    "    loss = torch.mean(losses)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30babe16-8ad6-48e0-b0a1-600f8b88a6cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Given this trained `density_estimator`, we can already generate samples from the posterior given observations (but we have to adhere to the shape specifications of the `DensityEstimator` explained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca964f37-b7aa-4570-80b3-64667b0a30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o = torch.as_tensor([[1.0, 1.0]])\n",
    "print(f\"Shape of x_o: {x_o.shape}            # Must have a batch dimension\")\n",
    "\n",
    "samples = density_estimator.sample((1000,), condition=x_o).detach()\n",
    "print(f\"Shape of samples: {samples.shape}  # Samples are returned with a batch dimension.\")\n",
    "\n",
    "samples = samples.squeeze(dim=1)\n",
    "print(f\"Shape of samples: {samples.shape}     # Removed batch dimension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37d472-4b6b-43f2-8fef-2ad967544a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pairplot(samples, limits=[[-3, 3], [-3, 3]], figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cab68-4196-47ee-a4c2-3ed2cd8a454b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Wrapping as a `DirectPosterior`\n",
    "\n",
    "You can also wrap the `DensityEstimator` as a `DirectPosterior`. The `DirectPosterior` is also returned by  `inference.build_posterior` and you have already learned how to use it in the [introduction tutorial](https://sbi-dev.github.io/sbi/dev/tutorials/00_getting_started/) and the [amortization tutotrial](https://sbi-dev.github.io/sbi/dev/tutorials/01_gaussian_amortized/). It adds the following functionality over the raw `DensityEstimator`:\n",
    "\n",
    "- automatically reject samples outside of the prior bounds  \n",
    "- compute the Maximum-a-posteriori (MAP) estimate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5e432-1399-4b26-8942-bb710a009651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference.posteriors import DirectPosterior\n",
    "\n",
    "posterior = DirectPosterior(density_estimator, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012870c0-7551-4895-9b3d-7cae3d103ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of x_o: {x_o.shape}\")\n",
    "samples = posterior.sample((1000,), x=x_o)\n",
    "print(f\"Shape of samples: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcbddf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note: For the `DirectPosterior`, the batch dimension is optional, i.e., it is possible to sample for multiple observations simultaneously. Use `.sample_batch` in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d496c-d4f0-43aa-96a3-103afa745416",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pairplot(samples, limits=[[-3, 3], [-3, 3]], figsize=(3, 3), upper=\"contour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c92a48-d6ff-4e06-a842-47c0e07de217",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Custom Data Loaders\n",
    "\n",
    "One helpful advantage of having access to the training loop is that you can now use your own DataLoaders during training of the density estimator. In this fashion, larger datasets can be used as input to `sbi` where `x` is potentially an image or something else. While this will require [embedding the input data](https://sbi-dev.github.io/sbi/latest/tutorials/04_embedding_networks/), a more fine grained control over loading the data is possible and allows to manage the memory requirement during training.\n",
    "\n",
    "First, we build a Dataset that complies with the `torch.util.data.Dataset` API. Note, the class below is meant for illustration purposes. In practice, this class can also read the data from disk etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe6e31-e682-419f-b88f-23b1e833eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPEData(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, nsamples:int,\n",
    "                 prior: torch.distributions.Distribution ,\n",
    "                 simulator: Callable,\n",
    "                 seed:int = 44):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.random.manual_seed(seed) #will set the seed device wide\n",
    "        self.prior = prior\n",
    "        self.simulator = simulator\n",
    "\n",
    "        self.theta = prior.sample((nsamples,))\n",
    "        self.x = simulator(self.theta)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.theta.shape[0]\n",
    "\n",
    "    def __getitem__(self, index:int):\n",
    "        return self.theta[index,...], self.x[index,...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8178235-291c-43e7-8376-98e42542d42d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now proceed to create a DataLoader and conduct our training loop as illustrated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2aa734-dfca-4691-b3b5-f5be889219ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NPEData(2048, prior, simulator)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759a702-6ce7-4e2c-8373-402fb30d7f49",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For sake of demonstration, let's create another estimator using a masked autoregressive flow (maf). For this, we create a second dataset and use only parts of the data to construct the maf estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61a11a-b9d6-4211-8456-cb4ff8755d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.neural_nets.net_builders import build_maf\n",
    "\n",
    "dummy_data = NPEData(64, prior, simulator, seed=43)\n",
    "dummy_loader = torch.utils.data.DataLoader(dummy_data, batch_size=4)\n",
    "dummy_theta, dummy_x = next(iter(dummy_loader))\n",
    "maf_estimator = build_maf(dummy_theta, dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6624f-ce79-43dd-b661-1cced75dca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "optw = AdamW(list(maf_estimator.parameters()), lr=5e-4)\n",
    "nepochs = 50\n",
    "\n",
    "for ep in range(nepochs):\n",
    "    for idx, (theta_batch, x_batch) in enumerate(train_loader):\n",
    "        optw.zero_grad()\n",
    "        losses = maf_estimator.loss(theta_batch, condition=x_batch)\n",
    "        loss = torch.mean(losses)\n",
    "        loss.backward()\n",
    "        optw.step()\n",
    "    if ep % 10 == 0:\n",
    "        print(\"last loss\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63ae5f-33e2-422b-9365-f183a53c09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare the trained estimator to the NSF from above\n",
    "samples = maf_estimator.sample((1000,), condition=x_o).detach()\n",
    "print(f\"Shape of samples: {samples.shape}  # Samples are returned with a batch dimension.\")\n",
    "\n",
    "samples = samples.squeeze(dim=1)\n",
    "print(f\"Shape of samples: {samples.shape}     # Removed batch dimension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd309901-76e1-4ebc-87e3-f1fa32161185",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pairplot(samples, limits=[[-3, 3], [-3, 3]], figsize=(3, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b9e60-f504-4837-9ce8-c5af63dd03bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Neural Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502a9df-dde9-4953-84ea-8058e05432fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The workflow for Neural Likelihood Estimation is very similar. Unlike for NPE, we have to sample with MCMC (or variational inference) though, so we will build an `MCMCPosterior` after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d401c-9410-4f39-9fd8-787a90a391cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.neural_nets.net_builders import build_nsf\n",
    "from sbi.inference.posteriors import MCMCPosterior\n",
    "from sbi.inference.potentials import likelihood_estimator_based_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb162e2-c4ed-4268-82f7-81ef79ab8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_estimator = build_nsf(x, theta)  # Note that the order of x and theta are reversed in comparison to NPE.\n",
    "\n",
    "# Training loop.\n",
    "opt = Adam(list(density_estimator.parameters()), lr=5e-4)\n",
    "for _ in range(100):\n",
    "    opt.zero_grad()\n",
    "    losses = density_estimator.loss(x, condition=theta)\n",
    "    loss = torch.mean(losses)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "# Build the posterior.\n",
    "potential, tf = likelihood_estimator_based_potential(density_estimator, prior, x_o)\n",
    "posterior = MCMCPosterior(\n",
    "    potential,\n",
    "    proposal=prior,\n",
    "    theta_transform=tf,\n",
    "    num_chains=50,\n",
    "    thin=1,\n",
    "    method=\"slice_np_vectorized\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb208afd-5ec0-4995-82ee-8611ade423e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample((1000,), x=x_o)\n",
    "_ = pairplot(samples, limits=[[-3, 3], [-3, 3]], figsize=(3, 3), upper=\"contour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb59c62-389e-4292-8cb5-1fbfc883076b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Neural Ratio Estimation\n",
    "\n",
    "Finally, for NRE, at this point, you have to implement the loss function yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078a8bb-c03f-4649-8035-632581f0b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.neural_nets.net_builders import build_resnet_classifier\n",
    "from sbi.inference.posteriors import MCMCPosterior\n",
    "from sbi.inference.potentials import ratio_estimator_based_potential\n",
    "from sbi import utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644654f-12e3-4655-a8fc-00f80c4dde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = build_resnet_classifier(x, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b13c5-e6f8-460f-80c4-bdec4b30e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(list(net.parameters()), lr=5e-4)\n",
    "\n",
    "\n",
    "def classifier_logits(net, theta, x, num_atoms):\n",
    "    batch_size = theta.shape[0]\n",
    "    repeated_x = utils.repeat_rows(x, num_atoms)\n",
    "    probs = ones(batch_size, batch_size) * (1 - eye(batch_size)) / (batch_size - 1)\n",
    "    choices = torch.multinomial(probs, num_samples=num_atoms - 1, replacement=False)\n",
    "    contrasting_theta = theta[choices]\n",
    "    atomic_theta = torch.cat((theta[:, None, :], contrasting_theta), dim=1).reshape(\n",
    "        batch_size * num_atoms, -1\n",
    "    )\n",
    "    return net(atomic_theta, repeated_x)\n",
    "\n",
    "\n",
    "num_atoms = 10\n",
    "for _ in range(300):\n",
    "    opt.zero_grad()\n",
    "    batch_size = theta.shape[0]\n",
    "    logits = classifier_logits(net, theta, x, num_atoms=num_atoms)\n",
    "    logits = logits.reshape(batch_size, num_atoms)\n",
    "    log_probs = logits[:, 0] - torch.logsumexp(logits, dim=-1)\n",
    "    loss = -torch.mean(log_probs)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d4cb3-b283-4b36-b259-5c8cc4b540c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential, tf = ratio_estimator_based_potential(net, prior, x_o)\n",
    "posterior = MCMCPosterior(\n",
    "    potential,\n",
    "    proposal=prior,\n",
    "    theta_transform=tf,\n",
    "    num_chains=100,\n",
    "    method=\"slice_np_vectorized\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b3de2-b630-4502-ace1-2a4b827451be",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample((1000,), x=x_o)\n",
    "_ = pairplot(samples, limits=[[-3, 3], [-3, 3]], figsize=(3, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "name": "18_training_interface.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": null,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
