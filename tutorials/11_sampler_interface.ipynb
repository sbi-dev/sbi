{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling algorithms in `sbi`\n",
    "\n",
    "Note: this tutorial requires that the user is already familiar with the [flexible interface](https://sbi-dev.github.io/sbi/tutorial/02_flexible_interface/).\n",
    "\n",
    "`sbi` implements three methods: SNPE, SNLE, and SNRE. When using SNPE, the trained neural network directly approximates the posterior. Thus, sampling from the posterior can be done by sampling from the trained neural network. The neural networks trained in SNLE and SNRE approximate the likelihood(-ratio). Thus, in order to draw samples from the posterior, one has to perform additional sampling steps, e.g. Markov-chain Monte-Carlo (MCMC). In `sbi`, the implemented samplers are:\n",
    "\n",
    "- Markov-chain Monte-Carlo (MCMC)\n",
    "\n",
    "- Rejection sampling\n",
    "\n",
    "- Variational inference (VI)\n",
    "\n",
    "Below, we will demonstrate how these samplers can be used in `sbi`. First, we train the neural network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from sbi.inference import SNLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining simulator, prior, and running inference\n",
    "\n",
    "Our simulator (model) accepts 2 parameters ($\\theta$) and outputs simulations of the same dimensionality, adding Gaussian noise to the parameter set. For each dimension of $\\theta$, we use a Gaussian _prior_ with a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 2\n",
    "prior = torch.distributions.MultivariateNormal(torch.zeros(num_dim), torch.eye(num_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference object. Here, SNLE is used.\n",
    "inference = SNLE(prior=prior, show_progress_bars=False)\n",
    "\n",
    "# generate simulations, pass to the inference object and train the likelihood estimator\n",
    "theta = prior.sample((1000,))\n",
    "x = theta + torch.randn((1000, num_dim))\n",
    "likelihood_estimator = inference.append_simulations(theta, x).train()\n",
    "\n",
    "# generate the first observation\n",
    "x_o = torch.randn((1, num_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the desired sampling technique, we provide its configuration to the `build_posterior()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9cbb3526ec457fadfd25ac301f0e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged with loss: 5.17\n",
      "Quality Score: -0.116 \t Good: Smaller than 0.5  Bad: Larger than 1.0 \t         NOTE: Less sensitive to mode collapse.\n"
     ]
    }
   ],
   "source": [
    "# sampling with MCMC\n",
    "sampling_algorithm = \"mcmc\"\n",
    "mcmc_method = \"slice_np\"  # alternatives: nuts,hmc\n",
    "posterior = inference.build_posterior(sample_with=sampling_algorithm,\n",
    "                                      mcmc_method=mcmc_method)\n",
    "\n",
    "# sampling with variational inference\n",
    "sampling_algorithm = \"vi\"\n",
    "vi_method = \"rKL\"  # alternative: fKL\n",
    "posterior = inference.build_posterior(sample_with=sampling_algorithm,\n",
    "                                      vi_method=vi_method)\n",
    "\n",
    "# unlike other methods, vi needs a training step for every observation.\n",
    "posterior = posterior.set_default_x(x_o).train()\n",
    "\n",
    "# sampling with rejection sampling\n",
    "sampling_algorithm = \"rejection\"\n",
    "posterior = inference.build_posterior(sample_with=sampling_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More flexibility in adjusting the sampler\n",
    "\n",
    "\n",
    "The above syntax enables straightforward experimentation with various sampling algorithms. For tailored sampling, you can modify sampler hyperparameters, such as the number of warm-up steps for MCMC, or develop a custom sampler from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main syntax (for SNLE and SNRE)\n",
    "\n",
    "As above, we begin by training the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for full flexibility in using the sampler, we do not use the `.build_posterior()` method. Instead, we explicitly define the potential function and the sampling algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import MCMCPosterior, likelihood_estimator_based_potential\n",
    "\n",
    "# define potential function and initizialize MCMC posterior object\n",
    "potential_fn, parameter_transform = likelihood_estimator_based_potential(\n",
    "    likelihood_estimator, prior, x_o\n",
    ")\n",
    "posterior = MCMCPosterior(\n",
    "    potential_fn, proposal=prior, theta_transform=parameter_transform, warmup_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _posterior_ is proportional to the product of likelihood and _prior_:                                               ð‘(ðœƒ|ð‘¥ð‘œ)âˆð‘(ð‘¥ð‘œ|ðœƒ)ð‘(ðœƒ). A _potential function_ is a function of the parameter ð‘“(ðœƒ) and defined as the logarithm of the right-hand side of this equation: ð‘“(ðœƒ)=log(ð‘(ð‘¥ð‘œ|ðœƒ)ð‘(ðœƒ))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use variational inference or rejection sampling, just replace the last line with `VIPosterior` or `RejectionPosterior`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b0cfdec1ba4afbbe5b42f7e4ff6d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged with loss: 5.18\n",
      "Quality Score: 0.104 \t Good: Smaller than 0.5  Bad: Larger than 1.0 \t         NOTE: Less sensitive to mode collapse.\n"
     ]
    }
   ],
   "source": [
    "from sbi.inference import RejectionPosterior, VIPosterior\n",
    "\n",
    "# define potential function and initizialize rejection posterior object\n",
    "posterior = VIPosterior(\n",
    "    potential_fn, prior=prior, theta_transform=parameter_transform\n",
    ").train()\n",
    "\n",
    "posterior = RejectionPosterior(\n",
    "    potential_fn, proposal=prior, theta_transform=parameter_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you could also plug the `potential_fn` into any sampler of your choice and not rely on any of the in-built `sbi`-samplers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further explanation\n",
    "\n",
    "The first lines are the same as for the flexible interface:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 59 epochs."
     ]
    }
   ],
   "source": [
    "# create inference object using SNLE, generate simulations and pass to the inference object \n",
    "inference = SNLE()\n",
    "likelihood_estimator = inference.append_simulations(theta, x).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'likelihood_estimator_based_potential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m potential_fn, parameter_transform \u001b[38;5;241m=\u001b[39m \u001b[43mlikelihood_estimator_based_potential\u001b[49m(\n\u001b[1;32m      2\u001b[0m     likelihood_estimator, prior, x_o\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'likelihood_estimator_based_potential' is not defined"
     ]
    }
   ],
   "source": [
    "# define potential function \n",
    "potential_fn, parameter_transform = likelihood_estimator_based_potential(\n",
    "    likelihood_estimator, prior, x_o\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `potential_fn`, you can evaluate the potential:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming that your parameters are 1D:\n",
    "potential = potential_fn(\n",
    "    torch.zeros(1, num_dim)\n",
    ")  # -> returns f(0) = log( p(x_o|0) p(0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other object that is returned by `likelihood_estimator_based_potential` is a `parameter_transform`. The `parameter_transform` is a [pytorch transform](https://github.com/pytorch/pytorch/blob/master/torch/distributions/transforms.py). The `parameter_transform` is a fixed transform that is can be applied to parameter `theta`. It transforms the parameters into unconstrained space (if the prior is bounded, e.g. `BoxUniform`), and standardizes the parameters (i.e. zero mean, one std). Using `parameter_transform` during sampling is optional, but it usually improves the performance of MCMC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# transform parameters into unconstrained space\n",
    "theta_tf = parameter_transform(torch.zeros(1, num_dim))\n",
    "theta_original = parameter_transform.inv(theta_tf)\n",
    "print(theta_original)  # -> tensor([[0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having obtained the `potential_fn`, we can sample from the posterior with MCMC or rejection sampling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = MCMCPosterior(\n",
    "    potential_fn, proposal=prior, theta_transform=parameter_transform\n",
    ")\n",
    "posterior = RejectionPosterior(potential_fn, proposal=prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main syntax for SNPE\n",
    "\n",
    "SNPE usually does not require MCMC or rejection sampling (if you still need it, you can use the same syntax as above with the `posterior_estimator_based_potential` function). Instead, SNPE samples from the neural network. If the support of the prior is bounded, some samples can lie outside of the support of the prior. The `DirectPosterior` class automatically rejects these samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 76 epochs."
     ]
    }
   ],
   "source": [
    "from sbi.inference import SNPE, DirectPosterior\n",
    "\n",
    "inference = SNPE()\n",
    "posterior_estimator = inference.append_simulations(theta, x).train()\n",
    "\n",
    "posterior = DirectPosterior(posterior_estimator, prior=prior)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
