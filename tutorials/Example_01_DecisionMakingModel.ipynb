{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBI for decision-making models\n",
    "\n",
    "In [a previous\n",
    "tutorial](https://sbi-dev.github.io/sbi/latest/tutorials/12_iid_data_and_permutation_invariant_embeddings.md),\n",
    "we showed how to use SBI with trial-based iid data. Such scenarios can arise,\n",
    "for example, in models of perceptual decision making. In addition to trial-based\n",
    "iid data points, these models often come with mixed data types and varying\n",
    "experimental conditions. Here, we show how `sbi` can be used to perform\n",
    "inference in such models with the `MNLE` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you find the original version of this notebook in the `sbi` repository under\n",
    "[tutorials/Example_01_DecisionMakingModel.ipynb](https://github.com/sbi-dev/sbi/blob/main/tutorials/Example_01_DecisionMakingModel.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial-based SBI with mixed data types\n",
    "\n",
    "In some cases, models with trial-based data additionally return data with mixed data types, e.g., continous and discrete data. For example, most computational models of decision-making have continuous reaction times and discrete choices as output.\n",
    "\n",
    "This can induce a problem when performing trial-based SBI that relies on learning a neural likelihood: It is challenging for most density estimators to handle both, continuous and discrete data at the same time.\n",
    "However, there is a recent SBI method for solving this problem, it's called **Mixed Neural Likelihood Estimation** (MNLE). It works just like NLE, but with mixed data types. The trick is that it learns two separate density estimators, one for the discrete part of the data, and one for the continuous part, and combines the two to obtain the final neural likelihood. Crucially, the continuous density estimator is trained conditioned on the output of the discrete one, such that statistical dependencies between the discrete and continuous data (e.g., between choices and reaction times) are modeled as well. The interested reader is referred to the original paper available [here](https://elifesciences.org/articles/77220).\n",
    "\n",
    "MNLE was recently added to `sbi` (see this [PR](https://github.com/mackelab/sbi/pull/638) and also [issue](https://github.com/mackelab/sbi/issues/845)) and follows the same API as `SNLE`.\n",
    "\n",
    "In this tutorial we will show how to apply `MNLE` to mixed data, and how to deal with varying experimental conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy problem for `MNLE`\n",
    "\n",
    "To illustrate `MNLE` we set up a toy simulator that outputs mixed data and for which we know the likelihood such we can obtain reference posterior samples via MCMC.\n",
    "\n",
    "**Simulator**: To simulate mixed data we do the following\n",
    "\n",
    "- Sample reaction time from `inverse Gamma`\n",
    "- Sample choices from `Binomial`\n",
    "- Return reaction time $rt \\in (0, \\infty)$ and choice index $c \\in \\{0, 1\\}$\n",
    "\n",
    "$$\n",
    "c \\sim \\text{Binomial}(\\rho) \\\\\n",
    "rt \\sim \\text{InverseGamma}(\\alpha=2, \\beta) \\\\\n",
    "$$\n",
    "\n",
    "**Prior**: The priors of the two parameters $\\rho$ and $\\beta$ are independent. We define a `Beta` prior over the probabilty parameter of the `Binomial` used in the simulator and a `Gamma` prior over the shape-parameter of the `inverse Gamma` used in the simulator:\n",
    "\n",
    "$$\n",
    "p(\\beta, \\rho) = p(\\beta) \\; p(\\rho) ; \\\\\n",
    "p(\\beta) = \\text{Gamma}(1, 0.5) \\\\\n",
    "p(\\text{probs}) = \\text{Beta}(2, 2)\n",
    "$$\n",
    "\n",
    "Because the `InverseGamma` and the `Binomial` likelihoods are well-defined we can perform MCMC on this problem and obtain reference-posterior samples.\n",
    "\n",
    "_Note: While the simulator we use in this tutorial only has a single continous and discrete dimension, MNLE also works with multi-dimensional continuous and discrete variables._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from example_01_utils import BinomialGammaPotential\n",
    "from pyro.distributions import InverseGamma\n",
    "from torch import Tensor\n",
    "from torch.distributions import Beta, Binomial, Gamma\n",
    "\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.inference import MNLE, MCMCPosterior\n",
    "from sbi.inference.potentials.likelihood_based_potential import LikelihoodBasedPotential\n",
    "from sbi.neural_nets import likelihood_nn\n",
    "from sbi.utils import BoxUniform, MultipleIndependent, mcmc_transform\n",
    "from sbi.utils.metrics import c2st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy simulator for mixed data\n",
    "def mixed_simulator(theta: Tensor, concentration_scaling: float = 1.0):\n",
    "    \"\"\"Returns a sample from a mixed distribution given parameters theta.\n",
    "\n",
    "    Args:\n",
    "        theta: batch of parameters, shape (batch_size, 1 + num_categories) concentration_scaling:\n",
    "        scaling factor for the concentration parameter of the InverseGamma\n",
    "        distribution, mimics an experimental condition.\n",
    "\n",
    "    \"\"\"\n",
    "    beta, rho = theta[:, :1], theta[:, 1:]\n",
    "\n",
    "    choices = Binomial(probs=rho).sample()\n",
    "    rts = InverseGamma(\n",
    "        concentration=concentration_scaling * torch.ones_like(beta), rate=beta\n",
    "    ).sample()\n",
    "\n",
    "    return torch.cat((rts, choices), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent prior.\n",
    "prior = MultipleIndependent(\n",
    "    [\n",
    "        Gamma(torch.tensor([1.0]), torch.tensor([0.5])),\n",
    "        Beta(torch.tensor([2.0]), torch.tensor([2.0])),\n",
    "    ],\n",
    "    validate_args=False,\n",
    ")\n",
    "prior_transform = mcmc_transform(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain reference-posterior samples via analytical likelihood and MCMC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "num_trials = 10\n",
    "num_samples = 1000\n",
    "theta_o = prior.sample((1,))\n",
    "x_o = mixed_simulator(theta_o.repeat(num_trials, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_kwargs = dict(\n",
    "    num_chains=100,\n",
    "    warmup_steps=100,\n",
    "    init_strategy=\"resample\",\n",
    "    thin=1,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "true_posterior = MCMCPosterior(\n",
    "    potential_fn=BinomialGammaPotential(prior, x_o),\n",
    "    proposal=prior,\n",
    "    theta_transform=prior_transform,\n",
    "    **mcmc_kwargs,\n",
    ")\n",
    "true_samples = true_posterior.sample((num_samples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MNLE and generate samples via MCMC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "num_simulations = 10000\n",
    "# For training the MNLE emulator we need to define a proposal distribution, the prior is\n",
    "# a good choice.\n",
    "proposal = prior\n",
    "theta = proposal.sample((num_simulations,))\n",
    "x = mixed_simulator(theta)\n",
    "\n",
    "# Train MNLE and obtain MCMC-based posterior.\n",
    "estimator_builder = likelihood_nn(model=\"mnle\", log_transform_x=True)\n",
    "trainer = MNLE(proposal, estimator_builder)\n",
    "estimator = trainer.append_simulations(theta, x).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build posterior from the trained estimator and prior.\n",
    "mnle_posterior = trainer.build_posterior(prior=prior, mcmc_parameters=mcmc_kwargs)\n",
    "\n",
    "mnle_samples = mnle_posterior.sample((num_samples,), x=x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare MNLE and reference posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them in one pairplot as contours (obtained via KDE on the samples).\n",
    "fig, ax = pairplot(\n",
    "    [\n",
    "        prior.sample((1000,)),\n",
    "        true_samples,\n",
    "        mnle_samples,\n",
    "    ],\n",
    "    points=theta_o,\n",
    "    diag=\"kde\",\n",
    "    upper=\"contour\",\n",
    "    upper_kwargs=dict(levels=[0.95]),\n",
    "    diag_kwargs=dict(bins=100),\n",
    "    fig_kwargs=dict(\n",
    "        points_offdiag=dict(marker=\"*\", markersize=10),\n",
    "        points_colors=[\"k\"],\n",
    "    ),\n",
    "    labels=[r\"$\\beta$\", r\"$\\rho$\"],\n",
    "    figsize=(6, 6),\n",
    ")\n",
    "\n",
    "plt.sca(ax[1, 1])\n",
    "plt.legend(\n",
    "    [\"Prior\", \"Reference\", \"MNLE\", r\"$\\theta_o$\"],\n",
    "    frameon=False,\n",
    "    fontsize=12,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the inferred `MNLE` posterior nicely matches the reference posterior, and how both inferred a posterior that is quite different from the prior.\n",
    "\n",
    "Because MNLE training is amortized we can obtain another posterior given a different observation with potentially a different number of trials, just by running MCMC again (without re-training `MNLE`):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat inference with different `x_o` that contains more trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 50\n",
    "x_o = mixed_simulator(theta_o.repeat(num_trials, 1))\n",
    "true_samples = true_posterior.sample((num_samples,), x=x_o, **mcmc_kwargs)\n",
    "mnle_samples = mnle_posterior.sample((num_samples,), x=x_o, **mcmc_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them in one pairplot as contours (obtained via KDE on the samples).\n",
    "fig, ax = pairplot(\n",
    "    [\n",
    "        prior.sample((1000,)),\n",
    "        true_samples,\n",
    "        mnle_samples,\n",
    "    ],\n",
    "    points=theta_o,\n",
    "    diag=\"kde\",\n",
    "    upper=\"contour\",\n",
    "    diag_kwargs=dict(bins=100),\n",
    "    upper_kwargs=dict(levels=[0.95]),\n",
    "    fig_kwargs=dict(\n",
    "        points_offdiag=dict(marker=\"*\", markersize=10),\n",
    "        points_colors=[\"k\"],\n",
    "    ),\n",
    "    labels=[r\"$\\beta$\", r\"$\\rho$\"],\n",
    "    figsize=(6, 6),\n",
    ")\n",
    "\n",
    "plt.sca(ax[1, 1])\n",
    "plt.legend(\n",
    "    [\"Prior\", \"Reference\", \"MNLE\", r\"$\\theta_o$\"],\n",
    "    frameon=False,\n",
    "    fontsize=12,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"c2st between true and MNLE posterior: {c2st(true_samples, mnle_samples).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can see that the posteriors match nicely. In addition, we observe that the posterior's (epistemic) uncertainty reduces as we increase the number of trials.\n",
    "\n",
    "Note: `MNLE` is trained on single-trial data. Theoretically, density estimation is perfectly accurate only in the limit of infinite training data. Thus, training with a finite amount of training data naturally induces a small bias in the density estimator.\n",
    "As we observed above, this bias is so small that we don't really notice it, e.g., the `c2st` scores were close to 0.5.\n",
    "However, when we increase the number of trials in `x_o` dramatically (on the order of 1000s) the small bias can accumulate over the trials and inference with `MNLE` can become less accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLE with experimental conditions\n",
    "\n",
    "In the perceptual decision-making research, it is common to design experiments with varying experimental decisions, e.g., to vary the difficulty of the task.\n",
    "During parameter inference, it can be beneficial to incorporate the experimental conditions.\n",
    "\n",
    "In MNLE, we are learning an emulator that should be able to generate synthetic experimental data including reaction times and choices given different experimental conditions.\n",
    "Thus, to make MNLE work with experimental conditions, we need to include them in the training process, i.e., treat them like auxiliary parameters of the simulator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a proposal that contains both, priors for the parameters and a discrte\n",
    "# prior over experimental conditions.\n",
    "proposal = MultipleIndependent(\n",
    "    [\n",
    "        Gamma(torch.tensor([1.0]), torch.tensor([0.5])),\n",
    "        Beta(torch.tensor([2.0]), torch.tensor([2.0])),\n",
    "        BoxUniform(torch.tensor([0.0]), torch.tensor([1.0])),\n",
    "    ],\n",
    "    validate_args=False,\n",
    ")\n",
    "\n",
    "# define a simulator wrapper in which the experimental condition are contained\n",
    "# in theta and passed to the simulator.\n",
    "def sim_wrapper(theta_and_conditions):\n",
    "    # simulate with experiment conditions\n",
    "    return mixed_simulator(\n",
    "        # we assume the first two parameters are beta and rho\n",
    "        theta=theta_and_conditions[:, :2],\n",
    "        # we treat the third concentration parameter as an experimental condition\n",
    "        concentration_scaling=theta_and_conditions[:, 2:],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data\n",
    "num_simulations = 10000\n",
    "num_samples = 1000\n",
    "theta = proposal.sample((num_simulations,))\n",
    "x = sim_wrapper(theta)\n",
    "assert x.shape == (num_simulations, 2)\n",
    "\n",
    "# simulate observed data and define ground truth parameters\n",
    "num_trials = 10\n",
    "# draw one ground truth parameter\n",
    "theta_o = proposal.sample((1,))[:, :2]\n",
    "# draw num_trials many different conditions\n",
    "conditions = proposal.sample((num_trials,))[:, 2:]\n",
    "# Theta is repeated for each trial, conditions are different for each trial.\n",
    "theta_and_conditions_o = torch.cat((theta_o.repeat(num_trials, 1), conditions), dim=1)\n",
    "x_o = sim_wrapper(theta_and_conditions_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain ground truth posterior via MCMC\n",
    "\n",
    "We obtain a ground-truth posterior via MCMC by using the analytical Binomial-Gamma\n",
    "likelihood as before. \n",
    "\n",
    "For that, we first the define the actual prior, i.e., the distribution over the\n",
    "parameter we want to infer (not the proposal). (dropping the uniform prior over\n",
    "experimental conditions).\n",
    "\n",
    "Additionally, we pass the entire batch of i.i.d. data `x_o` and matching batch of i.i.d.\n",
    "`conditions`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = MultipleIndependent(\n",
    "    [\n",
    "        Gamma(torch.tensor([1.0]), torch.tensor([0.5])),\n",
    "        Beta(torch.tensor([2.0]), torch.tensor([2.0])),\n",
    "    ],\n",
    "    validate_args=False,\n",
    ")\n",
    "prior_transform = mcmc_transform(prior)\n",
    "\n",
    "# We can now use the PotentialFunctionProvider to obtain a ground-truth\n",
    "# posterior via MCMC.\n",
    "true_posterior_samples = MCMCPosterior(\n",
    "    BinomialGammaPotential(\n",
    "        prior,\n",
    "        x_o,\n",
    "        concentration_scaling=conditions,\n",
    "    ),\n",
    "    theta_transform=prior_transform,\n",
    "    proposal=prior,\n",
    "    **mcmc_kwargs,\n",
    ").sample((num_samples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MNLE including experimental conditions\n",
    "\n",
    "Next, we use the combined parameters and conditions (`theta`) and the corresponding\n",
    "simulated data to train `MNLE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_builder = likelihood_nn(model=\"mnle\", log_transform_x=True)\n",
    "trainer = MNLE(proposal, estimator_builder)\n",
    "estimator = trainer.append_simulations(theta, x).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct conditional potential function\n",
    "\n",
    "We have now an emulator for the extended simulator, i.e., the one that has both the\n",
    "model parameters and the experimental condition as parameters. \n",
    "\n",
    "To obtain posterior\n",
    "samples conditioned on a particular experimental condition (and on x_o), we need to\n",
    "construct a corresponding potential function that can return the log likelihood of the\n",
    "model parameters, but conditioned on the experimental condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we define the potential function for the complete, unconditional MNLE-likelihood\n",
    "potential_fn = LikelihoodBasedPotential(estimator, proposal)\n",
    "# Then, we condition on the experimental conditions.\n",
    "conditioned_potential_fn = potential_fn.condition_on_theta(\n",
    "    conditions,  # pass only the conditions, must match the batch of iid data in x_o\n",
    "    dims_global_theta=[0, 1]  # pass the dimensions in the original theta that correspond to beta and rho\n",
    ")\n",
    "\n",
    "# Using this potential function, we can now obtain conditional samples.\n",
    "mnle_posterior = MCMCPosterior(\n",
    "    potential_fn=conditioned_potential_fn,  # pass the conditioned potential function\n",
    "    theta_transform=prior_transform,\n",
    "    proposal=prior,  # pass the prior, not the proposal.\n",
    "    **mcmc_kwargs\n",
    ")\n",
    "conditional_samples = mnle_posterior.sample((num_samples,), x=x_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can compare the ground truth conditional posterior with the\n",
    "# MNLE-conditional posterior.\n",
    "fig, ax = pairplot(\n",
    "    [\n",
    "        prior.sample((1000,)),\n",
    "        true_posterior_samples,\n",
    "        conditional_samples,\n",
    "    ],\n",
    "    points=theta_o,\n",
    "    diag=\"kde\",\n",
    "    upper=\"contour\",\n",
    "    diag_kwargs=dict(bins=100),\n",
    "    upper_kwargs=dict(levels=[0.95]),\n",
    "    fig_kwargs=dict(\n",
    "        points_offdiag=dict(marker=\"*\", markersize=10),\n",
    "        points_colors=[\"k\"],\n",
    "\n",
    "    ),\n",
    "    labels=[r\"$\\beta$\", r\"$\\rho$\"],\n",
    "    figsize=(6, 6),\n",
    ")\n",
    "\n",
    "plt.sca(ax[1, 1])\n",
    "plt.legend(\n",
    "    [\"Prior\", \"Reference\", \"MNLE\", r\"$\\theta_o$\"],\n",
    "    frameon=False,\n",
    "    fontsize=12,\n",
    ")\n",
    "print(f\"c2st between true and MNLE posterior: {c2st(true_posterior_samples, conditional_samples).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They match accurately, showing that we can indeed post-hoc condition the trained MNLE likelihood on different experimental conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with multiple subjects, trials, and conditions\n",
    "\n",
    "Note that we can also do inference for multiple `x_os` (e.g., subjects) with varying\n",
    "numbers of trails and experimental conditions - all without retraining the MNLE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "num_subjects = 3\n",
    "num_trials = [10, 20, 30]\n",
    "# draw one ground truth parameter\n",
    "theta_o = proposal.sample((num_subjects,))[:, :2]\n",
    "# Note that the trial conditions need to be the same for all subjects.\n",
    "\n",
    "# Simulate observed data for all subjects and trials.\n",
    "x_os = []\n",
    "conditions = []\n",
    "for i in range(num_subjects):\n",
    "    conditions.append(proposal.sample((num_trials[i],))[:, 2:])\n",
    "    # Theta is repeated for each trial, conditions are different for each trial.\n",
    "    theta_and_condition = torch.cat((theta_o[i].repeat(num_trials[i], 1), conditions[i]), dim=-1)\n",
    "    x_os.append(sim_wrapper(theta_and_condition))\n",
    "\n",
    "# loop over subjects (vectorized batched x and batched conditions is not supported yet)\n",
    "posterior_samples = []\n",
    "for idx in range(num_subjects):\n",
    "    # condition the potential\n",
    "    conditioned_potential_fn = potential_fn.condition_on_theta(\n",
    "        conditions[idx],\n",
    "        dims_global_theta=[0, 1]\n",
    "    )\n",
    "\n",
    "    # pass potential to sampler\n",
    "    mnle_posterior = MCMCPosterior(\n",
    "        potential_fn=conditioned_potential_fn,  # pass the conditioned potential function\n",
    "        theta_transform=prior_transform,\n",
    "        proposal=prior,  # pass the prior, not the proposal.\n",
    "        **mcmc_kwargs\n",
    "    )\n",
    "    posterior_samples.append(mnle_posterior.sample((num_samples,), x=x_os[idx], show_progress_bars=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all three posteriors in one pairplot.\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    [prior.sample((1000,))] + posterior_samples,\n",
    "    diag=\"kde\",\n",
    "    upper=\"contour\",\n",
    "    diag_kwargs=dict(bins=100),\n",
    "    upper_kwargs=dict(levels=[0.95]),\n",
    "    fig_kwargs=dict(\n",
    "        points_offdiag=dict(marker=\"*\", markersize=10),\n",
    "        points_colors=[\"k\"],\n",
    "\n",
    "    ),\n",
    "    labels=[r\"$\\beta$\", r\"$\\rho$\"],\n",
    "    figsize=(10, 10),\n",
    ")\n",
    "\n",
    "plt.sca(ax[1, 1])\n",
    "plt.legend(\n",
    "    [\"prior\"] + [f\"Subject {idx+1}\" for idx in range(num_subjects)],\n",
    "    frameon=False,\n",
    "    fontsize=12,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the posteriors are becoming more narrow with increasing number of trials\n",
    "(subject 1: 10 trials vs. subject 3: 30 trials)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
